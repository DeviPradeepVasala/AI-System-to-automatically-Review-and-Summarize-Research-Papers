{
  "pdf_path": "C:\\Users\\hp\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\nnU-Net_ a self-configuring method for deep learning-based biomedical image segmentation.pdf",
  "pdf_name": "nnU-Net_ a self-configuring method for deep learning-based biomedical image segmentation.pdf",
  "file_hash": "7bc25fa4aa786c3052ec174e4b564905d54a0a508348370b927649d9aae44ad1",
  "metadata": {
    "title": "",
    "author": "",
    "subject": "",
    "keywords": "",
    "creator": "LaTeX with hyperref package",
    "producer": "pdfTeX-1.40.17",
    "creation_date": "D:20200403010245Z",
    "modification_date": "D:20200403010245Z"
  },
  "raw_text": "Automated Design of Deep Learning Methods for\nBiomedical Image Segmentation\nFabian Isensee1,2†, Paul F. Jaeger1†, Simon A. A. Kohl3‡, Jens Petersen1,4, and Klaus H.\nMaier-Hein1,5*\n1Division of Medical Image Computing, German Cancer Research Center, Heidelberg\n2Faculty of Biosciences, University of Heidelberg, Heidelberg, Germany\n3DeepMind, London, United Kingdom\n4Faculty of Physics & Astronomy, University of Heidelberg, Heidelberg, Germany\n5Pattern Analysis and Learning Group, Heidelberg University Hospital, Department of Radiation\nOncology, Heidelberg, Germany\n*k.maier-hein@dkfz.de\nAbstract\nBiomedical imaging is a driver of scientiﬁc discovery and core component of\nmedical care, currently stimulated by the ﬁeld of deep learning. While semantic\nsegmentation algorithms enable 3D image analysis and quantiﬁcation in many\napplications, the design of respective specialised solutions is non-trivial and highly\ndependent on dataset properties and hardware conditions. We propose nnU-Net,\na deep learning framework that condenses the current domain knowledge and\nautonomously takes the key decisions required to transfer a basic architecture to dif-\nferent datasets and segmentation tasks. Without manual tuning, nnU-Net surpasses\nmost specialised deep learning pipelines in 19 public international competitions and\nsets a new state of the art in the majority of the 49 tasks. The results demonstrate\na vast hidden potential in the systematic adaptation of deep learning methods to\ndifferent datasets. We make nnU-Net publicly available as an open-source tool\nthat can effectively be used out-of-the-box, rendering state of the art segmentation\naccessible to non-experts and catalyzing scientiﬁc progress as a framework for\nautomated method design.\n1\nIntroduction\nSemantic segmentation transforms raw biomedical image data into meaningful, spatially structured\ninformation and thus plays an essential role for scientiﬁc discovery in the ﬁeld [9, 14]. At the same\ntime, semantic segmentation is an essential ingredient to numerous clinical applications [1, 27],\nincluding applications of artiﬁcial intelligence in diagnostic support systems [7, 3], therapy planning\n† Equal contribution. ‡ Work started while doing a PhD at the German Cancer Research Center.\nPreprint. Under review.\narXiv:1904.08128v2  [cs.CV]  2 Apr 2020\n\n\nsupport [28], intra-operative assistance [14] or tumor growth monitoring [19]. The high interest in\nautomatic segmentation methods manifests in a thriving research landscape, accounting for 70% of\ninternational image analysis competitions in the biomedical sector [23].\nDespite the recent success of deep learning-based segmentation methods, their applicability\nto speciﬁc image analysis problems of end-users is often limited. The task-speciﬁc design and\nconﬁguration of a method requires high levels of expertise and experience, with small errors leading\nto strong performance drops [22]. Especially in 3D biomedical imaging, where dataset properties like\nimaging modality, image size, (anisotropic) voxel spacing or class ratio vary drastically, the pipeline\ndesign can be cumbersome, because experience on what constitutes a successful conﬁguration\nmay not translate to the dataset at hand. The numerous expert decisions involved in designing\nand training a neural network range from the exact network architecture to the training schedule\nand methods for data augmentation or post-processing. Each sub-component is controlled by\nessential hyperparameters like learning rate, batch size, or class sampling [22]. An additional layer\nof complexity on the overall setup is posed by the hardware available for training and inference\n[21]. Algorithmic optimization of the codependent design choices in this high dimensional space\nof hyperparameters is technically demanding and ampliﬁes both the number of required training\ncases as well as compute resources by orders of magnitude [8]. As a consequence, the end-user is\ncommonly left with an iterative trial and error process during method design that is mostly driven\nby their individual experience, only scarcely documented and hard to replicate, inevitably evoking\nsuboptimal segmentation pipelines and methodological ﬁndings that do not generalize to other\ndatasets [22, 2].\nTo further complicate things, there is an unmanageable number of research papers that pro-\npose architecture variations and extensions for performance improvement. This bulk of studies is\nincomprehensible to the non-expert and difﬁcult to evaluate even for experts [22]. Approximately\n12000 studies cite the 2015 U-Net architecture on biomedical image segmentation [31], many of\nwhich propose extensions and advances. We put forward the hypothesis that a basic U-Net is still\nhard to beat if the corresponding pipeline is designed adequately.\nTo this end, we propose nnU-Net (“no new net”), which makes successful 3D biomedical\nimage segmentation accessible for biomedical research applications. nnU-Net automatically adapts\nto arbitrary datasets and enables out-of-the-box segmentation on account of two key contributions:\n1. We formulate the pipeline optimization problem in terms of a data ﬁngerprint (representing\nthe key properties of a dataset) and a pipeline ﬁngerprint (representing the key design choices\nof a segmentation algorithm).\n2. We make their relation explicit by condensing domain knowledge into a set of heuristic\nrules that robustly generate a high quality pipeline ﬁngerprint from a corresponding data\nﬁngerprint while considering associated hardware constraints.\nIn contrast to algorithmic approaches for method conﬁguration that are formulated as a task-speciﬁc\noptimization problem, nnU-Net readily executes systematic rules to generate deep learning methods\nfor previously unseen datasets without need for further optimization.\nIn the following, we demonstrate the superiority of this concept by presenting a new state\nof the art in numerous international challenges through application of our algorithm without manual\n2\n\n\nintervention.\nThe strong results underline the signiﬁcance of nnU-Net for users who require\nalgorithms for semantic segmentation on their custom datasets: as an open source tool, nnU-Net can\nsimply be downloaded and trained out-of-the box to generate state of the art segmentations without\nrequiring manual adaptation or expert knowledge. We further demonstrate shortcomings in the\ndesign process of current biomedical segmentation methods. Speciﬁcally, we take an in-depth look at\nthe 2019 Kidney and Kidney Tumor Segmentation (KiTS) semantic image segmentation challenge\nand demonstrate how important task-speciﬁc design and conﬁguration of a method are in comparison\nto choosing one of the many architectural extensions and advances previously proposed on top of the\nU-Net. By automating this design and conﬁguration process, nnU-Net fosters the ambition and the\nability of researchers to validate novel ideas on larger numbers of datasets, while at the same time\nserving as an ideal reference method when demonstrating methodological improvements.\n2\nResults\nnnU-Net is a deep learning framework that enables 3D semantic segmentation in many biomedical\nimaging applications, without requiring the design of respective specialised solutions. Exemplary\nsegmentation results generated by nnU-Net for a variety of datasets are shown in Figure 1.\nnnU-Net automatically adapts to any new dataset\nFigure 2a shows the current practice of adapt-\ning segmentation pipelines to a new dataset. This process is expert-driven and involves manual\ntrial-and-error experiments that are typically speciﬁc to the task at hand [22]. As shown in Figure 2b,\nnnU-Net addresses the adaptation process systematically. Therefore, we deﬁne a dataset ﬁngerprint\nas a standardized dataset representation comprising key properties such as image sizes, voxel spacing\ninformation or class ratios, and a pipeline ﬁngerprint as the entirety of choices being made during\nmethod design. nnU-Net is designed to generate a successful pipeline ﬁngerprint for a given dataset\nﬁngerprint. In nnU-Net, the pipeline ﬁngerprint is divided into three groups: blueprint, inferred and\nempirical parameters. The blueprint parameters represent fundamental design choices (such as using\na plain U-Net-like architecture template) as well as hyperparameters for which a robust default value\ncan simply be picked (for example loss function, training schedule and data augmentation). The\ninferred parameters encode the necessary adaptations to a new dataset and include modiﬁcations\nto the exact network topology, patch size, batch size and image preprocessing. The link between a\ndata ﬁngerprint and the inferred parameters is established via execution of a set of heuristic rules,\nwithout the need for expensive re-optimization when applied to unseen datasets. Note that many of\nthese design choices are co-dependent: The target image spacing, for instance, affects image size,\nwhich in return determines the size of patches the model should see during training, which affects\nthe network topology and has to be counterbalanced by the size of training mini-batches in order to\nnot exceed GPU memory limitations. nnU-Net strips the user of the burden to manually account for\nthese co-dependencies. The empirical parameters are autonomously identiﬁed via cross-validation\non the training cases. Per default, nnU-Net generates three different U-Net conﬁgurations: a 2D\nU-Net, a 3D U-Net that operates at full image resolution and a 3D U-Net cascade where the ﬁrst\nU-Net operates on downsampled images and the second is trained to reﬁne the segmentation maps\ncreated by the former at full resolution. After cross-validation nnU-Net empirically chooses the best\nperforming conﬁguration or ensemble. Finally, nnU-Net empirically opts for “non-largest component\nsuppression” as a postprocessing step if performance gains are measured. The output of nnU-Net’s\nautomated adaptation and training process are fully trained U-Net models that can be deployed to\nmake predictions on unseen images. We provide an in-depth description of the methodology behind\nnnU-Net in the online methods. The overarching design principles, i.e. our best-practice recommen-\n3\n\n\nFigure 1: nnU-Net handles a broad variety of datasets and target image properties. All examples\noriginate from the test sets of different international segmentation challenges that nnU-Net was applied\non. Target structures for each dataset are shown in 2D projected onto the raw data (left) and in 3D\ntogether with a volume rendering of the raw data (right). All visualizations are created with the\nMITK Workbench [29]. a: heart (green), aorta (red), trachea (blue) and esophagus (yellow) in CT\nimages (D18). b: synaptic clefts (green) in electron microscopy scans (D19). c: liver (yellow),\nspleen (orange), left/right kidney (blue/green) in T1 in-phase MRI (D16). d: thirteen abdominal\norgans in CT images (D11). e: liver (yellow) and liver tumors (green) in CT images (D14). f: right\nventricle (yellow), left ventricular cavity (blue) myocardium of left ventricle (green) in cine MRI\n(D13). g: prostate (yellow) in T2 MRI (D12). h: lung nodules (yellow) in CT images (D6). i: kidneys\n(yellow) and kidney tumors (green) in CT images (D17). j: edema (yellow), enhancing tumor (purple),\nnecrosis (green) in MRI (T1, T1 with contrast agent, T2, FLAIR) (D1). k: left ventricle (yellow) in\nMRI (D2). l: hepatic vessels (yellow) and liver tumors (green) in CT (D8). See Figure 5 for dataset\nreferences.\n4\n\n\nFigure 2: Manual and proposed automated conﬁguration of deep learning methods. a) Current\npractice of conﬁguring a deep learning method for biomedical segmentation: An iterative trial and\nerror process of manually choosing a set of hyperparameters and architecture conﬁgurations, training\nthe model, and monitoring performance of the model on a validation set. b) Proposed automated\nconﬁguration by nnU-Net: Dataset properties are summarized in a “dataset ﬁngerprint”. A set\nof heuristic rules operates on this ﬁngerprint to infer the data-dependent hyperparameters of the\npipeline. These are completed by blueprint parameters, the data-independent design choices to form\n“pipeline ﬁngerprints”. Three architectures are trained based on these pipeline ﬁngerprints in a 5-fold\ncross-validation. Finally, nnU-Net automatically selects the optimal ensemble of these architectures\nand performs postprocessing if required.\ndations for method adaptation to new datasets, are summarized in Supplementary Information B.\nAll segmentation pipelines generated by nnU-Net in the context of this manuscript are provided in\nSupplementary Information F.\nnnU-Net handles a wide variety of target structures and image properties\nWe demonstrate the\nvalue of nnU-Net as an out-of-the-box segmentation tool by applying it to 10 international biomedical\nimage segmentation challenges comprising 19 different datasets and 49 segmentation tasks across a\nvariety of organs, organ substructures, tumors, lesions and cellular structures in magnetic resonance\nimaging (MRI), computed tomography scans (CT) as well as electron microscopy (EM) images.\nChallenges are international competitions that can be seen as the equivalent to clinical trials for\nalgorithm benchmarking. Typically, they are hosted by individual researchers, institutes, or societies,\naiming to assess the performance of multiple algorithms in a standardized environment [23]. In\nall segmentation tasks, nnU-Net was trained from scratch using only the provided challenge data.\nWhile the methodology behind nnU-Net was developed on the 10 training sets provided by the\nMedical Segmentation Decathlon [32], the remaining datasets and tasks were used for independent\ntesting, i.e. nnU-Net was simply applied without further optimization. Qualitatively, we observe that\n5\n\n\nnnU-Net is able to handle a large disparity in dataset properties and diversity in target structures,\ni.e. generated pipeline conﬁgurations are in line with what human experts consider a reasonable or\nsensible setting (see Supplementary Information C.1and C.2). Examples for segmentation results\ngenerated by nnU-Net are presented in Figure 1.\nnnU-Net outperforms specialized pipelines in a range of diverse tasks\nMost international chal-\nlenges use the Soerensen-Dice coefﬁcient as a measure of overlap to quantify segmentation quality\n[13, 4, 25, 3]. Here, perfect agreement results in a Dice coefﬁcient of 1, whereas no agreement\nresults in a score of 0. Other metrics used by some of the challenges include the Normalized Surface\nDice (higher is better) [7] and the Hausdorff Distance (lower is better), both quantifying the distance\nbetween the borders of two segmentations. Figure 3 provides an overview of the quantitative results\nachieved by nnU-Net and the competing challenge teams across all 49 segmentation tasks. Despite\nits generic nature, nnU-Net outperforms most existing semantic segmentation solutions, even though\nthe latter were speciﬁcally optimized towards the respective task. Overall, nnU-Net sets a new state\nof the art in 29 out of 49 target structures and otherwise shows performances on par with or close to\nthe top leaderboard entries.\nDetails in pipeline conﬁguration have more impact on performance than architectural varia-\ntions\nTo highlight how important the task-speciﬁc design and conﬁguration of a method are in\ncomparison to choosing one of the many architectural extensions and advances previously proposed\non top of the U-Net, we put our results into context of current research by analyzing the participating\nalgorithms in the recent Kidney and Kidney Tumor Segmentation (KiTS) 2019 challenge hosted by\nthe Medical Image Computing and Computer Assisted Intervention (MICCAI) society [13]. The\nMICCAI society has consistently been hosting at least 50% of all annual biomedical image analysis\nchallenges [23]. With more than 100 competitors, the KiTS challenge was the largest competition at\nMICCAI 2019. Our analysis of the KiTS leaderboard1 (see Figure 4a) reveals several insights on the\ncurrent landscape of deep learning based segmentation method design: First, the top-15 methods were\noffspring of the (3D) U-Net architecture from 2016, conﬁrming its impact on the ﬁeld of biomedical\nimage segmentation. Second, the ﬁgure demonstrates that contributions using the same type of\nnetwork result in performances spread across the entire leaderboard. Third, when looking closer into\nthe top-15, none of the commonly used architectural modiﬁcations (e.g. residual connections [26, 10],\ndense connections [18, 15], attention mechanisms [30] or dilated convolutions [5, 24]) represent a\nnecessary condition for good performance on the KiTS task. By example this shows that many of\nthe previously introduced algorithm modiﬁcations may not generally be superior to a properly tuned\nbaseline method.\nFigure 4b underlines the importance of hyperparameter tuning by analyzing algorithms using the same\narchitecture variant as the challenge-winning contribution, a 3D U-Net with residual connections.\nWhile one of these methods won the challenge, other contributions based on the same principle cover\nthe entire range of evaluation scores and rankings. Key conﬁguration parameters were selected from\nrespective pipeline ﬁngerprints and are shown for all non-cascaded residual U-Nets, illustrating the\nco-dependent design choices that each team made during pipeline design. The drastically varying\nconﬁgurations submitted by contestants indicate the underlying complexity of the high-dimensional\noptimization problem that is implicitly posed by designing a deep learning method for biomedical 3D\nimage segmentation.\n1http://results.kits-challenge.org/miccai2019/\n6\n\n\nFigure 3: nnU-Net outperforms most specialized deep learning pipelines. Quantitative results\nfrom all international challenges that nnU-Net competed in. For each segmentation task, results\nachieved by nnU-Net are highlighted in red, competing teams are shown in blue. For each segmenta-\ntion task the respective rank is displayed in the bottom right corner as nnU-Net’s rank / total number\nof submissions. Axis scales: [DC] Dice coefﬁcient, [OH] other score (higher is better), [OL] other\nscore (lower is better). All leaderboards were accessed on December 12th 2019.\n7\n\n\nFigure 4: Pipeline ﬁngerprints from KITS 2019 [13] leaderboard entries. a) Coarse categoriza-\ntion of leaderboard entries by architecture variation. All top 15 contributions are encoder-decoder\narchitectures with skip-connections, 3D convolutions and output stride 1 (“3D U-Net-like”, purple).\nNo clear pattern arises from further sub-groupings into different architectural variations. Also,\nnone of the analyzed architectures guarantees good performance, indicating a large dependency of\nperformance beyond architecture type. b) Finer-grained key parameters selected from the pipeline\nﬁngerprints of all non-cascade 3D-U-Net-like architectures with residual connections (displayed on\nz-score normalized scale). The contributions vary drastically in their rankings as well as their ﬁnger-\nprints. Still, there is no evident relation between single parameters and performance. Abbreviations:\nCE = Cross entropy loss function, Dice = Soft Dice loss function, WBCE = Weighted binary cross\nentropy loss function.\nnnU-Net experimentally conﬁrms the importance of good hyperparameters over architectural vari-\nations on the KiTS dataset by setting a new state of the art on the open leaderboard (which also\nincludes the original challenge submissions analysed here) with a plain 3D U-Net architecture (see\nFigure 3). Our results from further international challenge participations conﬁrm this observation\nacross a variety of datasets.\nDifferent datasets require different pipeline conﬁgurations\nWe extract the data ﬁngerprints of\n19 biomedical segmentation datasets. As displayed in Figure 5, this documents an exceptional dataset\ndiversity in biomedical imaging, and reveals the fundamental reason behind the lack of out-of-the-box\n8\n\n\nFigure 5: Data ﬁngerprints across different challenge datasets. The data ﬁngerprints show the\nkey properties (displayed on z-score normalized scale) for the 19 datasets used in the nnU-Net experi-\nments (see Supplementary Material A for detailed dataset descriptions). Datasets vary tremendously\nin their properties, requiring intense method adaptation to the individual dataset and underlining\nthe need for evaluation on larger numbers of datasets when drawing general methodological conclu-\nsions. Abbreviations: EM = Electron Microscopy, CT = Computed Tomography, MRI = Magnetic\nResonance Imaging.\nsegmentation algorithms: The complexity of method design is ampliﬁed by the fact that suitable\npipeline settings either directly or indirectly depend on the data ﬁngerprint under potentially complex\nrelations. As a consequence, pipeline settings that are identiﬁed as optimal for one dataset (such\nas KiTS, see above) may not generalize to others, resulting in a need for (currently manual) re-\noptimization on each individual dataset. An example for conﬁguration parameters depending on\ndataset properties is the image size which affects the size of patches that the model sees during training,\nwhich in turn affects the required network topology (i.e. number of downsampling steps, size of\nconvolution ﬁlters, etc.). The network topology itself again inﬂuences several other hyperparameters\nin the pipeline.\nMultiple tasks enable robust design decisions\nnnU-Net is a framework that enables benchmark-\ning of new modiﬁcations or extensions of methods across multiple datasets without having to manually\nreconﬁgure the entire pipeline for each dataset. To demonstrate this, and also to support some of the\ncore design choices made in nnU-Net, we systematically tested the performance of common pipeline\nvariations in the nnU-Net blueprint parameters on 10 different datasets (Figure 6): the application\nof two alternative loss functions (Cross-entropy and TopK10 [35]), the introduction of residual\nconnections in the encoder [11], using three convolutions per resolution instead of two (resulting\nin a deeper network architecture), two modiﬁcations of the optimizer (a reduced momentum term\nand an alternative optimizer (Adam [20])), batch norm [17] instead of instance norm [33] and the\nomission of data augmentation. Ranking stability was estimated by bootstrapping as suggested by the\nchallengeR tool [34].\nThe volatility of the ranking between datasets demonstrates how single hyperparameter choices can\naffect segmentation performance depending on the dataset. The results clearly show that caution is\nrequired when drawing methodological conclusions from evaluations that are based on an insufﬁcient\nnumber of datasets. While ﬁve out of the nine variants achieved rank 1 in at least one of the\n9\n\n\nFigure 6: Evaluation of design decisions across multiple tasks. (a-j) Evaluation of exemplary\nmodel variations on ten datasets of the medical segmentation decathlon (D1-D10, see Figure 5 for\ndataset references). The analysis is done for every dataset by aggregating validation splits of the\nﬁve-fold cross-validation into one large validation set. 1000 virtual validation sets are generated\nvia bootstrapping (drawn with replacement). Algorithms are ranked on each virtual validation set,\nresulting in a distribution over rankings. The results indicate that evaluation of methodological\nvariations on too few datasets is prone to result in a misleading level of generality, since most\nperformance changes are not consistent over datasets. (k) The aggregation of rankings across datasets\nyields insights into what design decisions robustly generalize.\ndatasets, neither of them exhibits consistent improvements across the ten tasks. The original nnU-Net\nconﬁguration shows the best generalization and ranks ﬁrst when aggregating results of all datasets.\nIn current research practice, evaluation is rarely performed on more than two datasets and even then\nthe datasets come with largely overlapping properties (such as both being abdominal CT scans). As\nwe showed here, such evaluation is unsuitable for drawing general methodological conclusions. We\nrelate the lack of sufﬁciently broad evaluations to the manual tuning effort required when adapting\nexisting pipelines to individual datasets. nnU-Net alleviates this shortcoming in two ways: As a\nframework that can be extended to enable effective evaluation of new concepts across multiple tasks,\nand as a plug-and-play, standardized and state-of-the-art baseline to compare against.\n10\n\n\nnnU-Net is freely available and can be used out-of-the-box\nnnU-Net is freely available as an\nopen-source tool. It can be installed via Python Package Index (PyPI). The source code is publicly\navailable on Github (https://github.com/MIC-DKFZ/nnUNet). A comprehensive documentation\nis available together with the source code. Pretrained models for all presented datasets are available\nfor download at https://zenodo.org/record/3734294.\n3\nDiscussion\nWe presented nnU-Net, a deep learning framework for biomedical image analysis that automates\nmodel design for 3D semantic segmentation tasks. The method sets a new state of the art in the\nmajority of tasks it was evaluated on, outperforming all respective specialized processing pipelines.\nThe strong performance of nnU-Net is not achieved by a new network architecture, loss function or\ntraining scheme (hence the name nnU-Net - “no new net”), but by replacing the complex process of\nmanual pipeline optimization with a systematic approach based on explicit and interpretable heuristic\nrules. Requiring zero user-intervention, nnU-Net is the ﬁrst segmentation tool that can be applied\nout-of-the-box to a very large range of biomedical imaging datasets and is thus the ideal tool for\nusers who require access to semantic segmentation methods and do not have the expertise, time, or\ncompute resources required to manually adapt existing solutions to their problem.\nOur analysis on the KITS leaderboard as well as nnU-Net’s performance across 19 datasets\nconﬁrms our initial hypothesis that common architectural modiﬁcations proposed by the ﬁeld during\nthe last 5 years may not necessarily be required to achieve state-of-the-art segmentation performance.\nInstead, we observed that contributions using the same type of network result in performances\nspread across the entire leaderboard. This observation is in line with Litjens et al., who, in their\nreview from 2017, found that \"many researchers use the exact same architectures [...] but have\nwidely varying results\" [22]. There are several possible reasons for why performance improvements\nbased on architectural extensions proposed by the literature may not hold beyond the dataset they\nwere proposed on: many of them are evaluated on a limited amount of datasets, often as low as a\nsingle one. In practice this largely limits their success on unseen datasets with varying properties,\nbecause the quality of the hyperparameter conﬁguration often overshadows the effect of the evaluated\narchitectural modiﬁcation. This ﬁnding is in line with an observation by Litjens et al., who concluded\nthat \"the exact architecture is not the most important determinant in getting a good solution\" [22].\nMoreover, as shown above, it can be difﬁcult to tune existing baselines to a given dataset. This\nobstacle can unknowingly, but nonetheless unduly, make a new approach look better than the baseline,\nresulting in biased literature.\nIn this work, we demonstrated that nnU-Net is able to alleviate this bottleneck of current\nresearch in biomedical image segmentation in two ways: On the one hand, nnU-Net serves as a\nframework for methodological modiﬁcations enabling simple evaluation on an arbitrary number of\ndatasets. On the other hand, nnU-Net represents the ﬁrst standardized method that does not require\nmanual task-speciﬁc adaptation and as such can readily serve as a strong baseline on any new 3D\nsegmentation task.\nThe research performed in “AutoML” [16, 6] or “Neural architecture search” [8] has simi-\nlarities to our approach in that this line of research seeks to strip the ML user or researcher of the\nburden to manually ﬁnd good hyperparameters. In contrast to nnU-Net however, AutoML aims to\n11\n\n\nlearn hyperparameters directly from the data. This comes with practical difﬁculties such as enormous\nrequirements with respect to compute and data resources. Additionally, AutoML methods need to\noptimize the hyperparameters for each new task. The same disadvantages apply to “Grid Search” [2],\nwhere extensive trial and error sweeps in the hyperparameter landscape are performed to empirically\nﬁnd good conﬁgurations for a speciﬁc task. In contrast, nnU-Net transforms domain knowledge into\ninductive biases, thus shortcuts the high dimensional optimization of hyperparameters and minimizes\nrequired computational and data resources. As elaborated above, these heuristics are developed\non the basis of 10 different datasets of the Medical Segmentation Decathlon. The diversity within\nthese 10 datasets has proven sufﬁcient to achieve robustness to the variability encountered in all the\nremaining challenge participations. This is quite remarkable given the underlying complexity of\nmethod design and strongly conﬁrms the suitability of condensing the process in a few generally\napplicable rules that are simply executed when given a new dataset ﬁngerprint and do not require any\nfurther task-speciﬁc actions. The formal deﬁnition and also publishing of these explicit rules is a\nstep towards systematicity and interpretability in the task of hyperparameter selection, which has\npreviously been considered a “highly empirical exercise”, for which “no clear recipe can be given.”\n[22].\nDespite its strong performance across 49 diverse tasks, there might be segmentation tasks\nfor which nnU-Net’s automatic adaptation is suboptimal. For example, nnU-Net was developed\nwith a focus on the Dice coefﬁcient as performance metric. Some tasks, however, might require\nhighly domain speciﬁc target metrics for performance evaluation, which could inﬂuence method\ndesign.\nAlso, yet unconsidered dataset properties could exist which may cause suboptimal\nsegmentation performance. One example is the synaptic cleft segmentation task of the CREMI\nchallenge (https://cremi.org). While nnU-Net’s performance is highly competitive (rank 6/39),\nmanual adaptation of the loss function as well as electron microscopy-speciﬁc preprocessing may be\nnecessary to surpass state-of-the-art performance [12]. In principle, there are two ways of handling\ncases that are not yet optimally covered by nnU-Net: For potentially re-occurring cases, nnU-Net’s\nheuristics could be extended accordingly; for highly domain speciﬁc cases, nnU-Net should be seen\nas a good starting point for necessary modiﬁcations.\nIn summary, nnU-Net sets a new state of the art in various semantic segmentation chal-\nlenges and displays strong generalization characteristics without need for any manual intervention,\nsuch as the tuning of hyper-parameters. As pointed out by Litjens et al. and quantitatively conﬁrmed\nhere, hyper-parameter optimization constitutes a major difﬁculty for past and current research\nin biomedical image segmentation. nnU-Net automates the otherwise often unsystematic and\ncumbersome procedure and may thus help alleviate this burden. We propose to leverage nnU-Net as\nan out-of-the box tool for state-of-the-art segmentation, a framework for large-scale evaluation of\nnovel ideas without manual effort, and as a standardized baseline method to compare ideas against\nwithout the need for task-speciﬁc optimization.\n4\nAcknowledgements\nThis work was co-funded by the National Center for Tumor Diseases (NCT) in Heidelberg and\nthe Helmholtz Imaging Platform (HIP) of the German Cancer Consortium (DKTK). We thank our\ncolleagues at DKFZ who were involved in the various challenge contributions, especially Andre\nKlein, David Zimmerer, Jakob Wasserthal, Gregor Koehler, Tobias Norajitra and Sebastian Wirkert\n12\n\n\nwho contributed to the Decathlon submission. We also thank the MITK team who supported us in\nproducing all medical dataset visualizations. We are also thankful to all the challenge organizers,\nwho provided an important basis for our work. We want to especially mention Nicholas Heller, who\nenabled the collection of all the details from the KiTS challenge through excellent challenge design,\nand Emre Kavur from the CHAOS team, who generated comprehensive leaderboard information for\nus. We thank Manuel Wiesenfarth for his helpful advice concerning the ranking of methods and the\nvisualization of rankings. Last but not least, we thank Olaf Ronneberger and Lena Maier-Hein for\ntheir important feedback on this manuscript.\nReferences\n[1] H. J. Aerts, E. R. Velazquez, R. T. Leijenaar, C. Parmar, P. Grossmann, S. Carvalho, J. Bussink,\nR. Monshouwer, B. Haibe-Kains, D. Rietveld, et al. Decoding tumour phenotype by noninvasive\nimaging using a quantitative radiomics approach. Nature communications, 5(1):1–9, 2014.\n[2] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of machine\nlearning research, 13(Feb):281–305, 2012.\n[3] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir,\nO. Camara, M. A. G. Ballester, et al. Deep learning techniques for automatic mri cardiac multi-\nstructures segmentation and diagnosis: Is the problem solved? IEEE TMI, 37(11):2514–2525,\n2018.\n[4] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-\nA. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE\ntransactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n[6] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmenta-\ntion strategies from data. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 113–123, 2019.\n[7] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham,\nX. Glorot, B. O’Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis\nand referral in retinal disease. Nature medicine, 24(9):1342–1350, 2018.\n[8] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture search: A survey. Journal of Machine\nLearning Research, 20(55):1–21, 2019.\n[9] T. Falk, D. Mai, R. Bensch, Ö. Çiçek, A. Abdulkadir, Y. Marrakchi, A. Böhm, J. Deubner,\nZ. Jäckel, K. Seiwald, et al. U-net: deep learning for cell counting, detection, and morphometry.\nNature methods, 16(1):67–70, 2019.\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778,\n2016.\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016.\n13\n\n\n[12] L. Heinrich, J. Funke, C. Pape, J. Nunez-Iglesias, and S. Saalfeld. Synaptic cleft segmentation\nin non-isotropic volume electron microscopy of the complete drosophila brain. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 317–325.\nSpringer, 2018.\n[13] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[14] T. C. Hollon, B. Pandian, A. R. Adapa, E. Urias, A. V. Save, S. S. S. Khalsa, D. G. Eichberg,\nR. S. D’Amico, Z. U. Farooq, S. Lewis, et al. Near real-time intraoperative brain tumor diagnosis\nusing stimulated raman histology and deep neural networks. Nature Medicine, pages 1–7, 2020.\n[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n[16] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general\nalgorithm conﬁguration. In International conference on learning and intelligent optimization,\npages 507–523. Springer, 2011.\n[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[18] S. Jégou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers tiramisu:\nFully convolutional densenets for semantic segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition workshops, pages 11–19, 2017.\n[19] P. Kickingereder, F. Isensee, I. Tursunova, J. Petersen, U. Neuberger, D. Bonekamp, G. Brugnara,\nM. Schell, T. Kessler, M. Foltyn, et al. Automated quantitative tumour response assessment of\nmri in neuro-oncology with artiﬁcial neural networks: a multicentre, retrospective study. The\nLancet Oncology, 20(5):728–740, 2019.\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and\nY. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n[21] Y. LeCun. 1.1 deep learning hardware: Past, present, and future. In 2019 IEEE International\nSolid-State Circuits Conference-(ISSCC), pages 12–19. IEEE, 2019.\n[22] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. Van\nDer Laak, B. Van Ginneken, and C. I. Sánchez. A survey on deep learning in medical image\nanalysis. Medical image analysis, 42:60–88, 2017.\n[23] L. Maier-Hein, M. Eisenmann, A. Reinke, S. Onogur, M. Stankovic, P. Scholz, T. Arbel,\nH. Bogunovic, A. P. Bradley, A. Carass, et al. Why rankings of biomedical image analysis\ncompetitions should be interpreted with care. Nature communications, 9(1):5217, 2018.\n[24] R. McKinley, R. Meier, and R. Wiest. Ensembles of densely-connected cnns with label-\nuncertainty for brain tumor segmentation. In International MICCAI Brainlesion Workshop, pages\n456–465. Springer, 2018.\n14\n\n\n[25] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz,\nJ. Slotboom, R. Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats).\nIEEE transactions on medical imaging, 34(10):1993–2024, 2014.\n[26] F. Milletari, N. Navab, and S.-A. Ahmadi. V-net: Fully convolutional neural networks for\nvolumetric medical image segmentation. In International Conference on 3D Vision (3DV), pages\n565–571. IEEE, 2016.\n[27] U. Nestle, S. Kremp, A. Schaefer-Schuler, C. Sebastian-Welsch, D. Hellwig, C. Rübe, and C.-M.\nKirsch. Comparison of different methods for delineation of 18f-fdg pet–positive tissue for target\nvolume deﬁnition in radiotherapy of patients with non–small cell lung cancer. Journal of Nuclear\nMedicine, 46(8):1342–1348, 2005.\n[28] S. Nikolov, S. Blackwell, R. Mendes, J. De Fauw, C. Meyer, C. Hughes, H. Askham, B. Romera-\nParedes, A. Karthikesalingam, C. Chu, et al. Deep learning to achieve clinically applicable\nsegmentation of head and neck anatomy for radiotherapy. arXiv preprint arXiv:1809.04430,\n2018.\n[29] M. Nolden, S. Zelzer, A. Seitel, D. Wald, M. Müller, A. M. Franz, D. Maleike, M. Fangerau,\nM. Baumhauer, L. Maier-Hein, et al. The medical imaging interaction toolkit: challenges and\nadvances. International journal of computer assisted radiology and surgery, 8(4):607–620, 2013.\n[30] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y. Hammerla, B. Kainz, et al. Attention u-net: learning where to look for the pancreas. arXiv\npreprint arXiv:1804.03999, 2018.\n[31] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, pages 234–241. Springer, 2015.\n[32] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. van Ginneken, A. Kopp-\nSchneider, B. A. Landman, G. Litjens, B. Menze, et al. A large annotated medical image dataset\nfor the development and evaluation of segmentation algorithmsdelldatagrowth. arXiv preprint\narXiv:1902.09063, 2019.\n[33] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[34] M. Wiesenfarth, A. Reinke, B. A. Landman, M. J. Cardoso, L. Maier-Hein, and A. Kopp-\nSchneider. Methods and open-source toolkit for analyzing and visualizing challenge results.\narXiv preprint arXiv:1910.05121, 2019.\n[35] Z. Wu, C. Shen, and A. v. d. Hengel. Bridging category-level and instance-level semantic image\nsegmentation. arXiv preprint arXiv:1605.06885, 2016.\n15\n\n\nMethods\nA quick overview of the nnU-Net design principles can be found in the Supplemental Material B.\nThis section provides detailed information on how these principles are implemented.\nDataset ﬁngerprints\nAs a ﬁrst processing step, nnU-Net crops the provided training cases to their\nnonzero region. While this had no effect on most datasets in our experiments, it reduced the image\nsize of brain datasets such as D1 (Brain Tumor) and D15 (MSLes) substantially and thus improved\ncomputational efﬁciency. Based on the cropped training data, nnU-Net creates a dataset ﬁngerprint\nthat captures all relevant parameters and properties: image sizes (i.e. number of voxels per spatial\ndimension) before and after cropping, image spacings (i.e. the physical size of the voxels), modalities\n(read from metadata) and number of classes for all images as well as the total number of training\ncases. Furthermore, the ﬁngerprint includes the mean, standard deviation as well as the 0.5 and 99.5\npercentiles of the intensity values in the foreground regions, i.e. the voxels belonging to any of the\nclass labels, computed over all training cases.\nPipeline ﬁngerprints\nnnU-Net automizes the design of deep learning methods for biomedical im-\nage segmentation by generating a so-called pipeline ﬁngerprint that contains all relevant information.\nImportantly, nnU-Net reduces the design choices to the really essential ones and automatically infers\nthese choices using a set of heuristic rules. These rules condense the domain knowledge and operate\non the above-described data ﬁngerprint and the project-speciﬁc hardware constraints. These inferred\nparameters are complemented by blueprint parameters, which are data-independent, and empirical\nparameters, which are optimized during training.\nBlueprint parameters\nArchitecture template: All U-Net architectures conﬁgured by nnU-Net\noriginate from the same template. This template closely follows the original U-Net [16] and its\n3D counterpart [3]. According to our hypothesis that a well-conﬁgured plain U-Net is still hard to\nbeat, none of our U-Net conﬁgurations make use of recently proposed architectural variations such\nas residual connections [6, 7], dense connections [10, 12], attention mechanisms [14], squeeze and\nexcitation [9] or dilated convolutions [2]. Minor changes with respect to the original architecture were\nmade: To enable large patch sizes, the batch size of the networks in nnU-Net is small. In fact, most\n3D U-Net conﬁgurations were trained with a batch size of only 2 (see Supplementary Material Figure\nE.1a). Batch normalization [11], which is often used to speed up or stabilize the training, does not\nperform well with small batch sizes [20, 17]. We therefore use instance normalization [19] for all U-\nNet models. Furthermore, we replace ReLU with leaky ReLUs [13] (negative slope 0.01). Networks\nare trained with deep supervision: additional auxiliary losses are added in the decoder to all but the\ntwo lowest resolutions, allowing gradients to be injected deeper into the network and facilitating the\ntraining of all layers in the network. All U-Nets employ the very common conﬁguration of two blocks\nper resolution step in both encoder and decoder, with each block consisting of a convolution, followed\nby instance normalization and a leaky ReLU nonlinearity. Downsampling is implemented as strided\nconvolution (motivated by representational bottleneck, see [18]) and upsampling as convolution\ntransposed. As a tradeoff between performance and memory consumption, the initial number of\nfeature maps is set to 32 and doubled (halved) with each downsampling (upsampling) operation. To\nlimit the ﬁnal model size, the number of feature maps is additionally capped at 320 and 512 for 3D\nand 2D U-Nets, respectively.\nTraining schedule: Based on experience and as a trade-off between runtime and reward, all networks\nare trained for 1000 epochs with one epoch being deﬁned as iteration over 250 minibatches. Stochastic\n16\n\n\ngradient descent with nesterov momentum (µ = 0.99) and an initial learning rate of 0.01 is used for\nlearning network weights. The learning rate is decayed throughout the training following the ‘poly’\nlearning rate policy [2]: (1 −epoch/epochmax)0.9. The loss function is the sum of cross-entropy\nand Dice loss [4]. For each deep supervision output, a corresponding downsampled ground truth\nsegmentation mask is used for loss computation. The training objective is the sum of the losses at\nall resolutions: L = w1 · L1 + w2 · L2 + ... . Hereby, the weights halve with each decrease in\nresolution, resulting in w2 = 1/2 · w1; w3 = 1/4 · w1, etc. and are normalized to sum to 1. Samples\nfor the mini batches are chosen from random training cases. Oversampling is implemented to ensure\nrobust handling of class imbalances: 66.7% of samples are from random locations within the selected\ntraining case while 33.3% of patches are guaranteed to contain one of the foreground classes that\nare present in the selected training sample (randomly selected). The number of foreground patches\nis rounded with a forced minimum of 1 (resulting in 1 random and 1 foreground patch with batch\nsize 2). A variety of data augmentation techniques are applied on the ﬂy during training: rotations,\nscaling, Gaussian noise, Gaussian blur, brightness, contrast, simulation of low resolution, gamma and\nmirroring. Details are provided in Supplementary Information D.\nInference: Images are predicted with a sliding window approach, where the window size equals the\npatch size used during training. Adjacent predictions overlap by half the size of a patch. The accuracy\nof segmentation decreases towards the borders of the window. To suppress stitching artifacts and\nreduce the inﬂuence of positions close to the borders, a Gaussian importance weighting is applied,\nincreasing the weight of the center voxels in the softmax aggregation. Test time augmentation by\nmirroring along all axes is applied.\nInferred Parameters\nIntensity normalization: There are two different image intensity normal-\nization schemes supported by nnU-Net. The default setting for all modalities except CT images is\nz-scoring. For this option, during training and inference, each image is normalized independently\nby subtracting its mean, followed by division with its standard deviation. If cropping resulted in\nan average size decrease of 25% or more, a mask for central non-zero voxels is created and the\nnormalization is applied within that mask only, ignoring the surrounding zero voxels. For computed\ntomography (CT) images, nnU-Net employs a different scheme, as intensity values are quantitative\nand reﬂect physical properties of the tissue. It can therefore be beneﬁcial to retain this information by\nusing a global normalization scheme that is applied to all images. To this end, nnU-Net uses the 0.5\nand 99.5 percentiles of the foreground voxels for clipping as well as the global foreground mean a\nstandard deviation for normalization on all images.\nResampling: In some datasets, particularly in the medical domain, the voxel spacing (the physical\nspace the voxels represent) is heterogeneous. Convolutional neural networks operate on voxel grids\nand ignore this information. To cope with this heterogeneity, nnU-Net resamples all images to the\nsame target spacing (see paragraph below) using either third order spline, linear or nearest neighbor\ninterpolation. The default setting for image data is third order spline interpolation. For anisotropic\nimages (maximum axis spacing / minimum axis spacing > 3), in-plane resampling is done with third\norder spline whereas out of plane interpolation is done with nearest neighbor. Treating the out of\nplane axis differently in anisotropic cases suppresses resampling artifacts, as large contour changes\nbetween slices are much more common. Segmentation maps are resampled by converting them to\none hot encodings. Each channel is then interpolated with linear interpolation and the segmentation\nmask is retrieved by an argmax operation. Again, anisotropic cases are interpolated using “nearest\nneighbor” on the low resolution axis.\n17\n\n\nTarget spacing: The selected target spacing is a crucial parameter. Larger spacings result in smaller\nimages and thus a loss of details whereas smaller spacings result in larger images preventing the\nnetwork from accumulating sufﬁcient contextual information since the patch size is limited by the\ngiven GPU memory budget. Although this tradeoff is in part addressed by the 3D U-Net cascade\n(see below), a sensible target spacing for low and full resolution is still required. For the 3D full\nresolution U-Net, nnU-Net uses the median value of the spacings found in the training cases computed\nindependently for each axis as default target spacing. For anisotropic datasets, this default can result in\nsevere interpolation artifacts or in a substantial loss of information due to large variances in resolution\nacross the training data. Therefore, the target spacing of the lowest resolution axis is selected to be\nthe 10th percentile of the spacings found in the training cases if both voxel and spacing anisotropy\n(i.e. the ratio of lowest spacing axis to highest spacing axis) are larger than 3. For the 2D U-Net,\nnnU-Net generally operates on the two axes with the highest resolution. If all three axes are isotropic,\nthe two trailing axes are utilized for slice extraction. The target spacing is the median spacing of the\ntraining cases (computed independently for each axis). For slice-based processing, no resampling\nalong the out-of-plane axis is required.\nAdaptation of network topology, patch size and batch size: Finding an appropriate U-Net architecture\nconﬁguration is crucial for good segmentation performance. nnU-Net prioritizes large patch sizes\nwhile remaining within a predeﬁned GPU memory budget. Larger patch sizes allow for more\ncontextual information to be aggregated and thus typically increase segmentation performance.\nThey come, however, at the cost of a decreased batch size which results in noisier gradients during\nbackpropagation. To improve the stability of the training, we require a minimum batch size of 2 and\nchoose a large momentum term for network training (see blueprint parameters). Image spacing is also\nconsidered in the adaptation process: Downsampling operations may operate only on speciﬁc axes\nand convolutional kernels in the 3D U-Nets can operate on certain image planes only (pseudo-2D).\nThe network topology for all U-Net conﬁgurations is chosen on basis of the median image size after\nresampling as well as the target spacing the images were resampled to. A ﬂow chart for the adaptation\nprocess is presented in the Supplements in Figure E.1. The adaptation of the architecture template,\nwhich is described in more detail in the following, is computationally inexpensive. Due to the GPU\nmemory consumption estimate being based on feature map sizes, no GPU is required to run the\nadaptation process.\nInitialization: The patch size is initialized as the median image shape after resampling. If the patch\nsize is not divisible by 2nd for each axis, where nd is the number of downsampling operations, it is\npadded accordingly.\nArchitecture topology: The architecture is conﬁgured by determining the number of downsampling\noperations along each axis depending on the patch size and voxel spacing. Downsampling is\nperformed until further downsampling would reduce the feature map size to smaller than 4 voxels or\nthe feature map spacings become anisotropic. The downsampling strategy is determined by the voxel\nspacing: high resolution axes are downsampled separately until their resolution is within factor 2 of\nthe lower resolution axis. Subsequently, all axes are downsampled simultaneously. Downsampling is\nterminated for each axis individually, once the respective feature map constraint is triggered. The\ndefault kernel size for convolutions is 3 × 3 × 3 and 3 × 3 for 3D U-Net and 2D U-Net, respectively.\nIf there is an initial resolution discrepancy between axes (deﬁned as a spacing ratio larger than 2), the\nkernel size for the out-of-plane axis is set to 1 until the resolutions are within a factor of 2. Note that\nthe convolutional kernel size then remains at 3 for all axes.\n18\n\n\nAdaptation to GPU memory budget: The largest possible patch size during conﬁguration is limited\nby the amount of GPU memory. Since the patch size is initialized to the median image shape after\nresampling, it is initially too large to ﬁt into the GPU for most datasets. nnU-Net estimates the\nmemory consumption of a given architecture based on the size of the feature maps in the network,\ncomparing it to reference values of known memory consumption. The patch size is then reduced in\nan iterative process while updating the architecture conﬁguration accordingly in each step until the\nrequired budget is reached (see Figure E.1 in the Supplements). The reduction of the patch size is\nalways applied to the largest axis relative to the median image shape of the data. The reduction in\none step amounts to 2nd voxels of that axis, where nd is the number of downsampling operations.\nBatch size: As a ﬁnal step, the batch size is conﬁgured. If a reduction of patch size was performed\nthe batch size is set to 2. Otherwise, the remaining GPU memory headroom is utilized to increase the\nbatch size until the GPU is fully utilized. To prevent overﬁtting, the batch size is capped such that the\ntotal number of voxels in the minibatch do not exceed 5% of the total number of voxels of all training\ncases. Examples for generated U-Net architectures are presented in Supplementary Information C.1\nand C.2.\nConﬁguration of 3D U-Net cascade: Running a segmentation model on downsampled data increases\nthe size of patches in relation to the image and thus enables the network to accumulate more contextual\ninformation. This comes at the cost of a reduction in details in the generated segmentations and\nmay also cause errors if the segmentation target is very small or characterized by its texture. In a\nhypothetical scenario with unlimited GPU memory, it is thus generally favored to train models at\nfull resolution with a patch size that covers the entire image. The 3D U-Net cascade approximates\nthis approach by ﬁrst running a 3D U-Net on downsampled images and then training a second, full\nresolution 3D U-Net to reﬁne the segmentation maps of the former. This way, the “global”, low\nresolution network utilizes maximal contextual information to generate its segmentation output,\nwhich then serves as an additional input channel that guides the second, “local” U-Net. The cascade\nis triggered only for datasets where the patch size of the 3d full resolution U-Net covers less than\n12.5% of the median image shape. If this is the case, the target spacing for the downsampled data\nand the architecture of the associated 3D low resolution U-Net are conﬁgured jointly in an iterative\nprocess. The target spacing is initialized as the target spacing of the full resolution data. In order\nfor the patch size to cover a large proportion of the input image, the target spacing is then increased\nstepwise by 1% while updating the architecture conﬁguration accordingly in each step until the patch\nsize of the resulting network topology surpasses 25% of the current median image shape. If the\ncurrent spacing is anisotropic (factor 2 difference between lowest and highest resolution axis), only\nthe spacing of the higher resolution axes is increased. The conﬁguration of the second 3D U-Net of\nthe cascade is identical to the standalone 3D U-Net for which the conﬁguration process is described\nabove (except that the upsampled segmentation maps of the ﬁrst U-Net are concatenated to its input).\nFigure E.1b in the Supplements provides an overview of this optimization process.\nEmpirical parameters\nEnsembling and selection of U-Net conﬁguration(s): nnU-Net automat-\nically determines which (ensemble of) conﬁguration(s) to use for inference based on the average\nforeground Dice coefﬁcient computed via cross-validation on the training data. The selected model(s)\ncan be either a single U-Net (2D, 3D full resolution, 3D low resolution or the full resolution U-Net of\nthe cascade) or an ensemble of any two of these conﬁgurations. Models are ensembled by averaging\nsoftmax probabilities.\n19\n\n\nPostprocessing: Connected component-based postprocessing is commonly used in medical image\nsegmentation [1, 8]. Especially in organ segmentation it often helps to remove spurious false positive\ndetections by removing all but the largest connected component. nnU-Net follows this assumption\nand automatically benchmarks the effect of suppressing smaller components on the cross-validation\nresults. First, all foreground classes are treated as one component. If suppression of all but the largest\nregion improves the average foreground Dice coefﬁcient and does not reduce the Dice coefﬁcient\nfor any of the classes, this procedure is selected as the ﬁrst postprocessing step. Finally, nnU-Net\nbuilds on the outcome of this step and decides whether the same procedure should be performed for\nindividual classes.\nImplementation details\nnnU-Net is implemented in Python utilizing the PyTorch [15] framework.\nThe Batchgenerators library [5] is used for data augmentation. For reduction of computational\nburden and GPU memory footprint, mixed precision training is implemented with Nvidia Apex/Amp\n(https://github.com/NVIDIA/apex). For use as a framework, the source code is available\non GitHub (https://github.com/MIC-DKFZ/nnUNet). Users who seek to use nnU-Net as a\nstandardized benchmark or to run inference with our pretrained models can install nnU-Net via PyPI.\nFor a full description of how to use nnU-Net, please refer to the online documentation available on\nthe GitHub page.\nReporting summary\nFurther information on research design is available in the Nature Research Reporting Summary linked\nto this article.\nCode availability\nThe nnU-Net repository is available at: https://github.com/mic-dkfz/nnunet. Pre-traiend\nmodels for all datasets utilized in this study are available for download at https://zenodo.org/\nrecord/3734294.\nData availability\nAll 19 datasets used in this study are publicly available. References for web access as well as key\ndata properties can be found in the Supplementary Material A and F.\nReferences\n[1] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-\nA. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n[3] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger. 3d u-net: learning\ndense volumetric segmentation from sparse annotation. In International conference on medical\nimage computing and computer-assisted intervention, pages 424–432. Springer, 2016.\n20\n\n\n[4] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and C. Pal. The importance of skip\nconnections in biomedical image segmentation. In Deep Learning and Data Labeling for\nMedical Applications, pages 179–187. Springer, 2016.\n[5] I. Fabian, J. Paul, W. Jakob, Z. David, P. Jens, K. Simon, S. Justus, K. Andre, R. Tobias,\nW. Sebastian, N. Peter, D. Stefan, K. Gregor, and M.-H. Klaus. batchgenerators - a python\nframework for data augmentation, Jan. 2020.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\n778, 2016.\n[7] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016.\n[8] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[9] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 7132–7141, 2018.\n[10] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[12] S. Jégou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers\ntiramisu: Fully convolutional densenets for semantic segmentation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition workshops, pages 11–19, 2017.\n[13] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer nonlinearities improve neural network\nacoustic models. In Proc. icml, volume 30, page 3, 2013.\n[14] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y. Hammerla, B. Kainz, et al. Attention u-net: learning where to look for the pancreas. arXiv\npreprint arXiv:1804.03999, 2018.\n[15] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Systems, pages 8024–8035, 2019.\n[16] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, pages 234–241. Springer, 2015.\n[17] S. Singh and S. Krishnan. Filter response normalization layer: Eliminating batch dependence in\nthe training of deep neural networks. arXiv preprint arXiv:1911.09737, 2019.\n[18] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception archi-\ntecture for computer vision. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818–2826, 2016.\n21\n\n\n[19] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[20] Y. Wu and K. He. Group normalization. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 3–19, 2018.\n22\n\n\nSupplementary Information\nThis document contains supplementary information for the manuscript ’Automated Design\nof Deep Learning Methods for Biomedical Image Segmentation’.\nA\nDataset details\nTable A provides an overview of the datasets used in this manuscript including respective references\nfor data access. The numeric values presented here are computed based on the training cases for each\nof these datasets. They are the basis of the dataset ﬁngerprints presented in Figure 5.\nID\nDataset Name\nAssociated\nChallenges\nModalities\nMedian Shape\n(Spacing [mm])\nN\nClasses\nRarest\nClass Ratio\nN Training\nCases\nSegmentation Tasks\nD1\nBrain Tumour\n[15], [14]\nMRI (T1, T1c,\nT2, FLAIR)\n138x169x138\n(1, 1, 1)\n3\n7.310−3\n484\nedema, active tumor,\nnecrosis\nD2\nHeart\n[15]\nMRI\n115x320x232\n(1.37, 1.25, 1.25)\n1\n4.010−3\n20\nleft ventricle\nD3\nLiver\n[15], [2]\nCT\n432x512x512\n(1, 0.77, 0.77)\n2\n2.610−2\n131\nliver, liver tumors\nD4\nHippocampus\n[15]\nMRI\n36x50x35\n(1, 1, 1)\n2\n2.710−2\n260\nanterior and posterior\nhippocampus\nD5\nProstate\n[15]\nMRI\n(T2, ADC)\n20x320x319\n(3.6, 0.62, 0.62)\n2\n5.410−3\n32\nperipheral and\ntransition zone\nD6\nLung\n[15]\nCT\n252x512x512\n(1.24, 0.79, 0.79)\n1\n3.910−4\n63\nlung nodules\nD7\nPancreas\n[15]\nCT\n93x512x512\n(2.5, 0.80, 0.80)\n2\n2.010−3\n282\npancreas, pancreas\ncancer\nD8\nHepaticVessel\n[15]\nCT\n49x512x512\n(5, 0.80, 0.80)\n2\n1.110−3\n303\nhepatic vessels,\ntumors\nD9\nSpleen\n[15]\nCT\n90x512x512\n(5, 0.79, 0.79)\n1\n4.710−3\n41\nspleen\nD10\nColon\n[15]\nCT\n95x512x512\n(5, 0.78, 0.78)\n1\n5.610−4\n126\ncolon cancer\nD11\nAbdOrgSeg\n[12]\nCT\n128x512x512\n(3, 0.76, 0.76)\n13\n4.410−3\n30\n13 abdominal\norgans\nD12\nPromise\n[13]\nMRI\n24x320x320\n(3.6, 0.61, 0.61)\n1\n2.010−2\n50\nprostate\nD13\nACDC\n[1]\ncine MRI\n9x256x216\n(10, 1.56, 1.56)\n3\n1.210−2\n200\n(100x2) *\nleft ventricle, right\nventricle,\nmyocardium\nD14\nLiTS **\n[2]\nCT\n432x512x512\n(1, 0.77, 0.77)\n2\n2.610−2\n131\nliver, liver tumors\nD15\nMSLesion\n[4]\nMRI (FLAIR,\nMPRAGE, PD,\nT2)\n137x180x137\n(1, 1, 1)\n1\n1.710−3\n42\n(21x2) *\nmultiple sclerosis\nlesions\nD16\nCHAOS\n[11]\nMRI\n30x204x256\n(9, 1.66, 1.66)\n4\n3.310−2\n60\n(20 + 20x2) *\nliver, spleen, left and\nright kidney\nD17\nKiTS\n[7]\nCT\n107x512x512\n(3, 0.78, 0.78)\n2\n7.510−3\n206\nkidney, kidney\ntumor\nD18\nSegTHOR\n[16]\nCT\n178x512x512\n(2.5, 0.98, 0.98)\n4\n4.610−4\n40\nheart, aorta,\nesophagus, trachea\nD19\nCREMI\n[6]\nElectron\nMicroscopy\n125x1250x1250\n(40, 4, 4)\n1\n5.210−3\n3\nsynaptic clefts\n* multiple annotated examples per training case\n** almost identical to Decathlon Liver; Decathlon changed the training cases and test set slightly\nTable A.1: Overview over the challenge datasets used in this manuscript.\n23\n\n\nB\nnnU-Net Design Principles\nHere we present a brief overview of the design principles of nnU-Net on a conceptual level. Please\nrefer to the online methods for a more detailed information on how these guidelines are implemented.\nB.1\nBlueprint Parameters\n• Architecture Design decisions:\n– U-Net like architectures enable state of the art segmentation when the pipeline is\nwell-conﬁgured. According to our experience, sophisticated architectural variations are\nnot required to achieve state of the art performance.\n– Our architectures only use plain convolutions, instance normalization and Leaky non-\nlinearities. The order of operations in each computational block is conv - instance norm\n- leaky ReLU.\n– We use two computational blocks per resolution stage in both encoder and decoder.\n– Downsampling is done with strided convolutions (the convolution of the ﬁrst block of\nthe new resolution has stride >1), upsampling is done with convolutions transposed.\nWe should note that we did not observe substantial disparities in segmentation accuracy\nbetween this approach and alternatives (e.g. max pooling, bi/trilinear upsampling).\n• Selecting the best U-Net conﬁguration: It is difﬁcult to estimate which U-Net conﬁguration\nperforms best on what dataset. To address this, nnU-Net designs three separate conﬁgurations\nand automatically chooses the best one based on cross-validation (see inferred parameters).\nPredicting which conﬁgurations should be trained on which dataset is a future research\ndirection.\n– 2D U-Net: Runs on full resolution data. Expected to work well on anisotropic data,\nsuch as D5 (Prostate MRI) and D13 (ACDC, cine MRI) (for dataset references see\nTable A).\n– 3D full resolution U-Net: Runs on full resolution data. Patch size is limited by\navailability of GPU memory. Is overall the best performing conﬁguration (see results\nin F). For large data, however, the patch size may be too small to aggregate sufﬁcient\ncontextual information.\n– 3D U-Net cascade: Speciﬁcally targeted towards large data. First, coarse segmentation\nmaps are learned by a 3D U-Net that operates on low resolution data. These segmen-\ntation maps are then reﬁned by a second 3D U-Net that operates on full resolution\ndata.\n• Training Scheme\n– All trainings run for a ﬁxed length of 1000 epochs, where each epoch is deﬁned as 250\ntraining iterations (using the batch size conﬁgured by nnU-Net). Shorter trainings than\nthis default empirically result in diminished segmentation performance.\n– As for the opimizer, stochastic gradient descent with a high initial learning rate (0.01)\nand a large nesterov momentum (0.99) empirically provided the best results. The\nlearning rate is reduced during the training using the ’polyLR’ schedule as described in\n[5], which is an almost linear decrease to 0.\n– Data augmentation is essential to achieve state of the art performance. It is important\nto run the augmentations on the ﬂy and with associated probabilities to obtain a never\nending stream of unique examples (see Section D for details).\n24\n\n\n– Data in the biomedical domain suffers from class imbalance. Rare classes could end\nup being ignored because they are underrepresented during training. Oversampling\nforeground regions addresses this issue reliably. It should, however, not be overdone so\nthat the network also sees all the data variability of the background.\n– The Dice loss function is well suited to address the class imbalance, but comes with\nits own drawbacks. Dice loss optimizes the evaluation metric directly, but due to the\npatch based training, in practice merely approximates it. Furthermore, oversampling\nof classes skews the class distribution seen during training. Empirically, combining\nthe Dice loss with a cross-entropy loss improved training stability and segmentation\naccuracy. Therefore, the two loss terms are simply averaged.\n• Inference\n– Validation sets of all folds in the cross-validation are predicted by the single model\ntrained on the respective training data. The 5 models resulting from training on 5\nindividual folds are subsequently used as an ensemble for predicting test cases.\n– Inference is done patch based with the same patch size as used during training. Fully\nconvolutional inference is not recommended because it causes issues with zero-padded\nconvolutions and instance normalization.\n– To prevent stitching artifacts, adjacent predictions are done with a distance of patch_size\n/ 2. Predictions towards the border are less accurate, which is why we use Gaussian\nimportance weighting for softmax aggregation (the center voxels are weighted higher\nthen the border voxels).\nB.2\nInferred Parameters\nThese parameters are not ﬁxed across datasets, but conﬁgured on-the-ﬂy by nnU-Net according to the\ndata ﬁngerprint (low dimensional representation of dataset properties) of the task at hand.\n• Dynamic Network adaptation:\n– The network architecture needs to be adapted to the size and spacing of the input\npatches seen during training. This is necessary to ensure that the receptive ﬁeld of the\nnetwork covers the entire input.\n– We perform downsampling until the feature maps are relatively small (minimum is\n4 × 4(×4)) to ensure sufﬁcient context aggregation.\n– Due to having a ﬁxed number of blocks per resolution step in both the encoder and\ndecoder, the network depth is coupled to its input patch size. The number of convolu-\ntional layers in the network (excluding segmentation layers) is (5 ∗k + 2) where k is\nthe number of downsampling operations (5 per downsampling stems from 2 convs in\nthe encoder, 2 in the decoder plus the convolution transpose).\n– Additional loss functions are applied to all but the two lowest resolutions of the decoder\nto inject gradients deep into the network.\n– For anisotropic data, pooling is ﬁrst exclusively performed in-plane until the resolution\nmatches between the axes. Initially, 3D convolutions use a kernel size of 1 (making\nthem effectively 2D convolutions) in the out of plane axis to prevent aggregation of\ninformation across distant slices. Once an axes becomes too small, downsampling is\nstopped individually for this axis.\n• Conﬁguration of the input patch size:\n25\n\n\n– Should be as large as possible while still allowing a batch size of 2 (under a given GPU\nmemory constraint). This maximizes the context available for decision making in the\nnetwork.\n– Aspect ratio of patch size follows the median shape (in voxels) of resampled training\ncases.\n• Batch size:\n– Batch size is conﬁgured with a minimum of 2 to ensure robust optimization, since\nnoise in gradients increases with fewer sample in the minibatch.\n– If GPU memory headroom is available after patch size conﬁguration, the batch size is\nincreased until GPU memory is maxed out.\n• Target spacing and resampling:\n– For isotropic data, the median spacing of training cases (computed independently\nfor each axis) is set as default. Resampling with third order spline (data) and linear\ninterpolation (one hot encoded segmentation maps such as training annotations) give\ngood results.\n– For anisotropic data, the target spacing in the out of plane axis should be smaller than\nthe median, resulting in higher resolution in order to reduce resampling artifacts. To\nachieve this we set the target spacing as the 10th percentile of the spacings found for\nthis axis in the training cases. Resampling across the out of plane axis is done with\nnearest neighbor for both data and one-hot encoded segmentation maps.\n• Intensity normalization:\n– Z-score per image (mean substraction and division by standard deviation) is a good\ndefault.\n– We deviate from this default only for CT images, where a global normalization scheme\nis determined based on the intensities found in foreground voxels across all training\ncases.\nB.3\nEmpirical Parameters\nSome parameters cannot be inferred by simply looking at the dataset ﬁngerprint of the training cases.\nThese are determined empirically by monitoring validation performance after training.\n• Model selection: While the 3D full resolution U-Net shows overall best performance,\nselection of the best model for a speciﬁc task at hand can not be predicted with perfect\naccuracy. Therefore, nnU-Net generates three U-Net conﬁgurations and automatically picks\nthe best performing method (or ensemble of methods) after cross-validation.\n• Postprocessing: Often, particularly in medical data, the image contains only one instance\nof the target structure. This prior knowledge can often be exploited by running connected\ncomponent analysis on the predicted segmentation maps and removing all but the largest\ncomponent. Whether to apply this postprocessing is determined by monitoring validation\nperformance after cross-validation. Speciﬁcally, postprocessing is triggered for individual\nclasses where the Dice score is improved by removing all but the largest component.\n26\n\n\nFigure C.1: Network architectures generated by nnU-Net for the ACDC dataset (D13)\nC\nAnalysis of exemplary nnU-Net-generated pipelines\nIn this section we brieﬂy introduce the pipelines generated by nnU-Net for D13 (ACDC) and D14\n(LiTS) to create an intuitive understanding of nnU-Nets design principles and the motivation behind\nthem.\nC.1\nACDC\nFigure C.1 provides a summary of the pipelines that were automatically generated by nnU-Net for\nthis dataset.\nDataset Description\nThe Automated Cardiac Diagnosis Challenge (ACDC) [1] was hosted by\nMICCAI in 2017. Since then it is running as an open challenge with data and current leaderboard\navailable at https://acdc.creatis.insa-lyon.fr. In the segmentation part of the challenge,\nparticipating teams were asked to generate algorithms for segmenting the right ventricle, the left\nmyocardium and the left ventricular cavity from cine MRI. For each patient, reference segmentations\nfor two time steps within the cardiac cycle were provided. With 100 training patients, this amounts to\na total of 200 annotated images. One key property of cine MRI is that slice acquisition takes place\nacross multiple cardiac cycles and breath holds. This results in a limited number of slices and thus a\nlow out of plane resolution as well as the possibility for slice misalignments. Figure C.1 provides a\nsummary of the pipelines that were automatically generated by nnU-Net for this dataset. The typical\n27\n\n\nimage shape (here the median image size is computed for each axis independently) is 9 × 237 × 256\nvoxels at a spacing of 10 × 1.56 × 1.56 mm.\nIntensity Normalization\nWith the images being MRI, nnU-Net normalizes all images individually\nby subtracting their mean and dividing by their standard deviation.\n2D U-Net\nAs target spacing for the in-plane resolution, 1.56 × 1.56 mm is determined. This is\nidentical for the 2D and the 3D full resolution U-Net. Due to the 2D U-Net operating on slices only,\nthe out of plane resolution for this conﬁguration is not altered and remains heterogeneous within the\ntraining set. The 2D U-Net is conﬁgured as described in the Online Methods 4 to have a patch size of\n256 × 224 voxels, which fully covers the typical image shape after in-plane resampling (237 × 208).\n3D U-Net The size and spacing anisotropy of this dataset causes the out-of-plane target spacing\nof the 3D full resolution U-Net to be selected as 5mm, corresponding to the 10th percentile of the\nspacings found in the training cases. In datasets such as ACDC, the segmentation contour can change\nsubstantially between slices due to the large slice to slice distance. Choosing the target spacing\nto be lower results in more images that are upsampled for U-Net training and then downsampled\nfor the ﬁnal segmentation export. Preferring this variant over the median causes more images to\nbe downsampled for training and then upsampled for segmentation export and therefore reduces\ninterpolation artifacts substantially. Also note that resampling the out of plane axis is done with\nnearest neighbor interpolation.The median image shape after resampling for the 3D full resolution\nU-Net is 18 × 237 × 208 voxels. As described in the Online Methods 4 nnU-Net conﬁgures a patch\nsize of 20 × 256 × 224 for network training, which ﬁts into the memory budget with a batch size of 3.\nNote how the convolutional kernel sizes in the 3D U-Net start with (1 × 3 × 3) which is effectively a\n2D convolution for the initial layers (see also Figure C.1). The reasoning behind this is that due to the\nlarge discrepancy in voxel spacing, too many changes are expected across slices and the aggregation\nof imaging information may therefore not be beneﬁcial. Similarly, pooling is done in-plane only\n(conv kernel stride (1, 2, 2)) until the spacing between in-plane and out-of-plane axes are within a\nfactor of 2. Only after the spacings approximately match the pooling and the convolutional kernel\nsizes become isotropic.\n3D U-Net\ncascade Since the 3D U-Net already covers the whole median image shape, the U-Net\ncascade is not necessary and therefore omitted.\nTraining and Postprocessing\nDuring training, spatial augmentations for the 3D U-Net (such as\nscaling and rotation) are done in-plane only to prevent resampling of imaging information across\nslices which would cause interpolation artifacts. Each U-Net conﬁguration is trained in a ﬁve-fold\ncross-validation on the training cases. Note that we interfere with the splits in order to ensure that\npatients are properly stratiﬁed (since there are two images per patient). Thanks to the cross-validation,\nnnU-Net can use the entire training set for validation and ensembling. To this end, the validation splits\nof each of the ﬁve fold are aggregated. nnU-Net evaluates the performance (ensemble of models or\nsingle conﬁguration) by averaging the Dice scores over all foreground classes and cases, resulting in a\nsingle scalar value. Detailed results are omitted here for brevity (they are presented in Supplementary\nInformation F). Based on this evaluation scheme, the 2D U-Net obtains a score of 0.9165, the 3D full\nresolution a score of 0.9181 and the ensemble of the two a score of 0.9228. Therefore the ensemble\nis selected for predicting the test cases. Postprocessing is conﬁgured on the segmentation maps of\nthe ensemble. Removing all but the largest connected component was found beneﬁcial for the right\nventricle and the left ventricular cavity.\n28\n\n\nC.2\nLiTS\nFigure C.2 provides a summary of the pipelines that were automatically generated by nnU-Net for\nthis dataset.\nDataset Description\nThe Liver and Liver Tumor Segmentation challenge (LiTS) [2] was hosted by\nMICCAI in 2017. Due to the large, high quality dataset it provides, the challenge plays an important\nrole in concurrent research. The challenge is hosted at https://competitions.codalab.org/\ncompetitions/17094. The segmentation task in LiTS is the segmentation of the liver and liver\ntumors in abdominal CT scans. The challenge provides 131 training cases with reference annotations.\nThe test set has a size of 70 cases and the reference annotations are known only to the challenge\norganizers. The median image shape of the training cases is 432 × 512 × 512 voxels with a\ncorresponding voxel spacing of 1 × 0.77 × 0.77 mm.\nIntensity Normalization\nVoxel intensities in CT scans are linked to quantitative physical properties\nof the tissue. The intensities are therefore expected to be consistent between scanners. nnU-Net\nleverages this consistency by applying a global intensity normalization scheme (as opposed to\nACDC in Supplementary Information C.1, where cases are normalized individually using their mean\nand standard deviation). To this end, nnU-Net extracts intensity information as part of the dataset\nﬁngerprint: the intensities of the voxels belonging to any of the foreground classes (liver and liver\ntumor) are collected across all training cases. Then, the mean and standard deviations of these values\nas well as their 0.5 and 99.5 percentiles are computed. Subsequently, all images are normalized by\nclipping them to the 0.5 and 99.5 percentiles, followed by subtraction of the global mean and division\nby the global standard deviation.\n2D U-Net\nThe target spacing for the 2D U-Net is determined to be NA × 0.77 × 0.77 mm, which\ncorresponds to the median voxel spacing encountered in the training cases. Note that the 2D U-Net\noperates on slices only, so the out of plane axis is left untouched. Resampling the training cases\nresults in a median image shape of NA × 512 × 512 voxels (we indicate by NA that this axis is not\nresampled). Since this is the median shape, cases in the training set can be smaller or larger than that.\nThe 2D U-Net is conﬁgured to have an input patch size of 512 × 512 voxels and a batch size of 12.\n3D U-Net\nThe target spacing for the 3D U-Net is determined to be 1 × 0.77 × 0.77 mm,\nwhich corresponds to the median voxel spacing. Because the median spacing is nearly isotropic,\nnnU-Net does not use the 10th percentile for the out of plane axis as was the case for ACDC\n(see Supplementary Information C.1). The resampling strategy is decided on a per-image basis.\nIsotropic cases (maximum axis spacing / minimum axis spacing < 3) are resampled with third order\nspline interpolation for the image data and linear interpolation for the segmentations. Note that\nsegmentation maps are always converted into a one hot representation prior to resampling which\nis converted back to a segmentation map after the interpolation. For anisotropic images, nnU-Net\nresamples the out-of-plane axis separately, as was done in ACDC.\nAfter resampling, the median image shape is 482 × 512 × 512.\nnnU-Net prioritizes a large\npatch size over a large batch size (note that these are coupled under a given GPU memory budget) to\ncapture as much contextual information as possible. The 3D U-Net is thus conﬁgured to have a patch\nsize of 128 × 128 × 128 voxels and a batch size of 2, which is the minimum allowed according to\n29\n\n\nFigure C.2: Network architectures generated by nnU-Net for the LiTS dataset (D14)\n30\n\n\nnnU-Net heuristics. Since The input patches have nearly isotropic spacing, all convolutional kernel\nsizes and downsampling strides are isotropic (3 × 3 × 3 and 2 × 2 × 2, respectively).\n3D U-Net cascade\nAlthough nnU-Net prioritizes large input patches, the patch size of the 3D full\nresolution U-Net is too small to capture sufﬁcient contextual information (it only covers 1/60 of the\nvoxels of the median image shape after resampling). This can cause misclassiﬁcations of voxels\nbecause the patches are too ‘zoomed in’, making for instance the distinction between the spleen\nand the liver particularly hard. The 3D U-Net cascade is designed to tackle this problem by ﬁrst\ntraining a 3D U-Net on downsampled data and then reﬁning the low-resolution segmentation output\nwith a second U-Net that operates as full resolution. Using the process described in the Online\nMethods 4 as well as Figure E.1 b), the target spacing for the low resolution U-Net is determined to\nbe 2.47 × 1.9 × 1.9 mm, resulting in a median image shape of 195 × 207 × 207 voxels. The 3D low\nresolution operates on 128 × 128 × 128 patches with a batch size of 2. Note that while this setting is\nidentical to the 3D U-Net conﬁguration here, this is not necessarily the case for other datasets. If the\n3D full resolution U-Net data was anisotropic, nnU-Net would prioritize to downsample the higher\nresolution axes ﬁrst resulting in a deviating network architecture, patch size and batch size. After\nﬁve-fold cross-validation of the 3D low resolution U-Net, the segmentation maps of the respective\nvalidation sets are upsampled to the target spacing of the 3D full resolution U-Net. The full resolution\nU-Net of the cascade (which has an identical conﬁguration to the regular 3D full resolution U-Net) is\nthen trained to reﬁne the coarse segmentation maps and correct any errors it encounters. This is done\nby concatenating a one hot encoding of the upsampled segmentations to the input of the network.\nTraining and Postprocessing\nAll network conﬁgurations are trained as ﬁve fold cross-validation.\nnnU-Net again evaluates all conﬁgurations by computing the average Dice score across all foreground\nclasses, resulting in a scalar metric per conﬁguration. Based on this evaluation scheme, the scores are\n0.7625 for the 2D U-Net, 0.8044 for the 3D full resolution U-Net, 0.7796 for the 3D low resolution\nU-Net and 0.8017 for the full resolution 3D U-Net of the cascade. The best combination of two\nmodels was identiﬁed as the one between the low and full resolution U-Nets with a score of 0.8111.\nPostprocessing is conﬁgured on the segmentation maps of this ensemble. Removing all but the largest\nconnected component was found beneﬁcial for the combined foreground region (union of liver and\nliver tumor label) as well as for the liver label alone, as both resulted in small performance gains\nwhen empirically testing it on the training data.\nD\nDetails on nnU-Net’s Data Augmentation\nA variety of data augmentation techniques is applied during training. All augmentations are computed\non the ﬂy on the CPU using background workers. The data augmentation pipeline is implemented\nwith the publicly available batchgenerators framework 2. nnU-Net does not vary the parameters of\nthe data augmentation pipeline between datasets.\nSampled patches are initially larger than the patch size used for training.\nThis results in\nless out of boundary values (here 0) being introduced during data augmentation when rotation and\nscaling is applied. As a part of the rotation and scaling augmentation, patches are center-cropped to\nthe ﬁnal target patch size. To ensure that the borders of original images appear in the ﬁnal patches,\npreliminary crops may initially extend outside the boundary of the image.\n2https://github.com/MIC-DKFZ/batchgenerators\n31\n\n\nSpatial augmentations (rotation, scaling, low resolution simulation) are applied in 3D for\nthe 3D U-Nets and applied in 2D when training the 2D U-Net or a 3D U-Net with anisotropic patch\nsize. A patch size is considered anisotropic if the largest edge length of the patch size is at least three\ntimes larger than the smallest.\nTo increase the variability in generated patches, most augmentations are varied with param-\neters drawn randomly from predeﬁned ranges. In this context, x ∼U(a, b) indicates that x was\ndrawn from a uniform distribution between a and b. Furthermore, all augmentations are applied\nstochastically according to a predeﬁned probability.\nThe following augmentations are applied by nnU-Net (in the given order):\n1. Rotation and Scaling. Scaling and rotation are applied together for improved speed of\ncomputation. This approach reduces the amount of required data interpolations to one.\nScaling and rotation are applied with a probability of 0.2 each (resulting in probabilities of\n0.16 for only scaling, 0.16 for only rotation and 0.08 for both being triggered). If processing\nisotropic 3D patches, the angles of rotation (in degrees) αx, αy and αz are each drawn\nfrom U(−30, 30). If a patch is anisotropic or 2D, the angle of rotation is sampled from\nU(−180, 180). If the 2D patch size is anisotropic, the angle is sampled from U(−15, 15).\nScaling is implemented via multiplying coordinates with a scaling factor in the voxel grid.\nThus, scale factors smaller than one result in a \"zoom out\" effect while values larger one\nresult in a \"zoom in\" effect. The scaling factor is sampled from U(0.7, 1.4) for all patch\ntypes.\n2. Gaussian Noise. Zero centered additive Gaussian noise is added to each voxel in the sample\nindependently. This augmentation is applied with a probability of 0.15. The variance of the\nnoise is drawn from U(0, 0.1) (note that the voxel intensities in all samples are close to zero\nmean and unit variance due to intensity normalization).\n3. Gaussian Blur. Blurring is applied with a probability of 0.2 per sample. If this augmentation\nis triggered in a sample, blurring is applied with a probability of 0.5 for each of the\nassociated modalities (resulting in a combined probability of only 0.1 for samples with a\nsingle modality). The width (in voxels) of the Gaussian kernel σ is sampled from U(0.5, 1.5)\nindependently for each modality.\n4. Brightness. Voxel intensities are multiplied by x ∼U(0.7, 1.3) with a probability of 0.15.\n5. Contrast. Voxel intensities are multiplied by x ∼U(0.65, 1.5) with a probability of 0.15.\nFollowing multiplication, the values are clipped to their original value range.\n6. Simulation of low resolution. This augmentation is applied with a probability of 0.25 per\nsample and 0.5 per associated modality. Triggered modalities are downsampled by a factor\nof U(1, 2) using nearest neighbor interpolation and then sampled back up to their original\nsize with cubic interpolation. For 2D patches or anisotropic 3D patches, this augmentation\nis applied only in 2D leaving the out of plane axis (if applicable) in its original state.\n7. Gamma augmentation. This augmentation is applied with a probability of 0.15. The\npatch intensities are scaled to a factor of [0, 1] of their respective value range. Then, a\nnonlinear intensity transformation is applied per voxel: inew = iγ\nold with γ ∼U(0.7, 1.5).\nThe voxel intensities are subsequently scaled back to their original value range. With a\n32\n\n\nprobability of 0.15, this augmentation is applied with the voxel intensities being inverted\nprior to transformation: (1 −inew) = (1 −iold)γ.\n8. Mirroring. All patches are mirrored with a probability of 0.5 along all axes.\nFor the full resolution U-Net of the U-net cascade, nnU-Net additionally applies the following\naugmentations to the segmentation masks generated by the low resolution 3D U-net. Note that the\nsegmentations are stored as one hot encoding.\n1. Binary Operators. With probability 0.4, a binary operator is applied to all labels in the\npredicted masks. This operator is randomly chosen from [dilation, erosion, opening, closing].\nThe structure element is a sphere with radius r ∼U(1, 8). The operator is applied to the\nlabels in random order. Hereby, the one hot encoding property is retained. Dilation of one\nlabel, for example, will result in removal of all other labels in the dilated area.\n2. Removal of Connected Components. With probability 0.2, connected components that\nare smaller than 15% of the patch size are removed from the one hot encoding.\nE\nNetwork Architecture Conﬁguration\nFigure E.1 serves as a visual aid for the iterative process of architecture conﬁguration described in\nthe online methods.\nF\nSummary of nnU-Net Challenge Participations\nIn this section we provide details of all challenge participations.\nIn some participations, manual intervention regarding the format of input data or the cross-validation\ndata splits was required for compatibility with nnU-Net. For each dataset, we disclose all manual\ninterventions in this section. The most common cause for manual intervention was training cases that\nwere related to each other (such as multiple time points of the same patient) and thus required to be\nseparated for mutual exclusivity between data splits. A detailed description of how to perform this\nintervention is further provided along with the source code.\nFor each dataset, we run all applicable nnU-Net conﬁgurations (2D, 3D fullres, 3D lowres,\n3D cascade) in 5-fold cross-validation. All models are trained from scratch without pretraining and\ntrained only on the provided training data of the challenge without external training data. Note that\nother participants may be using external data in some competitions. For each dataset, nnU-Net\nsubsequently identiﬁes the ideal conﬁguration(s) based on cross-validation and ensembling. Finally,\nThe best conﬁguration is used to predict the test cases.\nThe pipeline generated by nnU-Net is provided for each dataset in the compact representa-\ntion described in Section F.2. We furthermore provide a table containing detailed cross-validation as\nwell as test set results.\nAll leaderboards were last accessed on December 12th, 2019.\n33\n\n\nFigure E.1: Workﬂow for network architecture conﬁguration. a) the conﬁguration of a U-Net\narchitecture given an input patch size and corresponding voxel spacing. Due to discontinuities in\nGPU memory consumption (due to changes in number of pooling operations and thus network depth),\nthe architecture conﬁguration cannot be solved analytically. b) Conﬁguration of the 3D low resolution\nU-Net of the U-Net cascade. The input patch size of the 3D lowres U-Net must cover at least 1/4 of\nthe median shape of the resampled trainig cases to ensure sufﬁcient contextual information. Higher\nresolution axes are downsampled ﬁrst, resulting in a potentially different aspect ratio of the data\nrelative to the full resolution data. Due to the patch size following this aspect ratio, the network\narchitecture of the low resolution U-Net may differ from the full resolution U-Net. This requires\nreconﬁguration of the network architecture as depicted in a) for each iteration. All computations are\nbased on memory consumption estimates resulting in fast computation times (sub 1s for conﬁguring\nall network architectures).\nF.1\nChallenge Inclusion Criteria\nWhen selecting challenges for participation, our goal was to apply nnU-Net to as many different\ndatasets as possible to demonstrate its robustness and ﬂexibility. We applied the following criteria to\nensure a rigorous and sound testing environment:\n1. The task of the challenge is semantic segmentation in any 3D imaging modality with images\nof any size.\n2. Training cases are provided to the challenge participants.\n3. Test cases are separate, with the ground truth not being available to the challenge participants.\n4. Comparison to results from other participants is possible (e.g. through standardized evalua-\ntion with an online platform and a public leaderboard).\n34\n\n\nFigure F.1: Decoding the architecture. We provide all generated architectures in a compact represen-\ntation from which they can be fully reconstructed if desired. The architecture displayed here can be\nrepresented by means of kernel sizes [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]] and\nstrides [[1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]] (see description in the text).\nThe competitions outlined below are the ones who qualiﬁed under these criteria and were thus selected\nfor evaluation of nnU-Net. To our knowledge, CREMI 3 is the only competition from the biological\ndomain that meets these criteria.\nF.2\nCompact Architecture Representation\nIn the following sections, network architectures generated by nnU-Net will be presented in a\ncompact representation consisting of two lists: one for the convolutional kernel sizes and one for\nthe downsampling strides. As we describe in this section, this representation can be used to fully\nreconstruct the entire network architecture. The condensed representation is chosen to prevent an\nexcessive amount of ﬁgures.\nFigure F.2 exemplary shows the 3D full resolution U-Net for the ACDC dataset (D13).\nThe architecture has 6 resolution stages. Each resolution stage in both encoder and decoder consists\nof two computational blocks. Each block is a sequence of (conv - instance norm - leaky ReLU), as\ndescribed in 4. In this ﬁgure, one such block is represented by an outlined blue box. Within each\nbox, the stride of the convolution is indicated by the ﬁrst three numbers (1,1,1 for the uppermost left\nbox) and the kernel size of the convolution is indicated by the second set of numbers (1,3,3 for the\nuppermost left box). Using this information, along with the template with which our architectures are\ndesigned, we can fully describe the presented architecture with the following lists:\n• Convolutional Kernel Sizes: The kernel sizes of this architecture are [[1, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]. Note that this list contains 6 elements, matching the\n6 resolutions encountered in the encoder. Each element in this list gives the kernel size of\nthe convolutional layers at this resolution (here this is three digits due to the convolutions\nbeing three dimensional). Within one resolution, both blocks use the same kernel size. The\nconvolutions in the decoder mirror the encoder (dropping the last entry in the list due to the\nbottleneck).\n3https://cremi.org/leaderboard/\n35\n\n\n• Downsampling strides: The strides for downsampling here are [[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]. Each downsampling step in the encoder is represented by one entry. A\nstride of 2 results in a downsampling of factor 2 along that axis which a stride of 1 leaves\nthe size unchanged. Note how the stride initially is [1, 2, 2] due to the spacing discrepancy.\nThis changes the initial spacing of 5 × 1.56 × 1.56 mm to a spacing of 5 × 3.12 × 3.12 mm\nin the second resolution step. The downsampling strides only apply to the ﬁrst convolution\nof each resolution stage in the encoder. The second convolution always has a stride of [1,\n1, 1]. Again, the decoder mirrors the encoder, but the stride is used as output stride of the\nconvolution transposed (resulting in appropriate upscaling of feature maps). Outputs of all\nconvolutions transposed have the same shape as the skip connection originating from the\nencoder.\nSegmentation outputs for auxiliary losses are added to all but the two lowest resolution steps.\nF.3\nMedical Segmentation Decathlon\nChallenge summary\nThe Medical Segmentation Decathlon4 [15] is a competition that spans 10\ndifferent segmentation tasks. These tasks are selected to cover a large proportion of the dataset\nvariability in the medical domain. The overarching goal of the competition was to encourage\nresearchers to develop algorithms that can work with these datasets out of the box without manual\nintervention. Each of the tasks comes with respective training and test data. A detailed description of\ndatasets can be found on the challenge homepage. Originally, the challenge was divided into two\nphases: In phase I, 7 datasets were provided to the participants for algorithm development. In phase\nII, the algorithms were applied to three additional and previously unseen datasets without further\nchanges. Challenge evaluation was performed for the two phases individually and winners were\ndetermined based on their performance on the test cases.\nInitial version of nnU-Net\nA preliminary version of nnU-Net was developed as part\nof our entry in this competition, where it achieved the ﬁrst rank in both phases (see\nhttp://medicaldecathlon.com/results.html).\nWe subsequently made the respective\nchallenge report available on arXiv [10].\nnnU-Net has since been reﬁned using all ten tasks of the Medical Segmentation Decathlon.\nThe current version of nnU-Net as presented in this publication was again submitted to the open\nleaderboard (https://decathlon-10.grand-challenge.org/evaluation/results/), and\nachieved the ﬁrst rank outperforming the initial nnU-Net as well as other methods that held the state\nof the art since the original competition [17].\nApplication of nnU-Net to the Medical Segmentation Decathlon\nnnU-Net was applied to all ten\ntasks of the Medical Segmentation Decathlon without any manual intervention.\nBrainTumour (D1)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n4http://medicaldecathlon.com/\n36\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\n-\nMedian image shape at\ntarget spacing:\nNA x 169 x 138\n138 169 138\n-\nPatch size:\n192 x 160\n128 x 128 x 128\n-\nBatch size:\n107\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.1: Network conﬁgurations generated by nnU-Net for the BrainTumour dataset from the\nMedical Segmentation Decathlon (D1). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\nedema\nnon-enhancing tumor\nenhancing tumour\nmean\n2D\n0.7957\n0.5985\n0.7825\n0.7256\n3D_fullres *\n0.8101\n0.6199\n0.7934\n0.7411\nBest Ensemble\n0.8106\n0.6179\n0.7926\n0.7404\nPostprocessed\n0.8101\n0.6199\n0.7934\n0.7411\nTest set\n0.68\n0.47\n0.68\n0.61\nTable F.2: Decathlon BrainTumour (D1) results. Note that all reported Dice scores (except the test\nset) were computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\") Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nHeart (D2)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.25 x 1.25\n1.37 x 1.25 x 1.25\n-\nMedian image shape at\ntarget spacing:\nNA x 320 x 232\n115 x 320 x 232\n-\nPatch size:\n320 x 256\n80 x 192 x 160\n-\nBatch size:\n40\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 1]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], ]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.3: Network conﬁgurations generated by nnU-Net for the Heart dataset from the Medical\nSegmentation Decathlon (D2). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\n37\n\n\nleft atrium\nmean\n2D\n0.9090\n0.9090\n3D_fullres *\n0.9328\n0.9328\nBest Ensemble\n0.9268\n0.9268\nPostprocessed\n0.9329\n0.9329\nTest set\n0.93\n0.93\nTable F.4: Decathlon Heart (D2) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nLiver (D3)\nNormalization: Clip to [−17, 201], then subtract 99.40 and ﬁnally divide by 39.36.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.7676 x 0.7676\n1 x 0.7676 x 0.7676\n2.47 x 1.90 x 1.90\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n482 x 512 x 512\n195 x 207 x 207\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.5: Network conﬁgurations generated by nnU-Net for the Liver dataset from the Medical\nSegmentation Decathlon (D3). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\nliver\ncancer\nmean\n2D\n0.9547\n0.5637\n0.7592\n3D_fullres\n0.9571\n0.6372\n0.7971\n3D_lowres\n0.9563\n0.6028\n0.7796\n3D cascade\n0.9600\n0.6386\n0.7993\nBest Ensemble*\n0.9613\n0.6564\n0.8088\nPostprocessed\n0.9621\n0.6600\n0.8111\nTest set\n0.96\n0.76\n0.86\nTable F.6: Decathlon Liver (D3) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D low\nresolution U-Net and the 3D full resolution U-Net.\nHippocampus (D4)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n38\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\n-\nMedian image shape at\ntarget spacing:\nNA x 50 x 35\n36 x 50 x 35\n-\nPatch size:\n56 x 40\n40 x 56 x 40\n-\nBatch size:\n366\n9\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3],\n[3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.7: Network conﬁgurations generated by nnU-Net for the Hippocampus dataset from the\nMedical Segmentation Decathlon (D4). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\nAnterior\nPosterior\nmean\n2D\n0.8787\n0.8595\n0.8691\n3D_fullres *\n0.8975\n0.8807\n0.8891\nBest Ensemble\n0.8962\n0.8790\n0.8876\nPostprocessed\n0.8975\n0.8807\n0.8891\nTest set\n0.90\n0.89\n0.895\nTable F.8: Decathlon Hippocampus (D4) results. Note that all reported Dice scores (except the test\nset) were computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nProstate (D5)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.62 x 0.62\n3.6 x 0.62 x 0.62\n-\nMedian image shape at\ntarget spacing:\nNA x 320 x 319\n20 x 320 x 319\n-\nPatch size:\n320 x 320\n20 x 320 x 256\n-\nBatch size:\n32\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2]]\n[[1, 2, 2], [1, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [1, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.9: Network conﬁgurations generated by nnU-Net for the Prostate dataset from the Medical\nSegmentation Decathlon (D5). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\n39\n\n\nPZ\nTZ\nmean\n2D\n0.6285\n0.8380\n0.7333\n3D_fullres\n0.6663\n0.8410\n0.7537\nBest Ensemble *\n0.6611\n0.8575\n0.7593\nPostprocessed\n0.6611\n0.8577\n0.7594\nTest set\n0.77\n0.90\n0.835\nTable F.10: Decathlon Prostate (D5) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nLung (D6)\nNormalization: Clip to [−1024, 325], then subtract −158.58 and ﬁnally divide by 324.70.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.79 x 0.79\n1.24 x 0.79 x 0.79\n2.35 x 1.48 x 1.48\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n252 x 512 x 512\n133 x 271 x 271\nPatch size:\n512 x 512\n80 x 192 x 160\n80 x 192 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.11: Network conﬁgurations generated by nnU-Net for the Lung dataset from the Medical\nSegmentation Decathlon (D6). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\ncancer\nmean\n2D\n0.4989\n0.4989\n3D_fullres\n0.7211\n0.7211\n3D_lowres\n0.7109\n0.7109\n3D cascade\n0.6980\n0.6980\nBest Ensemble*\n0.7241\n0.7241\nPostprocessed\n0.7241\n0.7241\nTest set\n0.74\n0.74\nTable F.12: Decathlon Lung (D6) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D low\nresolution U-Net and the 3D full resolution U-Net.\nPancreas (D7)\nNormalization: Clip to [−96.0, 215.0], then subtract 77.99 and ﬁnally divide by 75.40.\n40\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.8 x 0.8\n2.5 x 0.8 x 0.8\n2.58 x 1.29 x 1.29\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n96 x 512 x 512\n93 x 318 x 318\nPatch size:\n512 x 512\n40 x 224 x 224\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.13: Network conﬁgurations generated by nnU-Net for the Pancreas dataset from the Medical\nSegmentation Decathlon (D7). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\npancreas\ncancer\nmean\n2D\n0.7738\n0.3501\n0.5619\n3D_fullres\n0.8217\n0.5274\n0.6745\n3D_lowres\n0.8118\n0.5286\n0.6702\n3D cascade\n0.8101\n0.5380\n0.6741\nBest Ensemble *\n0.8214\n0.5428\n0.6821\nPostprocessed\n0.8214\n0.5428\n0.6821\nTest set\n0.82\n0.53\n0.675\nTable F.14: Decathlon Pancreas (D7) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D full\nresolution U-Net and the 3D U-Net cascade.\nHepatic Vessel (D8)\nNormalization: Clip to [−3, 243], then subtract 104.37 and ﬁnally divide by 52.62.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.8 x 0.8\n1.5 x 0.8 x 0.8\n2.42 x 1.29 x 1.29\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n150 x 512 x 512\n93 x 318 x 318\nPatch size:\n512 x 512\n64 x 192 x 192\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.15: Network conﬁgurations generated by nnU-Net for the HepaticVessel dataset from the\nMedical Segmentation Decathlon (D8). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\n41\n\n\nVessel\nTumour\nmean\n2D\n0.6180\n0.6359\n0.6269\n3D_fullres\n0.6456\n0.7217\n0.6837\n3D_lowres\n0.6294\n0.7079\n0.6687\n3D cascade\n0.6424\n0.7138\n0.6781\nBest Ensemble *\n0.6485\n0.7250\n0.6867\nPostprocessed\n0.6485\n0.7250\n0.6867\nTest set\n0.66\n0.72\n0.69\nTable F.16: Decathlon HepaticVessel (D8) results. Note that all reported Dice scores (except the test\nset) were computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D full\nresolution U-Net and the 3D low resolution U-Net.\nSpleen (D9)\nNormalization: Clip to [−41, 176], then subtract 99.29 and ﬁnally divide by 39.47.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.79 x 0.79\n1.6 x 0.79 x 0.79\n2.77 x 1.38 x 1.38\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n187 x 512 x 512\n108 x 293 x 293\nPatch size:\n512 x 512\n64 x 192 x 160\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.17: Network conﬁgurations generated by nnU-Net for the Spleen dataset from the Medical\nSegmentation Decathlon (D9). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\nspleen\nmean\n2D\n0.9492\n0.9492\n3D_fullres\n0.9638\n0.9638\n3D_lowres\n0.9683\n0.9683\n3D cascade\n0.9714\n0.9714\nBest Ensemble *\n0.9723\n0.9723\nPostprocessed\n0.9724\n0.9724\nTest set\n0.97\n0.97\nTable F.18: Decathlon Spleen (D9) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D\nU-Net cascade and the 3D full resolution U-Net.\nColon (D10)\nNormalization: Clip to [−30.0, 165.82], then subtract 62.18 and ﬁnally divide by 32.65.\n42\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.78 x 0.78\n3 x 0.78 x 0.78\n3.09 x 1.55 x 1.55\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n150 x 512 x 512\n146 x 258 x 258\nPatch size:\n512 x 512\n56 x 192 x 160\n96 x 160 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.19: Network conﬁgurations generated by nnU-Net for the Colon dataset from the Medical\nSegmentation Decathlon (D10). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\ncolon cancer primaries\nmean\n2D\n0.2852\n0.2852\n3D_fullres\n0.4553\n0.4553\n3D_lowres\n0.4538\n0.4538\n3D cascade *\n0.4937\n0.4937\nBest Ensemble\n0.4853\n0.4853\nPostprocessed\n0.4937\n0.4937\nTest set\n0.58\n0.58\nTable F.20: Decathlon Colon (D10) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D\nU-Net cascade and the 3D full resolution U-Net.\nF.4\nMulti Atlas Labeling Beyond the Cranial Vault: Abdomen (D11)\nChallenge summary\nThe Multi Atlas Labeling Beyond the Cranial Vault - Abdomen Challenge5\n[12] (denoted BCV for brevity) comprises 30 CT images for training and 20 for testing. The\nsegmentation target are thirteen different organs in the abdomen.\nApplication of nnU-Net to BCV\nnnU-Net was applied to the BCV challenge without any manual\nintervention.\nNormalization: Clip to [−958, 327], then subtract 82.92 and ﬁnally divide by 136.97.\n5https://www.synapse.org/Synapse:syn3193805/wiki/217752\n43\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.76 x 0.76\n3 x 0.76 x 0.76\n3.18 x 1.60 x 1.60\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n148 x 512 x 512\n140 x 243 x 243\nPatch size:\n512 x 512\n48 x 192 x 192\n80 x 160 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.21: Network conﬁgurations generated by nnU-Net for the BCV challenge (D131. For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\n1\n2\n3\n4\n5\n6\n7\n8\n2D\n0.8860\n0.8131\n0.8357\n0.6406\n0.7724\n0.9453\n0.8405\n0.9128\n3D_fullres\n0.9083\n0.8939\n0.8675\n0.6632\n0.7840\n0.9557\n0.8816\n0.9229\n3D_lowres\n0.9132\n0.9045\n0.9132\n0.6525\n0.7810\n0.9554\n0.8903\n0.9209\n3D cascade\n0.9166\n0.9069\n0.9137\n0.7036\n0.7885\n0.9587\n0.9037\n0.9215\nBest Ensemble *\n0.9135\n0.9065\n0.8971\n0.6955\n0.7897\n0.9589\n0.9026\n0.9248\nPostprocessed\n0.9135\n0.9065\n0.8971\n0.6959\n0.7897\n0.9590\n0.9026\n0.9248\nTest set\n0.9721\n0.9182\n0.9578\n0.7528\n0.8411\n0.9769\n0.9220\n0.9290\n9\n10\n11\n12\n13\nmean\n2D\n0.8140\n0.7046\n0.7367\n0.6269\n0.5909\n0.7784\n3D_fullres\n0.8638\n0.7659\n0.8176\n0.7148\n0.7238\n0.8279\n3D_lowres\n0.8571\n0.7469\n0.8003\n0.6688\n0.6851\n0.8223\n3D cascade\n0.8621\n0.7722\n0.8210\n0.7205\n0.7214\n0.8393\nBest Ensemble *\n0.8673\n0.7746\n0.8299\n0.7218\n0.7287\n0.8393\nPostprocessed\n0.8673\n0.7746\n0.8299\n0.7262\n0.7290\n0.8397\nTest set\n0.8809\n0.8317\n0.8515\n0.7887\n0.7674\n0.8762\nTable F.22: Multi Atlas Labeling Beyond the Cranial Vault Abdomen (D11) results. Note that all\nreported Dice scores (except the test set) were computed using ﬁve fold cross-validation on the training\ncases. Postprocessing was applied to the model marked with *. This model (incl postprocessing) was\nused for test set predictions. Note that the Dice scores for the test set are computed with the online\nplatform. Best ensemble on this dataset was the combination of the 3D U-Net cascade and the 3D\nfull resolution U-Net.\nF.5\nPROMISE12 (D12)\nChallenge summary\nThe segmentation target of the PROMISE12 challenge [13] is the prostate in\nT2 MRI images. 50 training cases with prostate annotations are provided for training. There are 30\ntest cases which need to be segmented by the challenge participants and are subsequently evaluated\non an online platform6.\nApplication of nnU-Net to PROMISE12\nnnU-Net was applied to the PROMISE12 challenge\nwithout any manual intervention.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n6https://promise12.grand-challenge.org/\n44\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.61 x 0.61\n2.2 x 0.61 x 0.61\n-\nMedian image shape at\ntarget spacing:\nNA x 327 x 327\n39 x 327 x 327\n-\nPatch size:\n384 x 384\n28 x 256 x 256\n-\nBatch size:\n22\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.23: Network conﬁgurations generated by nnU-Net for the PROMISE12 challenge (D12). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\nprostate\nmean\n2D\n0.8932\n0.8932\n3D_fullres\n0.8891\n0.8891\nBest Ensemble *\n0.9029\n0.9029\nPostprocessed\n0.9030\n0.9030\nTest set\n0.9194\n0.9194\nTable F.24: PROMISE12 (D12) results. Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\").\nNote that the scores for the test set are computed with the online platform. The evaluation score of our\ntest set submission is 89.6507. The test set Dice score reported in the table was computed from the de-\ntailed submission results (Detailed results available here https://promise12.grand-challenge.\norg/evaluation/results/89044a85-6c13-49f4-9742-dea65013e971/). Best ensemble on\nthis dataset was the combination of the 2D U-Net and the 3D full resolution U-Net.\nF.6\nThe Automatic Cardiac Diagnosis Challenge (ACDC) (D13)\nChallenge summary\nThe Automatic Cardiac Diagnosis Challenge7 [1] (ACDC) comprises 100\ntraining patients and 50 test patients. The target structures are the cavity of the right ventricle,\nthe myocardium of the left ventricle and the cavity of the left ventricle. All images are cine MRI\nsequences of which the enddiastolic (ED) and endsystolic (ES) time points of the cardiac cycle were\nto be segmented. With two time instances per patient, the effective number of training/test images is\n200/100.\nApplication of nnU-Net to ACDC\nSince two time instances of the same patient were provided,\nwe manually interfered with the split for the 5-fold cross-validation of our models to ensure mutual\nexclusivity of patients between folds. A part from that, nnU-Net was applied without manual\nintervention.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n7https://acdc.creatis.insa-lyon.fr\n45\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.56 x 1.56\n5 x 1.56 x 1.56\n-\nMedian image shape at\ntarget spacing:\nNA x 237 x 208\n18 x 237 x 208\n-\nPatch size:\n256 x 224\n20 x 256 x 224\n-\nBatch size:\n58\n3\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.25: Network conﬁgurations generated by nnU-Net for the ACDC challenge (D13). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nRV\nMLV\nLVC\nmean\n2D\n0.9053\n0.8991\n0.9433\n0.9159\n3D_fullres\n0.9059\n0.9022\n0.9458\n0.9179\nBest Ensemble *\n0.9145\n0.9059\n0.9479\n0.9227\nPostprocessed\n0.9145\n0.9059\n0.9479\n0.9228\nTest set\n0.9295\n0.9183\n0.9407\n0.9295\nTable F.26: ACDC results (D13). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\").\nNote that the Dice scores for the test set are computed with the online platform. The online platform\nreports the Dice scores for enddiastolic and endsystolic time points separately. We averaged these\nvalues for a more condensed presentation. Best ensemble on this dataset was the combination of the\n2D U-Net and the 3D full resolution U-Net.\nF.7\nLiver and Liver Tumor Segmentation Challenge (LiTS) (D14)\nChallenge summary\nThe Liver and Liver Tumor Segmentation challenge [3] provides 131 training\nCT images with ground truth annotations for the liver and liver tumors. 70 test images are provided\nwithout annotations. The predicted segmentation masks of the test cases are evaluated using the LiTS\nonline platform8.\nApplication of nnU-Net to LiTS\nnnU-Net was applied to the LiTS challenge without any manual\nintervention.\nNormalization: Clip to [−17, 201], then subtract 99.40 and ﬁnally divide by 39.39.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.77 x 0.77\n1 x 0.77 x 0.77\n2.47 x 1.90 x 1.90\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n482 x 512 x 512\n195 x 207 x 207\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.27: Network conﬁgurations generated by nnU-Net for the LiTS challenge (D14). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\n8https://competitions.codalab.org/competitions/17094\n46\n\n\nliver\ncancer\nmean\n2D\n0.9547\n0.5603\n0.7575\n3D_fullres\n0.9576\n0.6253\n0.7914\n3D_lowres\n0.9585\n0.6161\n0.7873\n3D cascade\n0.9609\n0.6294\n0.7951\nBest Ensemble*\n0.9618\n0.6539\n0.8078\nPostprocessed\n0.9631\n0.6543\n0.8087\nTest set\n0.9670\n0.7630\n0.8650\nTable F.28: LiTS results (D14). Note that all reported Dice scores (except the test set) were computed\nusing ﬁve fold cross-validation on the training cases. * marks the best performing model selected for\nsubsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\"). Note that\nthe Dice scores for the test set are computed with the online platform. Best ensemble on this dataset\nwas the combination of the 3D low resolution U-Net and the 3D full resolution U-Net.\nF.8\nLongitudinal multiple sclerosis lesion segmentation challenge (MSLesion) (D15)\nChallenge summary\nThe longitudinal multiple sclerosis lesion segmentation challenge [4] pro-\nvides 5 training patients. For each patient, 4 to 5 images acquired at different time points are provided\n(4 patients with 4 time points each and one patient with 5 time points for a total of 21 images).\nEach time point is annotated by two different experts, resulting in 42 training annotations (on 21\nimages). The test set contains 14 patients, again with several time points each, for a total of 61 MRI\nacquisitions. Test set predictions are evaluated using the online platform9. Each train and test image\nconsists of four MRI modalities: MPRAGE, FLAIR, Proton Density, T2.\nApplication of nnU-Net to MSLesion\nWe manually interfere with the splits in the cross-validation\nto ensure mutual exclusivity of patients between folds. Each image was annotated by two different\nexperts. We treat these annotations as separate training images (of the same patient), resulting in a\ntraining set size of 2 × 21 = 42. We do not use the longitudinal nature of the scans and treat each\nimage individually during training and inference.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\n-\nMedian image shape at\ntarget spacing:\nNA x 180 x 137\n137 x 180 x 137\n-\nPatch size:\n192 x 160\n112 x 128 x 96\n-\nBatch size:\n107\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]]\n[[1, 2, 1], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n-\nTable F.29: Network conﬁgurations generated by nnU-Net for the MSLesion challenge (D15). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\n9https://smart-stats-tools.org/lesion-challenge\n47\n\n\nlesion\nmean\n2D\n0.7339\n0.7339\n3D_fullres *\n0.7531\n0.7531\nBest Ensemble\n0.7494\n0.7494\nPostprocessed\n0.7531\n0.7531\nTest set\n0.6785\n0.6785\nTable F.30: MSLesion results (D15). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test\nset\"). Note that the Dice scores for the test set are computed with the online platform based on the\ndetailed results (which are available here https://smart-stats-tools.org/sites/lesion_\nchallenge/temp/top25/nnUNetV2_12032019_0903.csv). The ranking is based on a score,\nwhich includes other metrics as well (see [4] for details). The score of our submission is 92.874. Best\nensemble on this dataset was the combination of the 2D U-Net and the 3D full resolution U-Net.\nF.9\nCombined Healthy Abdominal Organ Segmentation (CHAOS) (D16)\nChallenge summary\nThe CHAOS challenge [11] is divided into ﬁve tasks. Here we focused on\nTasks 3 (MRI Liver segmentation) and Task 5 (MRI multiorgan segmentation). Tasks 1, 2 and 4 also\nincluded the use of CT images, a modality for which plenty of public data is available (see e.g. BCV\nand LiTS challenge). To isolate the algorithmic performance of nnU-Net relative to other participants\nwe decided to only use the tasks for which a contamination with external data was unlikely. The target\nstructures of Task 5 are the liver, the spleen and the left and right kidneys. The CHAOS challenge\nprovides 20 training cases. For each training case, there is a T2 images with a corresponding ground\ntruth annotation as well as a T1 acquisition with its own, separate ground truth annotation. The T1\nacquisition has two modalities which are co-registered: T1 in-phase and T1 out-phase. Task 3 is a\nsubset of Task 5 with only the liver being the segmentation target. The 20 test cases are evaluated\nusing the online platform10.\nApplication of nnU-Net to CHAOS\nnnU-Net only supports images with a constant number of\ninput modalities. The training cases in CHAOS have either one (T2) or two (T1 in & out phase)\nmodalities. To ensure compatibility with nnU-Net we could have either duplicated the T2 image and\ntrained with two input modalities or use only one input modality and treat T1 in phase and out phase\nas separate training examples. We opted for the latter because this variant results in more (albeit\nhighly correlated) training images. With 20 training patients being provided, this approach resulted\nin 60 training images. For the cross-validation we ensure that the split is being done on patient level.\nDuring inference, nnU-Net will generate two separate predictions for T1 in and out phase which\nneed to be consolidated for test set evaluation. We achieve this by simply averaging the softmax\nprobabilities between the two to generate the ﬁnal segmentation. We train nnU-Net only for Task 5.\nBecause task 3 represents a subset of Task 5, we extract the liver from our Task 5 predictions and\nsubmit it to Task 3.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n10https://chaos.grand-challenge.org/\n48\n\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.66 x 1.66\n5.95 x 1.66 x 1.66\n-\nMedian image shape at\ntarget spacing:\nNA x 195 x 262\n45 x 195 x 262\n-\nPatch size:\n224 x 320\n40 x 192 x 256\n-\nBatch size:\n45\n2\n-\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [1, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2], [1, 1, 2]]\n-\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.31: Network conﬁgurations generated by nnU-Net for the CHAOS challenge (D16). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2.\nliver\nright kidney\nleft kidney\nspleen\nmean\n2D\n0.9132\n0.8991\n0.8897\n0.8720\n0.8935\n3D_fullres\n0.9202\n0.9274\n0.9209\n0.8938\n0.9156\nBest Ensemble *\n0.9184\n0.9283\n0.9255\n0.8911\n0.9158\nPostprocessed\n0.9345\n0.9289\n0.9212\n0.894\n0.9197\nTest set\n-\n-\n-\n-\n-\nTable F.32: CHAOS results (D16). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. Postprocessing was applied to the\nmodel marked with *. This model (incl postprocessing) was used for test set predictions. Note that\nthe evaluation of the test set was performed with the online platform of the challenge which does not\nreport Dice scores for the individual organs. The score of our submission was 72.44 for Task 5 and\n75.10 for Task3 (see [11] for details). Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nF.10\nKidney and Kidney Tumor Segmentation (KiTS) (D17)\nChallenge summary\nThe Kidney and Kidney Tumor Segmentation challenge [8] was the largest\ncompetition (in terms of number of participants) at MICCAI 2019. The target structures are the\nkidneys and kidney tumors. 210 training and 90 test cases are provided by the challenge organizers.\nThe organizers provide the data both in their original geometry (with voxel spacing varying between\ncases) as well as interpolated to a common voxel spacing. Evaluation of the test set predictions is\ndone on the online platform11.\nWe participated in the original KiTS 2019 MICCAI challenge with a manually designed\nresidual 3D U-Net. This algorithm, described in [9] obtained the ﬁrst rank in the challenge. For this\nsubmission, we did slight modiﬁcations to the original training data: Cases 15 and 37 were conﬁrmed\nto be faulty by the challenge organizers (https://github.com/neheller/kits19/issues/21)\nwhich is why we replaced their respective segmentation masks with predictions of one of our\nnetworks. We furthermore excluded cases 23, 68, 125 and 133 because we suspected labeling\nerrors in these cases as well. At the time of conducting the experiments for this publication, no\nrevised segmentation masks were provided by the challenge organizers, which is why we re-used the\nmodiﬁed training dataset for training nnU-Net.\nAfter the challenge event at MICCAI 2019, an open leaderboard was created.\nThe original\n11https://kits19.grand-challenge.org/\n49\n\n\nchallenge leaderboard is retained at http://results.kits-challenge.org/miccai2019/. All\nsubmissions of the original KiTS challenge were mirrored to the open leaderboard. The submission\nof nnU-Net as performed in the context of this manuscript is done on the open leaderboard, where\nmany more competitors have entered since the challenge. As presented in Figure 3, nnU-Net sets a\nnew state of the art on the open leaderboard, thus also outperforming our initial, manually optimized\nsolution.\nApplication of nnU-Net to KiTS\nSince nnU-Net is designed to automatically deal with varying\nvoxel spacings within a dataset, we chose the original, non-interpolated image data as provided by\nthe organizers and let nnU-Net deal with the homogenization of voxel spacing. nnU-Net was applied\nto the KiTS challenge without any manual intervention.\nNormalization: Clip to [−79, 304], then subtract 100.93 and ﬁnally divide by 76.90.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.78 x 0.78\n0.78 x 0.78 x 0.78\n1.99 x 1.99 x 1.99\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n525 x 512 x 512\n206 x 201 x 201\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.33: Network conﬁgurations generated by nnU-Net for the KiTS challenge (D17). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nKidney\nTumor\nmean\n2D\n0.9613\n0.7563\n0.8588\n3D_fullres\n0.9702\n0.8367\n0.9035\n3D_lowres\n0.9629\n0.8420\n0.9025\n3D cascade\n0.9702\n0.8546\n0.9124\nBest Ensemble*\n0.9707\n0.8620\n0.9163\nPostprocessed\n0.9707\n0.8620\n0.9163\nTest set\n-\n0.8542\n-\nTable F.34: KiTS results (D17). Note that all reported Dice scores (except the test set) were computed\nusing ﬁve fold cross-validation on the training cases. Postprocessing was applied to the model marked\nwith *. This model (incl postprocessing) was used for test set predictions. Note that the Dice scores\nfor the test set are computed with the online platform which computes the kidney Dice score based of\nthe union of the kidney and tumor labels whereas nnU-Net always evaluates labels independently,\nresulting in a missing value for kindey in the table. The reported kindey Dice by the platform (which\nis not comparable with the value computed by nnU-Net) is 0.9793. Best ensemble on this dataset was\nthe combination of the 3D U-Net cascade and the 3D full resolution U-Net.\nF.11\nSegmentation of THoracic Organs at Risk in CT images (SegTHOR) (D18)\nChallenge summary\nIn the Segmentation of THoracic Organs at Risk in CT images [16] challenge,\nfour abdominal organs (the heart, the aorta, the trachea and the esopahgus) are to be segmented in CT\n50\n\n\nimages. 40 training images are provided for training and another 20 images are provided for testing.\nEvaluation of the test images is done using the online platform12.\nApplication of nnU-Net to SegTHOR\nnnU-Net was applied to the SegTHOR challenge without\nany manual intervention.\nNormalization: Clip to [−986, 271], then subtract 20.78 and ﬁnally divide by 180.50.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.89 x 0.89\n2.50 x 0.89 x 0.89\n3.51 x 1.76 x 1.76\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n171 x 512 x 512\n122 x 285 x 285\nPatch size:\n512 x 512\n64 x 192 x 160\n80 x 192 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.35: Network conﬁgurations generated by nnU-Net for the SegTHOR challenge (D18). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\nesophagus\nheart\ntrachea\naorta\nmean\n2D\n0.8181\n0.9407\n0.9077\n0.9277\n0.8986\n3D_fullres\n0.8495\n0.9527\n0.9055\n0.9426\n0.9126\n3D_lowres\n0.8110\n0.9464\n0.8930\n0.9284\n0.8947\n3D cascade\n0.8553\n0.9520\n0.9045\n0.9403\n0.9130\nBest Ensemble*\n0.8545\n0.9532\n0.9066\n0.9427\n0.9143\nPostprocessed\n0.8545\n0.9532\n0.9083\n0.9438\n0.9150\nTest set\n0.8890\n0.9570\n0.9228\n0.9510\n0.9300\nTable F.36: SegTHOR results (D18). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. Postprocessing was applied to the\nmodel marked with *. This model (incl postprocessing) was used for test set predictions. Note that\nthe Dice scores for the test set are computed with the online platform. Best ensemble on this dataset\nwas the combination of the 3D U-Net cascade and the 3D full resolution U-Net.\nF.12\nChallenge on Circuit Reconstruction from Electron Microscopy Images (CREMI)\n(D19)\nChallenge summary\nThe Challenge on Circuit Reconstruction from Electron Microscopy Images\nis subdivided into three tasks. The synaptic cleft segmentation task can be formulated as semantic\nsegmentation (as opposed to e.g. instance segmentation) and is thus compatible with nnU-Net. In this\ntask, the segmentation target is the cell membrane in locations where the cells are forming a synapse.\nThe dataset consists of serial section Transmission Electron Microscopy scans of the Drosophila\nmelanogaster brain. Three volumes are provided for training and another three are provided for\ntesting. Test set evaluation is done using the online platform13.\nApplication of nnU-Net to CREMI\nSince to the number of training images is lower than the\nnumber of splits, we cannot run a 5-fold cross-validation. Thus, we trained 5 model instances,\n12https://competitions.codalab.org/competitions/21145\n13https://cremi.org/\n51\n\n\neach of them on all three training volumes and subsequently ensembled these models for test set\nprediction. Because this training scheme leaves no validation data, selection of the best of three\nmodel conﬁgurations as performed by nnU-Net after cross-validation was not possible. Hence, we\nintervened by only conﬁguring and training the 3D full resolution conﬁguration.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\n-\n40 x 4 x 4\n-\nMedian image shape at\ntarget spacing:\n-\n125 x 1250 x 1250\n-\nPatch size:\n-\n24 x 256 x256\n-\nBatch size:\n-\n2\n-\nDownsampling strides:\n-\n[[1, 2, 2], [1, 2, 2], [1, 2, 2],\n[2, 2, 2], [2, 2, 2], [1, 2, 2]]\n-\nConvolution kernel sizes:\n-\n[[1, 3, 3], [1, 3, 3], [1, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\n-\nTable F.37: Network conﬁgurations generated by nnU-Net for the CREMI challenge (D19). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nResults\nBecause our training scheme for this challenge left no validation data, a performance\nestimate as given for the other datastes is not available for CREMI. The CREMI test set is evaluated\nby the online platform. The evaluation metric is the so called CREMI score, a description of which is\navailable here https://cremi.org/metrics/. Dice scores for the test set are not reported. The\nCREMI score of our test set submission was 74.96 (lower is better).\nG\nUsing nnU-Net with limited compute resources\nReduction of computational complexity was one of the key motivations driving the design of nnU-Net.\nThe effort of running all the conﬁgurations generated by nnU-Net should be manageable for most\nusers and researchers. There are, however, some shortcuts that can be be taken in case computational\nresources are extremely scarce.\nG.1\nReducing the number of network trainings\nDepending on whether the 3D U-Net cascade is conﬁgured for a given dataset, nnU-Net requires 10\n(2D and 3D U-Net with 5 models each) or 20 (2d, 3D, 3D cascade (low resolution and high resolution\nU-Net) with 5 models each) U-Net trainings to run, each of which takes a couple of days on a single\nGPU. While this approach guarantees the best possible performance, training all models may exceed\nreasonable computation time if only a single GPU is available. Therefore, we present two strategies\nto reduce the number of total network trainings when running nnU-Net.\nManual selection of U-Net conﬁgurations\nOverall, the 3D full resolution U-Net shows the best segmentation results. Thus, this conﬁguration is\na good starting point and could simply be selected as default choice. Users can decide whether to train\nthis conﬁguration using all training cases (to train a single model) or run a ﬁve-fold cross-validation\nand ensemble the 5 resulting models for test case predictions.\nIn some scenarios, other conﬁgurations than the 3D full resolution U-Net can yield best\n52\n\n\nperformance. Identifying such scenarios and selecting the respective most promising conﬁguration,\nhowever, requires domain knowledge for the dataset at hand. Datasets with highly anisotropic images\n(such as D12 PROMISE12), for instance, could be best suited for running a 2D U-Net. There is,\nhowever, no guarantee for this relation (see D13 ACDC). On datasets with very large images, the\n3D U-Net cascade seems to marginally outperform the 3D full resolution U-Net (for example D11,\nD14, D17, D18, ...) because it improves the capture of contextual information. Note that this is only\ntrue if the target structure requires a large receptive ﬁeld for optimal recognition. On CREMI (D19)\nfor example, despite large image sizes, only a limited ﬁeld of view is required, because the target\nstructure are relatively small synapses that can be identiﬁed using only local information, which is\nwhy we selected the 3D full resolution U-Net for this dataset (see Section F.12).\nNot running all conﬁgurations as 5-fold cross-validation\nAnother computation shortcut is to not run all models as 5-fold cross-validation. For instance, only\none split for each conﬁguration can be run (note, however, that the 3D low resolution U-Net of\nthe cascade is required to be run as a 5-fold cross-validation in order to generate low resolution\nsegmentation maps of all training cases for the second full resolution U-net of the cascade). Even\nwhen running multiple conﬁgurations to rely on empirical selection of conﬁgurations by nnU-Net,\nthis reduces the total number of models to be trained to 2 if no cascade is conﬁgured or 8 if the\ncascade is conﬁgured (the cascade requires 6 model trainings: 5 3D low resolution U-Nets and 1 full\nresolution 3D U-Net training). nnU-Net subsequently bases selection of the best conﬁguration on this\nsingle train-val split. Note that this strategy provides less reliable performance estimates and may\nresult in sub optimal conﬁguration choices. Finally, users can decide whether they wish to re-train the\nselected conﬁguration on the entire training data or run a ﬁve-fold cross-validation for this selected\nconﬁguration. The latter is expected to result in better test set performance because the 5 models can\nbe used as an ensemble.\nG.2\nReduction of GPU memory\nnnU-Net is conﬁgured to utilize 11GB of GPU memory. This requirement is, based on our experience,\na realistic requirement for a modern deep-learning capable GPU (such as a Nvidia GTX 1080 ti\n(11GB), Nvidia RTX 2080 ti (11GB), Nvidia TitanX(p) (12GB), Nvidia P100 (12/16 GB), Nvidia\nTitan RTX (24GB), Nvidia V100 (16/32 GB), ...). We strongly recommend using nnU-Net with\nthis default conﬁguration, because it has been tested extensively and, as we show in this manuscript,\nprovides excellent segmentation accuracy. Should users still desire to run nnU-Net on a smaller GPU,\nthe amount of GPU memory used for network conﬁguration can be adapted easily. Corresponding\ninstructions are provided along with the source code.\nReferences\n[1] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir,\nO. Camara, M. A. G. Ballester, et al. Deep learning techniques for automatic mri cardiac multi-\nstructures segmentation and diagnosis: Is the problem solved? IEEE TMI, 37(11):2514–2525,\n2018.\n[2] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-\nA. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n53\n\n\n[3] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-A. Heng,\nJ. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint arXiv:1901.04056,\n2019.\n[4] A. Carass, S. Roy, A. Jog, J. L. Cuzzocreo, E. Magrath, A. Gherman, J. Button, J. Nguyen,\nF. Prados, C. H. Sudre, et al. Longitudinal multiple sclerosis lesion segmentation: resource and\nchallenge. NeuroImage, 148:77–102, 2017.\n[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n[6] L. Heinrich, J. Funke, C. Pape, J. Nunez-Iglesias, and S. Saalfeld. Synaptic cleft segmentation\nin non-isotropic volume electron microscopy of the complete drosophila brain. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 317–325.\nSpringer, 2018.\n[7] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[8] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak, J. Rosen-\nberg, P. Blake, Z. Rengel, M. Oestreich, et al. The kits19 challenge data: 300 kidney tumor\ncases with clinical context, ct semantic segmentations, and surgical outcomes. arXiv preprint\narXiv:1904.00445, 2019.\n[9] F. Isensee and K. H. Maier-Hein.\nAn attempt at beating the 3d u-net.\narXiv preprint\narXiv:1908.02182, 2019.\n[10] F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl, J. Wasserthal, G. Koehler,\nT. Norajitra, S. Wirkert, et al. nnu-net: Self-adapting framework for u-net-based medical image\nsegmentation. arXiv preprint arXiv:1809.10486, 2018.\n[11] A. E. Kavur, N. S. Gezer, M. Barı¸s, P.-H. Conze, V. Groza, D. D. Pham, S. Chatterjee, P. Ernst,\nS. Özkan, B. Baydar, et al. Chaos challenge–combined (ct-mr) healthy abdominal organ\nsegmentation. arXiv preprint arXiv:2001.06535, 2020.\n[12] B. Landman, Z. Xu, J. Eugenio Igelsias, M. Styner, T. Langerak, and A. Klein. Miccai\nmulti-atlas labeling beyond the cranial vault–workshop and challenge, 2015.\n[13] G. Litjens, R. Toth, W. van de Ven, C. Hoeks, S. Kerkstra, B. van Ginneken, G. Vincent,\nG. Guillard, N. Birbeck, J. Zhang, et al. Evaluation of prostate segmentation algorithms for mri:\nthe promise12 challenge. Med Image Analysis, 18(2):359–373, 2014.\n[14] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz,\nJ. Slotboom, R. Wiest, et al. The multimodal brain tumor image segmentation benchmark\n(brats). IEEE transactions on medical imaging, 34(10):1993–2024, 2014.\n[15] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. van Ginneken, A. Kopp-\nSchneider, B. A. Landman, G. Litjens, B. Menze, et al. A large annotated medical image dataset\nfor the development and evaluation of segmentation algorithmsdelldatagrowth. arXiv preprint\narXiv:1902.09063, 2019.\n54\n\n\n[16] R. Trullo, C. Petitjean, B. Dubray, and S. Ruan. Multiorgan segmentation using distance-aware\nadversarial networks. Journal of Medical Imaging, 6(1):014001, 2019.\n[17] Q. Yu, D. Yang, H. Roth, Y. Bai, Y. Zhang, A. L. Yuille, and D. Xu. C2fnas: Coarse-to-ﬁne\nneural architecture search for 3d medical image segmentation. arXiv preprint arXiv:1912.09628,\n2019.\n55\n",
  "normalized_text": "Automated Design of Deep Learning Methods for\nBiomedical Image Segmentation\nFabian Isensee1,2†, Paul F. Jaeger1†, Simon A. A. Kohl3‡, Jens Petersen1,4, and Klaus H.\nMaier-Hein1,5*\n1Division of Medical Image Computing, German Cancer Research Center, Heidelberg\n2Faculty of Biosciences, University of Heidelberg, Heidelberg, Germany\n3DeepMind, London, United Kingdom\n4Faculty of Physics & Astronomy, University of Heidelberg, Heidelberg, Germany\n5Pattern Analysis and Learning Group, Heidelberg University Hospital, Department of Radiation\nOncology, Heidelberg, Germany\n*k.maier-hein@dkfz.de\nAbstract\nBiomedical imaging is a driver of scientiﬁc discovery and core component of\nmedical care, currently stimulated by the ﬁeld of deep learning. While semantic\nsegmentation algorithms enable 3D image analysis and quantiﬁcation in many\napplications, the design of respective specialised solutions is non-trivial and highly\ndependent on dataset properties and hardware conditions. We propose nnU-Net,\na deep learning framework that condenses the current domain knowledge and\nautonomously takes the key decisions required to transfer a basic architecture to different datasets and segmentation tasks. Without manual tuning, nnU-Net surpasses\nmost specialised deep learning pipelines in 19 public international competitions and\nsets a new state of the art in the majority of the 49 tasks. The results demonstrate\na vast hidden potential in the systematic adaptation of deep learning methods to\ndifferent datasets. We make nnU-Net publicly available as an open-source tool\nthat can effectively be used out-of-the-box, rendering state of the art segmentation\naccessible to non-experts and catalyzing scientiﬁc progress as a framework for\nautomated method design.\n1\nIntroduction\nSemantic segmentation transforms raw biomedical image data into meaningful, spatially structured\ninformation and thus plays an essential role for scientiﬁc discovery in the ﬁeld [9, 14]. At the same\ntime, semantic segmentation is an essential ingredient to numerous clinical applications [1, 27],\nincluding applications of artiﬁcial intelligence in diagnostic support systems [7, 3], therapy planning\n† Equal contribution. ‡ Work started while doing a PhD at the German Cancer Research Center.\nPreprint. Under review.\narXiv:1904.08128v2 [cs.CV] 2 Apr 2020\n\nsupport [28], intra-operative assistance [14] or tumor growth monitoring [19]. The high interest in\nautomatic segmentation methods manifests in a thriving research landscape, accounting for 70% of\ninternational image analysis competitions in the biomedical sector [23].\nDespite the recent success of deep learning-based segmentation methods, their applicability\nto speciﬁc image analysis problems of end-users is often limited. The task-speciﬁc design and\nconﬁguration of a method requires high levels of expertise and experience, with small errors leading\nto strong performance drops [22]. Especially in 3D biomedical imaging, where dataset properties like\nimaging modality, image size, (anisotropic) voxel spacing or class ratio vary drastically, the pipeline\ndesign can be cumbersome, because experience on what constitutes a successful conﬁguration\nmay not translate to the dataset at hand. The numerous expert decisions involved in designing\nand training a neural network range from the exact network architecture to the training schedule\nand methods for data augmentation or post-processing. Each sub-component is controlled by\nessential hyperparameters like learning rate, batch size, or class sampling [22]. An additional layer\nof complexity on the overall setup is posed by the hardware available for training and inference\n[21]. Algorithmic optimization of the codependent design choices in this high dimensional space\nof hyperparameters is technically demanding and ampliﬁes both the number of required training\ncases as well as compute resources by orders of magnitude [8]. As a consequence, the end-user is\ncommonly left with an iterative trial and error process during method design that is mostly driven\nby their individual experience, only scarcely documented and hard to replicate, inevitably evoking\nsuboptimal segmentation pipelines and methodological ﬁndings that do not generalize to other\ndatasets [22, 2].\nTo further complicate things, there is an unmanageable number of research papers that propose architecture variations and extensions for performance improvement. This bulk of studies is\nincomprehensible to the non-expert and difﬁcult to evaluate even for experts [22]. Approximately\n12000 studies cite the 2015 U-Net architecture on biomedical image segmentation [31], many of\nwhich propose extensions and advances. We put forward the hypothesis that a basic U-Net is still\nhard to beat if the corresponding pipeline is designed adequately.\nTo this end, we propose nnU-Net (“no new net”), which makes successful 3D biomedical\nimage segmentation accessible for biomedical research applications. nnU-Net automatically adapts\nto arbitrary datasets and enables out-of-the-box segmentation on account of two key contributions:\n1. We formulate the pipeline optimization problem in terms of a data ﬁngerprint (representing\nthe key properties of a dataset) and a pipeline ﬁngerprint (representing the key design choices\nof a segmentation algorithm).\n2. We make their relation explicit by condensing domain knowledge into a set of heuristic\nrules that robustly generate a high quality pipeline ﬁngerprint from a corresponding data\nﬁngerprint while considering associated hardware constraints.\nIn contrast to algorithmic approaches for method conﬁguration that are formulated as a task-speciﬁc\noptimization problem, nnU-Net readily executes systematic rules to generate deep learning methods\nfor previously unseen datasets without need for further optimization.\nIn the following, we demonstrate the superiority of this concept by presenting a new state\nof the art in numerous international challenges through application of our algorithm without manual\n2\n\nintervention.\nThe strong results underline the signiﬁcance of nnU-Net for users who require\nalgorithms for semantic segmentation on their custom datasets: as an open source tool, nnU-Net can\nsimply be downloaded and trained out-of-the box to generate state of the art segmentations without\nrequiring manual adaptation or expert knowledge. We further demonstrate shortcomings in the\ndesign process of current biomedical segmentation methods. Speciﬁcally, we take an in-depth look at\nthe 2019 Kidney and Kidney Tumor Segmentation (KiTS) semantic image segmentation challenge\nand demonstrate how important task-speciﬁc design and conﬁguration of a method are in comparison\nto choosing one of the many architectural extensions and advances previously proposed on top of the\nU-Net. By automating this design and conﬁguration process, nnU-Net fosters the ambition and the\nability of researchers to validate novel ideas on larger numbers of datasets, while at the same time\nserving as an ideal reference method when demonstrating methodological improvements.\n2\nResults\nnnU-Net is a deep learning framework that enables 3D semantic segmentation in many biomedical\nimaging applications, without requiring the design of respective specialised solutions. Exemplary\nsegmentation results generated by nnU-Net for a variety of datasets are shown in Figure 1.\nnnU-Net automatically adapts to any new dataset\nFigure 2a shows the current practice of adapting segmentation pipelines to a new dataset. This process is expert-driven and involves manual\ntrial-and-error experiments that are typically speciﬁc to the task at hand [22]. As shown in Figure 2b,\nnnU-Net addresses the adaptation process systematically. Therefore, we deﬁne a dataset ﬁngerprint\nas a standardized dataset representation comprising key properties such as image sizes, voxel spacing\ninformation or class ratios, and a pipeline ﬁngerprint as the entirety of choices being made during\nmethod design. nnU-Net is designed to generate a successful pipeline ﬁngerprint for a given dataset\nﬁngerprint. In nnU-Net, the pipeline ﬁngerprint is divided into three groups: blueprint, inferred and\nempirical parameters. The blueprint parameters represent fundamental design choices (such as using\na plain U-Net-like architecture template) as well as hyperparameters for which a robust default value\ncan simply be picked (for example loss function, training schedule and data augmentation). The\ninferred parameters encode the necessary adaptations to a new dataset and include modiﬁcations\nto the exact network topology, patch size, batch size and image preprocessing. The link between a\ndata ﬁngerprint and the inferred parameters is established via execution of a set of heuristic rules,\nwithout the need for expensive re-optimization when applied to unseen datasets. Note that many of\nthese design choices are co-dependent: The target image spacing, for instance, affects image size,\nwhich in return determines the size of patches the model should see during training, which affects\nthe network topology and has to be counterbalanced by the size of training mini-batches in order to\nnot exceed GPU memory limitations. nnU-Net strips the user of the burden to manually account for\nthese co-dependencies. The empirical parameters are autonomously identiﬁed via cross-validation\non the training cases. Per default, nnU-Net generates three different U-Net conﬁgurations: a 2D\nU-Net, a 3D U-Net that operates at full image resolution and a 3D U-Net cascade where the ﬁrst\nU-Net operates on downsampled images and the second is trained to reﬁne the segmentation maps\ncreated by the former at full resolution. After cross-validation nnU-Net empirically chooses the best\nperforming conﬁguration or ensemble. Finally, nnU-Net empirically opts for “non-largest component\nsuppression” as a postprocessing step if performance gains are measured. The output of nnU-Net’s\nautomated adaptation and training process are fully trained U-Net models that can be deployed to\nmake predictions on unseen images. We provide an in-depth description of the methodology behind\nnnU-Net in the online methods. The overarching design principles, i.e. our best-practice recommen3\n\nFigure 1: nnU-Net handles a broad variety of datasets and target image properties. All examples\noriginate from the test sets of different international segmentation challenges that nnU-Net was applied\non. Target structures for each dataset are shown in 2D projected onto the raw data (left) and in 3D\ntogether with a volume rendering of the raw data (right). All visualizations are created with the\nMITK Workbench [29]. a: heart (green), aorta (red), trachea (blue) and esophagus (yellow) in CT\nimages (D18). b: synaptic clefts (green) in electron microscopy scans (D19). c: liver (yellow),\nspleen (orange), left/right kidney (blue/green) in T1 in-phase MRI (D16). d: thirteen abdominal\norgans in CT images (D11). e: liver (yellow) and liver tumors (green) in CT images (D14). f: right\nventricle (yellow), left ventricular cavity (blue) myocardium of left ventricle (green) in cine MRI\n(D13). g: prostate (yellow) in T2 MRI (D12). h: lung nodules (yellow) in CT images (D6). i: kidneys\n(yellow) and kidney tumors (green) in CT images (D17). j: edema (yellow), enhancing tumor (purple),\nnecrosis (green) in MRI (T1, T1 with contrast agent, T2, FLAIR) (D1). k: left ventricle (yellow) in\nMRI (D2). l: hepatic vessels (yellow) and liver tumors (green) in CT (D8). See Figure 5 for dataset\nreferences.\n4\n\nFigure 2: Manual and proposed automated conﬁguration of deep learning methods. a) Current\npractice of conﬁguring a deep learning method for biomedical segmentation: An iterative trial and\nerror process of manually choosing a set of hyperparameters and architecture conﬁgurations, training\nthe model, and monitoring performance of the model on a validation set. b) Proposed automated\nconﬁguration by nnU-Net: Dataset properties are summarized in a “dataset ﬁngerprint”. A set\nof heuristic rules operates on this ﬁngerprint to infer the data-dependent hyperparameters of the\npipeline. These are completed by blueprint parameters, the data-independent design choices to form\n“pipeline ﬁngerprints”. Three architectures are trained based on these pipeline ﬁngerprints in a 5-fold\ncross-validation. Finally, nnU-Net automatically selects the optimal ensemble of these architectures\nand performs postprocessing if required.\ndations for method adaptation to new datasets, are summarized in Supplementary Information B.\nAll segmentation pipelines generated by nnU-Net in the context of this manuscript are provided in\nSupplementary Information F.\nnnU-Net handles a wide variety of target structures and image properties\nWe demonstrate the\nvalue of nnU-Net as an out-of-the-box segmentation tool by applying it to 10 international biomedical\nimage segmentation challenges comprising 19 different datasets and 49 segmentation tasks across a\nvariety of organs, organ substructures, tumors, lesions and cellular structures in magnetic resonance\nimaging (MRI), computed tomography scans (CT) as well as electron microscopy (EM) images.\nChallenges are international competitions that can be seen as the equivalent to clinical trials for\nalgorithm benchmarking. Typically, they are hosted by individual researchers, institutes, or societies,\naiming to assess the performance of multiple algorithms in a standardized environment [23]. In\nall segmentation tasks, nnU-Net was trained from scratch using only the provided challenge data.\nWhile the methodology behind nnU-Net was developed on the 10 training sets provided by the\nMedical Segmentation Decathlon [32], the remaining datasets and tasks were used for independent\ntesting, i.e. nnU-Net was simply applied without further optimization. Qualitatively, we observe that\n5\n\nnnU-Net is able to handle a large disparity in dataset properties and diversity in target structures,\ni.e. generated pipeline conﬁgurations are in line with what human experts consider a reasonable or\nsensible setting (see Supplementary Information C.1and C.2). Examples for segmentation results\ngenerated by nnU-Net are presented in Figure 1.\nnnU-Net outperforms specialized pipelines in a range of diverse tasks\nMost international challenges use the Soerensen-Dice coefﬁcient as a measure of overlap to quantify segmentation quality\n[13, 4, 25, 3]. Here, perfect agreement results in a Dice coefﬁcient of 1, whereas no agreement\nresults in a score of 0. Other metrics used by some of the challenges include the Normalized Surface\nDice (higher is better) [7] and the Hausdorff Distance (lower is better), both quantifying the distance\nbetween the borders of two segmentations. Figure 3 provides an overview of the quantitative results\nachieved by nnU-Net and the competing challenge teams across all 49 segmentation tasks. Despite\nits generic nature, nnU-Net outperforms most existing semantic segmentation solutions, even though\nthe latter were speciﬁcally optimized towards the respective task. Overall, nnU-Net sets a new state\nof the art in 29 out of 49 target structures and otherwise shows performances on par with or close to\nthe top leaderboard entries.\nDetails in pipeline conﬁguration have more impact on performance than architectural variations\nTo highlight how important the task-speciﬁc design and conﬁguration of a method are in\ncomparison to choosing one of the many architectural extensions and advances previously proposed\non top of the U-Net, we put our results into context of current research by analyzing the participating\nalgorithms in the recent Kidney and Kidney Tumor Segmentation (KiTS) 2019 challenge hosted by\nthe Medical Image Computing and Computer Assisted Intervention (MICCAI) society [13]. The\nMICCAI society has consistently been hosting at least 50% of all annual biomedical image analysis\nchallenges [23]. With more than 100 competitors, the KiTS challenge was the largest competition at\nMICCAI 2019. Our analysis of the KiTS leaderboard1 (see Figure 4a) reveals several insights on the\ncurrent landscape of deep learning based segmentation method design: First, the top-15 methods were\noffspring of the (3D) U-Net architecture from 2016, conﬁrming its impact on the ﬁeld of biomedical\nimage segmentation. Second, the ﬁgure demonstrates that contributions using the same type of\nnetwork result in performances spread across the entire leaderboard. Third, when looking closer into\nthe top-15, none of the commonly used architectural modiﬁcations (e.g. residual connections [26, 10],\ndense connections [18, 15], attention mechanisms [30] or dilated convolutions [5, 24]) represent a\nnecessary condition for good performance on the KiTS task. By example this shows that many of\nthe previously introduced algorithm modiﬁcations may not generally be superior to a properly tuned\nbaseline method.\nFigure 4b underlines the importance of hyperparameter tuning by analyzing algorithms using the same\narchitecture variant as the challenge-winning contribution, a 3D U-Net with residual connections.\nWhile one of these methods won the challenge, other contributions based on the same principle cover\nthe entire range of evaluation scores and rankings. Key conﬁguration parameters were selected from\nrespective pipeline ﬁngerprints and are shown for all non-cascaded residual U-Nets, illustrating the\nco-dependent design choices that each team made during pipeline design. The drastically varying\nconﬁgurations submitted by contestants indicate the underlying complexity of the high-dimensional\noptimization problem that is implicitly posed by designing a deep learning method for biomedical 3D\nimage segmentation.\n1http://results.kits-challenge.org/miccai2019/\n6\n\nFigure 3: nnU-Net outperforms most specialized deep learning pipelines. Quantitative results\nfrom all international challenges that nnU-Net competed in. For each segmentation task, results\nachieved by nnU-Net are highlighted in red, competing teams are shown in blue. For each segmentation task the respective rank is displayed in the bottom right corner as nnU-Net’s rank / total number\nof submissions. Axis scales: [DC] Dice coefﬁcient, [OH] other score (higher is better), [OL] other\nscore (lower is better). All leaderboards were accessed on December 12th 2019.\n7\n\nFigure 4: Pipeline ﬁngerprints from KITS 2019 [13] leaderboard entries. a) Coarse categorization of leaderboard entries by architecture variation. All top 15 contributions are encoder-decoder\narchitectures with skip-connections, 3D convolutions and output stride 1 (“3D U-Net-like”, purple).\nNo clear pattern arises from further sub-groupings into different architectural variations. Also,\nnone of the analyzed architectures guarantees good performance, indicating a large dependency of\nperformance beyond architecture type. b) Finer-grained key parameters selected from the pipeline\nﬁngerprints of all non-cascade 3D-U-Net-like architectures with residual connections (displayed on\nz-score normalized scale). The contributions vary drastically in their rankings as well as their ﬁngerprints. Still, there is no evident relation between single parameters and performance. Abbreviations:\nCE = Cross entropy loss function, Dice = Soft Dice loss function, WBCE = Weighted binary cross\nentropy loss function.\nnnU-Net experimentally conﬁrms the importance of good hyperparameters over architectural variations on the KiTS dataset by setting a new state of the art on the open leaderboard (which also\nincludes the original challenge submissions analysed here) with a plain 3D U-Net architecture (see\nFigure 3). Our results from further international challenge participations conﬁrm this observation\nacross a variety of datasets.\nDifferent datasets require different pipeline conﬁgurations\nWe extract the data ﬁngerprints of\n19 biomedical segmentation datasets. As displayed in Figure 5, this documents an exceptional dataset\ndiversity in biomedical imaging, and reveals the fundamental reason behind the lack of out-of-the-box\n8\n\nFigure 5: Data ﬁngerprints across different challenge datasets. The data ﬁngerprints show the\nkey properties (displayed on z-score normalized scale) for the 19 datasets used in the nnU-Net experiments (see Supplementary Material A for detailed dataset descriptions). Datasets vary tremendously\nin their properties, requiring intense method adaptation to the individual dataset and underlining\nthe need for evaluation on larger numbers of datasets when drawing general methodological conclusions. Abbreviations: EM = Electron Microscopy, CT = Computed Tomography, MRI = Magnetic\nResonance Imaging.\nsegmentation algorithms: The complexity of method design is ampliﬁed by the fact that suitable\npipeline settings either directly or indirectly depend on the data ﬁngerprint under potentially complex\nrelations. As a consequence, pipeline settings that are identiﬁed as optimal for one dataset (such\nas KiTS, see above) may not generalize to others, resulting in a need for (currently manual) reoptimization on each individual dataset. An example for conﬁguration parameters depending on\ndataset properties is the image size which affects the size of patches that the model sees during training,\nwhich in turn affects the required network topology (i.e. number of downsampling steps, size of\nconvolution ﬁlters, etc.). The network topology itself again inﬂuences several other hyperparameters\nin the pipeline.\nMultiple tasks enable robust design decisions\nnnU-Net is a framework that enables benchmarking of new modiﬁcations or extensions of methods across multiple datasets without having to manually\nreconﬁgure the entire pipeline for each dataset. To demonstrate this, and also to support some of the\ncore design choices made in nnU-Net, we systematically tested the performance of common pipeline\nvariations in the nnU-Net blueprint parameters on 10 different datasets (Figure 6): the application\nof two alternative loss functions (Cross-entropy and TopK10 [35]), the introduction of residual\nconnections in the encoder [11], using three convolutions per resolution instead of two (resulting\nin a deeper network architecture), two modiﬁcations of the optimizer (a reduced momentum term\nand an alternative optimizer (Adam [20])), batch norm [17] instead of instance norm [33] and the\nomission of data augmentation. Ranking stability was estimated by bootstrapping as suggested by the\nchallengeR tool [34].\nThe volatility of the ranking between datasets demonstrates how single hyperparameter choices can\naffect segmentation performance depending on the dataset. The results clearly show that caution is\nrequired when drawing methodological conclusions from evaluations that are based on an insufﬁcient\nnumber of datasets. While ﬁve out of the nine variants achieved rank 1 in at least one of the\n9\n\nFigure 6: Evaluation of design decisions across multiple tasks. (a-j) Evaluation of exemplary\nmodel variations on ten datasets of the medical segmentation decathlon (D1-D10, see Figure 5 for\ndataset references). The analysis is done for every dataset by aggregating validation splits of the\nﬁve-fold cross-validation into one large validation set. 1000 virtual validation sets are generated\nvia bootstrapping (drawn with replacement). Algorithms are ranked on each virtual validation set,\nresulting in a distribution over rankings. The results indicate that evaluation of methodological\nvariations on too few datasets is prone to result in a misleading level of generality, since most\nperformance changes are not consistent over datasets. (k) The aggregation of rankings across datasets\nyields insights into what design decisions robustly generalize.\ndatasets, neither of them exhibits consistent improvements across the ten tasks. The original nnU-Net\nconﬁguration shows the best generalization and ranks ﬁrst when aggregating results of all datasets.\nIn current research practice, evaluation is rarely performed on more than two datasets and even then\nthe datasets come with largely overlapping properties (such as both being abdominal CT scans). As\nwe showed here, such evaluation is unsuitable for drawing general methodological conclusions. We\nrelate the lack of sufﬁciently broad evaluations to the manual tuning effort required when adapting\nexisting pipelines to individual datasets. nnU-Net alleviates this shortcoming in two ways: As a\nframework that can be extended to enable effective evaluation of new concepts across multiple tasks,\nand as a plug-and-play, standardized and state-of-the-art baseline to compare against.\n10\n\nnnU-Net is freely available and can be used out-of-the-box\nnnU-Net is freely available as an\nopen-source tool. It can be installed via Python Package Index (PyPI). The source code is publicly\navailable on Github (https://github.com/MIC-DKFZ/nnUNet). A comprehensive documentation\nis available together with the source code. Pretrained models for all presented datasets are available\nfor download at https://zenodo.org/record/3734294.\n3\nDiscussion\nWe presented nnU-Net, a deep learning framework for biomedical image analysis that automates\nmodel design for 3D semantic segmentation tasks. The method sets a new state of the art in the\nmajority of tasks it was evaluated on, outperforming all respective specialized processing pipelines.\nThe strong performance of nnU-Net is not achieved by a new network architecture, loss function or\ntraining scheme (hence the name nnU-Net - “no new net”), but by replacing the complex process of\nmanual pipeline optimization with a systematic approach based on explicit and interpretable heuristic\nrules. Requiring zero user-intervention, nnU-Net is the ﬁrst segmentation tool that can be applied\nout-of-the-box to a very large range of biomedical imaging datasets and is thus the ideal tool for\nusers who require access to semantic segmentation methods and do not have the expertise, time, or\ncompute resources required to manually adapt existing solutions to their problem.\nOur analysis on the KITS leaderboard as well as nnU-Net’s performance across 19 datasets\nconﬁrms our initial hypothesis that common architectural modiﬁcations proposed by the ﬁeld during\nthe last 5 years may not necessarily be required to achieve state-of-the-art segmentation performance.\nInstead, we observed that contributions using the same type of network result in performances\nspread across the entire leaderboard. This observation is in line with Litjens et al., who, in their\nreview from 2017, found that \"many researchers use the exact same architectures [...] but have\nwidely varying results\" [22]. There are several possible reasons for why performance improvements\nbased on architectural extensions proposed by the literature may not hold beyond the dataset they\nwere proposed on: many of them are evaluated on a limited amount of datasets, often as low as a\nsingle one. In practice this largely limits their success on unseen datasets with varying properties,\nbecause the quality of the hyperparameter conﬁguration often overshadows the effect of the evaluated\narchitectural modiﬁcation. This ﬁnding is in line with an observation by Litjens et al., who concluded\nthat \"the exact architecture is not the most important determinant in getting a good solution\" [22].\nMoreover, as shown above, it can be difﬁcult to tune existing baselines to a given dataset. This\nobstacle can unknowingly, but nonetheless unduly, make a new approach look better than the baseline,\nresulting in biased literature.\nIn this work, we demonstrated that nnU-Net is able to alleviate this bottleneck of current\nresearch in biomedical image segmentation in two ways: On the one hand, nnU-Net serves as a\nframework for methodological modiﬁcations enabling simple evaluation on an arbitrary number of\ndatasets. On the other hand, nnU-Net represents the ﬁrst standardized method that does not require\nmanual task-speciﬁc adaptation and as such can readily serve as a strong baseline on any new 3D\nsegmentation task.\nThe research performed in “AutoML” [16, 6] or “Neural architecture search” [8] has similarities to our approach in that this line of research seeks to strip the ML user or researcher of the\nburden to manually ﬁnd good hyperparameters. In contrast to nnU-Net however, AutoML aims to\n11\n\nlearn hyperparameters directly from the data. This comes with practical difﬁculties such as enormous\nrequirements with respect to compute and data resources. Additionally, AutoML methods need to\noptimize the hyperparameters for each new task. The same disadvantages apply to “Grid Search” [2],\nwhere extensive trial and error sweeps in the hyperparameter landscape are performed to empirically\nﬁnd good conﬁgurations for a speciﬁc task. In contrast, nnU-Net transforms domain knowledge into\ninductive biases, thus shortcuts the high dimensional optimization of hyperparameters and minimizes\nrequired computational and data resources. As elaborated above, these heuristics are developed\non the basis of 10 different datasets of the Medical Segmentation Decathlon. The diversity within\nthese 10 datasets has proven sufﬁcient to achieve robustness to the variability encountered in all the\nremaining challenge participations. This is quite remarkable given the underlying complexity of\nmethod design and strongly conﬁrms the suitability of condensing the process in a few generally\napplicable rules that are simply executed when given a new dataset ﬁngerprint and do not require any\nfurther task-speciﬁc actions. The formal deﬁnition and also publishing of these explicit rules is a\nstep towards systematicity and interpretability in the task of hyperparameter selection, which has\npreviously been considered a “highly empirical exercise”, for which “no clear recipe can be given.”\n[22].\nDespite its strong performance across 49 diverse tasks, there might be segmentation tasks\nfor which nnU-Net’s automatic adaptation is suboptimal. For example, nnU-Net was developed\nwith a focus on the Dice coefﬁcient as performance metric. Some tasks, however, might require\nhighly domain speciﬁc target metrics for performance evaluation, which could inﬂuence method\ndesign.\nAlso, yet unconsidered dataset properties could exist which may cause suboptimal\nsegmentation performance. One example is the synaptic cleft segmentation task of the CREMI\nchallenge (https://cremi.org). While nnU-Net’s performance is highly competitive (rank 6/39),\nmanual adaptation of the loss function as well as electron microscopy-speciﬁc preprocessing may be\nnecessary to surpass state-of-the-art performance [12]. In principle, there are two ways of handling\ncases that are not yet optimally covered by nnU-Net: For potentially re-occurring cases, nnU-Net’s\nheuristics could be extended accordingly; for highly domain speciﬁc cases, nnU-Net should be seen\nas a good starting point for necessary modiﬁcations.\nIn summary, nnU-Net sets a new state of the art in various semantic segmentation challenges and displays strong generalization characteristics without need for any manual intervention,\nsuch as the tuning of hyper-parameters. As pointed out by Litjens et al. and quantitatively conﬁrmed\nhere, hyper-parameter optimization constitutes a major difﬁculty for past and current research\nin biomedical image segmentation. nnU-Net automates the otherwise often unsystematic and\ncumbersome procedure and may thus help alleviate this burden. We propose to leverage nnU-Net as\nan out-of-the box tool for state-of-the-art segmentation, a framework for large-scale evaluation of\nnovel ideas without manual effort, and as a standardized baseline method to compare ideas against\nwithout the need for task-speciﬁc optimization.\n4\nAcknowledgements\nThis work was co-funded by the National Center for Tumor Diseases (NCT) in Heidelberg and\nthe Helmholtz Imaging Platform (HIP) of the German Cancer Consortium (DKTK). We thank our\ncolleagues at DKFZ who were involved in the various challenge contributions, especially Andre\nKlein, David Zimmerer, Jakob Wasserthal, Gregor Koehler, Tobias Norajitra and Sebastian Wirkert\n12\n\nwho contributed to the Decathlon submission. We also thank the MITK team who supported us in\nproducing all medical dataset visualizations. We are also thankful to all the challenge organizers,\nwho provided an important basis for our work. We want to especially mention Nicholas Heller, who\nenabled the collection of all the details from the KiTS challenge through excellent challenge design,\nand Emre Kavur from the CHAOS team, who generated comprehensive leaderboard information for\nus. We thank Manuel Wiesenfarth for his helpful advice concerning the ranking of methods and the\nvisualization of rankings. Last but not least, we thank Olaf Ronneberger and Lena Maier-Hein for\ntheir important feedback on this manuscript.\nReferences\n[1] H. J. Aerts, E. R. Velazquez, R. T. Leijenaar, C. Parmar, P. Grossmann, S. Carvalho, J. Bussink,\nR. Monshouwer, B. Haibe-Kains, D. Rietveld, et al. Decoding tumour phenotype by noninvasive\nimaging using a quantitative radiomics approach. Nature communications, 5(1):1–9, 2014.\n[2] J. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. Journal of machine\nlearning research, 13(Feb):281–305, 2012.\n[3] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir,\nO. Camara, M. A. G. Ballester, et al. Deep learning techniques for automatic mri cardiac multistructures segmentation and diagnosis: Is the problem solved? IEEE TMI, 37(11):2514–2525,\n2018.\n[4] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.A. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE\ntransactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n[6] E. D. Cubuk, B. Zoph, D. Mane, V. Vasudevan, and Q. V. Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 113–123, 2019.\n[7] J. De Fauw, J. R. Ledsam, B. Romera-Paredes, S. Nikolov, N. Tomasev, S. Blackwell, H. Askham,\nX. Glorot, B. O’Donoghue, D. Visentin, et al. Clinically applicable deep learning for diagnosis\nand referral in retinal disease. Nature medicine, 24(9):1342–1350, 2018.\n[8] T. Elsken, J. H. Metzen, and F. Hutter. Neural architecture search: A survey. Journal of Machine\nLearning Research, 20(55):1–21, 2019.\n[9] T. Falk, D. Mai, R. Bensch, Ö. Çiçek, A. Abdulkadir, Y. Marrakchi, A. Böhm, J. Deubner,\nZ. Jäckel, K. Seiwald, et al. U-net: deep learning for cell counting, detection, and morphometry.\nNature methods, 16(1):67–70, 2019.\n[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–778,\n2016.\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016.\n13\n\n[12] L. Heinrich, J. Funke, C. Pape, J. Nunez-Iglesias, and S. Saalfeld. Synaptic cleft segmentation\nin non-isotropic volume electron microscopy of the complete drosophila brain. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 317–325.\nSpringer, 2018.\n[13] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[14] T. C. Hollon, B. Pandian, A. R. Adapa, E. Urias, A. V. Save, S. S. S. Khalsa, D. G. Eichberg,\nR. S. D’Amico, Z. U. Farooq, S. Lewis, et al. Near real-time intraoperative brain tumor diagnosis\nusing stimulated raman histology and deep neural networks. Nature Medicine, pages 1–7, 2020.\n[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n[16] F. Hutter, H. H. Hoos, and K. Leyton-Brown. Sequential model-based optimization for general\nalgorithm conﬁguration. In International conference on learning and intelligent optimization,\npages 507–523. Springer, 2011.\n[17] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[18] S. Jégou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers tiramisu:\nFully convolutional densenets for semantic segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition workshops, pages 11–19, 2017.\n[19] P. Kickingereder, F. Isensee, I. Tursunova, J. Petersen, U. Neuberger, D. Bonekamp, G. Brugnara,\nM. Schell, T. Kessler, M. Foltyn, et al. Automated quantitative tumour response assessment of\nmri in neuro-oncology with artiﬁcial neural networks: a multicentre, retrospective study. The\nLancet Oncology, 20(5):728–740, 2019.\n[20] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and\nY. LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\n[21] Y. LeCun. 1.1 deep learning hardware: Past, present, and future. In 2019 IEEE International\nSolid-State Circuits Conference-(ISSCC), pages 12–19. IEEE, 2019.\n[22] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. Van\nDer Laak, B. Van Ginneken, and C. I. Sánchez. A survey on deep learning in medical image\nanalysis. Medical image analysis, 42:60–88, 2017.\n[23] L. Maier-Hein, M. Eisenmann, A. Reinke, S. Onogur, M. Stankovic, P. Scholz, T. Arbel,\nH. Bogunovic, A. P. Bradley, A. Carass, et al. Why rankings of biomedical image analysis\ncompetitions should be interpreted with care. Nature communications, 9(1):5217, 2018.\n[24] R. McKinley, R. Meier, and R. Wiest. Ensembles of densely-connected cnns with labeluncertainty for brain tumor segmentation. In International MICCAI Brainlesion Workshop, pages\n456–465. Springer, 2018.\n14\n\n[25] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz,\nJ. Slotboom, R. Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats).\nIEEE transactions on medical imaging, 34(10):1993–2024, 2014.\n[26] F. Milletari, N. Navab, and S.-A. Ahmadi. V-net: Fully convolutional neural networks for\nvolumetric medical image segmentation. In International Conference on 3D Vision (3DV), pages\n565–571. IEEE, 2016.\n[27] U. Nestle, S. Kremp, A. Schaefer-Schuler, C. Sebastian-Welsch, D. Hellwig, C. Rübe, and C.-M.\nKirsch. Comparison of different methods for delineation of 18f-fdg pet–positive tissue for target\nvolume deﬁnition in radiotherapy of patients with non–small cell lung cancer. Journal of Nuclear\nMedicine, 46(8):1342–1348, 2005.\n[28] S. Nikolov, S. Blackwell, R. Mendes, J. De Fauw, C. Meyer, C. Hughes, H. Askham, B. RomeraParedes, A. Karthikesalingam, C. Chu, et al. Deep learning to achieve clinically applicable\nsegmentation of head and neck anatomy for radiotherapy. arXiv preprint arXiv:1809.04430,\n2018.\n[29] M. Nolden, S. Zelzer, A. Seitel, D. Wald, M. Müller, A. M. Franz, D. Maleike, M. Fangerau,\nM. Baumhauer, L. Maier-Hein, et al. The medical imaging interaction toolkit: challenges and\nadvances. International journal of computer assisted radiology and surgery, 8(4):607–620, 2013.\n[30] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y. Hammerla, B. Kainz, et al. Attention u-net: learning where to look for the pancreas. arXiv\npreprint arXiv:1804.03999, 2018.\n[31] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, pages 234–241. Springer, 2015.\n[32] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. van Ginneken, A. KoppSchneider, B. A. Landman, G. Litjens, B. Menze, et al. A large annotated medical image dataset\nfor the development and evaluation of segmentation algorithmsdelldatagrowth. arXiv preprint\narXiv:1902.09063, 2019.\n[33] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[34] M. Wiesenfarth, A. Reinke, B. A. Landman, M. J. Cardoso, L. Maier-Hein, and A. KoppSchneider. Methods and open-source toolkit for analyzing and visualizing challenge results.\narXiv preprint arXiv:1910.05121, 2019.\n[35] Z. Wu, C. Shen, and A. v. d. Hengel. Bridging category-level and instance-level semantic image\nsegmentation. arXiv preprint arXiv:1605.06885, 2016.\n15\n\nMethods\nA quick overview of the nnU-Net design principles can be found in the Supplemental Material B.\nThis section provides detailed information on how these principles are implemented.\nDataset ﬁngerprints\nAs a ﬁrst processing step, nnU-Net crops the provided training cases to their\nnonzero region. While this had no effect on most datasets in our experiments, it reduced the image\nsize of brain datasets such as D1 (Brain Tumor) and D15 (MSLes) substantially and thus improved\ncomputational efﬁciency. Based on the cropped training data, nnU-Net creates a dataset ﬁngerprint\nthat captures all relevant parameters and properties: image sizes (i.e. number of voxels per spatial\ndimension) before and after cropping, image spacings (i.e. the physical size of the voxels), modalities\n(read from metadata) and number of classes for all images as well as the total number of training\ncases. Furthermore, the ﬁngerprint includes the mean, standard deviation as well as the 0.5 and 99.5\npercentiles of the intensity values in the foreground regions, i.e. the voxels belonging to any of the\nclass labels, computed over all training cases.\nPipeline ﬁngerprints\nnnU-Net automizes the design of deep learning methods for biomedical image segmentation by generating a so-called pipeline ﬁngerprint that contains all relevant information.\nImportantly, nnU-Net reduces the design choices to the really essential ones and automatically infers\nthese choices using a set of heuristic rules. These rules condense the domain knowledge and operate\non the above-described data ﬁngerprint and the project-speciﬁc hardware constraints. These inferred\nparameters are complemented by blueprint parameters, which are data-independent, and empirical\nparameters, which are optimized during training.\nBlueprint parameters\nArchitecture template: All U-Net architectures conﬁgured by nnU-Net\noriginate from the same template. This template closely follows the original U-Net [16] and its\n3D counterpart [3]. According to our hypothesis that a well-conﬁgured plain U-Net is still hard to\nbeat, none of our U-Net conﬁgurations make use of recently proposed architectural variations such\nas residual connections [6, 7], dense connections [10, 12], attention mechanisms [14], squeeze and\nexcitation [9] or dilated convolutions [2]. Minor changes with respect to the original architecture were\nmade: To enable large patch sizes, the batch size of the networks in nnU-Net is small. In fact, most\n3D U-Net conﬁgurations were trained with a batch size of only 2 (see Supplementary Material Figure\nE.1a). Batch normalization [11], which is often used to speed up or stabilize the training, does not\nperform well with small batch sizes [20, 17]. We therefore use instance normalization [19] for all UNet models. Furthermore, we replace ReLU with leaky ReLUs [13] (negative slope 0.01). Networks\nare trained with deep supervision: additional auxiliary losses are added in the decoder to all but the\ntwo lowest resolutions, allowing gradients to be injected deeper into the network and facilitating the\ntraining of all layers in the network. All U-Nets employ the very common conﬁguration of two blocks\nper resolution step in both encoder and decoder, with each block consisting of a convolution, followed\nby instance normalization and a leaky ReLU nonlinearity. Downsampling is implemented as strided\nconvolution (motivated by representational bottleneck, see [18]) and upsampling as convolution\ntransposed. As a tradeoff between performance and memory consumption, the initial number of\nfeature maps is set to 32 and doubled (halved) with each downsampling (upsampling) operation. To\nlimit the ﬁnal model size, the number of feature maps is additionally capped at 320 and 512 for 3D\nand 2D U-Nets, respectively.\nTraining schedule: Based on experience and as a trade-off between runtime and reward, all networks\nare trained for 1000 epochs with one epoch being deﬁned as iteration over 250 minibatches. Stochastic\n16\n\ngradient descent with nesterov momentum (µ = 0.99) and an initial learning rate of 0.01 is used for\nlearning network weights. The learning rate is decayed throughout the training following the ‘poly’\nlearning rate policy [2]: (1 −epoch/epochmax)0.9. The loss function is the sum of cross-entropy\nand Dice loss [4]. For each deep supervision output, a corresponding downsampled ground truth\nsegmentation mask is used for loss computation. The training objective is the sum of the losses at\nall resolutions: L = w1 · L1 + w2 · L2 + ... . Hereby, the weights halve with each decrease in\nresolution, resulting in w2 = 1/2 · w1; w3 = 1/4 · w1, etc. and are normalized to sum to 1. Samples\nfor the mini batches are chosen from random training cases. Oversampling is implemented to ensure\nrobust handling of class imbalances: 66.7% of samples are from random locations within the selected\ntraining case while 33.3% of patches are guaranteed to contain one of the foreground classes that\nare present in the selected training sample (randomly selected). The number of foreground patches\nis rounded with a forced minimum of 1 (resulting in 1 random and 1 foreground patch with batch\nsize 2). A variety of data augmentation techniques are applied on the ﬂy during training: rotations,\nscaling, Gaussian noise, Gaussian blur, brightness, contrast, simulation of low resolution, gamma and\nmirroring. Details are provided in Supplementary Information D.\nInference: Images are predicted with a sliding window approach, where the window size equals the\npatch size used during training. Adjacent predictions overlap by half the size of a patch. The accuracy\nof segmentation decreases towards the borders of the window. To suppress stitching artifacts and\nreduce the inﬂuence of positions close to the borders, a Gaussian importance weighting is applied,\nincreasing the weight of the center voxels in the softmax aggregation. Test time augmentation by\nmirroring along all axes is applied.\nInferred Parameters\nIntensity normalization: There are two different image intensity normalization schemes supported by nnU-Net. The default setting for all modalities except CT images is\nz-scoring. For this option, during training and inference, each image is normalized independently\nby subtracting its mean, followed by division with its standard deviation. If cropping resulted in\nan average size decrease of 25% or more, a mask for central non-zero voxels is created and the\nnormalization is applied within that mask only, ignoring the surrounding zero voxels. For computed\ntomography (CT) images, nnU-Net employs a different scheme, as intensity values are quantitative\nand reﬂect physical properties of the tissue. It can therefore be beneﬁcial to retain this information by\nusing a global normalization scheme that is applied to all images. To this end, nnU-Net uses the 0.5\nand 99.5 percentiles of the foreground voxels for clipping as well as the global foreground mean a\nstandard deviation for normalization on all images.\nResampling: In some datasets, particularly in the medical domain, the voxel spacing (the physical\nspace the voxels represent) is heterogeneous. Convolutional neural networks operate on voxel grids\nand ignore this information. To cope with this heterogeneity, nnU-Net resamples all images to the\nsame target spacing (see paragraph below) using either third order spline, linear or nearest neighbor\ninterpolation. The default setting for image data is third order spline interpolation. For anisotropic\nimages (maximum axis spacing / minimum axis spacing > 3), in-plane resampling is done with third\norder spline whereas out of plane interpolation is done with nearest neighbor. Treating the out of\nplane axis differently in anisotropic cases suppresses resampling artifacts, as large contour changes\nbetween slices are much more common. Segmentation maps are resampled by converting them to\none hot encodings. Each channel is then interpolated with linear interpolation and the segmentation\nmask is retrieved by an argmax operation. Again, anisotropic cases are interpolated using “nearest\nneighbor” on the low resolution axis.\n17\n\nTarget spacing: The selected target spacing is a crucial parameter. Larger spacings result in smaller\nimages and thus a loss of details whereas smaller spacings result in larger images preventing the\nnetwork from accumulating sufﬁcient contextual information since the patch size is limited by the\ngiven GPU memory budget. Although this tradeoff is in part addressed by the 3D U-Net cascade\n(see below), a sensible target spacing for low and full resolution is still required. For the 3D full\nresolution U-Net, nnU-Net uses the median value of the spacings found in the training cases computed\nindependently for each axis as default target spacing. For anisotropic datasets, this default can result in\nsevere interpolation artifacts or in a substantial loss of information due to large variances in resolution\nacross the training data. Therefore, the target spacing of the lowest resolution axis is selected to be\nthe 10th percentile of the spacings found in the training cases if both voxel and spacing anisotropy\n(i.e. the ratio of lowest spacing axis to highest spacing axis) are larger than 3. For the 2D U-Net,\nnnU-Net generally operates on the two axes with the highest resolution. If all three axes are isotropic,\nthe two trailing axes are utilized for slice extraction. The target spacing is the median spacing of the\ntraining cases (computed independently for each axis). For slice-based processing, no resampling\nalong the out-of-plane axis is required.\nAdaptation of network topology, patch size and batch size: Finding an appropriate U-Net architecture\nconﬁguration is crucial for good segmentation performance. nnU-Net prioritizes large patch sizes\nwhile remaining within a predeﬁned GPU memory budget. Larger patch sizes allow for more\ncontextual information to be aggregated and thus typically increase segmentation performance.\nThey come, however, at the cost of a decreased batch size which results in noisier gradients during\nbackpropagation. To improve the stability of the training, we require a minimum batch size of 2 and\nchoose a large momentum term for network training (see blueprint parameters). Image spacing is also\nconsidered in the adaptation process: Downsampling operations may operate only on speciﬁc axes\nand convolutional kernels in the 3D U-Nets can operate on certain image planes only (pseudo-2D).\nThe network topology for all U-Net conﬁgurations is chosen on basis of the median image size after\nresampling as well as the target spacing the images were resampled to. A ﬂow chart for the adaptation\nprocess is presented in the Supplements in Figure E.1. The adaptation of the architecture template,\nwhich is described in more detail in the following, is computationally inexpensive. Due to the GPU\nmemory consumption estimate being based on feature map sizes, no GPU is required to run the\nadaptation process.\nInitialization: The patch size is initialized as the median image shape after resampling. If the patch\nsize is not divisible by 2nd for each axis, where nd is the number of downsampling operations, it is\npadded accordingly.\nArchitecture topology: The architecture is conﬁgured by determining the number of downsampling\noperations along each axis depending on the patch size and voxel spacing. Downsampling is\nperformed until further downsampling would reduce the feature map size to smaller than 4 voxels or\nthe feature map spacings become anisotropic. The downsampling strategy is determined by the voxel\nspacing: high resolution axes are downsampled separately until their resolution is within factor 2 of\nthe lower resolution axis. Subsequently, all axes are downsampled simultaneously. Downsampling is\nterminated for each axis individually, once the respective feature map constraint is triggered. The\ndefault kernel size for convolutions is 3 × 3 × 3 and 3 × 3 for 3D U-Net and 2D U-Net, respectively.\nIf there is an initial resolution discrepancy between axes (deﬁned as a spacing ratio larger than 2), the\nkernel size for the out-of-plane axis is set to 1 until the resolutions are within a factor of 2. Note that\nthe convolutional kernel size then remains at 3 for all axes.\n18\n\nAdaptation to GPU memory budget: The largest possible patch size during conﬁguration is limited\nby the amount of GPU memory. Since the patch size is initialized to the median image shape after\nresampling, it is initially too large to ﬁt into the GPU for most datasets. nnU-Net estimates the\nmemory consumption of a given architecture based on the size of the feature maps in the network,\ncomparing it to reference values of known memory consumption. The patch size is then reduced in\nan iterative process while updating the architecture conﬁguration accordingly in each step until the\nrequired budget is reached (see Figure E.1 in the Supplements). The reduction of the patch size is\nalways applied to the largest axis relative to the median image shape of the data. The reduction in\none step amounts to 2nd voxels of that axis, where nd is the number of downsampling operations.\nBatch size: As a ﬁnal step, the batch size is conﬁgured. If a reduction of patch size was performed\nthe batch size is set to 2. Otherwise, the remaining GPU memory headroom is utilized to increase the\nbatch size until the GPU is fully utilized. To prevent overﬁtting, the batch size is capped such that the\ntotal number of voxels in the minibatch do not exceed 5% of the total number of voxels of all training\ncases. Examples for generated U-Net architectures are presented in Supplementary Information C.1\nand C.2.\nConﬁguration of 3D U-Net cascade: Running a segmentation model on downsampled data increases\nthe size of patches in relation to the image and thus enables the network to accumulate more contextual\ninformation. This comes at the cost of a reduction in details in the generated segmentations and\nmay also cause errors if the segmentation target is very small or characterized by its texture. In a\nhypothetical scenario with unlimited GPU memory, it is thus generally favored to train models at\nfull resolution with a patch size that covers the entire image. The 3D U-Net cascade approximates\nthis approach by ﬁrst running a 3D U-Net on downsampled images and then training a second, full\nresolution 3D U-Net to reﬁne the segmentation maps of the former. This way, the “global”, low\nresolution network utilizes maximal contextual information to generate its segmentation output,\nwhich then serves as an additional input channel that guides the second, “local” U-Net. The cascade\nis triggered only for datasets where the patch size of the 3d full resolution U-Net covers less than\n12.5% of the median image shape. If this is the case, the target spacing for the downsampled data\nand the architecture of the associated 3D low resolution U-Net are conﬁgured jointly in an iterative\nprocess. The target spacing is initialized as the target spacing of the full resolution data. In order\nfor the patch size to cover a large proportion of the input image, the target spacing is then increased\nstepwise by 1% while updating the architecture conﬁguration accordingly in each step until the patch\nsize of the resulting network topology surpasses 25% of the current median image shape. If the\ncurrent spacing is anisotropic (factor 2 difference between lowest and highest resolution axis), only\nthe spacing of the higher resolution axes is increased. The conﬁguration of the second 3D U-Net of\nthe cascade is identical to the standalone 3D U-Net for which the conﬁguration process is described\nabove (except that the upsampled segmentation maps of the ﬁrst U-Net are concatenated to its input).\nFigure E.1b in the Supplements provides an overview of this optimization process.\nEmpirical parameters\nEnsembling and selection of U-Net conﬁguration(s): nnU-Net automatically determines which (ensemble of) conﬁguration(s) to use for inference based on the average\nforeground Dice coefﬁcient computed via cross-validation on the training data. The selected model(s)\ncan be either a single U-Net (2D, 3D full resolution, 3D low resolution or the full resolution U-Net of\nthe cascade) or an ensemble of any two of these conﬁgurations. Models are ensembled by averaging\nsoftmax probabilities.\n19\n\nPostprocessing: Connected component-based postprocessing is commonly used in medical image\nsegmentation [1, 8]. Especially in organ segmentation it often helps to remove spurious false positive\ndetections by removing all but the largest connected component. nnU-Net follows this assumption\nand automatically benchmarks the effect of suppressing smaller components on the cross-validation\nresults. First, all foreground classes are treated as one component. If suppression of all but the largest\nregion improves the average foreground Dice coefﬁcient and does not reduce the Dice coefﬁcient\nfor any of the classes, this procedure is selected as the ﬁrst postprocessing step. Finally, nnU-Net\nbuilds on the outcome of this step and decides whether the same procedure should be performed for\nindividual classes.\nImplementation details\nnnU-Net is implemented in Python utilizing the PyTorch [15] framework.\nThe Batchgenerators library [5] is used for data augmentation. For reduction of computational\nburden and GPU memory footprint, mixed precision training is implemented with Nvidia Apex/Amp\n(https://github.com/NVIDIA/apex). For use as a framework, the source code is available\non GitHub (https://github.com/MIC-DKFZ/nnUNet). Users who seek to use nnU-Net as a\nstandardized benchmark or to run inference with our pretrained models can install nnU-Net via PyPI.\nFor a full description of how to use nnU-Net, please refer to the online documentation available on\nthe GitHub page.\nReporting summary\nFurther information on research design is available in the Nature Research Reporting Summary linked\nto this article.\nCode availability\nThe nnU-Net repository is available at: https://github.com/mic-dkfz/nnunet. Pre-traiend\nmodels for all datasets utilized in this study are available for download at https://zenodo.org/\nrecord/3734294.\nData availability\nAll 19 datasets used in this study are publicly available. References for web access as well as key\ndata properties can be found in the Supplementary Material A and F.\nReferences\n[1] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.A. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n[2] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n[3] Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ronneberger. 3d u-net: learning\ndense volumetric segmentation from sparse annotation. In International conference on medical\nimage computing and computer-assisted intervention, pages 424–432. Springer, 2016.\n20\n\n[4] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and C. Pal. The importance of skip\nconnections in biomedical image segmentation. In Deep Learning and Data Labeling for\nMedical Applications, pages 179–187. Springer, 2016.\n[5] I. Fabian, J. Paul, W. Jakob, Z. David, P. Jens, K. Simon, S. Justus, K. Andre, R. Tobias,\nW. Sebastian, N. Peter, D. Stefan, K. Gregor, and M.-H. Klaus. batchgenerators - a python\nframework for data augmentation, Jan. 2020.\n[6] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770–\n778, 2016.\n[7] K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In European\nconference on computer vision, pages 630–645. Springer, 2016.\n[8] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[9] J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 7132–7141, 2018.\n[10] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger. Densely connected convolutional\nnetworks. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 4700–4708, 2017.\n[11] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[12] S. Jégou, M. Drozdzal, D. Vazquez, A. Romero, and Y. Bengio. The one hundred layers\ntiramisu: Fully convolutional densenets for semantic segmentation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition workshops, pages 11–19, 2017.\n[13] A. L. Maas, A. Y. Hannun, and A. Y. Ng. Rectiﬁer nonlinearities improve neural network\nacoustic models. In Proc. icml, volume 30, page 3, 2013.\n[14] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa, K. Mori, S. McDonagh,\nN. Y. Hammerla, B. Kainz, et al. Attention u-net: learning where to look for the pancreas. arXiv\npreprint arXiv:1804.03999, 2018.\n[15] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Systems, pages 8024–8035, 2019.\n[16] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image\nsegmentation. In MICCAI, pages 234–241. Springer, 2015.\n[17] S. Singh and S. Krishnan. Filter response normalization layer: Eliminating batch dependence in\nthe training of deep neural networks. arXiv preprint arXiv:1911.09737, 2019.\n[18] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2818–2826, 2016.\n21\n\n[19] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for\nfast stylization. arXiv preprint arXiv:1607.08022, 2016.\n[20] Y. Wu and K. He. Group normalization. In Proceedings of the European Conference on\nComputer Vision (ECCV), pages 3–19, 2018.\n22\n\nSupplementary Information\nThis document contains supplementary information for the manuscript ’Automated Design\nof Deep Learning Methods for Biomedical Image Segmentation’.\nA\nDataset details\nTable A provides an overview of the datasets used in this manuscript including respective references\nfor data access. The numeric values presented here are computed based on the training cases for each\nof these datasets. They are the basis of the dataset ﬁngerprints presented in Figure 5.\nID\nDataset Name\nAssociated\nChallenges\nModalities\nMedian Shape\n(Spacing [mm])\nN\nClasses\nRarest\nClass Ratio\nN Training\nCases\nSegmentation Tasks\nD1\nBrain Tumour\n[15], [14]\nMRI (T1, T1c,\nT2, FLAIR)\n138x169x138\n(1, 1, 1)\n3\n7.310−3\n484\nedema, active tumor,\nnecrosis\nD2\nHeart\n[15]\nMRI\n115x320x232\n(1.37, 1.25, 1.25)\n1\n4.010−3\n20\nleft ventricle\nD3\nLiver\n[15], [2]\nCT\n432x512x512\n(1, 0.77, 0.77)\n2\n2.610−2\n131\nliver, liver tumors\nD4\nHippocampus\n[15]\nMRI\n36x50x35\n(1, 1, 1)\n2\n2.710−2\n260\nanterior and posterior\nhippocampus\nD5\nProstate\n[15]\nMRI\n(T2, ADC)\n20x320x319\n(3.6, 0.62, 0.62)\n2\n5.410−3\n32\nperipheral and\ntransition zone\nD6\nLung\n[15]\nCT\n252x512x512\n(1.24, 0.79, 0.79)\n1\n3.910−4\n63\nlung nodules\nD7\nPancreas\n[15]\nCT\n93x512x512\n(2.5, 0.80, 0.80)\n2\n2.010−3\n282\npancreas, pancreas\ncancer\nD8\nHepaticVessel\n[15]\nCT\n49x512x512\n(5, 0.80, 0.80)\n2\n1.110−3\n303\nhepatic vessels,\ntumors\nD9\nSpleen\n[15]\nCT\n90x512x512\n(5, 0.79, 0.79)\n1\n4.710−3\n41\nspleen\nD10\nColon\n[15]\nCT\n95x512x512\n(5, 0.78, 0.78)\n1\n5.610−4\n126\ncolon cancer\nD11\nAbdOrgSeg\n[12]\nCT\n128x512x512\n(3, 0.76, 0.76)\n13\n4.410−3\n30\n13 abdominal\norgans\nD12\nPromise\n[13]\nMRI\n24x320x320\n(3.6, 0.61, 0.61)\n1\n2.010−2\n50\nprostate\nD13\nACDC\n[1]\ncine MRI\n9x256x216\n(10, 1.56, 1.56)\n3\n1.210−2\n200\n(100x2) *\nleft ventricle, right\nventricle,\nmyocardium\nD14\nLiTS **\n[2]\nCT\n432x512x512\n(1, 0.77, 0.77)\n2\n2.610−2\n131\nliver, liver tumors\nD15\nMSLesion\n[4]\nMRI (FLAIR,\nMPRAGE, PD,\nT2)\n137x180x137\n(1, 1, 1)\n1\n1.710−3\n42\n(21x2) *\nmultiple sclerosis\nlesions\nD16\nCHAOS\n[11]\nMRI\n30x204x256\n(9, 1.66, 1.66)\n4\n3.310−2\n60\n(20 + 20x2) *\nliver, spleen, left and\nright kidney\nD17\nKiTS\n[7]\nCT\n107x512x512\n(3, 0.78, 0.78)\n2\n7.510−3\n206\nkidney, kidney\ntumor\nD18\nSegTHOR\n[16]\nCT\n178x512x512\n(2.5, 0.98, 0.98)\n4\n4.610−4\n40\nheart, aorta,\nesophagus, trachea\nD19\nCREMI\n[6]\nElectron\nMicroscopy\n125x1250x1250\n(40, 4, 4)\n1\n5.210−3\n3\nsynaptic clefts\n* multiple annotated examples per training case\n** almost identical to Decathlon Liver; Decathlon changed the training cases and test set slightly\nTable A.1: Overview over the challenge datasets used in this manuscript.\n23\n\nB\nnnU-Net Design Principles\nHere we present a brief overview of the design principles of nnU-Net on a conceptual level. Please\nrefer to the online methods for a more detailed information on how these guidelines are implemented.\nB.1\nBlueprint Parameters\n• Architecture Design decisions:\n– U-Net like architectures enable state of the art segmentation when the pipeline is\nwell-conﬁgured. According to our experience, sophisticated architectural variations are\nnot required to achieve state of the art performance.\n– Our architectures only use plain convolutions, instance normalization and Leaky nonlinearities. The order of operations in each computational block is conv - instance norm\n- leaky ReLU.\n– We use two computational blocks per resolution stage in both encoder and decoder.\n– Downsampling is done with strided convolutions (the convolution of the ﬁrst block of\nthe new resolution has stride >1), upsampling is done with convolutions transposed.\nWe should note that we did not observe substantial disparities in segmentation accuracy\nbetween this approach and alternatives (e.g. max pooling, bi/trilinear upsampling).\n• Selecting the best U-Net conﬁguration: It is difﬁcult to estimate which U-Net conﬁguration\nperforms best on what dataset. To address this, nnU-Net designs three separate conﬁgurations\nand automatically chooses the best one based on cross-validation (see inferred parameters).\nPredicting which conﬁgurations should be trained on which dataset is a future research\ndirection.\n– 2D U-Net: Runs on full resolution data. Expected to work well on anisotropic data,\nsuch as D5 (Prostate MRI) and D13 (ACDC, cine MRI) (for dataset references see\nTable A).\n– 3D full resolution U-Net: Runs on full resolution data. Patch size is limited by\navailability of GPU memory. Is overall the best performing conﬁguration (see results\nin F). For large data, however, the patch size may be too small to aggregate sufﬁcient\ncontextual information.\n– 3D U-Net cascade: Speciﬁcally targeted towards large data. First, coarse segmentation\nmaps are learned by a 3D U-Net that operates on low resolution data. These segmentation maps are then reﬁned by a second 3D U-Net that operates on full resolution\ndata.\n• Training Scheme\n– All trainings run for a ﬁxed length of 1000 epochs, where each epoch is deﬁned as 250\ntraining iterations (using the batch size conﬁgured by nnU-Net). Shorter trainings than\nthis default empirically result in diminished segmentation performance.\n– As for the opimizer, stochastic gradient descent with a high initial learning rate (0.01)\nand a large nesterov momentum (0.99) empirically provided the best results. The\nlearning rate is reduced during the training using the ’polyLR’ schedule as described in\n[5], which is an almost linear decrease to 0.\n– Data augmentation is essential to achieve state of the art performance. It is important\nto run the augmentations on the ﬂy and with associated probabilities to obtain a never\nending stream of unique examples (see Section D for details).\n24\n\n– Data in the biomedical domain suffers from class imbalance. Rare classes could end\nup being ignored because they are underrepresented during training. Oversampling\nforeground regions addresses this issue reliably. It should, however, not be overdone so\nthat the network also sees all the data variability of the background.\n– The Dice loss function is well suited to address the class imbalance, but comes with\nits own drawbacks. Dice loss optimizes the evaluation metric directly, but due to the\npatch based training, in practice merely approximates it. Furthermore, oversampling\nof classes skews the class distribution seen during training. Empirically, combining\nthe Dice loss with a cross-entropy loss improved training stability and segmentation\naccuracy. Therefore, the two loss terms are simply averaged.\n• Inference\n– Validation sets of all folds in the cross-validation are predicted by the single model\ntrained on the respective training data. The 5 models resulting from training on 5\nindividual folds are subsequently used as an ensemble for predicting test cases.\n– Inference is done patch based with the same patch size as used during training. Fully\nconvolutional inference is not recommended because it causes issues with zero-padded\nconvolutions and instance normalization.\n– To prevent stitching artifacts, adjacent predictions are done with a distance of patch_size\n/ 2. Predictions towards the border are less accurate, which is why we use Gaussian\nimportance weighting for softmax aggregation (the center voxels are weighted higher\nthen the border voxels).\nB.2\nInferred Parameters\nThese parameters are not ﬁxed across datasets, but conﬁgured on-the-ﬂy by nnU-Net according to the\ndata ﬁngerprint (low dimensional representation of dataset properties) of the task at hand.\n• Dynamic Network adaptation:\n– The network architecture needs to be adapted to the size and spacing of the input\npatches seen during training. This is necessary to ensure that the receptive ﬁeld of the\nnetwork covers the entire input.\n– We perform downsampling until the feature maps are relatively small (minimum is\n4 × 4(×4)) to ensure sufﬁcient context aggregation.\n– Due to having a ﬁxed number of blocks per resolution step in both the encoder and\ndecoder, the network depth is coupled to its input patch size. The number of convolutional layers in the network (excluding segmentation layers) is (5 ∗k + 2) where k is\nthe number of downsampling operations (5 per downsampling stems from 2 convs in\nthe encoder, 2 in the decoder plus the convolution transpose).\n– Additional loss functions are applied to all but the two lowest resolutions of the decoder\nto inject gradients deep into the network.\n– For anisotropic data, pooling is ﬁrst exclusively performed in-plane until the resolution\nmatches between the axes. Initially, 3D convolutions use a kernel size of 1 (making\nthem effectively 2D convolutions) in the out of plane axis to prevent aggregation of\ninformation across distant slices. Once an axes becomes too small, downsampling is\nstopped individually for this axis.\n• Conﬁguration of the input patch size:\n25\n\n– Should be as large as possible while still allowing a batch size of 2 (under a given GPU\nmemory constraint). This maximizes the context available for decision making in the\nnetwork.\n– Aspect ratio of patch size follows the median shape (in voxels) of resampled training\ncases.\n• Batch size:\n– Batch size is conﬁgured with a minimum of 2 to ensure robust optimization, since\nnoise in gradients increases with fewer sample in the minibatch.\n– If GPU memory headroom is available after patch size conﬁguration, the batch size is\nincreased until GPU memory is maxed out.\n• Target spacing and resampling:\n– For isotropic data, the median spacing of training cases (computed independently\nfor each axis) is set as default. Resampling with third order spline (data) and linear\ninterpolation (one hot encoded segmentation maps such as training annotations) give\ngood results.\n– For anisotropic data, the target spacing in the out of plane axis should be smaller than\nthe median, resulting in higher resolution in order to reduce resampling artifacts. To\nachieve this we set the target spacing as the 10th percentile of the spacings found for\nthis axis in the training cases. Resampling across the out of plane axis is done with\nnearest neighbor for both data and one-hot encoded segmentation maps.\n• Intensity normalization:\n– Z-score per image (mean substraction and division by standard deviation) is a good\ndefault.\n– We deviate from this default only for CT images, where a global normalization scheme\nis determined based on the intensities found in foreground voxels across all training\ncases.\nB.3\nEmpirical Parameters\nSome parameters cannot be inferred by simply looking at the dataset ﬁngerprint of the training cases.\nThese are determined empirically by monitoring validation performance after training.\n• Model selection: While the 3D full resolution U-Net shows overall best performance,\nselection of the best model for a speciﬁc task at hand can not be predicted with perfect\naccuracy. Therefore, nnU-Net generates three U-Net conﬁgurations and automatically picks\nthe best performing method (or ensemble of methods) after cross-validation.\n• Postprocessing: Often, particularly in medical data, the image contains only one instance\nof the target structure. This prior knowledge can often be exploited by running connected\ncomponent analysis on the predicted segmentation maps and removing all but the largest\ncomponent. Whether to apply this postprocessing is determined by monitoring validation\nperformance after cross-validation. Speciﬁcally, postprocessing is triggered for individual\nclasses where the Dice score is improved by removing all but the largest component.\n26\n\nFigure C.1: Network architectures generated by nnU-Net for the ACDC dataset (D13)\nC\nAnalysis of exemplary nnU-Net-generated pipelines\nIn this section we brieﬂy introduce the pipelines generated by nnU-Net for D13 (ACDC) and D14\n(LiTS) to create an intuitive understanding of nnU-Nets design principles and the motivation behind\nthem.\nC.1\nACDC\nFigure C.1 provides a summary of the pipelines that were automatically generated by nnU-Net for\nthis dataset.\nDataset Description\nThe Automated Cardiac Diagnosis Challenge (ACDC) [1] was hosted by\nMICCAI in 2017. Since then it is running as an open challenge with data and current leaderboard\navailable at https://acdc.creatis.insa-lyon.fr. In the segmentation part of the challenge,\nparticipating teams were asked to generate algorithms for segmenting the right ventricle, the left\nmyocardium and the left ventricular cavity from cine MRI. For each patient, reference segmentations\nfor two time steps within the cardiac cycle were provided. With 100 training patients, this amounts to\na total of 200 annotated images. One key property of cine MRI is that slice acquisition takes place\nacross multiple cardiac cycles and breath holds. This results in a limited number of slices and thus a\nlow out of plane resolution as well as the possibility for slice misalignments. Figure C.1 provides a\nsummary of the pipelines that were automatically generated by nnU-Net for this dataset. The typical\n27\n\nimage shape (here the median image size is computed for each axis independently) is 9 × 237 × 256\nvoxels at a spacing of 10 × 1.56 × 1.56 mm.\nIntensity Normalization\nWith the images being MRI, nnU-Net normalizes all images individually\nby subtracting their mean and dividing by their standard deviation.\n2D U-Net\nAs target spacing for the in-plane resolution, 1.56 × 1.56 mm is determined. This is\nidentical for the 2D and the 3D full resolution U-Net. Due to the 2D U-Net operating on slices only,\nthe out of plane resolution for this conﬁguration is not altered and remains heterogeneous within the\ntraining set. The 2D U-Net is conﬁgured as described in the Online Methods 4 to have a patch size of\n256 × 224 voxels, which fully covers the typical image shape after in-plane resampling (237 × 208).\n3D U-Net The size and spacing anisotropy of this dataset causes the out-of-plane target spacing\nof the 3D full resolution U-Net to be selected as 5mm, corresponding to the 10th percentile of the\nspacings found in the training cases. In datasets such as ACDC, the segmentation contour can change\nsubstantially between slices due to the large slice to slice distance. Choosing the target spacing\nto be lower results in more images that are upsampled for U-Net training and then downsampled\nfor the ﬁnal segmentation export. Preferring this variant over the median causes more images to\nbe downsampled for training and then upsampled for segmentation export and therefore reduces\ninterpolation artifacts substantially. Also note that resampling the out of plane axis is done with\nnearest neighbor interpolation.The median image shape after resampling for the 3D full resolution\nU-Net is 18 × 237 × 208 voxels. As described in the Online Methods 4 nnU-Net conﬁgures a patch\nsize of 20 × 256 × 224 for network training, which ﬁts into the memory budget with a batch size of 3.\nNote how the convolutional kernel sizes in the 3D U-Net start with (1 × 3 × 3) which is effectively a\n2D convolution for the initial layers (see also Figure C.1). The reasoning behind this is that due to the\nlarge discrepancy in voxel spacing, too many changes are expected across slices and the aggregation\nof imaging information may therefore not be beneﬁcial. Similarly, pooling is done in-plane only\n(conv kernel stride (1, 2, 2)) until the spacing between in-plane and out-of-plane axes are within a\nfactor of 2. Only after the spacings approximately match the pooling and the convolutional kernel\nsizes become isotropic.\n3D U-Net\ncascade Since the 3D U-Net already covers the whole median image shape, the U-Net\ncascade is not necessary and therefore omitted.\nTraining and Postprocessing\nDuring training, spatial augmentations for the 3D U-Net (such as\nscaling and rotation) are done in-plane only to prevent resampling of imaging information across\nslices which would cause interpolation artifacts. Each U-Net conﬁguration is trained in a ﬁve-fold\ncross-validation on the training cases. Note that we interfere with the splits in order to ensure that\npatients are properly stratiﬁed (since there are two images per patient). Thanks to the cross-validation,\nnnU-Net can use the entire training set for validation and ensembling. To this end, the validation splits\nof each of the ﬁve fold are aggregated. nnU-Net evaluates the performance (ensemble of models or\nsingle conﬁguration) by averaging the Dice scores over all foreground classes and cases, resulting in a\nsingle scalar value. Detailed results are omitted here for brevity (they are presented in Supplementary\nInformation F). Based on this evaluation scheme, the 2D U-Net obtains a score of 0.9165, the 3D full\nresolution a score of 0.9181 and the ensemble of the two a score of 0.9228. Therefore the ensemble\nis selected for predicting the test cases. Postprocessing is conﬁgured on the segmentation maps of\nthe ensemble. Removing all but the largest connected component was found beneﬁcial for the right\nventricle and the left ventricular cavity.\n28\n\nC.2\nLiTS\nFigure C.2 provides a summary of the pipelines that were automatically generated by nnU-Net for\nthis dataset.\nDataset Description\nThe Liver and Liver Tumor Segmentation challenge (LiTS) [2] was hosted by\nMICCAI in 2017. Due to the large, high quality dataset it provides, the challenge plays an important\nrole in concurrent research. The challenge is hosted at https://competitions.codalab.org/\ncompetitions/17094. The segmentation task in LiTS is the segmentation of the liver and liver\ntumors in abdominal CT scans. The challenge provides 131 training cases with reference annotations.\nThe test set has a size of 70 cases and the reference annotations are known only to the challenge\norganizers. The median image shape of the training cases is 432 × 512 × 512 voxels with a\ncorresponding voxel spacing of 1 × 0.77 × 0.77 mm.\nIntensity Normalization\nVoxel intensities in CT scans are linked to quantitative physical properties\nof the tissue. The intensities are therefore expected to be consistent between scanners. nnU-Net\nleverages this consistency by applying a global intensity normalization scheme (as opposed to\nACDC in Supplementary Information C.1, where cases are normalized individually using their mean\nand standard deviation). To this end, nnU-Net extracts intensity information as part of the dataset\nﬁngerprint: the intensities of the voxels belonging to any of the foreground classes (liver and liver\ntumor) are collected across all training cases. Then, the mean and standard deviations of these values\nas well as their 0.5 and 99.5 percentiles are computed. Subsequently, all images are normalized by\nclipping them to the 0.5 and 99.5 percentiles, followed by subtraction of the global mean and division\nby the global standard deviation.\n2D U-Net\nThe target spacing for the 2D U-Net is determined to be NA × 0.77 × 0.77 mm, which\ncorresponds to the median voxel spacing encountered in the training cases. Note that the 2D U-Net\noperates on slices only, so the out of plane axis is left untouched. Resampling the training cases\nresults in a median image shape of NA × 512 × 512 voxels (we indicate by NA that this axis is not\nresampled). Since this is the median shape, cases in the training set can be smaller or larger than that.\nThe 2D U-Net is conﬁgured to have an input patch size of 512 × 512 voxels and a batch size of 12.\n3D U-Net\nThe target spacing for the 3D U-Net is determined to be 1 × 0.77 × 0.77 mm,\nwhich corresponds to the median voxel spacing. Because the median spacing is nearly isotropic,\nnnU-Net does not use the 10th percentile for the out of plane axis as was the case for ACDC\n(see Supplementary Information C.1). The resampling strategy is decided on a per-image basis.\nIsotropic cases (maximum axis spacing / minimum axis spacing < 3) are resampled with third order\nspline interpolation for the image data and linear interpolation for the segmentations. Note that\nsegmentation maps are always converted into a one hot representation prior to resampling which\nis converted back to a segmentation map after the interpolation. For anisotropic images, nnU-Net\nresamples the out-of-plane axis separately, as was done in ACDC.\nAfter resampling, the median image shape is 482 × 512 × 512.\nnnU-Net prioritizes a large\npatch size over a large batch size (note that these are coupled under a given GPU memory budget) to\ncapture as much contextual information as possible. The 3D U-Net is thus conﬁgured to have a patch\nsize of 128 × 128 × 128 voxels and a batch size of 2, which is the minimum allowed according to\n29\n\nFigure C.2: Network architectures generated by nnU-Net for the LiTS dataset (D14)\n30\n\nnnU-Net heuristics. Since The input patches have nearly isotropic spacing, all convolutional kernel\nsizes and downsampling strides are isotropic (3 × 3 × 3 and 2 × 2 × 2, respectively).\n3D U-Net cascade\nAlthough nnU-Net prioritizes large input patches, the patch size of the 3D full\nresolution U-Net is too small to capture sufﬁcient contextual information (it only covers 1/60 of the\nvoxels of the median image shape after resampling). This can cause misclassiﬁcations of voxels\nbecause the patches are too ‘zoomed in’, making for instance the distinction between the spleen\nand the liver particularly hard. The 3D U-Net cascade is designed to tackle this problem by ﬁrst\ntraining a 3D U-Net on downsampled data and then reﬁning the low-resolution segmentation output\nwith a second U-Net that operates as full resolution. Using the process described in the Online\nMethods 4 as well as Figure E.1 b), the target spacing for the low resolution U-Net is determined to\nbe 2.47 × 1.9 × 1.9 mm, resulting in a median image shape of 195 × 207 × 207 voxels. The 3D low\nresolution operates on 128 × 128 × 128 patches with a batch size of 2. Note that while this setting is\nidentical to the 3D U-Net conﬁguration here, this is not necessarily the case for other datasets. If the\n3D full resolution U-Net data was anisotropic, nnU-Net would prioritize to downsample the higher\nresolution axes ﬁrst resulting in a deviating network architecture, patch size and batch size. After\nﬁve-fold cross-validation of the 3D low resolution U-Net, the segmentation maps of the respective\nvalidation sets are upsampled to the target spacing of the 3D full resolution U-Net. The full resolution\nU-Net of the cascade (which has an identical conﬁguration to the regular 3D full resolution U-Net) is\nthen trained to reﬁne the coarse segmentation maps and correct any errors it encounters. This is done\nby concatenating a one hot encoding of the upsampled segmentations to the input of the network.\nTraining and Postprocessing\nAll network conﬁgurations are trained as ﬁve fold cross-validation.\nnnU-Net again evaluates all conﬁgurations by computing the average Dice score across all foreground\nclasses, resulting in a scalar metric per conﬁguration. Based on this evaluation scheme, the scores are\n0.7625 for the 2D U-Net, 0.8044 for the 3D full resolution U-Net, 0.7796 for the 3D low resolution\nU-Net and 0.8017 for the full resolution 3D U-Net of the cascade. The best combination of two\nmodels was identiﬁed as the one between the low and full resolution U-Nets with a score of 0.8111.\nPostprocessing is conﬁgured on the segmentation maps of this ensemble. Removing all but the largest\nconnected component was found beneﬁcial for the combined foreground region (union of liver and\nliver tumor label) as well as for the liver label alone, as both resulted in small performance gains\nwhen empirically testing it on the training data.\nD\nDetails on nnU-Net’s Data Augmentation\nA variety of data augmentation techniques is applied during training. All augmentations are computed\non the ﬂy on the CPU using background workers. The data augmentation pipeline is implemented\nwith the publicly available batchgenerators framework 2. nnU-Net does not vary the parameters of\nthe data augmentation pipeline between datasets.\nSampled patches are initially larger than the patch size used for training.\nThis results in\nless out of boundary values (here 0) being introduced during data augmentation when rotation and\nscaling is applied. As a part of the rotation and scaling augmentation, patches are center-cropped to\nthe ﬁnal target patch size. To ensure that the borders of original images appear in the ﬁnal patches,\npreliminary crops may initially extend outside the boundary of the image.\n2https://github.com/MIC-DKFZ/batchgenerators\n31\n\nSpatial augmentations (rotation, scaling, low resolution simulation) are applied in 3D for\nthe 3D U-Nets and applied in 2D when training the 2D U-Net or a 3D U-Net with anisotropic patch\nsize. A patch size is considered anisotropic if the largest edge length of the patch size is at least three\ntimes larger than the smallest.\nTo increase the variability in generated patches, most augmentations are varied with parameters drawn randomly from predeﬁned ranges. In this context, x ∼U(a, b) indicates that x was\ndrawn from a uniform distribution between a and b. Furthermore, all augmentations are applied\nstochastically according to a predeﬁned probability.\nThe following augmentations are applied by nnU-Net (in the given order):\n1. Rotation and Scaling. Scaling and rotation are applied together for improved speed of\ncomputation. This approach reduces the amount of required data interpolations to one.\nScaling and rotation are applied with a probability of 0.2 each (resulting in probabilities of\n0.16 for only scaling, 0.16 for only rotation and 0.08 for both being triggered). If processing\nisotropic 3D patches, the angles of rotation (in degrees) αx, αy and αz are each drawn\nfrom U(−30, 30). If a patch is anisotropic or 2D, the angle of rotation is sampled from\nU(−180, 180). If the 2D patch size is anisotropic, the angle is sampled from U(−15, 15).\nScaling is implemented via multiplying coordinates with a scaling factor in the voxel grid.\nThus, scale factors smaller than one result in a \"zoom out\" effect while values larger one\nresult in a \"zoom in\" effect. The scaling factor is sampled from U(0.7, 1.4) for all patch\ntypes.\n2. Gaussian Noise. Zero centered additive Gaussian noise is added to each voxel in the sample\nindependently. This augmentation is applied with a probability of 0.15. The variance of the\nnoise is drawn from U(0, 0.1) (note that the voxel intensities in all samples are close to zero\nmean and unit variance due to intensity normalization).\n3. Gaussian Blur. Blurring is applied with a probability of 0.2 per sample. If this augmentation\nis triggered in a sample, blurring is applied with a probability of 0.5 for each of the\nassociated modalities (resulting in a combined probability of only 0.1 for samples with a\nsingle modality). The width (in voxels) of the Gaussian kernel σ is sampled from U(0.5, 1.5)\nindependently for each modality.\n4. Brightness. Voxel intensities are multiplied by x ∼U(0.7, 1.3) with a probability of 0.15.\n5. Contrast. Voxel intensities are multiplied by x ∼U(0.65, 1.5) with a probability of 0.15.\nFollowing multiplication, the values are clipped to their original value range.\n6. Simulation of low resolution. This augmentation is applied with a probability of 0.25 per\nsample and 0.5 per associated modality. Triggered modalities are downsampled by a factor\nof U(1, 2) using nearest neighbor interpolation and then sampled back up to their original\nsize with cubic interpolation. For 2D patches or anisotropic 3D patches, this augmentation\nis applied only in 2D leaving the out of plane axis (if applicable) in its original state.\n7. Gamma augmentation. This augmentation is applied with a probability of 0.15. The\npatch intensities are scaled to a factor of [0, 1] of their respective value range. Then, a\nnonlinear intensity transformation is applied per voxel: inew = iγ\nold with γ ∼U(0.7, 1.5).\nThe voxel intensities are subsequently scaled back to their original value range. With a\n32\n\nprobability of 0.15, this augmentation is applied with the voxel intensities being inverted\nprior to transformation: (1 −inew) = (1 −iold)γ.\n8. Mirroring. All patches are mirrored with a probability of 0.5 along all axes.\nFor the full resolution U-Net of the U-net cascade, nnU-Net additionally applies the following\naugmentations to the segmentation masks generated by the low resolution 3D U-net. Note that the\nsegmentations are stored as one hot encoding.\n1. Binary Operators. With probability 0.4, a binary operator is applied to all labels in the\npredicted masks. This operator is randomly chosen from [dilation, erosion, opening, closing].\nThe structure element is a sphere with radius r ∼U(1, 8). The operator is applied to the\nlabels in random order. Hereby, the one hot encoding property is retained. Dilation of one\nlabel, for example, will result in removal of all other labels in the dilated area.\n2. Removal of Connected Components. With probability 0.2, connected components that\nare smaller than 15% of the patch size are removed from the one hot encoding.\nE\nNetwork Architecture Conﬁguration\nFigure E.1 serves as a visual aid for the iterative process of architecture conﬁguration described in\nthe online methods.\nF\nSummary of nnU-Net Challenge Participations\nIn this section we provide details of all challenge participations.\nIn some participations, manual intervention regarding the format of input data or the cross-validation\ndata splits was required for compatibility with nnU-Net. For each dataset, we disclose all manual\ninterventions in this section. The most common cause for manual intervention was training cases that\nwere related to each other (such as multiple time points of the same patient) and thus required to be\nseparated for mutual exclusivity between data splits. A detailed description of how to perform this\nintervention is further provided along with the source code.\nFor each dataset, we run all applicable nnU-Net conﬁgurations (2D, 3D fullres, 3D lowres,\n3D cascade) in 5-fold cross-validation. All models are trained from scratch without pretraining and\ntrained only on the provided training data of the challenge without external training data. Note that\nother participants may be using external data in some competitions. For each dataset, nnU-Net\nsubsequently identiﬁes the ideal conﬁguration(s) based on cross-validation and ensembling. Finally,\nThe best conﬁguration is used to predict the test cases.\nThe pipeline generated by nnU-Net is provided for each dataset in the compact representation described in Section F.2. We furthermore provide a table containing detailed cross-validation as\nwell as test set results.\nAll leaderboards were last accessed on December 12th, 2019.\n33\n\nFigure E.1: Workﬂow for network architecture conﬁguration. a) the conﬁguration of a U-Net\narchitecture given an input patch size and corresponding voxel spacing. Due to discontinuities in\nGPU memory consumption (due to changes in number of pooling operations and thus network depth),\nthe architecture conﬁguration cannot be solved analytically. b) Conﬁguration of the 3D low resolution\nU-Net of the U-Net cascade. The input patch size of the 3D lowres U-Net must cover at least 1/4 of\nthe median shape of the resampled trainig cases to ensure sufﬁcient contextual information. Higher\nresolution axes are downsampled ﬁrst, resulting in a potentially different aspect ratio of the data\nrelative to the full resolution data. Due to the patch size following this aspect ratio, the network\narchitecture of the low resolution U-Net may differ from the full resolution U-Net. This requires\nreconﬁguration of the network architecture as depicted in a) for each iteration. All computations are\nbased on memory consumption estimates resulting in fast computation times (sub 1s for conﬁguring\nall network architectures).\nF.1\nChallenge Inclusion Criteria\nWhen selecting challenges for participation, our goal was to apply nnU-Net to as many different\ndatasets as possible to demonstrate its robustness and ﬂexibility. We applied the following criteria to\nensure a rigorous and sound testing environment:\n1. The task of the challenge is semantic segmentation in any 3D imaging modality with images\nof any size.\n2. Training cases are provided to the challenge participants.\n3. Test cases are separate, with the ground truth not being available to the challenge participants.\n4. Comparison to results from other participants is possible (e.g. through standardized evaluation with an online platform and a public leaderboard).\n34\n\nFigure F.1: Decoding the architecture. We provide all generated architectures in a compact representation from which they can be fully reconstructed if desired. The architecture displayed here can be\nrepresented by means of kernel sizes [[1, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]] and\nstrides [[1, 2, 2], [2, 2, 2], [2, 2, 2], [1, 2, 2], [1, 2, 2]] (see description in the text).\nThe competitions outlined below are the ones who qualiﬁed under these criteria and were thus selected\nfor evaluation of nnU-Net. To our knowledge, CREMI 3 is the only competition from the biological\ndomain that meets these criteria.\nF.2\nCompact Architecture Representation\nIn the following sections, network architectures generated by nnU-Net will be presented in a\ncompact representation consisting of two lists: one for the convolutional kernel sizes and one for\nthe downsampling strides. As we describe in this section, this representation can be used to fully\nreconstruct the entire network architecture. The condensed representation is chosen to prevent an\nexcessive amount of ﬁgures.\nFigure F.2 exemplary shows the 3D full resolution U-Net for the ACDC dataset (D13).\nThe architecture has 6 resolution stages. Each resolution stage in both encoder and decoder consists\nof two computational blocks. Each block is a sequence of (conv - instance norm - leaky ReLU), as\ndescribed in 4. In this ﬁgure, one such block is represented by an outlined blue box. Within each\nbox, the stride of the convolution is indicated by the ﬁrst three numbers (1,1,1 for the uppermost left\nbox) and the kernel size of the convolution is indicated by the second set of numbers (1,3,3 for the\nuppermost left box). Using this information, along with the template with which our architectures are\ndesigned, we can fully describe the presented architecture with the following lists:\n• Convolutional Kernel Sizes: The kernel sizes of this architecture are [[1, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]]. Note that this list contains 6 elements, matching the\n6 resolutions encountered in the encoder. Each element in this list gives the kernel size of\nthe convolutional layers at this resolution (here this is three digits due to the convolutions\nbeing three dimensional). Within one resolution, both blocks use the same kernel size. The\nconvolutions in the decoder mirror the encoder (dropping the last entry in the list due to the\nbottleneck).\n3https://cremi.org/leaderboard/\n35\n\n• Downsampling strides: The strides for downsampling here are [[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]. Each downsampling step in the encoder is represented by one entry. A\nstride of 2 results in a downsampling of factor 2 along that axis which a stride of 1 leaves\nthe size unchanged. Note how the stride initially is [1, 2, 2] due to the spacing discrepancy.\nThis changes the initial spacing of 5 × 1.56 × 1.56 mm to a spacing of 5 × 3.12 × 3.12 mm\nin the second resolution step. The downsampling strides only apply to the ﬁrst convolution\nof each resolution stage in the encoder. The second convolution always has a stride of [1,\n1, 1]. Again, the decoder mirrors the encoder, but the stride is used as output stride of the\nconvolution transposed (resulting in appropriate upscaling of feature maps). Outputs of all\nconvolutions transposed have the same shape as the skip connection originating from the\nencoder.\nSegmentation outputs for auxiliary losses are added to all but the two lowest resolution steps.\nF.3\nMedical Segmentation Decathlon\nChallenge summary\nThe Medical Segmentation Decathlon4 [15] is a competition that spans 10\ndifferent segmentation tasks. These tasks are selected to cover a large proportion of the dataset\nvariability in the medical domain. The overarching goal of the competition was to encourage\nresearchers to develop algorithms that can work with these datasets out of the box without manual\nintervention. Each of the tasks comes with respective training and test data. A detailed description of\ndatasets can be found on the challenge homepage. Originally, the challenge was divided into two\nphases: In phase I, 7 datasets were provided to the participants for algorithm development. In phase\nII, the algorithms were applied to three additional and previously unseen datasets without further\nchanges. Challenge evaluation was performed for the two phases individually and winners were\ndetermined based on their performance on the test cases.\nInitial version of nnU-Net\nA preliminary version of nnU-Net was developed as part\nof our entry in this competition, where it achieved the ﬁrst rank in both phases (see\nhttp://medicaldecathlon.com/results.html).\nWe subsequently made the respective\nchallenge report available on arXiv [10].\nnnU-Net has since been reﬁned using all ten tasks of the Medical Segmentation Decathlon.\nThe current version of nnU-Net as presented in this publication was again submitted to the open\nleaderboard (https://decathlon-10.grand-challenge.org/evaluation/results/), and\nachieved the ﬁrst rank outperforming the initial nnU-Net as well as other methods that held the state\nof the art since the original competition [17].\nApplication of nnU-Net to the Medical Segmentation Decathlon\nnnU-Net was applied to all ten\ntasks of the Medical Segmentation Decathlon without any manual intervention.\nBrainTumour (D1)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n4http://medicaldecathlon.com/\n36\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\nMedian image shape at\ntarget spacing:\nNA x 169 x 138\n138 169 138\nPatch size:\n192 x 160\n128 x 128 x 128\nBatch size:\n107\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.1: Network conﬁgurations generated by nnU-Net for the BrainTumour dataset from the\nMedical Segmentation Decathlon (D1). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\nedema\nnon-enhancing tumor\nenhancing tumour\nmean\n2D\n0.7957\n0.5985\n0.7825\n0.7256\n3D_fullres *\n0.8101\n0.6199\n0.7934\n0.7411\nBest Ensemble\n0.8106\n0.6179\n0.7926\n0.7404\nPostprocessed\n0.8101\n0.6199\n0.7934\n0.7411\nTest set\n0.68\n0.47\n0.68\n0.61\nTable F.2: Decathlon BrainTumour (D1) results. Note that all reported Dice scores (except the test\nset) were computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\") Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nHeart (D2)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.25 x 1.25\n1.37 x 1.25 x 1.25\nMedian image shape at\ntarget spacing:\nNA x 320 x 232\n115 x 320 x 232\nPatch size:\n320 x 256\n80 x 192 x 160\nBatch size:\n40\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 1]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], ]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.3: Network conﬁgurations generated by nnU-Net for the Heart dataset from the Medical\nSegmentation Decathlon (D2). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\n37\n\nleft atrium\nmean\n2D\n0.9090\n0.9090\n3D_fullres *\n0.9328\n0.9328\nBest Ensemble\n0.9268\n0.9268\nPostprocessed\n0.9329\n0.9329\nTest set\n0.93\n0.93\nTable F.4: Decathlon Heart (D2) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nLiver (D3)\nNormalization: Clip to [−17, 201], then subtract 99.40 and ﬁnally divide by 39.36.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.7676 x 0.7676\n1 x 0.7676 x 0.7676\n2.47 x 1.90 x 1.90\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n482 x 512 x 512\n195 x 207 x 207\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.5: Network conﬁgurations generated by nnU-Net for the Liver dataset from the Medical\nSegmentation Decathlon (D3). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\nliver\ncancer\nmean\n2D\n0.9547\n0.5637\n0.7592\n3D_fullres\n0.9571\n0.6372\n0.7971\n3D_lowres\n0.9563\n0.6028\n0.7796\n3D cascade\n0.9600\n0.6386\n0.7993\nBest Ensemble*\n0.9613\n0.6564\n0.8088\nPostprocessed\n0.9621\n0.6600\n0.8111\nTest set\n0.96\n0.76\n0.86\nTable F.6: Decathlon Liver (D3) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D low\nresolution U-Net and the 3D full resolution U-Net.\nHippocampus (D4)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n38\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\nMedian image shape at\ntarget spacing:\nNA x 50 x 35\n36 x 50 x 35\nPatch size:\n56 x 40\n40 x 56 x 40\nBatch size:\n366\n9\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3],\n[3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\nTable F.7: Network conﬁgurations generated by nnU-Net for the Hippocampus dataset from the\nMedical Segmentation Decathlon (D4). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\nAnterior\nPosterior\nmean\n2D\n0.8787\n0.8595\n0.8691\n3D_fullres *\n0.8975\n0.8807\n0.8891\nBest Ensemble\n0.8962\n0.8790\n0.8876\nPostprocessed\n0.8975\n0.8807\n0.8891\nTest set\n0.90\n0.89\n0.895\nTable F.8: Decathlon Hippocampus (D4) results. Note that all reported Dice scores (except the test\nset) were computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nProstate (D5)\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.62 x 0.62\n3.6 x 0.62 x 0.62\nMedian image shape at\ntarget spacing:\nNA x 320 x 319\n20 x 320 x 319\nPatch size:\n320 x 320\n20 x 320 x 256\nBatch size:\n32\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2]]\n[[1, 2, 2], [1, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [1, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\nTable F.9: Network conﬁgurations generated by nnU-Net for the Prostate dataset from the Medical\nSegmentation Decathlon (D5). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\n39\n\nPZ\nTZ\nmean\n2D\n0.6285\n0.8380\n0.7333\n3D_fullres\n0.6663\n0.8410\n0.7537\nBest Ensemble *\n0.6611\n0.8575\n0.7593\nPostprocessed\n0.6611\n0.8577\n0.7594\nTest set\n0.77\n0.90\n0.835\nTable F.10: Decathlon Prostate (D5) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nLung (D6)\nNormalization: Clip to [−1024, 325], then subtract −158.58 and ﬁnally divide by 324.70.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.79 x 0.79\n1.24 x 0.79 x 0.79\n2.35 x 1.48 x 1.48\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n252 x 512 x 512\n133 x 271 x 271\nPatch size:\n512 x 512\n80 x 192 x 160\n80 x 192 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.11: Network conﬁgurations generated by nnU-Net for the Lung dataset from the Medical\nSegmentation Decathlon (D6). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\ncancer\nmean\n2D\n0.4989\n0.4989\n3D_fullres\n0.7211\n0.7211\n3D_lowres\n0.7109\n0.7109\n3D cascade\n0.6980\n0.6980\nBest Ensemble*\n0.7241\n0.7241\nPostprocessed\n0.7241\n0.7241\nTest set\n0.74\n0.74\nTable F.12: Decathlon Lung (D6) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D low\nresolution U-Net and the 3D full resolution U-Net.\nPancreas (D7)\nNormalization: Clip to [−96.0, 215.0], then subtract 77.99 and ﬁnally divide by 75.40.\n40\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.8 x 0.8\n2.5 x 0.8 x 0.8\n2.58 x 1.29 x 1.29\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n96 x 512 x 512\n93 x 318 x 318\nPatch size:\n512 x 512\n40 x 224 x 224\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.13: Network conﬁgurations generated by nnU-Net for the Pancreas dataset from the Medical\nSegmentation Decathlon (D7). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\npancreas\ncancer\nmean\n2D\n0.7738\n0.3501\n0.5619\n3D_fullres\n0.8217\n0.5274\n0.6745\n3D_lowres\n0.8118\n0.5286\n0.6702\n3D cascade\n0.8101\n0.5380\n0.6741\nBest Ensemble *\n0.8214\n0.5428\n0.6821\nPostprocessed\n0.8214\n0.5428\n0.6821\nTest set\n0.82\n0.53\n0.675\nTable F.14: Decathlon Pancreas (D7) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D full\nresolution U-Net and the 3D U-Net cascade.\nHepatic Vessel (D8)\nNormalization: Clip to [−3, 243], then subtract 104.37 and ﬁnally divide by 52.62.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.8 x 0.8\n1.5 x 0.8 x 0.8\n2.42 x 1.29 x 1.29\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n150 x 512 x 512\n93 x 318 x 318\nPatch size:\n512 x 512\n64 x 192 x 192\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.15: Network conﬁgurations generated by nnU-Net for the HepaticVessel dataset from the\nMedical Segmentation Decathlon (D8). For more information on how to decode downsampling\nstrides and kernel sizes into an architecture, see F.2\n41\n\nVessel\nTumour\nmean\n2D\n0.6180\n0.6359\n0.6269\n3D_fullres\n0.6456\n0.7217\n0.6837\n3D_lowres\n0.6294\n0.7079\n0.6687\n3D cascade\n0.6424\n0.7138\n0.6781\nBest Ensemble *\n0.6485\n0.7250\n0.6867\nPostprocessed\n0.6485\n0.7250\n0.6867\nTest set\n0.66\n0.72\n0.69\nTable F.16: Decathlon HepaticVessel (D8) results. Note that all reported Dice scores (except the test\nset) were computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D full\nresolution U-Net and the 3D low resolution U-Net.\nSpleen (D9)\nNormalization: Clip to [−41, 176], then subtract 99.29 and ﬁnally divide by 39.47.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.79 x 0.79\n1.6 x 0.79 x 0.79\n2.77 x 1.38 x 1.38\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n187 x 512 x 512\n108 x 293 x 293\nPatch size:\n512 x 512\n64 x 192 x 160\n64 x 192 x 192\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.17: Network conﬁgurations generated by nnU-Net for the Spleen dataset from the Medical\nSegmentation Decathlon (D9). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\nspleen\nmean\n2D\n0.9492\n0.9492\n3D_fullres\n0.9638\n0.9638\n3D_lowres\n0.9683\n0.9683\n3D cascade\n0.9714\n0.9714\nBest Ensemble *\n0.9723\n0.9723\nPostprocessed\n0.9724\n0.9724\nTest set\n0.97\n0.97\nTable F.18: Decathlon Spleen (D9) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D\nU-Net cascade and the 3D full resolution U-Net.\nColon (D10)\nNormalization: Clip to [−30.0, 165.82], then subtract 62.18 and ﬁnally divide by 32.65.\n42\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.78 x 0.78\n3 x 0.78 x 0.78\n3.09 x 1.55 x 1.55\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n150 x 512 x 512\n146 x 258 x 258\nPatch size:\n512 x 512\n56 x 192 x 160\n96 x 160 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.19: Network conﬁgurations generated by nnU-Net for the Colon dataset from the Medical\nSegmentation Decathlon (D10). For more information on how to decode downsampling strides and\nkernel sizes into an architecture, see F.2\ncolon cancer primaries\nmean\n2D\n0.2852\n0.2852\n3D_fullres\n0.4553\n0.4553\n3D_lowres\n0.4538\n0.4538\n3D cascade *\n0.4937\n0.4937\nBest Ensemble\n0.4853\n0.4853\nPostprocessed\n0.4937\n0.4937\nTest set\n0.58\n0.58\nTable F.20: Decathlon Colon (D10) results. Note that all reported Dice scores (except the test set)\nwere computed using ﬁve fold cross-validation on the training cases. * marks the best performing\nmodel selected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see\n\"Test set\"). Note that the Dice scores for the test set are computed with the online platform and only\ntwo signiﬁcant digits are reported. Best ensemble on this dataset was the combination of the 3D\nU-Net cascade and the 3D full resolution U-Net.\nF.4\nMulti Atlas Labeling Beyond the Cranial Vault: Abdomen (D11)\nChallenge summary\nThe Multi Atlas Labeling Beyond the Cranial Vault - Abdomen Challenge5\n[12] (denoted BCV for brevity) comprises 30 CT images for training and 20 for testing. The\nsegmentation target are thirteen different organs in the abdomen.\nApplication of nnU-Net to BCV\nnnU-Net was applied to the BCV challenge without any manual\nintervention.\nNormalization: Clip to [−958, 327], then subtract 82.92 and ﬁnally divide by 136.97.\n5https://www.synapse.org/Synapse:syn3193805/wiki/217752\n43\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.76 x 0.76\n3 x 0.76 x 0.76\n3.18 x 1.60 x 1.60\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n148 x 512 x 512\n140 x 243 x 243\nPatch size:\n512 x 512\n48 x 192 x 192\n80 x 160 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.21: Network conﬁgurations generated by nnU-Net for the BCV challenge (D131. For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\n1\n2\n3\n4\n5\n6\n7\n8\n2D\n0.8860\n0.8131\n0.8357\n0.6406\n0.7724\n0.9453\n0.8405\n0.9128\n3D_fullres\n0.9083\n0.8939\n0.8675\n0.6632\n0.7840\n0.9557\n0.8816\n0.9229\n3D_lowres\n0.9132\n0.9045\n0.9132\n0.6525\n0.7810\n0.9554\n0.8903\n0.9209\n3D cascade\n0.9166\n0.9069\n0.9137\n0.7036\n0.7885\n0.9587\n0.9037\n0.9215\nBest Ensemble *\n0.9135\n0.9065\n0.8971\n0.6955\n0.7897\n0.9589\n0.9026\n0.9248\nPostprocessed\n0.9135\n0.9065\n0.8971\n0.6959\n0.7897\n0.9590\n0.9026\n0.9248\nTest set\n0.9721\n0.9182\n0.9578\n0.7528\n0.8411\n0.9769\n0.9220\n0.9290\n9\n10\n11\n12\n13\nmean\n2D\n0.8140\n0.7046\n0.7367\n0.6269\n0.5909\n0.7784\n3D_fullres\n0.8638\n0.7659\n0.8176\n0.7148\n0.7238\n0.8279\n3D_lowres\n0.8571\n0.7469\n0.8003\n0.6688\n0.6851\n0.8223\n3D cascade\n0.8621\n0.7722\n0.8210\n0.7205\n0.7214\n0.8393\nBest Ensemble *\n0.8673\n0.7746\n0.8299\n0.7218\n0.7287\n0.8393\nPostprocessed\n0.8673\n0.7746\n0.8299\n0.7262\n0.7290\n0.8397\nTest set\n0.8809\n0.8317\n0.8515\n0.7887\n0.7674\n0.8762\nTable F.22: Multi Atlas Labeling Beyond the Cranial Vault Abdomen (D11) results. Note that all\nreported Dice scores (except the test set) were computed using ﬁve fold cross-validation on the training\ncases. Postprocessing was applied to the model marked with *. This model (incl postprocessing) was\nused for test set predictions. Note that the Dice scores for the test set are computed with the online\nplatform. Best ensemble on this dataset was the combination of the 3D U-Net cascade and the 3D\nfull resolution U-Net.\nF.5\nPROMISE12 (D12)\nChallenge summary\nThe segmentation target of the PROMISE12 challenge [13] is the prostate in\nT2 MRI images. 50 training cases with prostate annotations are provided for training. There are 30\ntest cases which need to be segmented by the challenge participants and are subsequently evaluated\non an online platform6.\nApplication of nnU-Net to PROMISE12\nnnU-Net was applied to the PROMISE12 challenge\nwithout any manual intervention.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n6https://promise12.grand-challenge.org/\n44\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.61 x 0.61\n2.2 x 0.61 x 0.61\nMedian image shape at\ntarget spacing:\nNA x 327 x 327\n39 x 327 x 327\nPatch size:\n384 x 384\n28 x 256 x 256\nBatch size:\n22\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.23: Network conﬁgurations generated by nnU-Net for the PROMISE12 challenge (D12). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\nprostate\nmean\n2D\n0.8932\n0.8932\n3D_fullres\n0.8891\n0.8891\nBest Ensemble *\n0.9029\n0.9029\nPostprocessed\n0.9030\n0.9030\nTest set\n0.9194\n0.9194\nTable F.24: PROMISE12 (D12) results. Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\").\nNote that the scores for the test set are computed with the online platform. The evaluation score of our\ntest set submission is 89.6507. The test set Dice score reported in the table was computed from the detailed submission results (Detailed results available here https://promise12.grand-challenge.\norg/evaluation/results/89044a85-6c13-49f4-9742-dea65013e971/). Best ensemble on\nthis dataset was the combination of the 2D U-Net and the 3D full resolution U-Net.\nF.6\nThe Automatic Cardiac Diagnosis Challenge (ACDC) (D13)\nChallenge summary\nThe Automatic Cardiac Diagnosis Challenge7 [1] (ACDC) comprises 100\ntraining patients and 50 test patients. The target structures are the cavity of the right ventricle,\nthe myocardium of the left ventricle and the cavity of the left ventricle. All images are cine MRI\nsequences of which the enddiastolic (ED) and endsystolic (ES) time points of the cardiac cycle were\nto be segmented. With two time instances per patient, the effective number of training/test images is\n200/100.\nApplication of nnU-Net to ACDC\nSince two time instances of the same patient were provided,\nwe manually interfered with the split for the 5-fold cross-validation of our models to ensure mutual\nexclusivity of patients between folds. A part from that, nnU-Net was applied without manual\nintervention.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n7https://acdc.creatis.insa-lyon.fr\n45\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.56 x 1.56\n5 x 1.56 x 1.56\nMedian image shape at\ntarget spacing:\nNA x 237 x 208\n18 x 237 x 208\nPatch size:\n256 x 224\n20 x 256 x 224\nBatch size:\n58\n3\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[1, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.25: Network conﬁgurations generated by nnU-Net for the ACDC challenge (D13). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nRV\nMLV\nLVC\nmean\n2D\n0.9053\n0.8991\n0.9433\n0.9159\n3D_fullres\n0.9059\n0.9022\n0.9458\n0.9179\nBest Ensemble *\n0.9145\n0.9059\n0.9479\n0.9227\nPostprocessed\n0.9145\n0.9059\n0.9479\n0.9228\nTest set\n0.9295\n0.9183\n0.9407\n0.9295\nTable F.26: ACDC results (D13). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\").\nNote that the Dice scores for the test set are computed with the online platform. The online platform\nreports the Dice scores for enddiastolic and endsystolic time points separately. We averaged these\nvalues for a more condensed presentation. Best ensemble on this dataset was the combination of the\n2D U-Net and the 3D full resolution U-Net.\nF.7\nLiver and Liver Tumor Segmentation Challenge (LiTS) (D14)\nChallenge summary\nThe Liver and Liver Tumor Segmentation challenge [3] provides 131 training\nCT images with ground truth annotations for the liver and liver tumors. 70 test images are provided\nwithout annotations. The predicted segmentation masks of the test cases are evaluated using the LiTS\nonline platform8.\nApplication of nnU-Net to LiTS\nnnU-Net was applied to the LiTS challenge without any manual\nintervention.\nNormalization: Clip to [−17, 201], then subtract 99.40 and ﬁnally divide by 39.39.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.77 x 0.77\n1 x 0.77 x 0.77\n2.47 x 1.90 x 1.90\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n482 x 512 x 512\n195 x 207 x 207\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.27: Network conﬁgurations generated by nnU-Net for the LiTS challenge (D14). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\n8https://competitions.codalab.org/competitions/17094\n46\n\nliver\ncancer\nmean\n2D\n0.9547\n0.5603\n0.7575\n3D_fullres\n0.9576\n0.6253\n0.7914\n3D_lowres\n0.9585\n0.6161\n0.7873\n3D cascade\n0.9609\n0.6294\n0.7951\nBest Ensemble*\n0.9618\n0.6539\n0.8078\nPostprocessed\n0.9631\n0.6543\n0.8087\nTest set\n0.9670\n0.7630\n0.8650\nTable F.28: LiTS results (D14). Note that all reported Dice scores (except the test set) were computed\nusing ﬁve fold cross-validation on the training cases. * marks the best performing model selected for\nsubsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test set\"). Note that\nthe Dice scores for the test set are computed with the online platform. Best ensemble on this dataset\nwas the combination of the 3D low resolution U-Net and the 3D full resolution U-Net.\nF.8\nLongitudinal multiple sclerosis lesion segmentation challenge (MSLesion) (D15)\nChallenge summary\nThe longitudinal multiple sclerosis lesion segmentation challenge [4] provides 5 training patients. For each patient, 4 to 5 images acquired at different time points are provided\n(4 patients with 4 time points each and one patient with 5 time points for a total of 21 images).\nEach time point is annotated by two different experts, resulting in 42 training annotations (on 21\nimages). The test set contains 14 patients, again with several time points each, for a total of 61 MRI\nacquisitions. Test set predictions are evaluated using the online platform9. Each train and test image\nconsists of four MRI modalities: MPRAGE, FLAIR, Proton Density, T2.\nApplication of nnU-Net to MSLesion\nWe manually interfere with the splits in the cross-validation\nto ensure mutual exclusivity of patients between folds. Each image was annotated by two different\nexperts. We treat these annotations as separate training images (of the same patient), resulting in a\ntraining set size of 2 × 21 = 42. We do not use the longitudinal nature of the scans and treat each\nimage individually during training and inference.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1 x 1\n1 x 1 x 1\nMedian image shape at\ntarget spacing:\nNA x 180 x 137\n137 x 180 x 137\nPatch size:\n192 x 160\n112 x 128 x 96\nBatch size:\n107\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2]]\n[[1, 2, 1], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.29: Network conﬁgurations generated by nnU-Net for the MSLesion challenge (D15). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\n9https://smart-stats-tools.org/lesion-challenge\n47\n\nlesion\nmean\n2D\n0.7339\n0.7339\n3D_fullres *\n0.7531\n0.7531\nBest Ensemble\n0.7494\n0.7494\nPostprocessed\n0.7531\n0.7531\nTest set\n0.6785\n0.6785\nTable F.30: MSLesion results (D15). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. * marks the best performing model\nselected for subsequent postprocessing (see \"Postprocessed\") and test set submission (see \"Test\nset\"). Note that the Dice scores for the test set are computed with the online platform based on the\ndetailed results (which are available here https://smart-stats-tools.org/sites/lesion_\nchallenge/temp/top25/nnUNetV2_12032019_0903.csv). The ranking is based on a score,\nwhich includes other metrics as well (see [4] for details). The score of our submission is 92.874. Best\nensemble on this dataset was the combination of the 2D U-Net and the 3D full resolution U-Net.\nF.9\nCombined Healthy Abdominal Organ Segmentation (CHAOS) (D16)\nChallenge summary\nThe CHAOS challenge [11] is divided into ﬁve tasks. Here we focused on\nTasks 3 (MRI Liver segmentation) and Task 5 (MRI multiorgan segmentation). Tasks 1, 2 and 4 also\nincluded the use of CT images, a modality for which plenty of public data is available (see e.g. BCV\nand LiTS challenge). To isolate the algorithmic performance of nnU-Net relative to other participants\nwe decided to only use the tasks for which a contamination with external data was unlikely. The target\nstructures of Task 5 are the liver, the spleen and the left and right kidneys. The CHAOS challenge\nprovides 20 training cases. For each training case, there is a T2 images with a corresponding ground\ntruth annotation as well as a T1 acquisition with its own, separate ground truth annotation. The T1\nacquisition has two modalities which are co-registered: T1 in-phase and T1 out-phase. Task 3 is a\nsubset of Task 5 with only the liver being the segmentation target. The 20 test cases are evaluated\nusing the online platform10.\nApplication of nnU-Net to CHAOS\nnnU-Net only supports images with a constant number of\ninput modalities. The training cases in CHAOS have either one (T2) or two (T1 in & out phase)\nmodalities. To ensure compatibility with nnU-Net we could have either duplicated the T2 image and\ntrained with two input modalities or use only one input modality and treat T1 in phase and out phase\nas separate training examples. We opted for the latter because this variant results in more (albeit\nhighly correlated) training images. With 20 training patients being provided, this approach resulted\nin 60 training images. For the cross-validation we ensure that the split is being done on patient level.\nDuring inference, nnU-Net will generate two separate predictions for T1 in and out phase which\nneed to be consolidated for test set evaluation. We achieve this by simply averaging the softmax\nprobabilities between the two to generate the ﬁnal segmentation. We train nnU-Net only for Task 5.\nBecause task 3 represents a subset of Task 5, we extract the liver from our Task 5 predictions and\nsubmit it to Task 3.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n10https://chaos.grand-challenge.org/\n48\n\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 1.66 x 1.66\n5.95 x 1.66 x 1.66\nMedian image shape at\ntarget spacing:\nNA x 195 x 262\n45 x 195 x 262\nPatch size:\n224 x 320\n40 x 192 x 256\nBatch size:\n45\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [1, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2], [1, 1, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\nTable F.31: Network conﬁgurations generated by nnU-Net for the CHAOS challenge (D16). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2.\nliver\nright kidney\nleft kidney\nspleen\nmean\n2D\n0.9132\n0.8991\n0.8897\n0.8720\n0.8935\n3D_fullres\n0.9202\n0.9274\n0.9209\n0.8938\n0.9156\nBest Ensemble *\n0.9184\n0.9283\n0.9255\n0.8911\n0.9158\nPostprocessed\n0.9345\n0.9289\n0.9212\n0.894\n0.9197\nTest set\nTable F.32: CHAOS results (D16). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. Postprocessing was applied to the\nmodel marked with *. This model (incl postprocessing) was used for test set predictions. Note that\nthe evaluation of the test set was performed with the online platform of the challenge which does not\nreport Dice scores for the individual organs. The score of our submission was 72.44 for Task 5 and\n75.10 for Task3 (see [11] for details). Best ensemble on this dataset was the combination of the 2D\nU-Net and the 3D full resolution U-Net.\nF.10\nKidney and Kidney Tumor Segmentation (KiTS) (D17)\nChallenge summary\nThe Kidney and Kidney Tumor Segmentation challenge [8] was the largest\ncompetition (in terms of number of participants) at MICCAI 2019. The target structures are the\nkidneys and kidney tumors. 210 training and 90 test cases are provided by the challenge organizers.\nThe organizers provide the data both in their original geometry (with voxel spacing varying between\ncases) as well as interpolated to a common voxel spacing. Evaluation of the test set predictions is\ndone on the online platform11.\nWe participated in the original KiTS 2019 MICCAI challenge with a manually designed\nresidual 3D U-Net. This algorithm, described in [9] obtained the ﬁrst rank in the challenge. For this\nsubmission, we did slight modiﬁcations to the original training data: Cases 15 and 37 were conﬁrmed\nto be faulty by the challenge organizers (https://github.com/neheller/kits19/issues/21)\nwhich is why we replaced their respective segmentation masks with predictions of one of our\nnetworks. We furthermore excluded cases 23, 68, 125 and 133 because we suspected labeling\nerrors in these cases as well. At the time of conducting the experiments for this publication, no\nrevised segmentation masks were provided by the challenge organizers, which is why we re-used the\nmodiﬁed training dataset for training nnU-Net.\nAfter the challenge event at MICCAI 2019, an open leaderboard was created.\nThe original\n11https://kits19.grand-challenge.org/\n49\n\nchallenge leaderboard is retained at http://results.kits-challenge.org/miccai2019/. All\nsubmissions of the original KiTS challenge were mirrored to the open leaderboard. The submission\nof nnU-Net as performed in the context of this manuscript is done on the open leaderboard, where\nmany more competitors have entered since the challenge. As presented in Figure 3, nnU-Net sets a\nnew state of the art on the open leaderboard, thus also outperforming our initial, manually optimized\nsolution.\nApplication of nnU-Net to KiTS\nSince nnU-Net is designed to automatically deal with varying\nvoxel spacings within a dataset, we chose the original, non-interpolated image data as provided by\nthe organizers and let nnU-Net deal with the homogenization of voxel spacing. nnU-Net was applied\nto the KiTS challenge without any manual intervention.\nNormalization: Clip to [−79, 304], then subtract 100.93 and ﬁnally divide by 76.90.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.78 x 0.78\n0.78 x 0.78 x 0.78\n1.99 x 1.99 x 1.99\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n525 x 512 x 512\n206 x 201 x 201\nPatch size:\n512 x 512\n128 x 128 x 128\n128 x 128 x 128\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.33: Network conﬁgurations generated by nnU-Net for the KiTS challenge (D17). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nKidney\nTumor\nmean\n2D\n0.9613\n0.7563\n0.8588\n3D_fullres\n0.9702\n0.8367\n0.9035\n3D_lowres\n0.9629\n0.8420\n0.9025\n3D cascade\n0.9702\n0.8546\n0.9124\nBest Ensemble*\n0.9707\n0.8620\n0.9163\nPostprocessed\n0.9707\n0.8620\n0.9163\nTest set\n0.8542\nTable F.34: KiTS results (D17). Note that all reported Dice scores (except the test set) were computed\nusing ﬁve fold cross-validation on the training cases. Postprocessing was applied to the model marked\nwith *. This model (incl postprocessing) was used for test set predictions. Note that the Dice scores\nfor the test set are computed with the online platform which computes the kidney Dice score based of\nthe union of the kidney and tumor labels whereas nnU-Net always evaluates labels independently,\nresulting in a missing value for kindey in the table. The reported kindey Dice by the platform (which\nis not comparable with the value computed by nnU-Net) is 0.9793. Best ensemble on this dataset was\nthe combination of the 3D U-Net cascade and the 3D full resolution U-Net.\nF.11\nSegmentation of THoracic Organs at Risk in CT images (SegTHOR) (D18)\nChallenge summary\nIn the Segmentation of THoracic Organs at Risk in CT images [16] challenge,\nfour abdominal organs (the heart, the aorta, the trachea and the esopahgus) are to be segmented in CT\n50\n\nimages. 40 training images are provided for training and another 20 images are provided for testing.\nEvaluation of the test images is done using the online platform12.\nApplication of nnU-Net to SegTHOR\nnnU-Net was applied to the SegTHOR challenge without\nany manual intervention.\nNormalization: Clip to [−986, 271], then subtract 20.78 and ﬁnally divide by 180.50.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\nNA x 0.89 x 0.89\n2.50 x 0.89 x 0.89\n3.51 x 1.76 x 1.76\nMedian image shape at\ntarget spacing:\nNA x 512 x 512\n171 x 512 x 512\n122 x 285 x 285\nPatch size:\n512 x 512\n64 x 192 x 160\n80 x 192 x 160\nBatch size:\n12\n2\n2\nDownsampling strides:\n[[2, 2], [2, 2], [2, 2], [2, 2],\n[2, 2], [2, 2], [2, 2]]\n[[1, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [2, 2, 2], [2, 2, 2]]\n[[2, 2, 2], [2, 2, 2], [2, 2, 2],\n[2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[3, 3], [3, 3], [3, 3], [3, 3],\n[3, 3], [3, 3], [3, 3], [3, 3]]\n[[1, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\n[[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3]]\nTable F.35: Network conﬁgurations generated by nnU-Net for the SegTHOR challenge (D18). For\nmore information on how to decode downsampling strides and kernel sizes into an architecture, see\nF.2\nesophagus\nheart\ntrachea\naorta\nmean\n2D\n0.8181\n0.9407\n0.9077\n0.9277\n0.8986\n3D_fullres\n0.8495\n0.9527\n0.9055\n0.9426\n0.9126\n3D_lowres\n0.8110\n0.9464\n0.8930\n0.9284\n0.8947\n3D cascade\n0.8553\n0.9520\n0.9045\n0.9403\n0.9130\nBest Ensemble*\n0.8545\n0.9532\n0.9066\n0.9427\n0.9143\nPostprocessed\n0.8545\n0.9532\n0.9083\n0.9438\n0.9150\nTest set\n0.8890\n0.9570\n0.9228\n0.9510\n0.9300\nTable F.36: SegTHOR results (D18). Note that all reported Dice scores (except the test set) were\ncomputed using ﬁve fold cross-validation on the training cases. Postprocessing was applied to the\nmodel marked with *. This model (incl postprocessing) was used for test set predictions. Note that\nthe Dice scores for the test set are computed with the online platform. Best ensemble on this dataset\nwas the combination of the 3D U-Net cascade and the 3D full resolution U-Net.\nF.12\nChallenge on Circuit Reconstruction from Electron Microscopy Images (CREMI)\n(D19)\nChallenge summary\nThe Challenge on Circuit Reconstruction from Electron Microscopy Images\nis subdivided into three tasks. The synaptic cleft segmentation task can be formulated as semantic\nsegmentation (as opposed to e.g. instance segmentation) and is thus compatible with nnU-Net. In this\ntask, the segmentation target is the cell membrane in locations where the cells are forming a synapse.\nThe dataset consists of serial section Transmission Electron Microscopy scans of the Drosophila\nmelanogaster brain. Three volumes are provided for training and another three are provided for\ntesting. Test set evaluation is done using the online platform13.\nApplication of nnU-Net to CREMI\nSince to the number of training images is lower than the\nnumber of splits, we cannot run a 5-fold cross-validation. Thus, we trained 5 model instances,\n12https://competitions.codalab.org/competitions/21145\n13https://cremi.org/\n51\n\neach of them on all three training volumes and subsequently ensembled these models for test set\nprediction. Because this training scheme leaves no validation data, selection of the best of three\nmodel conﬁgurations as performed by nnU-Net after cross-validation was not possible. Hence, we\nintervened by only conﬁguring and training the 3D full resolution conﬁguration.\nNormalization: Each image is normalized independently by subtracting its mean and dividing by its\nstandard deviation.\n2D U-Net\n3D full resolution U-Net\n3D low resolution U-Net\nTarget spacing (mm):\n40 x 4 x 4\nMedian image shape at\ntarget spacing:\n125 x 1250 x 1250\nPatch size:\n24 x 256 x256\nBatch size:\n2\nDownsampling strides:\n[[1, 2, 2], [1, 2, 2], [1, 2, 2],\n[2, 2, 2], [2, 2, 2], [1, 2, 2]]\nConvolution kernel sizes:\n[[1, 3, 3], [1, 3, 3], [1, 3, 3],\n[3, 3, 3], [3, 3, 3], [3, 3, 3],\n[3, 3, 3]]\nTable F.37: Network conﬁgurations generated by nnU-Net for the CREMI challenge (D19). For more\ninformation on how to decode downsampling strides and kernel sizes into an architecture, see F.2\nResults\nBecause our training scheme for this challenge left no validation data, a performance\nestimate as given for the other datastes is not available for CREMI. The CREMI test set is evaluated\nby the online platform. The evaluation metric is the so called CREMI score, a description of which is\navailable here https://cremi.org/metrics/. Dice scores for the test set are not reported. The\nCREMI score of our test set submission was 74.96 (lower is better).\nG\nUsing nnU-Net with limited compute resources\nReduction of computational complexity was one of the key motivations driving the design of nnU-Net.\nThe effort of running all the conﬁgurations generated by nnU-Net should be manageable for most\nusers and researchers. There are, however, some shortcuts that can be be taken in case computational\nresources are extremely scarce.\nG.1\nReducing the number of network trainings\nDepending on whether the 3D U-Net cascade is conﬁgured for a given dataset, nnU-Net requires 10\n(2D and 3D U-Net with 5 models each) or 20 (2d, 3D, 3D cascade (low resolution and high resolution\nU-Net) with 5 models each) U-Net trainings to run, each of which takes a couple of days on a single\nGPU. While this approach guarantees the best possible performance, training all models may exceed\nreasonable computation time if only a single GPU is available. Therefore, we present two strategies\nto reduce the number of total network trainings when running nnU-Net.\nManual selection of U-Net conﬁgurations\nOverall, the 3D full resolution U-Net shows the best segmentation results. Thus, this conﬁguration is\na good starting point and could simply be selected as default choice. Users can decide whether to train\nthis conﬁguration using all training cases (to train a single model) or run a ﬁve-fold cross-validation\nand ensemble the 5 resulting models for test case predictions.\nIn some scenarios, other conﬁgurations than the 3D full resolution U-Net can yield best\n52\n\nperformance. Identifying such scenarios and selecting the respective most promising conﬁguration,\nhowever, requires domain knowledge for the dataset at hand. Datasets with highly anisotropic images\n(such as D12 PROMISE12), for instance, could be best suited for running a 2D U-Net. There is,\nhowever, no guarantee for this relation (see D13 ACDC). On datasets with very large images, the\n3D U-Net cascade seems to marginally outperform the 3D full resolution U-Net (for example D11,\nD14, D17, D18, ...) because it improves the capture of contextual information. Note that this is only\ntrue if the target structure requires a large receptive ﬁeld for optimal recognition. On CREMI (D19)\nfor example, despite large image sizes, only a limited ﬁeld of view is required, because the target\nstructure are relatively small synapses that can be identiﬁed using only local information, which is\nwhy we selected the 3D full resolution U-Net for this dataset (see Section F.12).\nNot running all conﬁgurations as 5-fold cross-validation\nAnother computation shortcut is to not run all models as 5-fold cross-validation. For instance, only\none split for each conﬁguration can be run (note, however, that the 3D low resolution U-Net of\nthe cascade is required to be run as a 5-fold cross-validation in order to generate low resolution\nsegmentation maps of all training cases for the second full resolution U-net of the cascade). Even\nwhen running multiple conﬁgurations to rely on empirical selection of conﬁgurations by nnU-Net,\nthis reduces the total number of models to be trained to 2 if no cascade is conﬁgured or 8 if the\ncascade is conﬁgured (the cascade requires 6 model trainings: 5 3D low resolution U-Nets and 1 full\nresolution 3D U-Net training). nnU-Net subsequently bases selection of the best conﬁguration on this\nsingle train-val split. Note that this strategy provides less reliable performance estimates and may\nresult in sub optimal conﬁguration choices. Finally, users can decide whether they wish to re-train the\nselected conﬁguration on the entire training data or run a ﬁve-fold cross-validation for this selected\nconﬁguration. The latter is expected to result in better test set performance because the 5 models can\nbe used as an ensemble.\nG.2\nReduction of GPU memory\nnnU-Net is conﬁgured to utilize 11GB of GPU memory. This requirement is, based on our experience,\na realistic requirement for a modern deep-learning capable GPU (such as a Nvidia GTX 1080 ti\n(11GB), Nvidia RTX 2080 ti (11GB), Nvidia TitanX(p) (12GB), Nvidia P100 (12/16 GB), Nvidia\nTitan RTX (24GB), Nvidia V100 (16/32 GB), ...). We strongly recommend using nnU-Net with\nthis default conﬁguration, because it has been tested extensively and, as we show in this manuscript,\nprovides excellent segmentation accuracy. Should users still desire to run nnU-Net on a smaller GPU,\nthe amount of GPU memory used for network conﬁguration can be adapted easily. Corresponding\ninstructions are provided along with the source code.\nReferences\n[1] O. Bernard, A. Lalande, C. Zotti, F. Cervenansky, X. Yang, P.-A. Heng, I. Cetin, K. Lekadir,\nO. Camara, M. A. G. Ballester, et al. Deep learning techniques for automatic mri cardiac multistructures segmentation and diagnosis: Is the problem solved? IEEE TMI, 37(11):2514–2525,\n2018.\n[2] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.A. Heng, J. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint\narXiv:1901.04056ada, 2019.\n53\n\n[3] P. Bilic, P. F. Christ, E. Vorontsov, G. Chlebus, H. Chen, Q. Dou, C.-W. Fu, X. Han, P.-A. Heng,\nJ. Hesser, et al. The liver tumor segmentation benchmark (lits). arXiv preprint arXiv:1901.04056,\n2019.\n[4] A. Carass, S. Roy, A. Jog, J. L. Cuzzocreo, E. Magrath, A. Gherman, J. Button, J. Nguyen,\nF. Prados, C. H. Sudre, et al. Longitudinal multiple sclerosis lesion segmentation: resource and\nchallenge. NeuroImage, 148:77–102, 2017.\n[5] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic\nimage segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE transactions on pattern analysis and machine intelligence, 40(4):834–848, 2017.\n[6] L. Heinrich, J. Funke, C. Pape, J. Nunez-Iglesias, and S. Saalfeld. Synaptic cleft segmentation\nin non-isotropic volume electron microscopy of the complete drosophila brain. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 317–325.\nSpringer, 2018.\n[7] N. Heller, F. Isensee, K. H. Maier-Hein, X. Hou, C. Xie, F. Li, Y. Nan, G. Mu, Z. Lin, M. Han,\net al. The state of the art in kidney and kidney tumor segmentation in contrast-enhanced ct\nimaging: Results of the kits19 challenge. arXiv preprint arXiv:1912.01054, 2019.\n[8] N. Heller, N. Sathianathen, A. Kalapara, E. Walczak, K. Moore, H. Kaluzniak, J. Rosenberg, P. Blake, Z. Rengel, M. Oestreich, et al. The kits19 challenge data: 300 kidney tumor\ncases with clinical context, ct semantic segmentations, and surgical outcomes. arXiv preprint\narXiv:1904.00445, 2019.\n[9] F. Isensee and K. H. Maier-Hein.\nAn attempt at beating the 3d u-net.\narXiv preprint\narXiv:1908.02182, 2019.\n[10] F. Isensee, J. Petersen, A. Klein, D. Zimmerer, P. F. Jaeger, S. Kohl, J. Wasserthal, G. Koehler,\nT. Norajitra, S. Wirkert, et al. nnu-net: Self-adapting framework for u-net-based medical image\nsegmentation. arXiv preprint arXiv:1809.10486, 2018.\n[11] A. E. Kavur, N. S. Gezer, M. Barı¸s, P.-H. Conze, V. Groza, D. D. Pham, S. Chatterjee, P. Ernst,\nS. Özkan, B. Baydar, et al. Chaos challenge–combined (ct-mr) healthy abdominal organ\nsegmentation. arXiv preprint arXiv:2001.06535, 2020.\n[12] B. Landman, Z. Xu, J. Eugenio Igelsias, M. Styner, T. Langerak, and A. Klein. Miccai\nmulti-atlas labeling beyond the cranial vault–workshop and challenge, 2015.\n[13] G. Litjens, R. Toth, W. van de Ven, C. Hoeks, S. Kerkstra, B. van Ginneken, G. Vincent,\nG. Guillard, N. Birbeck, J. Zhang, et al. Evaluation of prostate segmentation algorithms for mri:\nthe promise12 challenge. Med Image Analysis, 18(2):359–373, 2014.\n[14] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz,\nJ. Slotboom, R. Wiest, et al. The multimodal brain tumor image segmentation benchmark\n(brats). IEEE transactions on medical imaging, 34(10):1993–2024, 2014.\n[15] A. L. Simpson, M. Antonelli, S. Bakas, M. Bilello, K. Farahani, B. van Ginneken, A. KoppSchneider, B. A. Landman, G. Litjens, B. Menze, et al. A large annotated medical image dataset\nfor the development and evaluation of segmentation algorithmsdelldatagrowth. arXiv preprint\narXiv:1902.09063, 2019.\n54\n\n[16] R. Trullo, C. Petitjean, B. Dubray, and S. Ruan. Multiorgan segmentation using distance-aware\nadversarial networks. Journal of Medical Imaging, 6(1):014001, 2019.\n[17] Q. Yu, D. Yang, H. Roth, Y. Bai, Y. Zhang, A. L. Yuille, and D. Xu. C2fnas: Coarse-to-ﬁne\nneural architecture search for 3d medical image segmentation. arXiv preprint arXiv:1912.09628,\n2019.\n55",
  "stats": {
    "raw_length": 152782,
    "normalized_length": 152507,
    "raw_lines": 3288,
    "normalized_lines": 3124
  }
}