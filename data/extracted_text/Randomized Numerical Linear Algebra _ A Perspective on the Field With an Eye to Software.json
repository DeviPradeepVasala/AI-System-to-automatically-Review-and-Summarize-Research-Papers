{
  "pdf_path": "C:\\Users\\hp\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\Randomized Numerical Linear Algebra _ A Perspective on the Field With an Eye to Software.pdf",
  "pdf_name": "Randomized Numerical Linear Algebra _ A Perspective on the Field With an Eye to Software.pdf",
  "file_hash": "15189e641c37183c9accea8aa139391510db58c693fa8ea1904ceb950af4ed04",
  "metadata": {
    "title": "",
    "author": "",
    "subject": "",
    "keywords": "",
    "creator": "LaTeX with hyperref",
    "producer": "pdfTeX-1.40.21",
    "creation_date": "D:20230414004406Z",
    "modification_date": "D:20230414004406Z"
  },
  "raw_text": "Randomized Numerical Linear Algebra\nA Perspective on the Field With an Eye to Software\nApril 12, 2023\narXiv:2302.11474v2  [math.NA]  12 Apr 2023\n\n\nPreface\nRandomized numerical linear algebra – RandNLA, for short – concerns the use of\nrandomization as a resource to develop improved algorithms for large-scale linear\nalgebra computations. The origins of contemporary RandNLA lay in theoretical\ncomputer science, where it blossomed from a simple idea: randomization provides\nan avenue for computing approximate solutions to linear algebra problems more\neﬃciently than deterministic algorithms. This idea proved fruitful in and was largely\ndriven by the development of scalable algorithms for machine learning and statistical\ndata analysis applications. However, the true potential of RandNLA only came into\nfocus once it began to integrate with the ﬁelds of numerical analysis and “classical”\nnumerical linear algebra.\nThrough the eﬀorts of many individuals, randomized\nalgorithms have been developed that provide full control over the accuracy of their\nsolutions and that can be every bit as reliable as algorithms that might be found in\nlibraries such as LAPACK.\nThe spectrum of possibilities oﬀered by RandNLA has created a virtuous cycle\nof contributions by numerical analysts, statisticians, theoretical computer scientists,\nand the machine learning community. Recent years have even seen the incorpora-\ntion of certain RandNLA methods into MATLAB, the NAG Library, NVIDIA’s\ncuSOLVER, and SciKit-Learn. In view of these developments, we believe the time\nis right to accelerate the adoption of RandNLA in the scientiﬁc community. In\nparticular, we believe the community stands to beneﬁt signiﬁcantly from a suit-\nably deﬁned “RandBLAS” and “RandLAPACK,” to serve as standard libraries for\nRandNLA, in much the same way that BLAS and LAPACK serve as standards for\ndeterministic linear algebra.\nThis monograph surveys the ﬁeld of RandNLA as a step toward building mean-\ningful RandBLAS and RandLAPACK libraries.\nSection 1 primes the reader for a\ndive into the ﬁeld and summarizes this monograph’s content at multiple levels of\ndetail. Section 2 focuses on RandBLAS, which is to be responsible for sketching.\nDetails of functionality suitable for RandLAPACK are covered in the ﬁve sections\nthat follow. Speciﬁcally, Sections 3 to 5 cover least squares and optimization, low-\nrank approximation, and other select problems that are well-understood in how\nthey beneﬁt from randomized algorithms. The remaining sections – on statistical\nleverage scores (Section 6) and tensor computations (Section 7) – read more like\ntraditional surveys. The diﬀerent ﬂavor of these latter sections reﬂects how, in our\nassessment, the literature on these topics is still maturing.\nWe provide a substantial amount of pseudo-code and supplementary material\nover the course of ﬁve appendices. Much of the pseudo-code has been tested via\npublicly available Matlab and Python implementations.\n\n\nAuthors\nRiley Murray, ICSI, LBNL, and University of California, Berkeley\nrjmurray@berkeley.edu\nJames Demmel, University of California, Berkeley\ndemmel@berkeley.edu\nMichael W. Mahoney, ICSI, LBNL, and University of California, Berkeley\nmmahoney@stat.berkeley.edu\nN. Benjamin Erichson, ICSI and Lawrence Berkeley National Laboratory\nerichson@icsi.berkeley.edu\nMaksim Melnichenko, University of Tennessee, Knoxville\nmmelnic1@vols.utk.edu\nOsman Asif Malik, Lawrence Berkeley National Laboratory\noamalik@lbl.gov\nLaura Grigori, INRIA Paris and J.L. Lions Laboratory, Sorbonne University\nlaura.grigori@inria.fr\nPiotr Luszczek, University of Tennessee, Knoxville\nluszczek@icl.utk.edu\nMicha l Derezi´nski, University of Michigan\nderezin@umich.edu\nMiles E. Lopes, University of California, Davis\nmelopes@ucdavis.edu\nTianyu Liang, University of California, Berkeley\ntianyul@berkeley.edu\nHengrui Luo, Lawrence Berkeley National Laboratory\nhrluo@lbl.gov\nJack Dongarra, University of Tennessee, Knoxville\ndongarra@icl.utk.edu\ni\n\n\nAcknowledgements\nMany individuals from the community gave detailed feedback on earlier versions of\nthis monograph that were not circulated publicly. These individuals include Mark\nTygert, Cameron Musco, Joel Tropp, Per-Gunnar Martinsson, Alex Townsend,\nDaniel Kressner, Alice Cortinovis, Ilse Ipsen, Sergey Voronin, Vivak Patel, Daniel\nMaldonado, Tammy Kolda, Florian Schaefer, Ramki Kannan, and Piyush Sao –\neach of them has our sincere gratitude for their assistance.\nIn addition, we thank the following people for providing input on the earliest\nstages of this project: Vivek Bharadwaj, Younghyun Cho, Jelani Nelson, Mark\nGates, Weslley da Silva Pereira, Julie Langou, and Julien Langou.\nThis work was partially funded by an NSF Collaborative Research Framework:\nBasic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collab-\noration (BALLISTIC), a project of the International Computer Science Institute,\nthe University of Tennessee’s ICL, the University of California at Berkeley, and\nthe University of Colorado at Denver (NSF Grant Nos. 2004235, 2004541, 2004763,\n2004850, respectively) [DDL+20]. Any opinions, ﬁndings, and conclusions or rec-\nommendations expressed in this material are those of the author(s) and do not\nnecessarily reﬂect the views of the National Science Foundation. MWM would also\nlike to thank the Oﬃce of Naval Research, which provided partial funding via a\nBasic Research Challenge on Randomized Numerical Linear Algebra.\nRelease History\n• 11/13/2022. The ﬁrst publicly-available draft of this monograph was circu-\nlated as a technical report at SC22.\n• 02/22/2023: arXiv V1. Improvements were made to Section 3.2.2 based on\ncomments from Joel Tropp.\nHelpful feedback from Robert Webber led to\nimprovements throughout Sections 4 and 5.2. Clariﬁcations and greater detail\nwere added to Appendix B.2 following comments from Ilse Ipsen.\n• 04/12/2023: arXiv V2. Section 5.1 was slightly revised based on valuable\ncomments from Oleg Balabanov.\nSection 5.3 was rewritten and expanded\nfollowing helpful discussions with Tyler Chen.\nSections 4.1.1 and 4.5 now\nmention an important piece of software that Mark Tygert brought to our\nattention. Revisions were made to Section 5.2.2 to more clearly and accurately\ncharacterize methods from the literature.\nii\n\n\nContents\n1\nIntroduction\n1\n1.1\nOur world . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nThis monograph, from an astronaut’s-eye view\n. . . . . . . . . . . .\n7\n1.3\nThis monograph, from a bird’s-eye view . . . . . . . . . . . . . . . .\n9\n1.4\nRecommended reading . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1.5\nNotation and terminology . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2\nBasic Sketching\n21\n2.1\nA high-level plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2.2\nHelpful things to know about sketching\n. . . . . . . . . . . . . . . .\n24\n2.3\nDense sketching operators . . . . . . . . . . . . . . . . . . . . . . . .\n30\n2.4\nSparse sketching operators . . . . . . . . . . . . . . . . . . . . . . . .\n32\n2.5\nSubsampled fast trigonometric transforms . . . . . . . . . . . . . . .\n35\n2.6\nMulti-sketch and quadratic-sketch routines . . . . . . . . . . . . . . .\n36\n3\nLeast Squares and Optimization\n39\n3.1\nProblem classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.2\nDrivers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n3.3\nComputational routines\n. . . . . . . . . . . . . . . . . . . . . . . . .\n51\n3.4\nOther optimization functionality\n. . . . . . . . . . . . . . . . . . . .\n58\n3.5\nExisting libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n4\nLow-rank Approximation\n63\n4.1\nProblem classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n4.2\nDrivers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.3\nComputational routines\n. . . . . . . . . . . . . . . . . . . . . . . . .\n80\n4.4\nOther low-rank approximations . . . . . . . . . . . . . . . . . . . . .\n89\n4.5\nExisting libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n5\nFurther Possibilities for Drivers\n93\n5.1\nMulti-purpose matrix decompositions . . . . . . . . . . . . . . . . . .\n94\n5.2\nSolving unstructured linear systems . . . . . . . . . . . . . . . . . . . 100\n5.3\nTrace estimation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6\nAdvanced Sketching: Leverage Score Sampling\n111\n6.1\nDeﬁnitions and background . . . . . . . . . . . . . . . . . . . . . . . 112\n6.2\nApproximation schemes\n. . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.3\nSpecial topics and further reading . . . . . . . . . . . . . . . . . . . . 120\niii\n\n\n7\nAdvanced Sketching: Tensor Product Structures\n123\n7.1\nThe Kronecker and Khatri–Rao products\n. . . . . . . . . . . . . . . 124\n7.2\nSketching operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n7.3\nPartial updates to Kronecker product sketches\n. . . . . . . . . . . . 130\nA Details on Basic Sketching\n135\nA.1 Subspace embeddings and eﬀective distortion . . . . . . . . . . . . . 135\nA.2 Short-axis-sparse sketching operators . . . . . . . . . . . . . . . . . . 137\nA.3 Theory for sketching by row selection . . . . . . . . . . . . . . . . . . 140\nB Details on Least Squares and Optimization\n143\nB.1\nQuality of preconditioners . . . . . . . . . . . . . . . . . . . . . . . . 143\nB.2\nBasic error analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\nB.3\nIll-posed saddle point problems . . . . . . . . . . . . . . . . . . . . . 152\nB.4\nMinimizing regularized quadratics\n. . . . . . . . . . . . . . . . . . . 153\nC Details on Low-Rank Approximation\n157\nC.1 Theory for submatrix-oriented decompositions . . . . . . . . . . . . . 157\nC.2 Computational routines\n. . . . . . . . . . . . . . . . . . . . . . . . . 160\nD Correctness of Preconditioned Cholesky QRCP\n169\nE Bootstrap Methods for Error Estimation\n171\nE.1\nBootstrap methods in a nutshell\n. . . . . . . . . . . . . . . . . . . . 172\nE.2\nSketch-and-solve least squares . . . . . . . . . . . . . . . . . . . . . . 173\nE.3\nSketch-and-solve one-sided SVD . . . . . . . . . . . . . . . . . . . . . 174\nBibliography\n195\niv\n\n\nSection 1\nIntroduction\n1.1 Our world ...................................................................\n1\n1.1.1 Four value propositions of randomization ......................\n4\n1.1.2 What is, and isn’t, subject to randomness .....................\n6\n1.2 This monograph, from an astronaut’s-eye view\n.........\n7\n1.3 This monograph, from a bird’s-eye view .....................\n9\n1.4 Recommended reading ................................................\n13\n1.4.1 Tutorials, light on prerequisites ...................................\n13\n1.4.2 Broad and proof-heavy resources .................................\n14\n1.4.3 Perspectives on theory, light on proofs .........................\n14\n1.4.4 Deep investigations of speciﬁc topics ............................\n15\n1.4.5 Randomized numerical linear algebra: Foundations and Al-\ngorithms, by Martisson and Tropp ...............................\n15\n1.5 Notation and terminology ...........................................\n17\nThis introductory section has three principal goals: to motivate our subject and\nclarify common misconceptions that surround it (§1.1); to explain this monograph’s\nscope and overarching structure (§1.2); and to help direct the reader’s attention\nthrough section-by-section summaries (§1.3). Many readers may beneﬁt from our\n“survey of surveys” (§1.4), and all should at least brieﬂy consult the section on\nnotation and deﬁnitions (§1.5).\n1.1\nOur world\nNumerical linear algebra (NLA) concerns algorithms for computations on matrices\nwith numerical entries. Originally driven by applications in the physical sciences,\nit now provides the foundation for vast swaths of applied and computational math-\nematics. The cultural norms in this ﬁeld developed many years ago, in large part\nfrom recurring themes in problem formulations and algorithmically-useful structures\nin matrices. However, more recently, NLA has also been motivated by developments\nin machine learning and data science. Applications in these ﬁelds also have their\nown themes of problem formulations and structures in data, often of a very diﬀerent\nnature than those in more classical applications.\n1\n\n\nIntroduction\n1.1. Our world\nA dire situation.\nWhile communities that rely on NLA now vary widely, they share\none essential property: a ravenous appetite for solving larger and larger problems.\nFor decades, this hunger was satiated by complementary innovations in hardware\nand software. However, this progress should not be taken for granted. In particular,\nthere are two factors that increasingly present obstacles to scaling linear algebra\ncomputations to the next level.\n• Space and power constraints in hardware. Chips today have billions of tran-\nsistors, and these transistors are packed into a very small amount of space.\nIt takes power to run these transistors at gigahertz frequencies, and power\ngenerates heat. It is hard for one hot thing to dissipate heat when surrounded\nby millions of other hot things. Too much heat can fry a chip.\nThese constraints are known to industry and research community alike, and\noften referred to as the breakdown of the Dennard’s Law [DGR+74; Boh07]\nand the sunsetting of Moore’s Law [Moo65]. The former represents the infa-\nmous power wall due to the inability of dissipating the heat produced by the\nprocessors leading to ﬂattened curve of clock frequency increases. The latter\nintroduced the post-Moore era of heterogeneous computing [VBG+18] leading\nto plethora specialized hardware targeting individual application spaces.\nThe end result of all this?\nA situation where “more powerful processors”\nare just scaled-out versions of “less powerful processors,” for both commod-\nity and server-tier hardware. Any algorithm that does not parallelize well\nis fundamentally limited in its ability to leverage these advances.\nIf one’s\npockets are deep enough, then one can try to get around this with purpose-\nbuilt accelerators. But even then, there remains the matter of programming\nthose accelerators, and high-performance implementations of classical NLA\nalgorithms are anything but simple.\n• NLA’s maturity as a ﬁeld. Software can only improve so much without algo-\nrithmic innovations. At the same time, linear algebra is a very well-studied\ntopic, and most algorithmic breakthroughs in recent years have required care-\nfully exploiting structures present in speciﬁc problems. Identifying new and\nuseful problem structures has been increasingly diﬃcult, often requiring deep\nknowledge of NLA alongside substantial domain expertise.\nIf we are to continue scaling our capabilities in matrix computations, then it is\nessential that we leverage all technologies that are on the table.\nAn underutilized technology.\nThis monograph concerns randomized numerical lin-\near algebra, or RandNLA, for short.\nAlgorithms in this realm oﬀer compelling\nadvantages in a wide variety of settings. Some provide an unrivaled combination of\neﬃciency and reliability in computing approximate solutions for massive problems.\nOthers provide ﬁne-grained control when balancing accuracy and computational\ncost, as is essential for practitioners who are operating at the limits of what their\nmachines can handle. In many cases, the practicality of these algorithms can be seen\neven with elementary MATLAB or Python implementations, which increases their\nsuitability for adapting to new hardware by leveraging similarly powerful abstrac-\ntion layers.\nFinally, although truly high-performance implementations are more\ncomplicated, they remain relatively easy to implement when given the right build-\ning blocks.\nPage 2\narXiv Version 2\n\n\n1.1. Our world\nIntroduction\nBut we are getting ahead of ourselves. What do we mean by “randomized al-\ngorithms,” as the term is used within RandNLA? First and foremost, these are\nalgorithms that are probabilistic in nature. They use randomness as part of their\ninternal logic to make decisions or compute estimates, which they can go on to use\nin any number of ways. These algorithms do not presume a distribution over pos-\nsible inputs, nor do they assume the inputs somehow possess intrinsic uncertainty.\nRather, they use randomness as a tool, to ﬁnd and exploit structures in problem\ndata that would seem “hidden” from the perspective of classical NLA.\nWhat’s this about “ﬁnding hidden structures?”\nConsider the problem of highly overde-\ntermined least squares, i.e., the problem of solving\nmin\nx∈Rn ∥Ax −b∥2\n2,\n(1.1)\nwhere A has m ≫n rows. It is well-known that if the columns of A are orthonormal\nthen (1.1) can be solved in O(mn) time by setting x = A∗b, where A∗is the\ntranspose of A. The trouble, of course, is that the columns of A are very unlikely\nto be orthogonal in any interesting application, and the standard algorithms for\nsolving this problem take O(mn2) time.\nHowever, what if, by some miracle, we could easily ﬁnd an n × n matrix C for\nwhich AC−1 was column-orthonormal? In this case, we could compute the exact\nsolution x = (C−1)(C−1)∗A∗b in time\nO\n\u0000mn + n3\u0001\nby suitably factoring C. Now, randomized algorithms do not work miracles, but at\ntimes they can approach the miraculous. In the speciﬁc case of (1.1), randomization\ncan be used to quickly identify a basis in which A is nearly column-orthonormal.\nAny such basis can be incorporated into a standard iterative method from classical\nNLA. With such an approach, one can reliably solve (1.1) to ϵ-error in time\nO\n\u0000mn log\n\u0000 1\nϵ\n\u0001\n+ n3\u0001\n(where we ask forgiveness for being vague about the meaning of “ϵ”).\nBack to the big picture.\nThe approach to least squares described above has been\nknown for well over ten years now. Since then, an entire suite of compelling results\non RandNLA has been established. What’s more, the literature also documents the\nexistence of high-performance proof-of-concept implementations that testify to the\npracticality of these methods. Indeed, as we explain below, randomized algorithms\nhave been developed to address the same basic challenges as classical methods. Ran-\ndomized algorithms are also very well-suited to address many upcoming challenges\nwith which classical algorithms struggle. RandNLA as a ﬁeld is slowly achieving a\ncertain level of maturity.\nDespite this, substantial interdisciplinary gaps have impeded technology transfer\nfrom those doing research in RandNLA to those who might beneﬁt from it. This\nstems partly from the absence of work that organizes the RandNLA literature in a\nway that supports the development of high-quality software.\nThis monograph is our attempt at addressing that absence. With it, we aim\nto provide a principled and practical foundation for developing high-quality soft-\nware to address future needs of large-scale linear algebra computations, for scien-\ntiﬁc computing, large-scale data analysis and machine learning, and other related\narXiv Version 2\nPage 3\n\n\nIntroduction\n1.1. Our world\napplications. Our particular approach is informed by plans to develop such high-\nquality libraries – a “RandBLAS” and “RandLAPACK,” if you will. Towards this end,\nwe have implemented and tested many of the algorithms described herein in both\nMATLAB1 and Python.2 We provide more context on our approach and scope in\nSection 1.2. But ﬁrst, we elaborate on the value propositions of RandNLA and the\nrole of randomness in these algorithms.\n1.1.1\nFour value propositions of randomization\nOur goal here is to introduce (and only introduce!) some value propositions for\nRandNLA. We do this for as broad an audience as possible and we have attempted\nto keep our introductions short. While these descriptions are unlikely to convince\na skeptic, they should at least set the agenda for a debate.\nBackground: time complexity and FLOP counts.\nIn the sequential RAM model of\ncomputing, an algorithm’s time complexity is its worst-case total number of reads,\nwrites, and elementary arithmetic operations, as a function of input size. Precise\nexpressions for time complexity can be hard to come by and diﬃcult to parse.\nTherefore it is standard to describe complexity asymptotically, with big-O notation.\nIn NLA, we also care about how the size of an algorithm’s input aﬀects the num-\nber of arithmetic operations that it requires. Arithmetic operations are presumed\nto be ﬂoating point operations (“ﬂops”) by default, and it is common to refer to an\nalgorithm’s ﬂop count as a function of input size. Flop counts almost always agree\nasymptotically with time complexity. But, in contrast with time complexity, ﬂop\ncounts are often given with explicit constant factors. In determining the constant\nfactors accurately one must consider subtleties such as whether a fused multiply-\nadd instruction counts as one or two “ﬂops.” We do not consider such subtleties in\nthis monograph.\nFighting the scourge of superlinear complexity.\nThe dimensions of matrices arising in\napplications typically have semantic meanings. They might represent the number\nof points in a dataset, or they might be aﬀected by the “ﬁdelity” of a linear model\nfor some nonlinear phenomenon. A scientist who relies on matrix computations will\ninevitably want to increase the size of their dataset or the ﬁdelity of their model.\nThis is often diﬃcult because the complexity of classical algorithms for high-level\nlinear algebra problems rarely scale linearly with the semantic notion of problem\nsize. This brings us to the ﬁrst value proposition of RandNLA.\nFor many important linear algebra problems, randomization oﬀers en-\ntirely new avenues of computing approximate solutions with linear or\nnearly-linear complexity.\nTo get a sense of why this matters, suppose that one needs to compute a Cholesky\ndecomposition of a dense matrix of order n. The standard algorithm for this takes\nn3/3 ﬂops.\nAt time of writing, a higher-end laptop can do this calculation for\nn = 10,000 in about one second. However, if n is the semantic notion of problem\nsize, and if one wants to solve a problem ten times as large, then the calculation\nwith n = 100,000 takes over 15 minutes.\n1https://github.com/BallisticLA/MARLA\n2https://github.com/BallisticLA/PARLA\nPage 4\narXiv Version 2\n\n\n1.1. Our world\nIntroduction\nThere are two lessons in that simple example.\nThe ﬁrst is that superlinear\ncomplexity can be crippling when it comes to solving larger linear algebra problems.\nThe second is that an informed user does well to think of their problem size in a\nmore realistic way. In the case of this example, one should go into the problem\nthinking in terms of the number of free parameters in an n × n positive deﬁnite\nmatrix: around 50 million when n = 10,000 and around 5 billion when n = 100,000.\nRemark 1.1.1. One of RandNLA’s success stories is a fast algorithm for computing\nsparse approximate Cholesky decompositions of so-called graph Laplacians. In order\nto keep the length of this monograph under control, we have opted not to include\nalgorithms that only apply to sparse matrices. However, we do provide algorithms\nfor computing approximate eigendecompositions of regularized positive semideﬁnite\nmatrices, and these algorithms can be used to solve linear systems faster than\nCholesky in certain applications.\nResisting the siren call of galactic algorithms.\nThe problem of multiplying two n×n\nmatrices is one of the most fundamental in all of NLA. If we only consider asymp-\ntotics, then the fastest algorithms for this task run in less than O(n2.38) time.\nHowever, the fastest method that is practical (Strassen’s algorithm), runs in time\nO(nlog2 7).\nThe trouble with these “fast” algorithms is that they have massive constants\nhidden in their big-O complexity.\nSuch algorithms are called galactic, owing to\ncommon comparisons between the size of their hidden constants and the number\nof stars or atoms in the galaxy.\nAnd with this, we arrive at the second value\nproposition of RandNLA.\nFor a handful of important linear algebra problems, the asymptotically\nfastest (non-galactic) algorithms for computing accurate solutions are,\nin fact, randomized.\nHighly overdetermined least squares (see page 3) is one such problem.\nStriking the Achilles’ heel of the RAM model.\nThe RAM model of computing, al-\nthough useful, is not high-ﬁdelity. Indeed, even in the setting of a shared-memory\nmulti-core machine, it fails to account for the fact that moving data from main\nmemory, through diﬀerent levels of cache, and onward to processor registers is much\nmore expensive than elementary arithmetic on the same data. This fact has been\nappreciated even in the earliest days of LAPACK’s development, over 30 years ago.\nIts principal consequence is that even if the time complexities of two algorithms\nmatch up to and including constant factors, their performance by wallclock time\ncan diﬀer by orders of magnitude. This is the third value proposition of RandNLA.\nRandomization creates a wealth of opportunities to reduce and redirect\ndata movement. Randomized algorithms based on this principle are sig-\nniﬁcantly faster than the best-available deterministic methods by wall-\nclock time.\nRandomized algorithms for computing full QR decompositions with column pivoting\nﬁt this description.\narXiv Version 2\nPage 5\n\n\nIntroduction\n1.1. Our world\nFinite-precision arithmetic: once a curse, now a blessing.\nFinite-precision arithmetic\nand exact arithmetic are diﬀerent beasts, and this has real consequences for NLA.\nFor one thing, this limitation introduces many technicalities in understanding ac-\ncuracy guarantees, even for seemingly straightforward problems like LU decompo-\nsition. It is tempting to view it as a curse. However, if we accept it as given, then it\ncan be used to our advantage. Certain computations can be performed with lower\nprecision without compromising the accuracy of a ﬁnal result.\nThis perspective brings us to our ﬁnal value proposition, stated in terms of the\nconcept of sketching, deﬁned momentarily.\nIn RandNLA, it is natural to perform computations on sketches of ma-\ntrices in lower-precision arithmetic.\nDepending on how the sketch is\nconstructed, one can be (nearly) certain of avoiding degenerate situa-\ntions that are known to cause common deterministic algorithms to fail.\n1.1.2\nWhat is, and isn’t, subject to randomness\nSampling sketching operators from sketching distributions.\nWe are concerned with\nalgorithms that use random linear dimension reduction maps called sketching oper-\nators. The sketching operators used in RandNLA come in a wide variety of forms.\nThey can be as simple as operators for selecting rows or columns from a matrix,\nand they can be even more complicated than algorithms for computing Fast Fourier\nTransforms. We refer to a distribution over sketching operators as a sketching dis-\ntribution. Given this terminology, we can highlight the following essential fact.\nFor the vast majority of RandNLA algorithms, randomization is only\nused when sampling from the sketching distribution.\nFrom an implementation standpoint, one should know that while sketching dis-\ntributions can be quite complicated, the sampling process always builds on some\nkind of basic random number generator. Upon specifying a seed for the random\nnumber generator involved in sampling, RandNLA algorithms become every bit as\ndeterministic as classical algorithms.\nForming and processing sketches.\nWhen a sketching operator is applied to a large\ndata matrix, it produces a smaller matrix called a sketch. A wealth of diﬀerent\noutcomes can be achieved through diﬀerent methods for processing a sketch and\nusing the processed representation downstream.\nSome processing schemes inevitably yield rough approximations to the\nsolution of a given problem. Other processing schemes can lead to high-\naccuracy approximations, if not exact solutions, under mild assump-\ntions.\nAcross these regimes, one of the most popular trends in algorithm analysis is to\nemploy a two-part approach. In the ﬁrst part, the task is to characterize algorithm\noutput in terms of some simple property of the sketch. In the second part, one can\nemploy results from random matrix theory to bound the probability that the sketch\nwill possess the desired property.\nPage 6\narXiv Version 2\n\n\n1.2. This monograph, from an astronaut’s-eye view\nIntroduction\nConﬁdently managing uncertainty.\nThe performance of a numerical algorithm is\ncharacterized by the accuracy of its solutions and the cost it incurs to produce\nthose solutions. Naturally, one can expect some variation in algorithm performance\nwhen using randomized methods. Luckily, we have the following.\nMost randomized algorithms “gamble” with only one of the two perfor-\nmance metrics, accuracy or cost. Through optional algorithm parame-\nters, users retain ﬁne-grained control over one of these two metrics.\nFurthermore, when cost is controllable, the algorithm parameters can be adjusted to\ninﬂuence accuracy; when accuracy is controllable, they can be adjusted to inﬂuence\ncost. The eﬀects of these inﬂuences can sometimes be masked by variability in run-\nto-run performance. However, there is a general trend in RandNLA algorithms of\nbecoming more predictable as they are applied to larger problems. At large enough\nscales, many randomized algorithms are nearly as predictable as deterministic ones.\n1.2\nThis monograph, from an astronaut’s-eye view\nThis monograph started as a development plan for two C++ libraries for RandNLA,\nprimarily working within a shared-memory dense-matrix data model. We prepared\na preliminary plan for these libraries in short order by leveraging existing surveys.\nHowever, after pausing our writing for some number of months to receive feedback\nfrom community members, we found ourselves with many unanswered questions\nthat would aﬀect our implementations. Before long we found ourselves in a cycle\nof diving ever-deeper into the RandNLA literature with an eye to implementation,\neach time coming up with more answers and more questions.\nThis monograph does not answer every question we came across in the forego-\ning months. Rather, it represents what we know at a time when the best way to\nanswer our remaining questions is to focus on developing the libraries themselves.\nTherefore we provide the reader with this — a monograph that aggregates mate-\nrial from over 300 references on classical and randomized NLA — which functions\npartly as a survey and partly as original research. In it, we present new (unifying)\ntaxonomies, candidate application areas, and even a handful of novel theoretical\nresults and algorithms.\nAlthough its scope has greatly increased, the original purpose of this monograph\ninforms its structure. It also contains a number of clear statements about plans for\nour C++ libraries. Therefore, while we do not want to give the impression that\nthis monograph’s value depends on its connections to speciﬁc pieces of software, we\nprovide the following remarks on our planned libraries up-front.\nThe ﬁrst library, RandBLAS, concerns basic sketching and is the subject\nof Section 2. Our hope is that RandBLAS will grow to become a commu-\nnity standard for RandNLA, in the sense that its API would see wider\nadoption than any single implementation thereof. In order to achieve\nthis goal we think it is important to keep its scope narrowly focused.\nThe second library, RandLAPACK, concerns algorithms for traditional\nlinear algebra problems (Sections 3 to 5, on least-squares and optimiza-\ntion, low-rank approximation, and additional possibilities, respectively)\nand advanced sketching functionality (Sections 6 and 7). The design\narXiv Version 2\nPage 7\n\n\nIntroduction\n1.2. This monograph, from an astronaut’s-eye view\nspaces of algorithms for these tasks are large, and we believe that power-\nful abstractions are needed for a library to leverage this fact. Consistent\nwith this, we are developing RandLAPACK in an object-oriented pro-\ngramming style wherein algorithms are objects. Such a style is naturally\ninstantiated with functors when working in C++.\nWe have written this monograph to be modular and accessible, without sac-\nriﬁcing depth.\nThe modularity manifests in how there are almost no technical\ndependencies across Sections 3 to 7. For the sake of accessibility, each section gives\nbackground on its core subject. We use two strategies to provide accessibility with-\nout sacriﬁcing depth. First, we make liberal use of appendices. In them, the reader\ncan ﬁnd proofs, background on special topics, low-level algorithm implementation\nnotes, and high-level algorithm pseudocode. Second, our citations regularly indicate\nprecisely where a given concept can be found in a manuscript. Therefore, if we give\ntoo brief a treatment on a topic of interest, the reader will know exactly where to\nlook to learn more.\nA word on “drivers” and “computational routines”\nWe designate most algorithms as either drivers or computational routines. These\nterms are borrowed from LAPACK’s API. In general, drivers solve higher-level prob-\nlems than computational routines, and their implementations tend to use a small\nnumber of computational routines. In our context,\ndrivers are only for traditional linear algebra problems,\nwhile\ncomputational routines address a mix of traditional linear algebra prob-\nlems and specialized problems that are only of interest in RandNLA.\nSections 3 and 4 cover drivers and the computational routines behind them; they\nare the most comprehensive sections in this monograph.\nSection 5 also covers\ndrivers, but at less depth than the two that precede it.\nIn particular, it does\nnot identify algorithmic building blocks that would be considered computational\nroutines.\nMeanwhile, the advanced sketching functionality in Sections 6 and 7\nwould only be considered for computational routines.\nOne reason why we use the “driver” and “computational routine” taxonomy is\nto push much of the RandNLA design space into computational routines. This is\nessential to keeping drivers simple and few in number. However, it has a side eﬀect:\nsince choices made in the computational routines decisively aﬀect the drivers, it is\nhard to state theoretical guarantees for the drivers without being prescriptive on\nthe choice of computational routine. This is compounded by two factors. First, we\nprefer to not be prescriptive on choices of computational routines within drivers,\nsince there is always a possibility that some problems beneﬁt more from some ap-\nproaches than others. Second, even if we recommended speciﬁc implementations, it\nwould be very complicated to characterize their performance with consideration to\nthe full range of possibilities for their tuning parameters.\nAs a result of all this, we make relatively few statements about performance\nguarantees or computational complexity of driver-level algorithms. While this is a\nlimitation of our approach, we believe it is not severe. One can supplement this\nmonograph with a variety of resources discussed in Section 1.4.\nPage 8\narXiv Version 2\n\n\n1.3. This monograph, from a bird’s-eye view\nIntroduction\n1.3\nThis monograph, from a bird’s-eye view\nSection-by-section summaries are provided below to help direct the reader’s atten-\ntion. While space limitations prevent them from being comprehensive, they are\neﬀective for what they are. They assume familiarity with standard linear algebra\nconcepts, including least squares models, singular value decomposition, Hermitian\nmatrices, eigendecomposition, and positive (semi)deﬁniteness. We deﬁne all of these\nconcepts in Section 1.5 for completeness. Finally, as one disclaimer, some problem\nformulations below have slight diﬀerences from those used in the sections them-\nselves.\nEssential notation and conventions.\nThe adjoint of a linear operator A is denoted\nby A∗. When A is a real matrix, the adjoint is simply the transpose. Vectors have\ncolumn orientations by default, so the standard inner product of two vectors u, v\nis u∗v.\nWe sometimes call a vector of length n an n-vector. If we refer to an m × n\nmatrix as “tall” then the reader can be certain that m ≥n and reasonably expect\nthat m > n. If m is much larger than n and we want to emphasize this fact, then\nwe write m ≫n and would call an m × n matrix “very tall.” We use analogous\nconventions for “wide” and “very wide” matrices.\nBasic Sketching (Section 2)\nThis section documents our work toward developing a RandBLAS standard.\nIt\nbegins with remarks on the Basic Linear Algebra Subprograms (BLAS), which are\nto classical NLA as we hope the RandBLAS will be to RandNLA.\nSection 2.1 addresses high-level design questions for a RandBLAS standard. By\nstarting with a simple premise, we arrive at the conclusion that it should provide\nfunctionality for data-oblivious sketching (that is, sketching without consideration\nto the numerical properties of the data). We then oﬀer our thoughts on how such a\nlibrary should be organized and how it should handle random number generation.\nSection 2.2 summarizes a variety of concepts in sketching.\nIn it, we answer\nquestions such as the following.\n• What are the geometric interpretations of sketching?\n• How does one measure the quality of a sketch?\n• What are the “standard” properties for the ﬁrst and second moments of sketch-\ning operator distributions? When and how are these properties important in\nRandNLA algorithms?\nDetail-oriented readers should consider Section 2.2 alongside Appendix A.1, which\npresents a novel concept called eﬀective distortion that is useful in characterizing\nthe behavior of randomized algorithms for least squares and related problems.\nSections 2.3 to 2.5 review the three types of sketching operator distributions that\nthe RandBLAS might support. These types of distributions consist of dense sketch-\ning operators (e.g., Gaussian matrices), sparse sketching operators, and sketch-\ning operators based on subsampled fast trigonometric transforms (such as discrete\nFourier, discrete cosine, and Walsh-Hadamard transforms). As we explain in Sec-\ntion 2.4, we consider row-sampling and column-sampling as particular types of\narXiv Version 2\nPage 9\n\n\nIntroduction\n1.3. This monograph, from a bird’s-eye view\nsparse sketching. The interested reader is referred to Appendix A.2 for details on\na class of sparse sketching operators that is distinct from row or column sampling.\nThese details include notes on high-performance implementations that have not\nappeared in earlier literature.\nOur chapter on basic sketching concludes with Section 2.6, which presents a\nhandful of elementary sketching operations that are not naturally represented by\na linear transformation that acts only on the columns or only on the rows of a\nmatrix. These operations arise in the fastest randomized algorithms for low-rank\napproximation.\nLeast Squares and Optimization (Section 3)\nThis is one of three sections that cover driver-level functionality, and it is one of\ntwo that discuss drivers and computational routines. It is narrower in scope but\ngreater in depth than the other sections that address drivers.\nProblem classes.\nIn Section 3.1 we consider a variety of least squares problems\nwithin a common framework. The framework describes all problems in terms of an\nm×n data matrix A where m ≥n. Given A, any pair of vectors (b, c) of respective\nlengths (m, n) can be considered along with a parameter µ ≥0 to deﬁne “primal”\nand “dual” saddle point problems. The primal problem is always\nmin\nx∈Rn\n\b\n∥Ax −b∥2\n2 + µ∥x∥2\n2 + 2c∗x\n\t\n.\n(Pµ)\nThe dual problem takes one of two forms, depending on the value of µ:\nmin\ny∈Rm\n\b\n∥A∗y −c∥2\n2 + µ∥y −b∥2\n2\n\t\nif µ > 0\nmin\ny∈Rm\n\b\n∥y −b∥2\n2 : A∗y = c\n\t\nif µ = 0\n\n\n.\n(Dµ)\nSpecial cases of these problems include overdetermined and underdetermined least\nsquares, as well as ridge regression with tall or wide matrices. Appendix B.2 gives\nbackground on accuracy metrics, sensitivity analysis, and error estimation methods\nthat apply to the most prominent problems under this umbrella.\nSection 3.1 considers one type of problem that does not ﬁt nicely into the above\nframework. Speciﬁcally, for a positive semideﬁnite linear operator G and a positive\nparameter µ, it also considers the regularized quadratic problem\nmin\nw w∗(G + µI)w −2h∗w.\n(Rµ)\nWe note that (Pµ) and (Dµ) can be cast to this form when µ is positive. However,\nto make this reformulation would be to obfuscate the structure in a saddle point\nproblem, rather than reveal it.\nDrivers.\nWe start in Section 3.2.1 by covering a low-accuracy method for overde-\ntermined least squares known as sketch-and-solve. This method is remarkable for\nthe simplicity of its description and its analysis. It is also the ﬁrst place where\nour newly-proposed concept of eﬀective distortion provides improved insight into\nalgorithm behavior.\nPage 10\narXiv Version 2\n\n\n1.3. This monograph, from a bird’s-eye view\nIntroduction\nSections 3.2.2 and 3.2.3 concern methods for solving problems (Pµ), (Dµ), and\n(Rµ) to high accuracy. These methods use randomization to ﬁnd a preconditioner.\nThe preconditioner is used to implicitly change the coordinate system that describes\nthe optimization problem, in such a way that the preconditioned problem can easily\nbe solved by iterative methods from classical NLA. These methods are intended for\nuse with certain problem structures (e.g. m ≫n) that we clearly identify.\nThe broader idea of sketch-and-solve algorithms has been successfully used for\nkernel ridge regression (KRR – see Appendix B.4.1 for a primer). In Section 3.2.4,\nwe reinterpret two algorithms for approximate KRR as sketch-and-solve algorithms\nfor (Rµ). We further identify how the sketched problems amount to saddle point\nproblems with m ≫n. Appendix B.4.2 details how the saddle point framework is\nuseful in the more complicated of these two settings.\nComputational routines.\nThe computational routines that we cover in Section 3.3\nonly pertain to drivers based on random preconditioning. We kick oﬀour discussion\nin Section 3.3.1 with background on saddle point problems. Then, Section 3.3.2\naddresses preconditioner generation for saddle point problems when m ≫n. It\nopens with a theoretical result (Proposition 3.3.1) characterizing the spectrum of\nthe preconditioned data matrix A (see also Appendix B.1) before providing a com-\nprehensive overview of implementation considerations. Special attention is paid to\nhow one can generate the preconditioner when µ > 0 at no added cost compared\nto when µ = 0. In Section 3.3.3, we extend recently proposed methods from the\nliterature to deﬁne novel low-memory preconditioners for regularized saddle point\nproblems. Finally, Section 3.3.4 reviews a suite of deterministic iterative algorithms\nfrom classical NLA that are needed for randomized preconditioning algorithms.\nLow-rank Approximation (Section 4)\nLow-rank approximation problems take the following form.\nGiven as input an m × n target matrix A, compute suitably structured\nfactor matrices E, F, and G where\nˆA\n:=\nE\nF\nG\nm × n\nm × k\nk × k\nk × n\napproximates A. The accuracy of the approximation ˆA ≈A may vary\nfrom one application to another, but we require that k ≪min{m, n}.\nThis section summarizes the massive design spaces of randomized algorithms for\nsuch problems, as documented in the existing literature. One of its core contribu-\ntions is to clarify what parts of this design space are relevant in what situations.\nProblem classes.\nSection 4.1 starts by explaining the signiﬁcance of the SVD and\neigendecomposition in relation to principal component analysis.\nFrom there, it\nintroduces the reader to a handful of submatrix-oriented decompositions – CUR,\none-sided interpolative decompositions (one-sided ID), and two-sided interpolative\ndecompositions (two-sided ID) – along with their applications. Section 4.1 concludes\nwith guidance on how one should and should-not quantify approximation error in\nlow-rank approximation problems. We note that this background is much more\ndetailed than that Section 3.1 provided on least squares and optimization problems.\nThis extra background will be important for many readers.\narXiv Version 2\nPage 11\n\n\nIntroduction\n1.3. This monograph, from a bird’s-eye view\nDrivers.\nSection 4.2 gives concise yet comprehensive overviews for RandNLA al-\ngorithms for SVD and Hermitian eigendecomposition (§4.2.1 and 4.2.2) as well as\nCUR and two-sided interpolative decomposition (§4.2.3). In the process, we take\ncare to prevent misunderstandings in what we mean by a Nystr¨om approximation\nof a positive semideﬁnite matrix. Pseudocode is provided for at least one algorithm\nfor each of these problems.\nComputational routines.\nAs is typical for surveys on this topic, we identify QB\ndecomposition (§4.3.2) and column subset selection (CSS) / one-sided ID (§4.3.4) as\nthe basic building blocks for most drivers. We also isolate power iteration (§4.3.1)\nand partial column-pivoted matrix decompositions (§4.3.3) as subproblems with\nnontrivial design spaces that are important to low-rank approximation.\nSome of the building blocks covered cumulatively from Sections 4.3.1 to 4.3.4 can\nbe used to compute low-rank approximations iteratively. If one seeks an approxima-\ntion that is accurate to within some given tolerance, then these iterative algorithms\nrequire methods for estimating norms of linear operators; we cover such norm es-\ntimation methods brieﬂy in Section 4.3.5. Appendix C.2 contains pseudocode for\nseven computational routines and details their dependency structure.\nFurther Possibilities for Drivers (Section 5)\nThis section covers a handful of independent topics.\nSection 5.1 covers multi-purpose matrix decompositions. Section 5.1.1 explains\na simple algorithm for computing an unpivoted QR decomposition of a tall-and-\nthin matrix of full column rank; the algorithm uses randomization to precondition\nCholesky QR for numerical stability. Section 5.1.2 ﬁrst describes an existing algo-\nrithm from the literature for Householder QRCP of matrices with any aspect ratio,\nand then presents an extension of preconditioned Cholesky QR that incorporates\npivoting and allows for rank-deﬁcient matrices. Section 5.1.3 summarizes methods\nfor computing decompositions known by various names (UTV, URV, QLP) that all\naim to serve as cheaper surrogates for the SVD.\nSection 5.2 addresses randomized algorithms for the solution of unstructured\nlinear systems. This includes direct methods based on accelerating (or safely by-\npassing) pivoting in matrix decompositions (§5.2.1) as well as iterative methods\n(§5.2.2). Some of these iterative methods were developed fairly recently and are a\nsubject of considerable practical interest.\nSection 5.3 considers the problem of estimating the trace of a linear opera-\ntor. This problem is unique in the context of this monograph, since it makes no\nsense to consider in the shared-memory dense-matrix data model. We have opted\nto cover it anyway since randomized methods are extremely eﬀective for it. Sec-\ntion 5.3.1 introduces the elementary Girard–Hutchinson estimator developed in the\nlate 1980s. Section 5.3.2 covers methods that beneﬁt from contemporary develop-\nments on randomized algorithms for low-rank approximation. Finally, Section 5.3.3\ncovers methods for computing the trace of f(B) where B is a Hermitian matrix and\nf is a matrix function. We give a signiﬁcant amount of background material to help\nthe newcomer understand the methods in this last category.\nPage 12\narXiv Version 2\n\n\n1.4. Recommended reading\nIntroduction\nAdvanced Sketching: Leverage Score Sampling (Section 6)\nLeverage scores constitute measures of importance for the rows or columns of a\nmatrix. They can be used to deﬁne data-aware sketching operators that implement\nrow or column sampling.\nSection 6.1 introduces three types of leverage scores: standard leverage scores,\nsubspace leverage scores, and ridge leverage scores. We explain how each type is\nsuitable for sketching with diﬀerent downstream tasks in mind. For example, a\nproposition in Section 6.1.1 bounds the probability that a row-sampling operator\nsatisﬁes a subspace embedding property for the range of a matrix A. The bound\nshows that if rows are sampled according to a distribution q, then it becomes more\nlikely that the subspace embedding property holds as q approaches A’s standard\nleverage score distribution.\nSection 6.2 covers randomized algorithms for approximating leverage scores.\nSuch approximation methods are important since leverage scores are expensive to\ncompute except when working with highly structured problem data. The structure\nof these algorithms bears similarities to those seen in earlier sections. For example,\nSection 6.2.2 explains how a longstanding algorithm for approximating subspace\nleverage scores can be extended with QB approaches from Section 4.3.2.\nAdvanced Sketching: Tensor Product Structures (Section 7)\nTensor computations are the domain of multilinear algebra. As such, it is reasonable\nto exclude them from the scope of a standard library from RandNLA. However,\nat the same time, it is reasonable for a RandNLA library to support the core\nsubproblems in tensor computations that are linear algebraic in nature. Sketching\nimplicit matrices with tensor product structure ﬁts this description.\nThis section reviews eﬃcient methods for sketching matrices with Kronecker\nproduct or Khatri–Rao product structures (see §7.1 for deﬁnitions). The material\nin 7.2.1–7.2.4 concerns data-oblivious sketching distributions that are similar to\nthose from Section 2 but modiﬁed for the tensor product setting. Section 7.2.5, by\ncontrast, concerns data-aware sketching methods based on leverage score sampling.\nNotably, there are methods to eﬃciently sample from the exact leverage score distri-\nbutions of tall matrices with Kronecker and Khatri–Rao product structures without\nexplicitly forming those matrices.\nFor completeness, Section 7.3 discusses motivating applications (speciﬁcally, ten-\nsor decomposition algorithms) that entail sketching matrices with these structures.\n1.4\nRecommended reading\nThis monograph is heavily inﬂuenced by a recent and sweeping survey by Martinsson\nand Tropp [MT20]. We draw detailed comparisons to that work in Section 1.4.5.\nBut ﬁrst, we give remarks on other resources of note for learning about RandNLA.\n1.4.1\nTutorials, light on prerequisites\nRandNLA: randomized numerical linear algebra, by Drineas and Mahoney [DM16].\nDepending on one’s background (and schedule!) this article can be read in one sit-\nting. It requires no knowledge of NLA or probability. In fact, it does not even\narXiv Version 2\nPage 13\n\n\nIntroduction\n1.4. Recommended reading\npresume that the reader already cares about matrix computations. It starts with\nbasic ideas of matrix approximation by subsampling, explains the eﬀect of sampling\nin diﬀerent data-aware ways, and frames general data-oblivious sketching as “pre-\nprocessing followed by uniform subsampling.” It summarizes, at a very high level,\nsigniﬁcant results of RandNLA in least squares, low-rank approximation, and the\nsolution of structured linear systems known as Laplacian systems.\nLectures on randomized numerical linear algebra, by Drineas and Mahoney [DM18].\nThis book chapter is useful for those who want to see representative banner re-\nsults in RandNLA with proofs. It covers algorithms for least squares and low-rank\napproximation. Its proofs emphasize decoupling deterministic and probabilistic as-\npects of analysis. Among resources that engage with the theory of RandNLA, it\nis notable for its brevity and its self-contained introductions to linear algebra and\nprobability.\n1.4.2\nBroad and proof-heavy resources\nSketching as a tool for numerical linear algebra, by Woodruﬀ[Woo14].\nThis monograph proceeds one problem at a time, starting with ℓ2 regression, then\non to ℓ1 regression, then low-rank approximation, and ﬁnally graph sparsiﬁcation.\nIt develops the technical machinery needed for each of these settings, at various\nlevels of detail. Among resources that address RandNLA theory, it is notable for\nits treatment of lower bounds (i.e., limitations of randomized algorithms).\nAn introduction to matrix concentration inequalities, by Tropp [Tro15].\nThis monograph gives an introduction to the theory of matrix concentration and\nits applications. It is not about RandNLA per se, but several of its applications do\nfocus on RandNLA. The course notes [Tro19] build on this monograph, exploring\ntheory and applications of matrix concentration developed after [Tro15] was written.\nLecture notes on randomized linear algebra, by Mahoney [Mah16].\nThese notes are fairly comprehensive in their coverage of results in RandNLA up\nto 2013. They address matrix concentration, approximate matrix multiplication,\nsubspace embedding properties of sketching distributions, as well as various algo-\nrithms for least squares and low-rank approximation. These notes are distinct from\n[Woo14] in that they address theory and practice. (Of course, being course notes,\nthey are not suitable as a formal reference.)\n1.4.3\nPerspectives on theory, light on proofs\nRandomized algorithms for matrices and data, by Mahoney [Mah11].\nThis monograph heavily emphasizes concepts, interpretations, and qualitative proof\nstrategies. It is a good resource for those who want to know what RandNLA can oﬀer\nin terms of theory for the least squares and low-rank approximation. It is notable\nfor the eﬀort it expends to connect RandNLA theory to theoretical developments\nin other disciplines.\nPage 14\narXiv Version 2\n\n\n1.4. Recommended reading\nIntroduction\nDeterminantal point processes in randomized numerical linear algebra, by Derezi´nski\nand Mahoney [DM21a].\nThis article gives an overview of RandNLA theory from the perspective of determi-\nnantal point processes and statistical data analysis. Among the many resources for\nlearning about RandNLA, it is notable for oﬀering a distinctly prospective (rather\nthan retrospective) viewpoint.\n1.4.4\nDeep investigations of speciﬁc topics\nFinding structure with randomness: probabilistic algorithms for constructing approxi-\nmate matrix decompositions, by Halko, Martinsson, and Tropp [HMT11].\nAs of late 2022, this article is the single most inﬂuential resource on RandNLA. Its\nintroduction includes a history of how randomized algorithms have been used in\nnumerical computing, as well as a brief summary of (then) active areas of research\nin RandNLA. Following the introduction, it focuses exclusively on low-rank approx-\nimation. It is extremely thorough in its treatment of both theory and practice.\nThis article is now somewhat out of date and is partially subsumed by [MT20].\nHowever, it is still of distinct value for the fact that it proves all of its main results\n(and in certain cases, by novel methods). It also includes some algorithms that are\nnot found in [MT20].\nRandomized algorithms in numerical linear algebra, by Kannan and Vempala [KV17a].\nThis survey provides a detailed theory of row and column sampling methods. It\nalso includes methods for tensor computations.\nRandomized methods for matrix computations, by Martinsson [Mar18].\nThis book chapter focuses on practical aspects of randomized algorithms for low-\nrank approximation. In this regard, it is important to note that while [HMT11]\nprovided thorough coverage of this topic at the time, the more recent [Mar18] reviews\nimportant practical advances developed after 2011. Among resources that provide\nan in-depth investigation into low-rank approximation, is notable for how it also\nincludes algorithms for full-rank matrix decomposition.\n1.4.5\nRandomized numerical linear algebra:\nFoundations and Algorithms\nMartinsson and Tropp’s recent Acta Numerica survey, [MT20], covers a wide range\nof topics, each with substantial technical and historical depth. We have beneﬁted\nfrom it tremendously in developing our plans for RandBLAS and RandLAPACK.\nBecause we have found this resource so useful – and, at the same time, because we\nhave gone through the trouble of writing a distinct monograph that is just as long\n– we think there is value in highlighting how it diﬀers from our work.\nBasic sketching.\nBy comparison to [MT20], we focus more on implementation than\non theory. The outcome of this is the broadest-yet review of the literature relevant\nto the implementation of sketching methods. In the appendices, we provide novel\ntechnical contributions to sketching theory and practice.\nSee Section 2 and Appendix A, [MT20, §7 – §9].\narXiv Version 2\nPage 15\n\n\nIntroduction\n1.4. Recommended reading\nLeast squares and optimization.\nOur coverage of these concepts is comprehensive,\ninsofar as optimization can be reduced to linear algebra. It also includes a number\nof novel technical contributions and a review of relevant software. By comparison,\n[MT20] provides very limited coverage of this area, as acknowledged in [MT20, §1.6].\nSee Section 3 and Appendix B, [MT20, §10].\nLow-rank approximation.\nOur approach here is very diﬀerent than that of [MT20].\nIt provides eﬀective scaﬀolding for a reader to get a handle on the vast literature on\nlow-rank approximation. However, it comes at the price of creating fewer opportu-\nnities for mathematical explanations. Separately, our coverage here is distinguished\nby providing an overview of software that implements randomized algorithms for\nlow-rank approximation.\nSee Section 4 and Appendix C, [MT20, §11 – §15].\nFull-rank matrix decompositions.\nBy comparison to [MT20], we emphasize a broader\nrange of matrix decompositions and more algorithms for computing them. One of\nthe algorithms we cover is novel and is accompanied by proofs that characterize its\nbehavior. For the algorithms covered here and in [MT20], the latter provides more\nmathematical detail.\nSee Section 5.1 and Section 5.2.1, Appendix D, [MT20, §16].\nKernel methods.\nRandomized methods have proven very eﬀective in processing\nmachine learning models based on positive deﬁnite kernels. They are also eﬀective\nin approximating matrices from scientiﬁc computing induced by indeﬁnite kernels.\nBoth of these topics are addressed in [MT20]. We only address the former topic,\nand we do so in a way that emphasizes the resulting linear algebra problems.\nSee Sections 3.2.2, 3.2.4, and 6.1.3, Appendix B.4.1, [MT20, §19, §20].\nLinear system solvers.\nWe cover slightly more material for solving unstructured\nlinear systems than [MT20]. However, we do not cover methods that are speciﬁc to\nsparse problems. As a result, we do not cover a prominent method for approximate\nCholesky decompositions of sparse graph Laplacians.\nSee Sections 3.2.3 and 5.2, [MT20, §17, §18].\nTrace estimation.\nWe have the luxury of being able to cover recently-developed\nalgorithms that were not available when [MT20] was written. This includes two\nmethods that provide the ﬁrst major advances in trace estimation since the late\n1980s. We provide less depth than [MT20] on average, with the notable exception\nof stochastic Lanczos quadrature.\nSee Section 5.3, [MT20, §4, §6].\nPage 16\narXiv Version 2\n\n\n1.5. Notation and terminology\nIntroduction\nAdvanced sketching.\nBoth this monograph and [MT20] cover leverage score sam-\npling and sketching operators with tensor product structures. We cover these topics\nin substantially more detail, spending a full ten pages on each of them. We do this\npartly because these topics complement one another: implicit matrices with ten-\nsor product structures are among the best candidates for practical leverage score\nsampling, nearly on par with kernel matrices from machine learning.\nSee Sections 6 and 7, [MT20, §7.4, §9.4, §9.6, §19.2.3].\n1.5\nNotation and terminology\nOur notation is summarized in Table 1.1; we also deﬁne some of this notation below\nas we explain basic concepts.\nMatrices and vectors\nLet A be an m×n matrix or linear operator. We use A∗to denote its adjoint (trans-\npose, in the real case) and A† to denote its pseudo-inverse. It is called Hermitian\nif A∗= A and positive semideﬁnite if it is Hermitian and all of its eigenvalues are\nnonnegative. We often abbreviate “positive semideﬁnite” with “psd.”\nWe sometimes ﬁnd it convenient to write A ∈Rm×n. However, it should be\nunderstood that the methods in this monograph generally apply to both real and\ncomplex matrices. We therefore tend to deﬁne a matrix by phrases like “A is m-by-\nn” or “an m-by-n matrix A.” We often call a vector of length n an n-vector; vectors\nare oriented as columns by default.\nFor m ≥n, a QR decomposition of A consists of an m × n column-orthonormal\nmatrix Q and an upper-triangular matrix R for which A = QR. Those familiar\nwith the NLA literature will note that this is typically called the economic QR\ndecomposition. If A has rank k < min(m, n), then we also consider it valid for\nQ to be m × k and for R to be k × n. We also consider QR decomposition with\ncolumn pivoting (QRCP). To describe QRCP, we say that if J = (j1, . . . , jn) is a\npermutation of JnK, then\nA[:, J] = [aj1, aj2, . . . , ajn]\nwhere ai is the ith column of A. In this notation, QRCP produces an index vector\nJ and factors (Q, R) that provide a QR decomposition of A[:, J].\nNow let A have rank k. Its singular value decomposition (SVD) takes the form\nA = UΣV∗, where the matrices (U, V) have k orthonormal columns and Σ =\ndiag(σ1, . . . , σk) is a square matrix with sorted entries σ1 ≥· · · ≥σk > 0. The SVD\ncan also be written as a sum of rank-one matrices: A = Pk\ni=1 σiuiv∗\ni , where (ui, vi)\nare the ith columns of (U, V) respectively. Those familiar with the NLA literature\nwill note that this is typically called the compact SVD.\nProbability and our usage of the term “random.”\nA Rademacher random variable uniformly takes values in {+1, −1}. The initialism\n“iid” expands to independent and identically distributed.\nWe often abuse terminology and say that a matrix “randomly” performs some\noperation. In reality, matrices only perform deterministic calculations, and ran-\ndomness only comes into play when the matrix is ﬁrst constructed. This convention\narXiv Version 2\nPage 17\n\n\nIntroduction\n1.5. Notation and terminology\nextends to “matrices” that are abstract linear operators, in which case randomness\nis only involved in constructing the data that deﬁnes the operator.\nUnqualiﬁed use of the term “random” before performing an action with a ﬁnite\nset of outcomes (such as sampling components from a vector, applying a permuta-\ntion, etc...) means the randomness is uniform over the space of possible actions.\nPage 18\narXiv Version 2\n\n\n1.5. Notation and terminology\nIntroduction\nTable 1.1: Notation\nArrays and indexing\nAij or A[i, j]\n(i, j)th entry of a matrix A\nai\nor A[:, i]\nith column of A\nvi\nor v[i]\nith component of a vector v\nJmK\nindex set of integers from 1 to m\nI or J\npartial permutation vector for indexing into an array\n|I|\nlength of an index vector\nA[I, :]\nsubmatrix consisting of (permuted) rows of A\nA[:, J]\nsubmatrix consisting of (permuted) columns of A\n:k\nindex into the leading k elements of an array,\nalong an axis of length at least k\nk:\nindex into the trailing n −k + 1 elements of an array,\nalong an axis of length n ≥k\nReserved symbols\nS\nsketching operator\nIk\nidentity matrix of size k × k\nδi\nith standard basis vector of implied dimension\n0n\nzero vector of length n\n0m×n\nzero matrix of size m × n\nLinear algebra\n∥x∥2 or ∥x∥\nEuclidean norm of a vector x\n∥A∥2\nspectral norm of A\n∥A∥F\nFrobenius norm of A\ncond(A)\nEuclidean condition number of A\nλi(A)\nith largest eigenvalue of A\nσi(A)\nith largest singular value of A\nA∗\nadjoint (transpose, in the real case) of A\nA†\nMoore–Penrose pseudoinverse of A\nA1/2\nHermitian matrix square root\nA ⪯B\nthe matrix B −A is positive semideﬁnite\nMatrix decomposition conventions\nA = QR\nQR decomposition (economic, by default)\n(Q, R, J) = qrcp(A)\nQR with column-pivoting; A[:, J] = QR.\nA = UΣV∗\nsingular value decomposition (compact, by default)\nR = chol(G)\nupper triangular Cholesky factor of G = R∗R\nProbability\nX ∼D\nX is a random variable following a distribution D\nE[X]\nexpected value of a random matrix X\nvar(X)\nvariance of a random variable X\nPr{E}\nprobability of the event E\narXiv Version 2\nPage 19\n\n\nIntroduction\n1.5. Notation and terminology\nPage 20\narXiv Version 2\n\n\nSection 2\nBasic Sketching\n2.1 A high-level plan .........................................................\n22\n2.1.1 Random number generation ........................................\n23\n2.1.2 Portability, reproducibility and exception handling .........\n24\n2.2 Helpful things to know about sketching ......................\n24\n2.2.1 Geometric interpretations of sketching ..........................\n25\n2.2.2 Sketch quality ..........................................................\n26\n2.2.3 (In)essential properties of sketching distributions ...........\n29\n2.3 Dense sketching operators ..........................................\n30\n2.4 Sparse sketching operators ..........................................\n32\n2.4.1 Short-axis-sparse sketching operators ...........................\n33\n2.4.2 Long-axis-sparse sketching operators ............................\n34\n2.5 Subsampled fast trigonometric transforms ..................\n35\n2.6 Multi-sketch and quadratic-sketch routines ................\n36\nThe BLAS (Basic Linear Algebra Subprograms) were originally a collection of\nFortran routines for computations including vector scaling, vector addition, and\napplying Givens rotations [LHK+79].\nThey were later extended to operations\nsuch as matrix-vector multiplication and triangular solves [DDH+88] as well as\nmatrix-matrix multiplication, block triangular solves, and symmetric rank-k up-\ndates [DDH+90]. These routines have subsequently been organized into three levels\ncalled BLAS 1, BLAS 2, and BLAS 3.\nOver the years the BLAS have evolved into a community standard, with im-\nplementations targeting diﬀerent machine architectures in many programming lan-\nguages. This standardization has been instrumental in the development of linear\nalgebra libraries – from the early days of LINPACK, through to LAPACK, and on\nto modern libraries such as PLASMA and SLATE [DMB+79; DDD+87; ABB+99;\nADD+09; DGH+19; KWG+17; AAB+17]. It has also reduced the coupling be-\ntween hardware and software design for NLA. Indeed, the spirit of the BLAS has\nbeen adapted to accommodate dramatic changes in prevailing architectures, such\nas those faced by ScaLAPACK and MAGMA [CDO+95; CDD+96; TDB10; NTD10].\nThis section summarizes our progress on the design of a “RandBLAS” library,\n21\n\n\nBasic Sketching\n2.1. A high-level plan\nwhich is to be to RandNLA as BLAS is to classical NLA. Section 2.1 begins by\nspeaking to high-level scope and design considerations. From there, Section 2.2\nsummarizes sketching concepts that remain important throughout this monograph;\nwe encourage the reader to not dwell too long on this section and instead return\nto it as-needed later on. Sections 2.3 through 2.6 present our plans for sketching\ndense data matrices. In brief: our near-term plans are for the RandBLAS to sup-\nport sketching operators which could naturally be represented by dense arrays or\nby sparse matrices with certain structures; we consider row sampling and column\nsampling as particular types of sparse sketching.\n2.1\nA high-level plan\nWe begin with a simple premise.\nThe RandBLAS’ deﬁning purpose should be to facilitate implementation\nof high-level RandNLA algorithms.\nThis premise works to reduce the RandBLAS’ scope, as there are “basic” operations\nin RandNLA which do not support this purpose.1 Another way that we reduce the\nscope of the RandBLAS is to only consider sketching dense data matrices. It may be\nreasonable to lift this restriction in the future, and consider methods for producing\ndense sketches of sparse data matrices.\nOur premise for the RandBLAS suggests that it should be concerned with data-\noblivious sketching – that is, sketching without consideration to the numerical prop-\nerties of a dataset. We identify three categories of operations on this topic:\n• sampling a random sketching operator from a prescribed distribution,\n• applying a sampled sketching operator to a data matrix, and\n• sketching that is not naturally expressed as applying a single a linear operator\nto a data matrix.\nThese categories are somewhat analogous to BLAS 1, BLAS 2, and BLAS 3, insofar\nas their implementations admit more and more opportunities for machine-speciﬁc\nperformance optimizations.\nAt this time, however, we do not advocate for any\nformalization of “RandBLAS levels.”\nWe note that data-oblivious sketching is not the only kind of sketching of value in\nRandNLA. Indeed, data-aware sketching operators such as those derived from power\niteration are extremely important for low-rank approximation (see Section 4.3.1).\nMethods for row or column sampling based on leverage scores are also useful for ker-\nnel ridge regression and certain tensor computations; see Sections 6 and 7. Although\nimportant, most of the functionality for producing or applying these sketching op-\nerators should be addressed in higher-level libraries.\nIn the material under the next two headings, we address the questions of how\nto handle random number generation and reproducibility in the RandBLAS.\n1For example, the problem of accepting two matrices and using randomization to approximate\ntheir product is certainly basic, and it is of conceptual value [DKM06a]. However, it is rarely used\nas an explicit building block in higher-level RandNLA algorithms.\nPage 22\narXiv Version 2\n\n\n2.1. A high-level plan\nBasic Sketching\n2.1.1\nRandom number generation\nFor reproducibility’s sake it is important that the RandBLAS include a speciﬁcation\nfor random number generators (RNGs).\nWe believe the RandBLAS should use counter-based random number generators\n(CBRNGs), which were ﬁrst proposed in [SMD+11]. A CBRNG returns a random\nnumber upon being called with two integer parameters: the counter and the key.\nThe time required for the CBRNG to return does not depend on either of these\nparameters. A serial application can set the key at the outset of the program and\nnever change it.\nParallel applications (particularly parallel simulations) can use\ndiﬀerent keys across diﬀerent threads. Sequential calls to the CBRNG with a ﬁxed\nkey should use diﬀerent values for the counter. For a ﬁxed key, a CBRNG with a\np-bit integer counter deﬁnes a stream of random numbers with period length 2p.\nIn our context, CBRNGs are preferable to traditional state-based RNGs such\nas the Mersenne Twister. A key reason for this is that CBRNGs maximize ﬂexibil-\nity in the order in which a sketching operator is generated. For example, given a\nuser-provided counter oﬀset c which acts as a random seed, the (i, j)th entry of a\ndense d×m sketching operator can be generated with counter c+(i+dj). The fact\nthat these computations are embarrassingly parallel will be important for vendors\ndeveloping optimized RandBLAS implementations. We note that this ﬂexibility also\nprovides an advantage over widely-used linear congruential RNGs, which have sep-\narate shortcomings of performing very poorly on statistical tests [SMD+11, §2.2.1].\nParticular examples of CBRNGs include Philox, ARS, and Threeﬁsh, each of\nwhich was deﬁned in [SMD+11] and implemented in the Random123 library. These\nCBRNGs have periods of 2128, can support 264 diﬀerent keys, and pass standard\nstatistical tests for random number generators. Random123 provides the core of the\nsketching layer of the LibSkylark RandNLA library [KAI+15]. Implementations of\nPhilox and ARS can also be found in MKL Vector Statistics [Int19, §6.5].\nShift-register RNGs\nWe have observed that the CBRNGs in Random123 are signiﬁcantly more expen-\nsive than the state-based shift-register RNGs developed by Blackman and Vigna\n[BV21]. In fact, Blackman and Vigna’s generators are so fast that we have been\nable to implement a method for applying a Gaussian sketching operator to a sparse\nmatrix that beats Intel MKL’s sparse-times-dense matrix multiplication methods.\nHowever, in the application where we observed that performance, processing the\nsketch downstream was more expensive than computing the sketch in the ﬁrst place.\nTherefore, while CBRNGs were substantially more expensive in that application,\ntheir longer runtimes were inconsequential in that case. This longer runtime can be\nviewed as a price we pay for prioritizing reproducibility of sketching across compute\nenvironments with diﬀerent levels of parallelism.\nThe overall situation is this:\nState-based RNGs may be preferable to CBRNGs if sketching is the bot-\ntleneck in a RandNLA algorithm and where the cost of random number\ngeneration decisively aﬀects the cost of sketching. At this time we have\nno evidence that high-performance implementations of RandNLA algo-\nrithms run into such bottlenecks. Such evidence may arise in the future\nand warrant reconsideration to fast state-based RNGs for the Rand-\narXiv Version 2\nPage 23\n\n\nBasic Sketching\n2.2. Helpful things to know about sketching\nBLAS, particularly if major advances are made in hardware-accelerated\nsketching algorithms.\n2.1.2\nPortability, reproducibility and exception handling\nWe believe it is important that the RandBLAS lends itself to portability across\nprogramming languages.\nTherefore we plan for the RandBLAS to have a proce-\ndural API and make use of no special data structures beyond elementary structs.\nHigher-level libraries should take responsibility for exposing RandBLAS function-\nality with sophisticated abstractions. In particular, we plan for RandLAPACK to\nexpose RandBLAS functionality through a suitable object-oriented linear operator\ninterface. A key goal of this interface will be to make it possible to implement high-\nlevel RandNLA algorithms with minimal assumptions on the sketching operator’s\ndistribution. Such an interface will also reduce the coupling between determining\nRandBLAS’s procedural API and prototyping RandLAPACK.\nDebugging high-performance numerical code is notoriously diﬃcult. Care must\nbe taken in the design of the RandBLAS so as to not contribute to this diﬃculty.\nIndeed, it is essential that the RandBLAS be reproducible to the greatest extent\npossible. The actual extent of the reproducibility will depend on factors outside\nof our control. For example – the RandBLAS cannot oﬀer bitwise reproducibility\nguarantees unless the BLAS does the same (see Remark 2.1.1). Therefore the main\nchallenge for reproducibility for RandBLAS is in random number generation; this\nchallenge can be resolved comprehensively through the aforementioned CBRNGs.\nA key source of exceptions in NLA is the presence of NaNs or Infs in problem\ndata. Extremely sparse sketching matrices (such as those from Section 2.4.2) might\nnot even read every entry of a data matrix, and so they might miss a NaN or\nInf. Those routines will be clearly marked as carrying this risk. The majority\nof routines in the RandBLAS and RandLAPACK will not carry this risk: they will\npropagate NaNs and Infs. (See [DDG+22] for a more detailed discussion of how\nthe BLAS and LAPACK (should) deal with exceptions.) For any such routine the\nexact behavior will depend on how the random sketching operator interacts with the\nproblem data. For example, if a data matrix containing multiple Infs is sketched\ntwice using diﬀerent random seeds, then it is possible that an entry of the ﬁrst\nsketch is an Inf while the corresponding entry of the second sketch is a NaN.\nRemark 2.1.1. Making the BLAS bitwise reproducible is challenging because ﬂoating-\npoint addition is not associative, and the order of summation can vary depending on\nthe use of parallelism, vectorization, and other matters [RDA18]. Summation algo-\nrithms that guarantee bitwise reproducibility do exist [ADN20]. These algorithms\nmay become practical on hardware that implements the latest IEEE 754 ﬂoating\npoint standard, which includes a recommended instruction for bitwise-reproducible\nsummation [IEE19]. However, we leave these matters to future work.\n2.2\nHelpful things to know about sketching\nThe purpose of sketching is to enact dimension-reduction so that computations\nof interest can be performed on a smaller matrix called a sketch. While precise\ncomputations performed on the sketch can vary dramatically, the simple statement\nof sketching’s purpose lets us deduce the following facts.\nPage 24\narXiv Version 2\n\n\n2.2. Helpful things to know about sketching\nBasic Sketching\n• Sketching operators applied to the left of a data matrix must must be wide\n(i.e., they must have more columns than rows).\n• Sketching operators applied to the right of a data matrix must be tall (i.e.,\nthey must have more rows than columns).\nThis is to say, in left-sketching we require that SA has fewer rows than A, and in\nright-sketching we require that AS has fewer columns than A. These facts are true\nregardless of the aspect ratio of the data matrix; see Figure 2.1 for an illustration.\nThe facts are important because sketching operators in the literature are often\ndeﬁned under the assumption of left-sketching.\nBefore we proceed further, we reiterate some important advice.\nWe encourage the reader to not dwell too long on this section (Section 2.2)\nand instead return to it as needed later on.\nWith that, Section 2.2.1 explains geometric interpretations of sketching from the left\nand right. It also introduces the concepts of “sketching in the embedding regime”\nand “sketching in the sampling regime.” Section 2.2.2 covers concepts of subspace\nembedding distortion and the oblivious subspace embedding property – these are\ncentral to RandNLA theory, but they play a modest role in this monograph. Sec-\ntion 2.2.3 states properties of sketching distributions that should hold as part of a\n‘sanity check’ for whether a proposed distribution is reasonable.\nSA\nS\nA\nAS\nA\nS\n(a)\n(b)\nFigure 2.1: The left plot (a) shows that a sketching operator S applied to the left\nof a matrix A is wide, whereas the right plot (b) shows that a sketching operator S\napplied to the right of a matrix A is tall. These stated properties hold universally;\nthere are no exceptions for any kind of sketching. Separately, we note that both cases\nin the ﬁgure illustrate sketching in the sampling regime in the sense of Section 2.2.1.\n2.2.1\nGeometric interpretations of sketching\nPrototypical left-sketching and right-sketching\nSketching A from the left preserves its number of columns. Therefore, it is suitable\nfor things such as estimating right singular vectors. We often interpret a left-sketch\nSA as a compression of the range of A to a space of lower ambient dimension. In\nthe special case when rank(SA) = rank(A), there is a sense in which the quality of\nthe compression is completely independent from the spectrum of A.\nSketching A from the right preserves its number of rows, and is suitable for things\nsuch as estimating left singular vectors. Conceptually, the right-sketch AS can be\ninterpreted as a sample from the range of A using a test matrix S. In the special\ncase when rank(AS) ≪rank(A) then it is appropriate to think of this as a “lossy\nsample” from a much larger “population,” and it is natural to want this sample to\ncapture as much information as possible for some sub-population of interest.\narXiv Version 2\nPage 25\n\n\nBasic Sketching\n2.2. Helpful things to know about sketching\nEquivalence of left-sketching and right-sketching\nLeft-sketching and right-sketching can be reduced to one another by replacing A\nand S by their adjoints. For example, a left-sketch SA can be viewed as a sample\nfrom the row space of A, equivalent to the right-sketch A∗S∗. Conversely, a right-\nsketch AS can be viewed as a compression of the row space of A, equivalent to the\nleft-sketch S∗A∗. Therefore it is artiﬁcial to strongly distinguish sketching operators\nby whether they are ﬁrst deﬁned for left-sketching or right-sketching.\nThis leads us to an important point.\nIf Dd,m is a distribution over wide d×m sketching operators, it is canon-\nically extended to a distribution over tall n × d sketching operators by\nsampling T from Dd,n and then returning the adjoint S = T∗.\nThe notation in the statement above is carefully chosen: since our “data matrices”\nare typically m × n, a typical left-sketching operator requires m columns, and a\ntypical right-sketching operator requires n rows.\nThe embedding and sampling regimes\nWhile it is artiﬁcial to associate a sketching distribution only with left-sketching\nor only with right-sketching, there are indeed families of sketching operators that\nare suited to qualitatively diﬀerent situations. The following terms help with our\ndiscussion of such families.\nSketching in the embedding regime is the use of a sketching operator\nthat is larger than the data to be sketched. Sketching in the sampling\nregime is the use of a sketching operator that is far smaller than the\ndata to be sketched.\nIn the above deﬁnitions one quantiﬁes the size of an operator (or matrix) by the\nproduct of its number of rows and number of columns.\nIn Section 3, we will see that sketching in the embedding regime is nearly univer-\nsal in randomized algorithms for least squares and related problems. In Section 4,\nwe will see that sketching in the sampling regime is the foundation of randomized\nalgorithms for low-rank approximation. Over these sections we tend to see sketch-\ning in the embedding regime happen from the left, and sketching in the sampling\nregime happen from the right. We stress that these tendencies are consequences of\nexposition; they do not always hold when developing or using RandNLA software.\n2.2.2\nSketch quality\nLet L be a subset of some high-dimensional Euclidean space Rm, S be a sketching\noperator deﬁned on Rm, and consider the sketch SL. Intuitively, SL should be\nuseful if its geometry is somehow “eﬀectively the same” as that of L.\nHere we\ndiscuss the preferred ways to quantify changes to geometry in RandNLA. We focus\non methods suitable for when L is a linear subspace, but we also consider when L\nis a ﬁnite point set.\nWe acknowledge up-front that it only does so much good to measure the quality\nof an individual sketch. Indeed, in order to make predictive statements about the\nbehavior of algorithms, it is necessary to understand how the distribution of a\nsketching operator S ∼D induces a distribution over measures of sketch quality in\nPage 26\narXiv Version 2\n\n\n2.2. Helpful things to know about sketching\nBasic Sketching\na given application. It is further necessary to analyze families of distributions Dd,m\nparameterized by an embedding dimension d, since the size of a sketch is often a\nkey parameter that a user can control.\nSubspace embeddings\nLet S be a d×m sketching operator and L be a linear subspace of Rm. We say that\nS embeds L into Rd and that it does so with distortion δ ∈[0, 1] if x ∈L implies\n(1 −δ)∥x∥2 ≤∥Sx∥2 ≤(1 + δ)∥x∥2.\n(2.1)\nWe often call such an operator an δ-embedding.\nThe concept of a subspace embedding was ﬁrst used implicitly in RandNLA by\n[DMM06]; the sketching operators used in [DMM06] were based on a type of data-\naware sketching called leverage score sampling (discussed in Section 6). The ﬁrst\nexplicit deﬁnition of subspace embeddings was given by [Sar06], who focused on\ndata-oblivious sketching. We address data-oblivious subspace embeddings in detail\nmomentarily.\nThe most transparent use of subspace embedding distortion arises when L is the\nrange of a matrix A. In this context, S is a δ-embedding for L if and only if the\nfollowing two-sided linear matrix inequality holds:\n(1 −δ)2A∗A ⪯(SA)∗(SA) ⪯(1 + δ)2A∗A.\n(2.2)\nIn other words, the distortion of S as an embedding for range(A) is a measurement\nof how well the Gram matrix of SA approximates that of A.\nNote that in order for S to be a subspace embedding for L it is necessary that\nd ≥dim(L). Therefore if L is the range of an m × n matrix of full-column-rank,\nthe requirement that d ≥dim(L) means that subspace embeddings can only be\nachieved when “sketching in the embedding regime,” in the sense of Section 2.2.1.\nFurthermore, substantial dimension reduction can only be achieved in this frame-\nwork when m ≫n.\nEﬀective distortion\nSubspace embedding distortion is the most common measure of sketch quality, but\nit is not without its limitations. Its greatest limitation is that it is not invariant\nunder scaling of S (i.e., it is not invariant under replacing S ←tS for t ̸= 0). This is\na signiﬁcant limitation since many RandNLA algorithms are invariant under scaling\nof S; existing theoretical analyses of RandNLA algorithms simply do not take this\ninto account.\nIn Appendix A.1 we explore a novel concept of eﬀective distortion that resolves\nthe scale-sensitivity problem. Formally, the eﬀective distortion of a sketching oper-\nator S for a subspace L is\nDe(S; L) = inf{ δ : 0 ≤δ ≤1, 0 < t\n(2.3)\ntS is a δ-embedding for L}.\nIn words, this is the minimum distortion that any sketching operator tS can achieve\nfor L, optimizing over t > 0. We brieﬂy reference this concept in our discussion of\nalgorithms for least squares and optimization (§3.2). Appendix B.1 makes deeper\nconnections between eﬀective distortion and randomized preconditioning methods\nfor least squares.\narXiv Version 2\nPage 27\n\n\nBasic Sketching\n2.2. Helpful things to know about sketching\nOblivious subspace embeddings\nData-oblivious subspace embedding (OSEs) were ﬁrst used in RandNLA in [Sar06]\nand were largely popularized by [Woo14].\nThere is a clean way to describe the\n“reliability” of a sketching distribution in this setting.\nConsider a distribution D over wide d × m matrices. We say that D has\nthe OSE property with parameters (δ, n, p) if, for every n-dimensional\nsubspace L ⊂Rm, we have\nPr{S ∼D is a δ-embedding for L} ≥1 −p.\nTheoretical analyses of sketching distributions often concern bounding d as a func-\ntion of (δ, n, p) to ensure that D satisﬁes the OSE property. Naturally, all else equal,\nwe would like to achieve the OSE property for smaller values of d.\nTheoretical results can be used to select d in practice for very well-behaved distri-\nbutions, particularly the Gaussian distribution. Results for the more sophisticated\ndistributions (such as those of sparse sketching operators) tend to be pessimistic\ncompared to what is observed in practice. Some of this pessimism stems from the\nexistence of esoteric constructions which indeed call for large embedding dimensions.\nSetting these constructions aside, we have reason to be optimistic since distortion is\nactually not the ideal measure of sketch quality in many settings. Indeed, eﬀective\ndistortion is far more relevant for least squares and optimization, and it will always\nbe no larger than the standard notion of distortion.\nAll in all, there is something of an art to choosing the best sketching distribution\nfor a particular RandNLA task. Luckily, for most RandNLA algorithms it is far\nfrom necessary to choose the “best” sketching distribution; good results can be\nobtained even when setting distribution parameters by simple rules of thumb.\nJohnson–Lindenstrauss embeddings\nLet S be a d × m sketching operator and L be a ﬁnite point set in Rm. We say that\nS is a Johnson–Lindenstrauss embedding (or “JL embedding”) for L with distortion\nδ if, for all distinct x, y in L, we have\n1 −δ ≤∥S(x −y)∥2\n2\n∥x −y∥2\n2\n≤1 + δ.\nThis property is named for a seminal result by William Johnson and Joram Lin-\ndenstrauss, who used randomization to prove the existence of operators satisfying\nthis property where d is logarithmic in |L| and linear in 1/δ2 [JL84].\nThe JL Lemma (as the result is now known) is remarkable for two reasons.\nFirst, the requisite value for d did not depend on the ambient dimension m and\nwas only logarithmic in |L|. Second, the construction of the transformation S was\ndata-oblivious – a scaled orthogonal projection. This latter fact led to questions\nabout how one might deﬁne alternative distributions over sketching operators, with\nthe aim of\n1. being simpler to implement than a scaled orthogonal projection, and\n2. attaining similar “data-oblivious JL properties.”\nPage 28\narXiv Version 2\n\n\n2.2. Helpful things to know about sketching\nBasic Sketching\nIt so happened that many constructions could achieve these goals. For example,\n[IM98] and [DG03] relaxed the condition of being a (scaled) orthogonal projector to\nS having iid Gaussian entries, which still results in a rotationally-invariant distri-\nbution. As another example, [Ach03] relaxed the rotational invariance by choosing\nthe entries of S to be scaled Rademacher random variables.\n2.2.3\n(In)essential properties of sketching distributions\nDistributions D over wide sketching operators are typically designed so that, for\nS ∼D, the mean and covariance matrices are\nE[S] = 0\nand\nE[S∗S] = I.\nThe property that E[S] = 0 is important – if not ubiquitous – in RandNLA. How-\never, there is some ﬂexibility in the latter property, as in most situations it suﬃces\nfor the covariance matrix to be a scalar multiple of the identity.\nTo understand why we have ﬂexibility in the scale of the covariance matrix,\nconsider how E[S∗S] = I is equivalent to S preserving squared Euclidean norms\nin expectation. As it happens, the vast majority of algorithms mentioned in this\nmonograph do not need sketching operators to preserve norms. Rather, they rely\non sketching preserving relative norms, in the sense that ∥Su∥2/∥Sv∥2 should be\nclose to ∥u∥2/∥v∥2 for all vectors u, v in a set of interest. Such a property is clearly\nunaﬀected if every entry of S scaled by a ﬁxed nonzero constant (i.e., if S is replaced\nby tS for some t ̸= 0).\nThis section uses scale-agnosticism to help describe sketching distributions with\nreduced emphasis on whether the operator is wide or tall. For example, if the entries\nof S are iid mean-zero random variables of ﬁnite variance, then both E[S∗S] and\nE[SS∗] are scalar multiples of the identity matrix. Speaking loosely, the former\nproperty justiﬁes using S to sketch from the left and the latter property justiﬁes\nusing S∗to sketch from the right.\nWith this observation in mind, this section ignores most matters of scaling that\nis applied equally to all entries of a sketching operator. This manifests in how we\nregularly describe sketching operators as having entries in [−1, 1] even though it is\nmore common to have entries in [−v, v] for some positive v (which is set to achieve\nan identity covariance matrix). Note that this does not confer freedom to scale\nindividual rows, columns, or entries of a sketching operator separately from one\nanother.\nThe main places where scaling matters are in algorithms for norm estimation\n(see Section 4.3.5) and algorithms which only sketch a portion of the data in a larger\nproblem. The subtleties in this latter situation warrant a detailed explanation.\nScale sensitivity: partial sketching\nLet G be an n × n psd matrix and A be a very tall m × n matrix. Suppose that we\napproximate\nH = A∗A + G\nby a partial sketch\nHsk = (SoA)∗(SoA) + G\nwhere So is a d × m sketching operator. How should we understand the statistical\nproperties of Hsk as an estimator for H?\narXiv Version 2\nPage 29\n\n\nBasic Sketching\n2.3. Dense sketching operators\nAt the simplest level we can turn to the idea of subspace embedding distortion.\nUsing the characterization of distortion in (2.2), we could study the distribution of\nthe minimum δ ∈(0, 1) for which\n(1 −δ)2H ⪯Hsk ⪯(1 + δ)2H.\nOne can go beyond distortion by lifting to a higher-dimensional space. Letting\n√\nG\ndenote the Hermitian square root of G, we deﬁne the augmented sketching operator\nand augmented data matrix\nS =\n\u0014So\n0\n0\nI\n\u0015\nand\nAG =\n\u0014 A\n√\nG\n\u0015\nThis lets us express H = A∗\nGAG and Hsk = (SAG)∗(SAG). Therefore the statistical\nproperties of Hsk as an approximation to H can be understood in terms of how S\npreserves (or distorts) the range of AG.\nScale sensitivity: row sampling from block matrices\nThe concept of partial sketching can arise when sketching block matrices, which\nare indeed encountered in many applications. For example, it is widely appreciated\nthat a ridge regression problem with tall m × n data matrix A and regularization\nparameter µ can be lifted to an ordinary least squares problem with data matrix\nAµ := [A; √µI].\nSuppose we want to sketch Aµ by a row sampling operator S.\nIt is natural\nto treat the lower n rows of Aµ diﬀerently than its upper m rows. In particular,\nit is natural for S to be an operator that produces SA = [SoA; √µI] with some\nother d × m row sampling operator So. Here, even if So sampled rows from A uni-\nformly at random, the map Aµ 7→SAµ would not sample uniformly at random from\nAµ. Therefore there is a sense in which partial sketching is a way of incorporating\nnon-uniform row sampling into other sketching distributions; see [DKM06a] for the\norigins of this interpretation. In the context of this speciﬁc example, the nonuni-\nformity would necessitate that So be scaled to have entries in {0, ±1/\n√\nd}.\nWe\nrefer the reader to Section 2.4.2 and Section 6.1.1 for more discussion on sketching\noperators that implement row sampling.\n2.3\nDense sketching operators\nThe RandBLAS should provide methods for sampling sketching operators with iid\nentries drawn from distinguished distributions. Across this broad category, we be-\nlieve the following types of operators stand out:\n• Rademacher sketching operators: entries are ±1 with equal probability;\n• uniform sketching operators: entries are uniform over [−1, 1];\n• Gaussian sketching operators: entries follow the standard normal distribution.\nWe believe the RandBLAS should also support sampling row-orthonormal or column-\northonormal matrices uniformly at random from the set of all such matrices.\nPage 30\narXiv Version 2\n\n\n2.3. Dense sketching operators\nBasic Sketching\nThe theoretical results for Gaussian operators are especially strong. However,\nthere is little practical diﬀerence in the performance of RandNLA algorithms be-\ntween any of the three entrywise iid operators given above. This is reﬂected in\nimplementations such as [LLS+17] that only use uniform sketching operators. The\npractical equivalence between these types of sketching operators also has theoreti-\ncal support through universality principles in high-dimensional probability [Ver18],\n[OT17], [MT20, §8.8], [DLL+20].\nIn what follows we speak to implementation\ndetails and the intended use cases for these operators.\nSampling iid-dense sketching operators\nSampling from the Rademacher or uniform distributions is the most basic operation\nof random number generators. Methods for sampling from the Gaussian distribution\ninvolve transforming random variables sampled uniformly from [0, 1]. There are two\ntransformations of interest for the RandBLAS: Box-Muller [BM58]; and the Ziggurat\ntransform [MT00]. The former should be included in the RandBLAS because it is\neasy to implement and parallelizes well. The latter method is far more eﬃcient on\na single thread, and it has been used within RandNLA (see [MSM14]), but it does\nnot parallelize well [Ngu07, §37.2.3]. We postpone any recommendation for whether\nit should be an option in the RandBLAS.\nRandNLA algorithms tend to be very robust to the quality of the random num-\nber generator. As a result, it is not necessary for us to sample from the Gaussian\ndistribution with high statistical accuracy. This is due in part to the aforementioned\nuniversality principles, and it can be seen through the success of sub-Gaussian distri-\nbutions as an analysis framework in high-dimensional probability [Ver18, §2]. From\nan implementation standpoint, there is likely no need to sample from the Gaussian\ndistribution beyond single precision [Mar22a]. It is worth exploring if even lower\nprecisions (e.g., half-precision) would suﬃce for practical purposes.\nApplying iid-dense sketching operators\nIf a dense sketching operator is realized explicitly in memory then it can (and\nshould) be applied by an appropriate BLAS function, most likely gemm.\nMany\nRandNLA algorithms provide good practical performance even with such simple\nimplementations, although there is potential for reduced memory or communication\nrequirements if a sketching operator is applied without ever fully allocating it in-\nmemory. There is a large design space for such algorithms with iid-dense sketching\noperators when using counter-based random number generators (see Section 2.1.1).\nSuch functionality could appear in an initial version of a RandBLAS standard. The\nreference implementations of such functions could start as mere wrappers around\nroutines to generate a sketching operator and then apply that operator via gemm.\nSampling and applying Haar operators\nIf we suppose left-sketching, then the Haar distribution is the uniform distribution\nover row-orthonormal matrices. If we instead suppose right-sketching, then it is the\nuniform distribution over column-orthonormal matrices. We call these operators\n“dense” because if one is sampled and then formed explicitly, it will be dense with\nprobability one.\narXiv Version 2\nPage 31\n\n\nBasic Sketching\n2.4. Sparse sketching operators\nThere are two qualitative approaches to sampling from this distribution. The\nnaive approach essentially requires sampling from a Gaussian distribution and per-\nforming a QR factorization, at a total cost of O(d2m); see [Li92, §1 - §4] and more\ngeneral methods in [Mez07]. A more eﬃcient approach – which costs only O(dm)\ntime – involves constructing the operator as a composition of suitable Householder\nreﬂectors [Ste80]. This approach has the secondary beneﬁt of not needing to form\nthe sketching operator explicitly.\nHaar operators are of interest not just for sketching in RandNLA algorithms but\nalso for generating test data for evaluating other sketching operators. As such, we\nbelieve they are natural to include in a ﬁrst version of a RandBLAS standard.\nIntended use-cases\nUsing terminology from Section 2.2.1, dense sketching operators are commonly used\nfor “sketching in the sampling regime.” In particular, they are the workhorses of\nrandomized algorithms for low-rank approximation. They also have applications\nin certain randomized algorithms for ridge regression and some full-rank matrix\ndecomposition problems.\nThese distributions are much less useful for sketching dense matrices “in the\nembedding regime” (again in the sense of Section 2.2.1).\nThis is because they\nare more expensive to apply to dense matrices than many other types of sketching\noperators. These types of sketching operators might be of interest in the embedding\nregime if applied to sparse or otherwise structured data matrices.\n2.4\nSparse sketching operators\nThe RandNLA literature describes many types of sparse sketching operators, almost\nalways under the convention of sketching from the left. We think it is important to\ndeﬁne sketching distributions in a way that is agnostic to sketching from the left or\nright. Indeed, while we often focus on left-sketching for ease of exposition, asserting\nthat this is “without loss of generality” ignores the plight of the user tasked with\nright-sketching.\nIn order to achieve our desired agnosticism, we use a taxonomy for sparse sketch-\ning operators which has not appeared in prior literature. To describe it, we use the\nterm short-axis vector in reference to the columns of a wide matrix or rows of a\ntall matrix. The term long-axis vector is deﬁned analogously, as the rows of a wide\nmatrix or columns of a tall matrix. In these terms, we have the following families\nof sparse sketching operators.\n• Short-axis-sparse sketching operators. The short-axis vectors of these opera-\ntors are independent of one another. Each short-axis vector has a ﬁxed (and\nvery small) number of nonzeros. Typically, the indices of the nonzeros in each\nshort-axis vector are sampled uniformly without replacement.\n• Long-axis-sparse sketching operators. The long-axis vectors of these operators\nare independent of one another. For a given long-axis vector, the indices for its\nnonzeros are sampled with replacement according to a prescribed probability\ndistribution (which can be uniform). The value of a given nonzero is aﬀected\nby the number of times its index appears in the sample for that vector.\nPage 32\narXiv Version 2\n\n\n2.4. Sparse sketching operators\nBasic Sketching\n• Iid-sparse sketching operators.\nMathematically, these can be described as\nstarting with an iid-dense sketching operator and “zeroing-out” entries in an\niid-manner with some high probability. (From an implementation standpoint\nthis would work the other way around, randomly choosing a few entries to\nmake nonzero.)\nWhen abbreviations are necessary, we suggest that short-axis-sparse sketching op-\nerators be called SASOs and that long-axis-sparse sketching operators be called\nLASOs. Most of our use of such abbreviations appears here and in Appendix A.2.\nA visualization of these types of sketching operators is given in Figure 2.2.\nBefore proceeding further we should say that we are not in favor of including iid-\nsparse sketching operators in the RandBLAS. Our ﬁrst reason for this is that their\ntheoretical guarantees are not as strong as either SASOs (see the discussion at the\nend of [Tro20, §7.4] and remarks in [Lib09, §2.4]) or LASOs [DLD+21; DLP+21].\nOur second reason is that their lack of predictable structure makes it harder to\nimplement eﬃcient parallel algorithms for applying these operators. Therefore in\nwhat follows we only give details on SASOs and LASOs.\nA\nS\nA\nS\nA\nS\n(a)\n(b)\n(c)\nFigure 2.2: Illustration of a SASO (a) with 3 non-zero entries per row, LASO (b)\nwith 3 non-zero entries per column, and an iid-sparse sketching operator (c) with\niid non-zero entries.\n2.4.1\nShort-axis-sparse sketching operators\nSASOs include sketching operators known as sparse Johnson–Lindenstrauss trans-\nforms, the Clarkson–Woodruﬀtransform, CountSketch, and OSNAPs [KN12; CW13;\nMM13; NN13]. These constructions are all described assuming we sketch from the\nleft, and as such, they are all stated for wide sketching operators. They are described\nas having a ﬁxed number of nonzeros within each column. The more general notion\n(for sketching from the left or right) is to say there is a ﬁxed number of nonzeros\nper short-axis vector.\nThe short-axis vectors of a SASO should be independent of one another. One\ncan select the locations of nonzero elements in diﬀerent ways; we are interested in\ntwo methods from [KN12]. For a wide d × m operator, we can\n1. sample k indices uniformly from JdK without replacement, once for each col-\numn, or\n2. divide JdK into k contiguous subsets of equal size, and then for each column\nwe select one index from each of the k index sets.\nThese deﬁnitions are extended from wide sketching operators to tall sketching op-\nerators in the natural way.\narXiv Version 2\nPage 33\n\n\nBasic Sketching\n2.4. Sparse sketching operators\nFor either method, the nonzero values in a SASO’s short-axis vector are canon-\nically independent Rademachers. Alternatively, they can be drawn from other sub-\nGaussian distributions. For example, in the wide case, drawing the nonzeros inde-\npendently and uniformly from a union of disjoint intervals, such as [−2, −1] ∪[1, 2],\ncan protect against the possibility of a given row of S being orthogonal to a column\nof a matrix to be sketched [Tyg22].\nDetails SASOs are provided in Appendix A.2. This includes implementation\nnotes, a short historical summary of relevant theory, and remarks on setting the\nsparsity parameter k. On the topic of theory, we note here that the state-of-the-art\nresults for SASOs are due to Cohen [Coh16]. More information can be found in\nthe lecture notes [Mah11; Mah16; DM18], [Tro20, §7.4] and the surveys [Woo14],\n[MT20, §9.2].\nRemark 2.4.1 (Naming conventions). The concept of what we would call a “wide\nSASO” is referred to in the literature as an OSNAP. We have a slight preference for\n“SASO” over “OSNAP” for two reasons. First, it pairs naturally with the abbre-\nviation LASO for long-axis-sparse sketching operators, which is valuable for tax-\nonomizing sparse sketching operators. Second, the literature consistently describes\nOSNAPs as having a ﬁxed number of nonzeros per column. While this description\nis appropriate for left sketching, it is not appropriate for right sketching.\n2.4.2\nLong-axis-sparse sketching operators\nThis category includes row and column sampling, LESS embeddings [DLD+21], and\nLESS-uniform operators [DLP+21].\nIn the wide case, a LASO has independent rows and a ﬁxed upper bound on the\nnumber of nonzeros per row. All rows are sampled with reference to a distribution\np over JmK (which can be uniform) and a positive integer k. Construction begins by\nsampling t1, . . . , tk from JmK with replacement according to p. Then we initialize\nS[i, :] =\n1\n√\ndk\n\u0012s\nb1\np1\n, . . . ,\ns\nbm\npm\n\u0013\n,\n(2.4)\nwhere bj is the number of times the index j appeared in the sample (t1, . . . , tk).\nWe ﬁnish constructing the row by multiplying each nonzero entry by an iid copy\nof a mean-zero random variable of unit variance (e.g., a standard Gaussian random\nvariable). Such a LASO will have at most k nonzeros per row and hence at most\ndk nonzeros in total. Note that this is much smaller than mk nonzeros required by\na SASO with the same parameters.\nThe quality of sketches produced by LASOs when p is uniform depends on the\nproperties of the matrix to be sketched. Speciﬁcally, it will depend on the leverage\nscores of the matrix.\nThe leverage score concept, introduced in Section 6.1, is\nimportant for constructing data-aware sketching operators that implement row or\ncolumn sampling. If p is the leverage score distribution of some matrix then the\nsketching operator is known as a Leverage Score Sparsiﬁed (LESS) embedding for\nthat matrix [DLD+21]. The term LESS-uniform has been used for long-axis-sparse\noperators that use the uniform distribution for p [DLP+21].\nRemark 2.4.2 (Scale). The scaling factor 1/\n√\ndk appearing in the initialization (2.4)\nis the same for all rows of S (in the wide case, i.e., for each long-axis vector). This\nfactor is necessary so that once the nonzeros in S are multiplied by mean-zero unit-\nvariance random variables, we have E[S∗S] = Im. This scaling matters when one\nPage 34\narXiv Version 2\n\n\n2.5. Subsampled fast trigonometric transforms\nBasic Sketching\ncares about subspace embedding distortion or when one is only sketching a portion\nof the problem data (see Section 2.2.3).\nThis scaling has no eﬀect on eﬀective\ndistortion if p is uniform.\n2.5\nSubsampled fast trigonometric transforms\nFast trigonometric transforms (or fast trig transforms) are orthogonal or unitary\noperators that take m-vectors to m-vectors in O(m log m) time or better. The most\nimportant examples in this class are the Discrete Fourier Transform (for complex-\nvalued inputs) and the Discrete Cosine Transform (for real-valued inputs). The\nWalsh-Hadamard Transform is also notable; although it only exists when m is a\npower of two, it is equivalent to a Kronecker product of log2 m Discrete Fourier\nTransforms of size 2 × 2, and the standard algorithm for applying it involves no\nmultiplications and entails no branching.\nTraditionally, trig transforms are valued for their ability to map dense input\nvectors with a periodic structure into sparse output vectors. Within RandNLA,\nwe are interested in them for the opposite reason: the fact that they map inputs\nthat lack periodic structure to dense outputs. This behavior is useful because if we\npreprocess an input to destroy any periodic structure with high probability, the re-\nsulting output should be easier to approximate by random coordinate subsampling.\nThis leads to the idea of a subsampled randomized fast trig transforms or SRFTs.\nTraditional SRFTs\nFormally, a d × m SRFT takes the form\nS =\np\nm/d RFD,\nwhere D is a diagonal matrix of independent Rademachers, F is a fast trig transform\nthat maps m-vectors to m-vectors, and R randomly samples d components from\nan m-vector [AC06; AC09]. For added robustness one can deﬁne SRFTs slightly\ndiﬀerently, replacing S by SΠ for a permutation matrix Π [MT20].\nSRFTs are appealing for their eﬃciency and theoretical guarantees. Speaking\nto the former aspect, a d × m SRFT can be applied to an m × n matrix in as\nlittle as O(mn log d) time by using methods for subsampled fast trig transforms\n[WLR+08, §3.3], [Lib09, §3.3]. Theoretical guarantees for SRFTs are usually es-\ntablished assuming F is the Walsh-Hadamard transform [DMM+11; Tro11; BG13].\nThese guarantees are especially appealing since they do not rely on tuning param-\neters such as sparsity parameters required by sketching operators from Section 2.4.\nThe trouble with SRFTs is that they are notoriously diﬃcult to implement\neﬃciently. Even their best-case O(mn log d) complexity is higher than the O(mnk)\ncomplexity of a SASO that is wide with k ≪log d nonzeros per column. However,\nSRFTs have an advantage when it comes to memory: if one overwrites A by the\nm × n matrix B :=\np\nm/d FDA in O(mn log m) time, then SA can be accessed as a\nsubmatrix of rows of B without losing access to A or A∗as linear operators. Further\ninvestigation is needed to determine the true value of this in-place nondestructive\nimplementation. For the time being, we do not believe that traditional SRFTs are\nessential for a preliminary RandBLAS standard.\narXiv Version 2\nPage 35\n\n\nBasic Sketching\n2.6. Multi-sketch and quadratic-sketch routines\nBlock SRFTs\nLet p be a positive integer, r = m/p be greater than d, and R be a matrix that\nrandomly samples d components from an r-vector.\nFor each index i ∈JpK, we\nintroduce a d × r sketching operator\nSi =\np\nr/d Dpost\ni\nRFDpre\ni\n,\nwhere Dpost\ni\nand Dpre\ni\nare diagonal matrices ﬁlled with independent Rademachers.\nThe block SRFT [BBG+22] is deﬁned columnwise as S = [S1\nS2\n. . .\nSp].\nBlock SRFTs can eﬀectively leverage parallel hardware using serial implementa-\ntions of the fast trig transform. For concreteness, suppose A is m×n and distributed\nblock row-wise among p processors. We apply the block SRFT S by the formula\nSA =\nX\ni∈JpK\nSiAi,\nwhere Ai is the block of rows of A stored on processor i. The multiplication SiAi,\ncomputed locally on each processor, is followed by a reduction operation among\nprocessors to sum the local contributions.\nWe are undecided as to whether block SRFTs are appropriate for a preliminary\nRandBLAS standard. Their comparative ease of implementation is favorable. How-\never, they are problematic in that the deﬁnition of the distribution changes as we\nvary p, which complicates reproducibility across platforms.\nHistorical remarks and further reading\nThe development of SRFTs began with fast Johnson–Lindenstrauss transforms\n(FJLTs) [AC06], which replace the matrix “R” in the SRFT construction by a par-\nticular type of sparse matrix. FJLTs were ﬁrst used in RandNLA for least squares\nand low-rank approximation by [Sar06]. The jump from FJLTs to SRFTs was made\nindependently in [DMM+11] and [WLR+08] for usage in least squares and low-rank\napproximation, respectively.\nFor more background on this topic we refer the reader to the book [Mah11],\nthe lecture notes [Mah16], [DM18], [Tro20, §7.5], and the survey [MT20, §9.3].\nWe also note that SRFTs are sometimes called randomized orthornomal systems\n[PW16; PW17; OPA19] or (with slight abuse of terminology) FJLTs. Finally, we\npoint out that a type of “SRFTs without subsampling” has been successfully used\nto approximate Gaussian matrices needed in random features approaches to kernel\nridge regression [LSS13].\nRemark 2.5.1 (Navigating the literature). The reader should be aware that [AC09] is\nthe journal version of [AC06]. Additionally, while [LWM+07] also describes SRFTs,\nit was actually written after [WLR+08].\n2.6\nMulti-sketch and quadratic-sketch routines\nFor many years, the performance bottleneck in NLA algorithms has been data\nmovement, rather than FLOPs performed on the data.\nFor example, a general\nmatrix-matrix multiply with n × n matrices would do O(n3) data movement if\nimplemented naively with three nested loops, but can be up to a factor of\n√\nM\nsmaller, where M is the cache size, if appropriately implemented using loop tiling.\nPage 36\narXiv Version 2\n\n\n2.6. Multi-sketch and quadratic-sketch routines\nBasic Sketching\nIn our context of RandNLA, the fastest randomized algorithms for low-rank\nmatrix approximation involve computing multiple sketches of a data matrix. Such\nmulti-sketching presents new challenges and opportunities in the development of\noptimized implementations with minimal data movement.\nWe believe the RandBLAS should include functionality for at least three types\nof multi-sketching, listed below. The end of Section 4.2.1 points to algorithms that\nuse these primitives. In all cases of which we are aware, these primitives are only\nused for sketching in the sampling regime.\n1. Generate S and compute Y1 = AS and Y2 = A∗AS. We illustrate the use of\nthis primitive explicitly in Algorithm 12.\n2. Generate independent S1, S2, and compute Y1 = AS1 and Y2 = S2A. Algo-\nrithms which use this primitive typically need to retain S1 or S2 for later use\n[HMT11, pg. 251], [YGL+17, Algorithm 2], [TYU+17b, § 1.4].\n3. Generate independent S1, S2, S3, S4, and compute Y1 = AS1, Y2 = S2A, and\nY3 = S3AS4.\nHaving identiﬁed these operations as basic building blocks, we arrive at the following\nquestion.\nWhat combination of sketching distributions should be supported in\nmulti-sketching of types 2 and 3?\nFor the former type (i.e., type 2), it is important to support at least the case that\nboth S1 and S2 are dense sketching operators, and it may be useful to support\nwhen both are fast operators. At this point, we do not see an advantage for one of\n(S1, S2) to be a fast operator and for the other to be dense. For the latter type (i.e.,\ntype 3), [TYU+17b, §7.3.2] suggests that (S1, S2) be Gaussian and that (S3, S4) be\nSRFTs. We believe that it would be reasonable to use SASOs in place of SRFTs in\nthis context.\nThe RandBLAS should provide methods to compute sketches that are quadratic\nin the data matrix.\nBy “quadratic sketch,” we mean a linear sketch of A∗A or\nAA∗. This operation is ubiquitous in algorithms for low-rank approximation. As\nwith multi-sketching, all uses of quadratic sketching (of which we are aware) entail\nsketching in the sampling regime. It is not possible to fundamentally accelerate this\nkind of sketching by using fast sketching operators.2 Therefore it would be reason-\nable for RandBLAS’s quadratic sketching methods to only support dense sketching\noperators.\n(The preceding comments also apply to type 1 multi-sketching.)\nIn\nessence, this asks for a high-performance implementation of the composition of the\nBLAS 3 functions syrk and gemm: (A, S) 7→AA∗S. There is a substantial amount\nof structure in quadratic sketching that could be leveraged for reduced data move-\nment, which suggests that the RandBLAS would beneﬁt signiﬁcantly from having\noptimized routines for this functionality.\n2This point has also been made in a related setting [MT20, §11.6.1].\narXiv Version 2\nPage 37\n\n\nBasic Sketching\n2.6. Multi-sketch and quadratic-sketch routines\nPage 38\narXiv Version 2\n\n\nSection 3\nLeast Squares and Optimization\n3.1 Problem classes ...........................................................\n40\n3.1.1 Minimizing regularized quadratics ...............................\n41\n3.1.2 Solving least squares and basic saddle point problems .....\n41\n3.2 Drivers ........................................................................\n43\n3.2.1 Sketch-and-solve for overdetermined least squares\n.........\n43\n3.2.2 Sketch-and-precondition for least squares and saddle point\nproblems .................................................................\n44\n3.2.3 Nystr¨om PCG for minimizing regularized quadratics ......\n49\n3.2.4 Sketch-and-solve for minimizing regularized quadratics ...\n50\n3.3 Computational routines ..............................................\n51\n3.3.1 Technical background: optimality conditions for saddle point\nproblems .................................................................\n51\n3.3.2 Preconditioning least squares and saddle point problems:\ntall data matrices ......................................................\n53\n3.3.3 Preconditioning least squares and saddle point problems:\ndata matrices with fast spectral decay ..........................\n56\n3.3.4 Deterministic preconditioned iterative solvers ................\n57\n3.4 Other optimization functionality .................................\n58\n3.5 Existing libraries .........................................................\n60\nNumerical linear algebra is the backbone of the most widely-used algorithms\nfor continuous optimization. Continuous optimization, in turn, is a workhorse for\nmany scientiﬁc computing, machine learning, and data science applications.\nThe connections between optimization and linear algebra are often introduced\nwith least squares problems. Such problems have been used as a tool for curve ﬁtting\nsince the days of Gauss and Legendre over 200 years ago — several decades before\nCayley even deﬁned linear algebraic concepts such as the matrix-inverse! These\nproblems are also remarkable because algorithms for solving them easily generalize\nto more complicated settings. Indeed, one of this section’s key messages is that, by\nadopting a suitable perspective, one can use randomization in essentially the same\nway to solve a wealth of diﬀerent quadratic optimization problems.\n39\n\n\nLeast Squares and Optimization\n3.1. Problem classes\nOur perspective entails describing all least squares problems in terms of an m×n\ndata matrix A with at least as many rows as columns. Speciﬁcally, we express the\noverdetermined problem as\nmin\nx∈Rn ∥Ax −b∥2\n2\nfor a vector b in Rm, while we express the underdetermined problem as\nmin\ny∈Rm{∥y∥2\n2 | A∗y = c}\nfor a vector c in Rn. Of course, both of these models could be expressed in the\ncorresponding “argmin” formulation. We generally prefer the “min” formulation\nfor the optimization problem itself and use “argmin” only for the set of optimal\nsolutions.\nSection 3.1 introduces the problems we consider: minimization of regularized\nquadratics and various generalizations of least squares problems. For each problem,\nit provides high-level comments on structures and desired outcomes that can make\nrandomized algorithms preferable to classical ones.\nSection 3.2 covers the drivers for these problems based on RandNLA. It de-\ntails the problem structures that stand to beneﬁt from a particular driver, and\nit highlights other linear algebra problems that largely reduce to solving problems\namenable to these drivers. Section 3.3 details some essential computational routines\nthat would power the drivers.\nThe rest of the Section 3 is largely supplemental. Section 3.4 reviews randomized\noptimization algorithms that we ﬁnd notable but out-of-scope, as well as one type of\ndeterministic computational routine that is potentially useful for (but not required\nby) the drivers. We conclude by describing existing RandNLA libraries for least\nsquares and optimization in Section 3.5.\n3.1\nProblem classes\nThis section covers drivers for two related classes of optimization problems: mini-\nmizing regularized positive deﬁnite quadratics (§3.1.1) and certain generalizations\nof overdetermined and underdetermined least squares which we refer to as saddle\npoint problems (§3.1.2). Problems in both classes can naturally be transformed to\nequivalent linear algebra problems.1 Functionality for solving these problems can\neasily provide the foundation for managing the core linear algebra kernels of larger\noptimization algorithms.\nHow can we measure the accuracy of an approximate solution?\nThe problem of quantifying the error of an approximate solution to a least squares\nor saddle point problem is very important. While we would like to address this topic\nup-front, a proper discussion requires technical background that we only cover in\nSection 3.3. Furthermore, even given this background, there are various subtleties\nand special cases that would be laborious to describe here.\nTherefore we defer\nthe important topic of error metrics for least squares and related problems to Ap-\npendix B.2.\n1As we explain later, the saddle point problems are equivalent to so-called saddle point systems,\nwhich are well-studied in the NLA literature; see [BGL05; OA17]\nPage 40\narXiv Version 2\n\n\n3.1. Problem classes\nLeast Squares and Optimization\n3.1.1\nMinimizing regularized quadratics\nLet G be a positive semideﬁnite (psd) linear operator, and let µ be a positive\nregularization parameter. One of the main topics of this section is algorithms for\ncomputing approximate solutions to problems of the form\nmin\nx x∗(G + µI) x −2h∗x.\n(3.1)\nNote that solving (3.1) is equivalent to solving (G + µI) x = h. We refer to such\nproblems in diﬀerent contexts throughout this section. In some contexts, we say\nthat G is n × n, and in others, we say it is m × m.\nThis section covers algorithms for solving these problems to varying degrees of\naccuracy.\n• Methods for solving to higher accuracy will access G repeatedly by matrix-\nmatrix and matrix-vector multiplication.\n• Methods for solving to lower accuracy may vary in how they access G: they\nmay only entail selecting a subset of its columns; or they may perform a single\nmatrix-matrix multiplication GS with a tall and thin matrix S.\nNote that the low accuracy methods may be useful in machine learning contexts\nsuch as kernel ridge regression (KRR), where an inaccurate solution to (3.1) can\nstill be useful for downstream computational tasks.\nRemark 3.1.1. If the linear operator G implements the action of an implicit Gram\nmatrix A∗A (with A known) then it would be preferable to reformulate (3.1) as\n(3.2), below, with b = 0 and c = h.\nAmenable problem structures\nThe suitability of methods we describe for problem (3.1) will depend on how many\neigenvalues of G are larger than µ. Supposing G is n × n, it is desirable that the\nnumber of such eigenvalues is much less than n.\nThe data (G, µ) that arise in\npractical KRR problems usually have this property.\nIn an ideal setting, the user would have an estimate for the number of eigenvalues\nof G that are larger than µ. This is not a strong requirement when G is accessible by\nrepeated matrix-vector multiplication, in which case the accuracy of the estimate\nis unimportant. The standard RandNLA algorithm in this situation can easily be\nmodiﬁed to recycle the work in solving (3.1) for one value of µ towards solving (3.1)\nfor another value of µ.\n3.1.2\nSolving least squares and basic saddle point problems\nWe are interested in certain generalizations of overdetermined and underdetermined\nleast squares problems. The generalizations facilitate natural speciﬁcation of linear\nterms in composite quadratic objectives, which is a common primitive in many\nsecond-order optimization algorithms.\nWe frame these problems as complementary formulations of a common saddle\npoint problem. The deﬁning data for such a problem consists of a tall m × n matrix\nA, an m-vector b, an n-vector c, and a scalar µ ≥0. For simplicity, our descriptions\nin this paragraph assume A is full-rank. The primal saddle point problem is\nmin\nx∈Rn ∥Ax −b∥2\n2 + µ∥x∥2\n2 + 2c∗x.\n(3.2)\narXiv Version 2\nPage 41\n\n\nLeast Squares and Optimization\n3.1. Problem classes\nWhen µ is positive, the dual saddle point problem is\nmin\ny∈Rm ∥A∗y −c∥2\n2 + µ∥y −b∥2\n2.\n(3.3)\nIn the limit as µ tends to zero, Eq. (3.3) canonically becomes\nmin\ny∈Rm{∥y −b∥2\n2 : A∗y = c}.\n(3.4)\nNote that the primal problem reduces to ridge regression when c is zero, and it\nreduces to overdetermined least squares when both c and µ are zero. When b is\nzero, and depending on the value of µ, the dual problem amounts to ridge regression\nwith a wide data matrix or to basic underdetermined least squares.\nPros and cons of this viewpoint\nAdopting this more general optimization-based viewpoint on least squares problems\nhas two major beneﬁts.\n• It extends least squares problems to include linear terms in the objective. The\nlinear term in the primal problem is obvious. The linear terms in (3.3) and\n(3.4) are obtained by expanding ∥y −b∥2\n2 = ∥y∥2\n2 −2b∗y + ∥b∥2\n2 and ignoring\nthe constant term ∥b∥2\n2.\n• It renders the primal and dual problems equivalent for most algorithmic pur-\nposes. The equivalence is based on formulating the optimality conditions for\nthese problems in a so-called saddle point system over the variables (x, y).\nSection 3.3.1 details this equivalence.\nIt must be noted that the saddle point problems we consider can be ill-posed when\nµ is zero and A is rank-deﬁcient. Speciﬁcally, when µ = 0 and c is not orthogonal\nto the kernel of A, the primal problem (3.2) has no optimal solution and the dual\nproblem (3.4) has no feasible solution. In this setting, we assign canonical solutions\nby considering the limit as µ tends to zero. Appendix B.3 addresses the existence\nand form of these limiting solutions. The outcome of the limiting analysis is that\nwhen µ = 0, we obtain canonical solutions\nx = (A∗A)†(A∗b −c)\nand\ny = (A∗)†c + (I −AA†)b,\n(3.5)\nwhich are related through the identity y = b −Ax.\nAmenable problem structures\nThis section focuses on methods for solving these optimization problems to high\naccuracy. Indeed, later in this section we make the novel observation that methods\nfor solving problems (3.2)–(3.4) to high accuracy can be used as the core subroutine\nin solving (3.1) to low accuracy at extremely large scales. If m ≫n, then these\nmethods are eﬃcient regardless of numerical aspects of the problem data (A, b, c, µ);\nproblems such as poor numerical conditioning will be relevant insofar as they con-\ntribute to ﬂoating-point rounding errors in these eﬃcient algorithms. If m is only\nslightly larger than n, then the methods we describe will only be eﬀective when\nG := A∗A and µ have the properties alluded to in Section 3.1.1. These properties\nare detailed later in this section.\nPage 42\narXiv Version 2\n\n\n3.2. Drivers\nLeast Squares and Optimization\n3.2\nDrivers\nHere we present four families of drivers for the problems described in Section 3.1.\nTwo of the driver families belong to a paradigm in the RandNLA literature known\nas sketch-and-precondition. Algorithms in these families are capable of computing\naccurate approximations of a problem’s true solution. The other two driver families\nbelong to a paradigm known as sketch-and-solve.\nThey are less expensive than\nsketch-and-precondition methods (to varying degrees) but they are only suitable\nfor producing rough approximations of a problem’s true solution. The sketch-and-\nsolve drivers described in Section 3.2.4 are novel in that they rely on separate\nsketch-and-precondition methods for their core subroutines.\n3.2.1\nSketch-and-solve for overdetermined least squares\nSketch-and-solve is a broad paradigm within RandNLA, and algorithms based on\nit have been central to early developments in the area [Mah11; Woo14; DM16;\nDM21a]. Its most notable manifestations have been for overdetermined least squares\n[DMM06; Sar06; DMM+11; CW13], overdetermined ℓ1 and general ℓp regression\n[DDH+09; YMM16], and ridge regression [ACW17a; WGM18].\nWe focus here on least squares for concreteness. In this case, one samples a\nsketching operator S, and returns\n(SA)†(Sb) ∈arg min\nx\n∥S(Ax −b)∥2\n2\n(3.6)\nas a proxy for the solution to minx ∥Ax −b∥2\n2. The quality of this solution can be\nbounded with the concept of subspace embeddings from Section 2.2.2. In particular,\nif S is a subspace embedding for V = range([A, b]) with distortion δ, then\n∥A(SA)†(Sb) −b∥2 ≤\n\u00121 + δ\n1 −δ\n\u0013\n∥AA†b −b∥2.\n(3.7)\nNote that (3.6) is invariant under scaling of S. This implies that (3.7) also holds\nwhen δ is the eﬀective distortion of S for V ; see (2.3) and Appendix A.1.\nImplementation considerations and a viable application are given below.\nMethods for the sketched subproblem\nDirect methods for (3.6) require computing an orthogonal decomposition of SA,\nsuch as a QR decomposition or an SVD, in O(dn2) time. In this context, sketch-\nand-solve can be used as a preprocessing step for sketch-and-precondition methods\nat essentially no added cost. Indeed, this preprocessing step was used in [RT08].\nTherefore if a direct method is being considered for sketch-and-solve, then sketch-\nand-precondition methods should also be viable when m ∈O(dn).\nOne can in principle apply an iterative solver to the problem deﬁned by (SA, Sb).\nThis strategy avoids the cost of factoring SA, and it reduces the per iteration cost\nrelative to running the iterative solver on the original problem. This is typically\nimplemented without preconditioning (but see [YCR+18]), in which case it leaves\nthe dependence on the condition number of the original problem, and so it can only\nbe recommended for problems where the condition number is known to be small.\narXiv Version 2\nPage 43\n\n\nLeast Squares and Optimization\n3.2. Drivers\nError estimation\nSince sketch-and-solve algorithms for overdetermined least squares are most suitable\nfor computing rough approximations to a problem’s true solution, it is important\nto have methods for estimating this error. Such estimates can either be used to\ninform downstream processing of the approximate solution or to determine if a more\naccurate solution (computed by a more expensive algorithm) might be needed. It\nis especially important that these methods work well in regimes where sketch-and-\nsolve has a compelling computational proﬁle, such as when m ≫dn. Appendix E.2\nprovides one such estimator based on the principle of bootstrapping from statistics.\nApplication to tensor decomposition\nThe beneﬁts of sketch-and-solve for least squares manifest most prominently when\nthe following conditions are satisﬁed simultaneously: (1) m is extremely large, so\nA is not stored explicitly, and (2) A supports relatively cheap access to individual\nrows A[i, :]. Among other places, this situation arises in alternating least squares\napproaches to tensor decomposition. We touch upon that topic in Section 7.3.1,\nparticularly in the remarks after (7.17).\n3.2.2\nSketch-and-precondition for least squares and saddle point\nproblems\nThe sketch-and-precondition approach to overdetermined least squares was intro-\nduced by Rokhlin and Tygert [RT08]. When the m × n matrix A is very tall, the\nmethod is capable of producing accurate solutions with less expense than direct\nmethods. It starts by computing a d × n sketch Ask = SA in the embedding regime\n(i.e., d ≳n). The sketch is decomposed by QR with column pivoting AskΠ = QR,\nwhich deﬁnes a preconditioner M = ΠR−1. If the parameters for the sketching oper-\nator distribution were chosen appropriately, then AM will be nearly-orthogonal with\nhigh probability.2 The near-orthogonality of AM ensures rapid convergence of an\niterative method for the least squares problem’s preconditioned normal equations.\nIf Tsk denotes the time complexity of computing SA, then the typical asymptotic\nFLOP count to solve to ϵ-error is\nO(Tsk + dn2 + mn log(1/ϵ))\n(3.8)\nImportantly, this complexity has no dependence on the condition number of A.\nThis approach was extended with stronger theoretical guarantees, support for\nmore general least squares problems, and high-performance implementations through\nBlendenpik [AMT10] and LSRN [MSM14]. It has also recently been used to solve\npositive deﬁnite systems arising in linear programming algorithms [CLA+20]. All\nof these methods produce preconditioners M where AM is nearly-orthogonal and\nare intended for the regime where A is very tall.\nWhat constitutes “very tall” depends on the algorithm’s implementation and\nthe hardware that runs it. It is easy to implement these algorithms in Matlab or\nPython so that, on a personal laptop, they are competitive with LAPACK’s direct\nmethods when m ≥50n ≥105; see also Section 3.5.\n2The condition number of AM and the eﬀective distortion of S for range(A) completely char-\nacterize one another; see Appendices A.1 and B.1.\nPage 44\narXiv Version 2\n\n\n3.2. Drivers\nLeast Squares and Optimization\nYour attention, please!\nIf a saddle point problem features regularization (i.e., if µ > 0) and if A has rapid\nspectral decay, then randomized methods can be used to ﬁnd a good preconditioner\nin far less than O(n3) time, no matter the speciﬁc value of m ≥n. This is possible\nby borrowing ideas from Nystr¨om preconditioning [FTU21], which we introduce in\nSection 3.2.3 for the related problem of minimizing regularized quadratics. As a\nnovel contribution, Section 3.3.3 explains how Nystr¨om preconditioning can natu-\nrally be adapted to saddle point problems. Therefore while the material here (in\nSection 3.2.2) focuses on the case m ≫n, one should be aware that this requirement\ncan be relaxed.\nAlgorithms\nSketch-and-precondition algorithms can take diﬀerent approaches to sketching, pre-\nconditioner generation, and choice of the eventual iterative solver.\n• Blendenpik used SRFT sketching operators, obtained its preconditioner by un-\npivoted QR of Ask, and used LSQR [PS82] as its underlying iterative method.\n• LSRN used Gaussian sketching operators, obtained its preconditioner through\nan SVD of Ask, and defaulted to the Chebyshev semi-iterative method [GV61]\nfor its iterative solver.\nThese two examples hint at the huge range of possibilities for the implementation\nof sketch-and-precondition algorithms. Indeed, we discuss preconditioners in detail\nover Sections 3.3.2 and 3.3.3, and we review a suite of possible deterministic iterative\nmethods in Section 3.3.4. For now, we give Algorithms 1 and 2 (below) as footholds\nfor understanding the various design considerations.\nFor simplicity’s sake, both of these algorithms use a black-box function\nz = iterative ls solver(F, g, ϵ, L, zo)\nwhich computes an approximate solution to minz ∥Fz −g∥2\n2. The exact semantics\nof this function are unimportant for our present purpose. Its general semantics are\nthat the solver initializes an iterative procedure at zo and that it runs until either\nan implementation-dependent error tolerance ϵ is met or an iteration limit L is\nreached. Typical implementations would measure error with a suitably normalized\nversion of the normal equation residual ∥F∗(Fz −g) ∥2. If κ denotes the condition\nnumber of F then typical convergence rates are such that error ∥F(z−F†g)∥2 decays\nmultiplicatively by a factor of (κ −1)/(κ + 1) with each iteration.\narXiv Version 2\nPage 45\n\n\nLeast Squares and Optimization\n3.2. Drivers\nBesides the use of a common iterative solver, both algorithms below initialize\nthe iterative solver at the solution from a sketch-and-solve approach in the vein of\nSection 3.2.1. The time needed to perform this presolve step is negligible, but it\nshould save several iterations when solving to a prescribed accuracy. It also plays an\nimportant role in handling overdetermined least squares problems when b is in the\nrange of A. In such contexts, the sketch-and-solve result actually solves the least\nsquares problem exactly provided that rank(SA) = rank(A); this stands in contrast\nto using a preconditioned iterative method initialized at the origin, which would not\nbe able to achieve relative error guarantees for ∥Ax −b∥against ∥(I −AA†)b∥= 0.\nAlgorithm 1 SPO1: a Blendenpik-like approach to overdetermined least squares\n1: function SPO1(A, b, ϵ, L)\nInputs:\nA is m × n and b is an m-vector. We require m ≥n and expect\nm ≫n. The iterative solver’s termination criteria are governed by ϵ\nand L: it stops if the solution reaches error ϵ ≥0 according to the\nsolver’s error metric, or if the solver completes L ≥1 iterations.\nOutput:\nAn approximate solution to (3.2), with c = 0 and µ = 0.\nAbstract subroutines and tuning parameters:\nSketchOpGen generates an oblivious sketching operator.\nsampling factor ≥1 is the size of the embedding dimension relative to n.\n2:\nd = min{⌈n · sampling factor⌉, m}\n3:\nS = SketchOpGen(d, m)\n4:\n[Ask, bsk] = S[A, b]\n5:\nQ, R = qr econ(Ask)\n6:\nzo = Q∗bsk\n# R−1zo solves minx{∥S(Ax −b)∥2\n2}\n7:\nAprecond = AR−1 # as a linear operator\n8:\nz = iterative ls solver(Aprecond, b, ϵ, L, zo)\n9:\nreturn R−1z\nWhile Algorithm 1 is standard, Algorithm 2 is somewhat novel. Using the same\ndata that might be computed during a standard sketch-and-precondition algorithm\nfor simple overdetermined least squares, it transforms any saddle point problem —\nprimal or dual — into an equivalent primal saddle point problem with c = 0. To\nour knowledge, no such conversion routines have been described in the literature.\nThe conversion is advantageous because it opens the possibility of using iterative\nsolvers with excellent numerical properties that are speciﬁc to least squares prob-\nlems. The validity of the algorithm’s transformation is explained towards the end\nof Section 3.3.1.\nPage 46\narXiv Version 2\n\n\n3.2. Drivers\nLeast Squares and Optimization\nAlgorithm 2 SPS2 : sketch, transform a saddle point problem to least squares,\nand precondition. A more eﬃcient version of this algorithm can be obtained using\nour observations on SVD-based preconditioning in Section 3.3.2.\n1: function SPS2(A, b, c, µ, ϵ, L)\nInputs:\nA is m × n, b is an m-vector, c is an n-vector, and µ is a nonnegative\nregularization parameter.\nWe require m ≥n and expect m ≫n.\nThe iterative solver’s termination criteria are governed by ϵ and L: it\nstops if the solution reaches error ϵ ≥0 according to its internal error\nmetric, or if it completes L ≥1 iterations.\nOutput:\nApproximate solutions to (3.2) and its dual problem.\nAbstract subroutines and tuning parameters:\nSketchOpGen generates an oblivious sketching operator.\nsampling factor ≥1 is the size of the embedding dimension relative to n.\n2:\nd = min{⌈n · sampling factor⌉, m}\n3:\nS = SketchOpGen(d, m)\n4:\nif µ > 0 then\n5:\nS =\n\u0014S\n0\n0\nIn\n\u0015\n,\nA =\n\u0014 A\n√µIn\n\u0015\n,\nb =\n\u0014b\n0\n\u0015\n6:\nAsk = SA\n7:\nU, Σ, V∗= svd(Ask)\n8:\nM = VΣ†\n9:\nbmod = b\n10:\nif c ̸= 0 then\n11:\nˆv = UΣ†V∗c # ˆv solves minv{∥v∥2\n2 : A∗S∗v = c}\n12:\nbshift = S∗ˆv\n# A∗bshift = c\n13:\nbmod = bmod −bshift\n14:\nzo = U∗Sbmod\n# Mzo solves min{∥S (Ax −bmod) ∥2\n2}\n15:\nAprecond = AM\n# deﬁne implicitly, as a linear operator\n16:\nz = iterative ls solver(Aprecond, bmod, ϵ, L, zo)\n17:\nx = Mz\n18:\ny = b[: m] −A[: m, :]x\n19:\nreturn x, y\narXiv Version 2\nPage 47\n\n\nLeast Squares and Optimization\n3.2. Drivers\nWe wrap up our introduction to sketch-and-precondition algorithms by speaking\nto their tradeoﬀs with sketch-and-solve. It is easy to see that if m ≪n2 and we\nperform sketch-and-solve using a direct method for Eq. (3.6), then performing an\nadditional constant number of steps of sketch-and-precondition’s iterative phase\ndoes not increase the FLOP count by even so much as a constant factor. However,\nif we are in the regime where m ≥n2, then even a single step of an iterative\nmethod in sketch-and-precondition can cost as much as an entire sketch-and-solve\nalgorithm. Therefore when an accurate solution is not required and m ≥n2, it may\nbe preferable to use sketch-and-solve rather than sketch-and-precondition.\nApplications\nOne application of these algorithms is to carry out the core subroutine in iterative\nmethods for solving linear systems by block projection. We explain the nature of\nthis connection later on, in Section 5.2.2.\nTo explain the next application, we need some context. Classical linear algebra\ntechniques to solve a KRR problem with m datapoints require O(m2) storage and\nO(m3) time. Rahimi and Recht’s random feature maps provide a framework for\nreplacing such a KRR problem with a more tractable ridge regression problem\n[RR07].\nA data matrix in a random features ridge regression problem is m × n\n(for a tuning parameter n < m) and is characterized by the KRR datapoints and\nfunctions f1, . . . , fn drawn from a suitable random distribution. The ith row in this\nmatrix is obtained by evaluating f1, . . . , fn on the ith KRR datapoint.\nThe randomness in random features ridge regression is not “sketching” in the\nsense meant by this monograph. Still, this approach is notable in our context be-\ncause it provides a source of models that are amenable to the methodology described\nabove. The Nystr¨om preconditioning methodology (see Sections 3.2.3 and 3.3.3) has\nbeen reported to be especially eﬀective for such problems when n ≲m [FTU21].\nWhen is sketch-and-precondition asymptotically faster than QR?\nHere we detail the runtime of sketch-and-precondition algorithms under the assump-\ntion of sketching with SRFTs. These sketching operators were used in the original\nsketch-and-precondition paper [RT08] and subsequently by [AMT10]. We focus on\nthem here because they have no tuning parameters besides their embedding dimen-\nsion. Minimizing the number of tuning parameters helps us make comparisons to\ndirect solvers based on QR decomposition that run in time O(mn2).\nRecall from Section 2.5 that it takes O(mn log d) time to apply a d × m SRFT\nto an m × n matrix. We can plug Tsk = mn log d into (3.8) to see that the “typical”\nruntime for sketch-and-precondition with an SRFT is\nO(mn log d + dn2 + mn log(1/ϵ)).\n(3.9)\nThis runtime is only “typical” because it does not address subtleties stemming from\nrandomness. In the algorithm’s true runtime, there is a random multiplicative factor\nF on the mn log(1/ϵ) term in (3.9). The distribution of F depends on (d, n) in a\ncomplicated way. In formal algorithm analysis, one describes how to choose d to\nupper-bound the probability that F exceeds some universal constant C. Then one\ncan say that (3.9) does describe the algorithm’s true runtime with some probability.\nThe convention in the ﬁeld is to describe how to choose d so the probability that\nF ≤C tends to one as problem size increases.\nPage 48\narXiv Version 2\n\n\n3.2. Drivers\nLeast Squares and Optimization\n[RT08] observed that taking d = sn for small constants s (e.g., s = 4) suf-\nﬁced for (3.9) to accurately describe algorithm runtime in practice. However, the\ntheoretical analysis in [RT08] needed to take d ∈Ω(n2) to bound F with high proba-\nbility. Therefore the best theoretical runtime guarantee for sketch-and-precondition\nwas originally obtained by plugging d = n2 into (3.9). The theoretical guarantees\nimproved following developments in the analysis of SRFTs. Speciﬁcally, [AMT10]\nobserved that a transparent application of a result by [NDT09] could be used to\nprove that d ∈Ω(n log n) suﬃced to bound F with high probability. Therefore\none can plug d = n log n into (3.9) to obtain a bound for algorithm runtime in\nterms of (m, n, ϵ) that holds with high probability. This is the appropriate bound\nto use when comparing the theoretical asymptotic runtime of SRFT-based sketch-\nand-precondition to other algorithms. However, in practice, it is still preferred to\nuse d = sn for some small s > 1, since the resulting preconditioned matrices tend\nto be extremely well-conditioned.\n3.2.3\nNystr¨om PCG for minimizing regularized quadratics\nNystr¨om preconditioned conjugate gradient (Nystr¨om PCG) is a recently-proposed\nmethod for solving problems of the form (3.1) to fairly high accuracy [FTU21].\nWe describe it as a method to compute approximate solutions to linear systems\n(G + µI)x = h where G is n × n and psd.\nThe randomness in Nystr¨om PCG is encapsulated in an initial phase where it\ncomputes a low-rank approximation of G by a so-called “Nystr¨om approximation.”\nWe defer discussion on such approximations (including the potentially-confusing\nnaming convention) to Section 4.2.2.\nFor our purposes, what matters is that a\nrank-ℓNystr¨om approximation leads to a preconditioner P which can be stored in\nO(ℓn) space and applied in O(ℓn) time.\nNow let κ denote the condition number of Gp := P−1/2(G + µI)P−1/2. It is\nwell-known that each iteration of PCG requires one matrix-vector multiply with\nG, one matrix-vector multiply with P−1, and reduces the error of the candidate\nsolution to (3.1) by a multiplicative factor (√κ−1)/(√κ+1). As we discuss below,\none can expect that κ will be O(1) if the ℓth-largest eigenvalue of G is smaller than\nµ.\nIndeed, Nystr¨om PCG is most eﬀective for problems when this threshold is\ncrossed at some ℓ≪n. As a practical matter, users will not need to select the\napproximation rank parameter ℓmanually in order to use Nystr¨om PCG; [FTU21,\nAlgorithm E.2] is a specialized adaptive method for Nystr¨om approximation that\ncan determine an appropriate value for ℓgiven (G, µ).\nDetails on the preconditioner\nWe presume access to a low-rank approximation\nˆG = V diag(λ)V∗\n(3.10)\nwhere V is a column-orthonormal n × ℓmatrix that approximates the dominant ℓ\neigenvectors of G and λ1 ≥· · · ≥λℓ> 0 are the approximated eigenvalues. The\ndata (V, λ, µ) is then used to deﬁne a preconditioner\nP−1 = V diag(λ + µ)−1V∗+ (µ + λℓ)−1(In −VV∗).\n(3.11)\narXiv Version 2\nPage 49\n\n\nLeast Squares and Optimization\n3.2. Drivers\nAlternatively, following [FTU21] to the letter, P−1 can be the result of multiplying\nthe expression above by (µ + λℓ). Under this latter convention, P−1 acts as the\nidentity on range(V)⊥.\nWhile the form of this preconditioner may appear mysterious, its appropriateness\ncan be seen by considering a simple idealized setting. To make a precise statement\non this topic we adopt notation where λi(G) is the ith-largest eigenvalue of G.\nAssuming that (V, λ) are very good estimates for the top ℓeigenpairs of G and that\nλℓ(G) ≈λℓ+1(G), the condition number of Gp should be near\nκℓ(G, µ) := (λℓ(G) + µ)/(λn(G) + µ).\nTaking this for granted, the preconditioner (3.11) can only be eﬀective if ℓ≪n\nis large enough so that κℓ(G, µ) is bounded by a small constant. Using the fact\nthat κℓ(G, u) ≤1 + λℓ(G)/µ, we can simplify the criteria and say that a good\npreconditioner is possible when λℓ(G)/µ is O(1).\nRemark 3.2.1. The argument above can be made more rigorous by assuming that V\nis an n × (ℓ−1) matrix that contains the exact leading ℓ−1 eigenvectors of G, and\nthat λ1, . . . , λℓare the exact leading ℓeigenvalues of G. In this case, the condition\nnumber of Gp will be equal to κℓ(G, µ), which will be at most 1 + λℓ/µ.\n3.2.4\nSketch-and-solve for minimizing regularized quadratics\nRandomization oﬀers several avenues for solving problems of the form (3.1) to mod-\nest accuracy. We describe two possible methods here through novel interpretations\nof existing work on KRR. Our descriptions of the methods keep the focus on linear\nalgebra, and we refer the reader to Appendix B.4.1 for information on the KRR\nformalism. We note that our formulations of these methods are novel in how they\napply sketch-and-precondition as the core subroutine in what is otherwise a sketch-\nand-solve style driver. Such “nested randomization” is a relatively under-explored\nand potentially powerful algorithm design paradigm.\nFor notation, we shall say that G is m × m, that µ = mλ for some λ > 0, and\nthat the optimization variable in (3.1) is denoted by “α” rather than “x.”\nA one-shot fallback on Nystr¨om approximations\nRather than solving (3.1) directly, it has been suggested that one solve\n(AA∗+ mλI) ˆα = h,\nwhere AA∗is a Nystr¨om approximation of G [AM15]. The computation of A only\nrequires access to G by a single sketch GS for a tall m×n sketching operator S. In the\nKRR context, it is especially popular for S to be a column sampling operator [WS00;\nKMT09b; GM16]. Section 6.1.3 discusses how such column-selection sketches GS\ncan be computed adaptively using the concept of ridge leverage scores. Regardless\nof how the approximation is obtained, there is an equivalence between computing\nˆα and solving a dual saddle point problem with matrix A and other data (b, c, µ) =\n(h, 0, mλ). That dual saddle point problem can naturally be approached by sketch-\nand-precondition methods from Section 3.2.2. The preconditioner generation steps\nin this context are subtle and addressed in Appendix B.4.2.\nPage 50\narXiv Version 2\n\n\n3.3. Computational routines\nLeast Squares and Optimization\nApplying a random subspace constraint\nBy taking the gradient of the objective function in (3.1) and multiplying the gradient\nby the positive deﬁnite matrix G, we can recast (3.1) as minimizing\nQ(α) = α∗(G2 + mλG)α −2h∗Gα.\nIn [YPW17], a sketch-and-solve approach to the problem of minimizing this loss\nfunction is proposed. Speciﬁcally, one minimizes Q(α) subject to a constraint that\nα is in the range of a very tall m × n sketching operator S.\nThe constrained\nminimization problem is equivalent to minimizing z 7→Q(Sz) over n-vectors z.\nThis in turn is equivalent to solving a highly overdetermined least squares problem,\nwith an (m + n) × n data matrix A = [GS;\n√\nmλR] where R is any matrix for\nwhich R∗R = S∗GS. This problem can clearly be handled by our methods from\nSection 3.2.2.\nRemark 3.2.2. We note that [YPW17] presumes access to the sketches h∗GS, S∗GS,\nand S∗G2S, and advocates for solving the resulting n-dimensional minimization\nproblem by a direct method in O(n3) time. However, no guidance is given on how\nto compute the sketch S∗G2S. From what we can tell, the most eﬃcient way of\ndoing this would be to form the Gram matrix at cost O(mn2) assuming access to\nthe sketch GS. (Our usage of (m, n) is swapped relative to [YPW17].)\n3.3\nComputational routines\nTo contextualize the computational routines that follow, we begin in Section 3.3.1\nwith a brief discussion of optimality conditions for saddle point problems. From\nthere, we present in Sections 3.3.2 and 3.3.3 two families of methods for generating\npreconditioners needed by saddle point drivers; our presentation of both families\nincludes novel observations that lead to improved eﬃciency and numerical stability.\nThen in Section 3.3.4 we discuss deterministic preconditioned iterative methods\nfor positive deﬁnite systems and saddle point problems. Such iterative methods are\napplicable to all drivers from the previous section (although less so for Section 3.2.1).\nRoutines not detailed here\nThe driver from Section 3.2.3 requires methods to compute Nystr¨om approxima-\ntions, which are described in Section 4. In addition, the drivers from Section 3.2.4\nwould beneﬁt from specialized data-aware methods for sketching kernel matrices,\nwhich are discussed in Section 6. We also note that this section does not describe\ncomputational routines for sketch-and-solve type drivers.\nThis is because those\ndrivers are extraordinarily simple to implement and there is no need to isolate their\nbuilding blocks into separate computational routines.\n3.3.1\nTechnical background: optimality conditions for saddle point\nproblems\nHere, we give a handful of characterizations of optimal solutions for saddle point\nproblems. Let us begin by calling an n-vector x primal-optimal if it solves (3.2).\nAnalogously, an m-vector y shall be called dual-optimal if it solves (3.3) when µ is\npositive or (3.4) when µ is zero.\narXiv Version 2\nPage 51\n\n\nLeast Squares and Optimization\n3.3. Computational routines\nPrimal-dual optimal solutions can be characterized with saddle point systems.\nThese are a class of 2 × 2 block linear systems that arise broadly in computational\nmathematics and especially in optimization. General introductions to these systems\ncan be found in the survey [BGL05] and the book [OA17]. We are interested in\nsaddle point systems of the form\n\u0014 I\nA\nA∗\n−µI\n\u0015 \u0014y\nx\n\u0015\n=\n\u0014b\nc\n\u0015\n.\n(3.12)\nA solution to such a system always exists when µ is positive or when the tall matrix\nA is full-rank. Given that assumption, it can be shown that a point ˜x is primal-\noptimal if and only if there is a ˜y for which (˜x, ˜y) solve (3.12). Similarly, a point ˜y\nis dual-optimal if and only if there is an ˜x for which (˜x, ˜y) solve (3.12).\nSaddle point systems are often reformulated into equivalent positive semideﬁnite\nsystems. The reformulation takes the system’s upper block to deﬁne y = b −Ax,\nand then substitutes that expression into the system’s lower block. This gives us\nthe normal equations\n(A∗A + µI)x = A∗b −c.\n(3.13)\nTherefore one can solve (3.12) by ﬁrst solving (3.13) and then setting y = b −Ax.\nSuch an approach to underdetermined least squares is suggested by Bj¨orck in his\nbooks [Bj¨o96; Bj¨o15].\nThinking in terms of the normal equations helps with the design of precon-\nditioners. When accurate solutions are desired, however, it is preferable to em-\nploy reformulations that reduce the need for matrix-vector products with the linear\noperator A∗A. Such reformulations start by deﬁning an augmented data matrix\nAµ = [A; √µIn]. For dual saddle point problems, one solves\nmin{ ∥∆y∥2\n2 : ∆y ∈Rm+n, (Aµ)∗∆y = c −A∗b},\n(3.14)\nand subsequently recovers the dual-optimal solution y = [b1 + ∆y1; . . . ; bm + ∆ym].\nFor primal saddle point problems, one computes some bshift ∈Rm+n satisfying\n(Aµ)∗bshift = c and then deﬁnes bµ = [b; 0n] −bshift. Any solution to the resulting\nproblem\nmin\nx∈Rn\n\b\n∥Aµx −bµ∥2\n2\n\t\n(3.15)\nis primal-optimal. Of course, this reformulation is only useful if we have a cheap way\nto compute bshift. As it happens, however, randomized methods for preconditioner\ngeneration provide methods to compute a near-minimum-norm solution to A∗u = c\nin O(mn) extra time compared to when c = 0. We illustrated this process earlier\nwith an SVD-based preconditioner in Algorithm 2.\nInconsistent saddle point systems\nSuppose that µ is zero, so as to allow for the possibility that (3.12) is consistent.\nUnder this assumption, (3.12) is inconsistent if and only if c is not in the range\nof A∗. When framed in this way, we have that (3.12) is inconsistent if and only if\n(3.4) has no feasible solution. What’s more, since c ̸∈range(A∗) is equivalent to\nc ̸∈ker(A)⊥, we see that inconsistency of (3.12) is equivalent to (3.2) having no\noptimal solution. Therefore a saddle point system is consistent if and only if its\nassociated saddle point problems are well-posed; for ill-posed problems, recall that\nwe canonically assign solutions per (3.5).\nPage 52\narXiv Version 2\n\n\n3.3. Computational routines\nLeast Squares and Optimization\n3.3.2\nPreconditioning least squares and saddle point problems:\ntall data matrices\nThere is a simple unifying framework for preconditioner generation of the kind used\nin [RT08; AMT10; MSM14]. The framework is applicable to any least squares or\nsaddle point problem (3.2)–(3.4) in the regime m ≫n. We describe its general form\nbelow and then turn to its concrete instantiations.\nSketch and orthogonalize\nTo describe our framework, begin by deﬁning a sketch Ask = SA where the sketching\noperator S has d ≳n rows. We also deﬁne the augmented matrices\nAµ =\n\u0014 A\n√µI\n\u0015\nand\nAsk\nµ =\n\u0014\nAsk\n√µI\n\u0015\n.\nThese augmented matrices are only used as a formalism. They reﬂect the inﬂu-\nence of the normal equations (3.13) on preconditioner design. We emphasize that\nwe speciﬁcally allow for µ = 0 and one need not form these augmented matrices\nexplicitly in memory.\nNext, we introduce two key terms.\nWe say that a matrix M orthogonalizes Ask\nµ if the columns of Ask\nµ M are\nan orthonormal basis for the range of Ask\nµ . Such a matrix is called a\nvalid preconditioner for Aµ if, in addition, rank(Ask\nµ ) = rank(Aµ).\nWe note that the rank requirement of a valid preconditioner is nearly universal\nin practice. For example, it holds with probability one for uniform and Gaussian\noperators (§2.3). We conjecture that it holds with exponentially-high probability\nfor suitable SASOs (§2.4.1) and for SRFTs (§2.5).\nHow good are these preconditioners?\nIn our context, M is a good preconditioner if\nthe spectrum of AµM can be divided into a small number of tightly clustered groups.\nGiven the tools at our disposal in RandNLA, we mostly aim for the spectrum of\nthis matrix to be tightly clustered into a single group, i.e., for its condition number\nto be small. In this regard, we can provide the following principle.\nIf M is a valid preconditioner for Aµ, then the condition number of AµM\ndoes not depend that of Aµ.\nThis principle can be formalized with the following proposition, which we state\nwithout regularization for the sake of clarity.\nProposition 3.3.1. Let U be a matrix whose columns form an orthonormal basis\nfor the range of A. If M is a valid preconditioner for A, then the spectrum of AM\nis equal to that of (SU)†.\n[RT08, Theorem 1] gives a very similar statement under the assumption that A is\nfull-rank. [MSM14, Lemma 4.2] improved upon [RT08] by supporting the rank-\ndeﬁcient case, at the price of strong assumptions on the sketching operator and the\nform of the preconditioner. In Appendix B.1 we provide what to our knowledge is\nthe ﬁrst proof of Proposition 3.3.1 in its general form; we also explain its application\nto regularized problems.\narXiv Version 2\nPage 53\n\n\nLeast Squares and Optimization\n3.3. Computational routines\nUp next.\nWe now turn to how one can compute orthogonalizers. To keep things at\na reasonable length we only speak to QR-based and SVD-based methods, although\nothers could also be used.\nOur goal is to give a general overview that includes\ntime and space complexity considerations. As to the latter consideration, we must\nnote that these preconditioners have insubstantial space requirements when A is\ndense and m ≫d ≳n.\nSeparately, we note that details of the preconditioner\ngeneration process can aﬀect the sketch-and-solve preprocessing step in sketch-and-\nprecondition algorithms. For more information on theoretical properties of these\npreconditioners in a RandNLA context, we refer the reader to [CFS21].\nQR-based preconditioning in the full-rank case\nQR-based preconditioning when µ = 0 is very simple; one need only run Householder\nQR on Ask and return M = R−1 as a linear operator. We note that specialized\nmethods for QR decomposition of very tall matrices would not be appropriate here,\nsince the d × n matrix Ask will have d ≳n. Householder-type representations of\nAsk’s QR decomposition are especially useful since they require a modest amount\nof added workspace on top of storing Ask.\nThe case with µ > 0 is more complicated if we want to avoid forming Ask\nµ\nexplicitly. To describe it, suppose we have an initial QR decomposition Ask = QoRo.\nIt is easy to show that the factor R from a QR decomposition of Ask\nµ is the same as\nthe triangular factor from a QR decomposition of ˆR := [Ro; √µI]. This observation\nis useful because there are specialized algorithms for QR decomposition of matrices\ngiven by an implicit vertical concatenation of a triangular matrix and a diagonal\nmatrix; these specialized algorithms only require O(n) additional workspace. The\nfactor Q from a QR decomposition of Ask\nµ can also be recovered with this approach,\nalthough the representation would be somewhat complicated.\nIf A is not too ill-conditioned then the same preconditioner can be obtained by\nCholesky-decomposing the regularized Gram matrix\n(Ask\nµ )∗(Ask\nµ ) = (Ask)∗(Ask) + µI,\nsince the upper-triangular Cholesky factor of that matrix is the same as the factor\nR from the QR decomposition of Ask\nµ . This approach is simple to implement, and\nits time and space requirements are unaﬀected by whether or not µ is zero.\nA\nsophisticated implementation could even try to form the regularized Gram matrix\nwithout allocating dn space for Ask as an intermediate quantity. Although, it is\nclear that unless such a sophisticated implementation is used, there is no material\nmemory savings compared to the Q-less QR approach described above. This ap-\nproach also aﬀects sketch-and-solve preprocessing by requiring that we solve the\nnormal equations, which is not a numerically stable approach [Bj¨o96].\nQR-based preconditioning in the rank-deﬁcient case\nSuppose for ease of exposition that µ = 0 and let k = rank(Ask) ≲n. One can use\na variety of methods to compute preconditioners that are morally triangular in the\nsense that they are of the form M = PR−1 for an n × k partial-permutation matrix\nP and a triangular matrix R. As long as the preconditioner orthogonalizes Ask,\nwe can postprocess zsol = argmin ∥AMz −b∥2\n2 to obtain xsol = Mz which solves\nmin ∥Ax −b∥2\n2.\nPage 54\narXiv Version 2\n\n\n3.3. Computational routines\nLeast Squares and Optimization\nThe subtlety here is that when k < n there is a nontrivial aﬃne subspace of\noptimal solutions to min ∥Ax −b∥2\n2. Our stated goal in the rank-deﬁcient case is\nto ﬁnd the minimum-norm solution to the least squares problem (see Eq. (3.5)).\nUnfortunately, if we assume that b has no role in deﬁning M, then it is clearly\nimpossible to guarantee that the norm of the recovered solution is anywhere near\nthe minimum possible among all minimizers of ∥Ax −b∥2\n2.\nSVD-based preconditioners\nLet us denote the SVD of Ask by U diag(σ)V∗.\nFirst we consider preconditioner generation when µ = 0. In this case we must\naccount for the fact that Ask might be rank-deﬁcient. Letting k denote the rank of\nAsk, the SVD-based preconditioner is the n × k matrix\nM =\n\u0014 v1\nσ1\n, . . . , vk\nσk\n\u0015\n.\nThis construction is important, because it can be shown that if z⋆solves\nmin\nz ∥AMz −b∥2\n2 + c∗Mz\n(3.16)\nthen x = Mz⋆satisﬁes (3.5). We note in particular that (3.16) has a unique optimal\nsolution and so computing z⋆is a well-posed problem.\nSVD-based preconditioning is conceptually simpler when µ is positive, since\nin that case it does not matter if Ask is rank-deﬁcient. However, it is harder to\neﬃciently implement compared to when µ = 0. Here we present an eﬃcient con-\nstruction based on the relationship between the SVD of a matrix and the eigende-\ncomposition of its Gram matrix. Speciﬁcally, recall that the right singular vectors\nof a matrix F are the eigenvectors of F∗F, and that the singular values of F are the\nsquare roots of the eigenvalues of F∗F.\nWhen used in our context this fact implies that the right singular vectors of Ask\nµ\nare equal to those of Ask, and that its singular values are\nˆσi =\nq\nσ2\ni + µ.\nThese observations alone are suﬃcient to recover the preconditioner\nM = V diag\n\u0012 1\nˆσ1\n, . . . , 1\nˆσn\n\u0013\nwhich orthogonalizes Ask\nµ .\nAs a ﬁnal point we consider the problem of recovering the left singular vectors\nof Ask\nµ given the SVD of Ask. This is useful in settings such as Algorithm 2 for\npresolve and problem transformation purposes. Moreover, it can actually be done\neﬃciently. If we deﬁne\nD1 = diag\n\u0012σ1\nˆσ1\n, . . . , σn\nˆσn\n\u0013\nand\nD2 = diag\n\u0012√µ\nˆσ1\n, . . . ,\n√µ\nˆσn\n\u0013\nthen by assumption on M the left singular vectors of Ask\nµ are given by\n\u0014\nAsk\n√µI\n\u0015\nM =\n\u0014\nAskM\n√µM\n\u0015\n=\n\u0014UD1\nVD2\n\u0015\n.\narXiv Version 2\nPage 55\n\n\nLeast Squares and Optimization\n3.3. Computational routines\nWe note that the column-orthonormality of this matrix can easily be veriﬁed by its\nrightmost representation.\nTo recap, we have introduced three key beneﬁts of SVD-based preconditioning\nfor tall least squares and saddle point problems. First, it can be used to ﬁnd the\nminimum norm solutions in (3.5) in the rank-deﬁcient case. Second, an SVD-based\npreconditioner can be computed in the presence of regularization given only the\nsingular values and right singular vectors of Ask. Third, the SVD of Ask is suﬃcient\nto recover the SVD of Ask\nµ , which facilitates sketch-and-solve as a preprocessing step\nin sketch-and-precondition.\nRemark 3.3.2 (Computational complexity). The default algorithm for SVD is cur-\nrently divide-and-conquer [GE95]. Two somewhat-outdated algorithms for comput-\ning the SVD are described in [GV13, §8.6]; [GV13, Figure 8.6.1] gives complexity\nestimates for these algorithms depending on whether the left singular vectors need\nto be computed.\n3.3.3\nPreconditioning least squares and saddle point problems:\ndata matrices with fast spectral decay\nInterpreting the Nystr¨om preconditioner.\nRecall from Section 3.2.3 that the Nystr¨om\npreconditioning approach to solving (G + µI)x = h starts by constructing a low-\nrank approximation of G. That approximation deﬁnes a preconditioner P satisfying\nthree properties:\n1. P is positive deﬁnite.\n2. (G + µI)P−1 is well-conditioned on a subspace L that contains G’s dominant\neigenspaces.\n3. P acts as the identity on L⊥(the orthogonal complement of L).\nSuch a preconditioner will be eﬀective when the action of G on L⊥is “not too\npronounced” compared to that of µI. More formally, if we deﬁne the restricted\nspectral norm of G on L⊥\n∥G∥L⊥= max{∥Gz∥2 : z ∈L⊥, ∥z∥2 = 1}\nthen the preconditioner will be eﬀective if ∥G∥L⊥/µ is O(1).\nAdaptation to saddle point problems.\nNystr¨om preconditioners can naively be used\nfor regularized saddle point problems by taking G = A∗A and considering the\nnormal equations (3.13). However, the numerical properties of iterative least squares\nsolvers that only access A∗A tend to be less robust than those of iterative solvers\nthat access A and A∗separately (i.e., solvers such as LSQR). This motivates having\nan extension of the Nystr¨om preconditioner to be compatible with the latter type\nof solver.\nTowards this end, let us express P−1 with a (possibly non-symmetric) matrix\nsquare-root M, satisfying the relation P−1 = MM∗. We appeal to the well-known\nfact that running PCG on (G + µI)z = h with preconditioner P is equivalent to\nrunning the “unpreconditioned” CG algorithm on\nM∗(G + µI)Mz = M∗h.\nPage 56\narXiv Version 2\n\n\n3.3. Computational routines\nLeast Squares and Optimization\nWhen framed in this way, we can ask how M should relate to A and µ so that it\nwould be a good preconditioner if it were used on the normal equations.\nTo answer this question we work with the augmented matrix Aµ = [A; √µIn].\nThe basic criteria for a P as a Nystr¨om preconditioner for the normal equations\n(3.13) can be stated with M as follows:\n1. AµM should be well-conditioned on a subspace L that includes the dominant\nright singular vectors of Aµ.\n2. we should have AµMx = Aµx for all x ∈L⊥.\nWhether such a preconditioner will be eﬀective can be stated with the “restricted\nspectral norm” as deﬁned above. Speciﬁcally, M should be eﬀective if the above\nconditions hold and ∥A∥L⊥/√µ is O(1). We note that the requisite matrix M can\nbe constructed eﬃciently by similar principles as methods for low-rank SVD in\nSection 4, and we leave the details to future work.\n3.3.4\nDeterministic preconditioned iterative solvers\nMost of the drivers described in Section 3.2 amount to using randomization to obtain\na preconditioner and then calling a traditional iterative solver that can make use\nof that randomized preconditioner. Here we list some iterative solvers that could\nbe of use for these drivers. We note up front that many factors can aﬀect the ideal\nchoice of iterative method in a given setting.\n• CG [HS52] is the most broadly applicable solver in our context. It applies to\nthe regularized positive deﬁnite system (3.1) and the normal equations of the\nprimal saddle point problem (3.13).\n• CGLS [HS52] applies when c is zero. It is equivalent to CG on the normal\nequations in exact arithmetic, but is more stable than CG in ﬁnite-precision\narithmetic.\n• LSQR [PS82] applies when at least one of c or b is zero. When considered for\noverdetermined problems it is algebraically (but not numerically) equivalent\nto CGLS. It is more stable than CG [Bj¨o96, § 7.6.3], [Bj¨o15, § 4.5.4] and\nCGLS [PS82, § 9] for ill-conditioned problems.\n• CS (the Chebyshev semi-iterative method) [GV61] applies to the same class\nof systems as CG. It has fewer synchronization points in each iteration and so\ncan take better advantage of parallelism. It requires knowledge of an upper\nbound and a lower bound on the eigenvalues of the system matrix. We refer\nthe reader to [Bj¨o96, § 7.2.5], [Bj¨o15, § 4.1.7] for information on this method.\n• LSMR [FS11] applies to the same problems as LSQR. For overdetermined\nleast squares it is algebraically equivalent to MINRES on the normal equa-\ntions. In that context, the residual of the normal equations will decrease with\neach iteration, which makes it safer to stop early compared to LSQR.\nThese algorithms vary in how they accommodate preconditioners. Some require\nimplicitly preconditioning the problem data, calling the “unpreconditioned” solver,\nthen applying some (cheap) postprocessing to the returned solution. We note that\nit is necessary to “precondition” any regularization term in the problem’s objective\narXiv Version 2\nPage 57\n\n\nLeast Squares and Optimization\n3.4. Other optimization functionality\nwhen using such an algorithm.3 For other algorithms, a preconditioner is supplied\nalongside the problem data, and the algorithm returns a solution that requires no\npostprocessing.\nThe diﬀerence between these situations is that diﬀerent quanti-\nties end up being available for use in termination criteria (at least for oﬀ-the-shelf\nimplementations). We emphasize that appropriate choices of termination criteria\ncan be crucial for iterative solvers to work eﬀectively, and we refer the reader to\nAppendix B.2 for discussion on this and other topics.\nAny standard library implementing the drivers from Section 3 should include\ncomputational methods for (preconditioned) CG and LSQR. LSQR is most natu-\nrally applied to dual saddle point problems by reformulation to (3.14) and to primal\nsaddle point problems by reformulation to (3.15). More full-featured RandNLA li-\nbraries would do well to include implementations of CS or LSMR, and “blocked”\nversions of iterative solvers. Such blocked methods apply to linear systems and least\nsquares problems with multiple right-hand sides; they take better advantage of par-\nallel hardware and have slightly faster convergence rates than their non-blocked\ncounterparts.\n3.4\nOther optimization functionality\nHere, we brieﬂy discuss other RandNLA algorithms of note for least squares or\noptimization, often commenting on how they ﬁt into our plans for RandLAPACK.\nSome of these algorithms are out-of-scope for a linear algebra library but can be\ndirectly facilitated by the drivers we described in Section 3.\nFacilitating second-order optimization algorithms\nMany second-order optimization algorithms need to solve sequences of saddle point\nsystems, where A, b, c vary continuously from one iteration to the next. RandLA-\nPACK will support such use-cases indirectly through methods that help amortize\nthe dominant computational cost of a single randomized algorithm across multi-\nple saddle point solves. See [PW17; RM19] for uses of RandNLA for second-order\noptimization.\nThe most common way for A to vary is by a reweighting: when A = WAo for a\nﬁxed matrix Ao and an iteration-dependent matrix W. The matrix W is typically\n(but not universally) a matrix square root of the Hessian of some separable convex\nfunction. The randomized algorithms described in this section will only be useful for\nsuch problems when W and its adjoint can be applied to m-vectors in O(m) time.\nThis condition is satisﬁed in limited but important situations such as in algorithms\nfor logistic regression, linear programming, and iteratively-reweighted least squares.\nStochastic Newton and subsampled Newton methods\nNewton Sketch is a prototype algorithm developed over two papers [PW16; PW17]\nwhich is closely related to subsampled Newton methods [XRM17; YXR+18; RM19].\nEach is suited to optimization problems that feature non-quadratic objective func-\ntions or problems with constraints other than linear equations.\nThese methods\nentail sampling a new sketching operator (and applying it to a new data matrix) in\n3That is, if we precondition a ridge regression problem, then it is necessary to precondition the\naugmented matrix [A; √µI] in an unregularized version of the problem.\nPage 58\narXiv Version 2\n\n\n3.4. Other optimization functionality\nLeast Squares and Optimization\neach iteration, with the aim of approximating the Hessian of the objective at the\ngiven iterate. The algorithms described in this section can easily serve as the main\nsubroutine in Newton Sketch and subsampled Newton methods.\nNewton Sketch has a natural specialization for least squares which entails sam-\npling and applying only one sketching operator. This specialization can be viewed\nas sketch-and-precondition, where the iterative method for solving the saddle point\nsystem is based on preconditioned steepest-descent. The asymptotic convergence\nof this approach can be established in various ways [OPA19; LP19; Tro20]. It has\nbeen shown that “traditional” sketch-and-precondition methods (based on CG or\nthe Chebyshev semi-iterative method) exhibit faster convergence [LP19]. Therefore\nwe do not expect to incorporate this method into RandLAPACK.\nThere are two recently proposed extensions of Newton Sketch that may be suit-\nable for solving the saddle point problems described in Section 3.1.2: Hessian av-\neraging [NDM22] and stochastic variance reduced Newton (SVRN) [Der22b]. The\nperformance proﬁles of these methods are better when A is very tall. When special-\nized to least squares, the former method amounts to preconditioned steepest-descent\nwhere the preconditioner is updated at each iteration. By comparison (again in the\nleast squares setting), SVRN amounts to steepest-descent with a ﬁxed precondi-\ntioner that incorporates variance-reduced sketching methods (adapted from [JZ13])\nto approximate the gradient at each iteration.\nRandom features preconditioning for KRR\nA random-features approach for computing accurate solutions to problems of the\nform (3.1) in the context of KRR is proposed in [ACW17b]. Speciﬁcally, [ACW17b]\nadvocates for using random features to obtain a preconditioner for use in an iter-\native method such as PCG. Since any such iterative solver requires access to G by\nmatrix-vector multiplication, Nystr¨om PCG can be applied to the same problems\nas this random-features preconditioning. Empirical results strongly suggest that the\nNystr¨om approach has better performance than random-features preconditioning\non shared-memory machines [FTU21]. Thus, we do not plan for RandLAPACK to\nsupport random-features preconditioning at this time.\nUtilities for iterative reﬁnement\nIterative reﬁnement can be used as a tool to compensate for rounding errors in\notherwise reliable linear system solvers. These methods typically work by computing\nresiduals to higher precision than that used by the solver, running the solver with the\nupdated residual, and then adding the new solution to the original solution [Bj¨o96,\n§2.9.2].4 LAPACK has some procedures of this kind. See [Hig97] and [DHK+06] for\ntheoretical and practical analyses.\nThe best way to use iterative reﬁnement routines to support randomized algo-\nrithms in this section is yet to be determined. On the one hand, it may suﬃce to in-\nclude methods similar to those in LAPACK. However, sketch-and-precondition algo-\nrithms might pose unique numerical problems that require diﬀerent techniques. See\n[AMT10, §5.7] for some discussion of numerical issues in the sketch-and-precondition\ncontext. It might also be natural for a RandNLA library to have more iterative re-\n4In some situations, it can suﬃce to recompute the residual with the same precision used by\nthe underlying solver [Bj¨o96, §2.9.3].\narXiv Version 2\nPage 59\n\n\nLeast Squares and Optimization\n3.5. Existing libraries\nﬁnement methods than LAPACK in order to better exploit low-precision arithmetic\nand accelerators.\n3.5\nExisting libraries\nWe know of four high-performance libraries with sketch-and-precondition methods\nfor least squares: Blendenpik [AMT10], LSRN [MSM14], LibSkylark [KAI+15], and\nSki-LLS [CFS21].5 To our knowledge, LibSkylark is the only RandNLA library which\nsupports least squares and low-rank approximation (see Section 4.5). None of these\nlibraries support saddle point problems of the kind we consider, and none of them\nmake use of Nystr¨om preconditioning.\nBlendenpik.\nThis library is written in C and callable from Matlab; it is currently\navailable on the Matlab File Exchange. It uses LSQR as the deterministic iterative\nsolver, and obtains the preconditioner by running QR on a sketch Ask = SA, where\nS is an SRFT. Blendenpik also adaptively calls LAPACK if a problem is deemed too\npoorly scaled or if the iterative method performs poorly. It was shown to outperform\nan unspeciﬁed LAPACK least squares solver on a machine with 8GB RAM and an\nAMD Opteron 242 processor [AMT10].\nLSRN.\nThis comprises a C++ implementation callable from Matlab and a Python\nimplementation. The C++ implementation was shown to outperform LAPACK’s\nDGELSD on large dense problems, and Matlab’s backslash (SuiteSparseQR) on sparse\nproblems. The Python implementation has demonstrated that LSRN scales well on\nAmazon Elastic Compute Cloud clusters. We note that the Python implementation\nrelies on an auxiliary Python package with a custom C-extension for sampling from\nthe Gaussian distribution via the ziggurat method.\nLibSkylark.\nThis library is written in C++ and is available on GitHub. Its sup-\nport for least squares problems is very general and includes a few deterministic\npreconditioned iterative solvers. Its sketch-and-precondition functionality includes\nimplementations in the styles of Blendenpik and LSRN. LibSkylark has a Python\ninterface, but only for Python 2.7. Its linear algebra kernels are implemented partly\nin the Elemental distributed linear algebra library [PMG+13]. Unfortunately, Ele-\nmental is no longer maintained.\nSki-LLS.\nThis is a recently developed C++ library for solving dense and sparse\nhighly overdetermined least squares problems. It is distinguished by its ﬂexibility in\npreconditioner generation. In particular, it supports sketching by SRFTs, Gaussian\noperators, and SASOs. It also supports factoring the sketch SA by several methods,\nincluding a standard SVD algorithm, a randomized algorithm for full-rank column-\npivoted QR (see Section 5.1.2), and a standard algorithm for sparse QR. We record\nthe following (adapted) quote from the GitHub repository that hosts this software:\nSki-LLS is faster and more robust than Blendenpik and LAPACK on\nlarge over-determined data matrices, e.g., matrices having 40,000 rows\nand 4,000 columns.\nSki-LLS is 10 times faster than Sparse QR and\n5Note that “Blendenpik” and “LSRN” are names for algorithms and libraries.\nPage 60\narXiv Version 2\n\n\n3.5. Existing libraries\nLeast Squares and Optimization\nincomplete-Cholesky preconditioned LSQR on sparse data matrices that\nare ill-conditioned and suﬃciently large, e.g., with 120,000 rows, 5,000\ncolumns, and 1% non-zeros.\nFalkon.\nFinally, we note the recently developed Falkon library for sketch-and-solve\napproaches to KRR powered by multi-GPU machines [MCR+20; MCD+22]. While\nthis library works outside of our primary data model, it is of interest to anyone\ndeveloping software for KRR based on RandNLA.\narXiv Version 2\nPage 61\n\n\nLeast Squares and Optimization\n3.5. Existing libraries\nPage 62\narXiv Version 2\n\n\nSection 4\nLow-rank Approximation\n4.1 Problem classes ...........................................................\n64\n4.1.1 Spectral decompositions .............................................\n65\n4.1.2 Submatrix-oriented decompositions ..............................\n68\n4.1.3 On accuracy metrics ..................................................\n72\n4.2 Drivers ........................................................................\n73\n4.2.1 Methods for SVD ......................................................\n74\n4.2.2 Methods for Hermitian eigendecomposition ...................\n75\n4.2.3 Methods for CUR and two-sided ID .............................\n78\n4.3 Computational routines ..............................................\n80\n4.3.1 Power iteration .........................................................\n81\n4.3.2 Orthogonal projections: QB and rangeﬁnders ................\n81\n4.3.3 Column-pivoted matrix decompositions ........................\n83\n4.3.4 One-sided ID and CSS ...............................................\n85\n4.3.5 Estimating matrix norms ...........................................\n88\n4.3.6 Oblique projections ...................................................\n89\n4.4 Other low-rank approximations ..................................\n89\n4.5 Existing libraries .........................................................\n91\nModern scientiﬁc computing, machine learning, and data science applications\ngenerate massive matrices that need to be processed for reduced run time, reduced\nstorage requirements, or improved interpretability. Low-rank approximation is a\nworkhorse approach for achieving these goals. Here, given a target matrix A, the\ntask is to produce a suitably factored representation of a low-rank matrix ˆA of the\nsame dimensions which approximates the matrix A.\nWe can express the main aspects of a low-rank approximation as computing\nfactor matrices E and F where\nA\n≈\nˆA\n:=\nE\nF\nm × n\nm × n\nm × k\nk × n\n(4.1)\nfor some k ≪min{m, n}. We note that it is very common to have a k × k “inner\nfactor” that appears in between E and F above.\n63\n\n\nLow-rank Approximation\n4.1. Problem classes\nSuch representations facilitate data interpretation by choosing the factors to\nhave useful structure, such as having orthonormal columns or rows, or being sub-\nmatrices of the target. The extent of storage reduction from low-rank approxima-\ntion depends on whether A is dense or sparse. In the dense case, ˆA is stored in\nO(mk + nk) space. In the sparse case, one representation consists of a dense k × k\ninner factor, a slice of k rows of A, and a slice of k columns of A.\nThe rank k used in a low-rank approximation is a tuning-parameter that the\nuser can control to trade-oﬀbetween approximation accuracy and data compression.\nThe best choice of this parameter depends on context. For instance, one may want\nto choose k small enough to graphically visualize coherent structure in the target.\nIn such a setting one would not expect that ˆA is close to A in an absolute sense, but\none can still ask that the distance is near the minimum among all approximations\nwith the desired structure and rank. Alternatively, one might know that A can be\nwell-approximated by a low-rank matrix, and yet not know the rank necessary to\nachieve a good approximation. Such matrices are called numerically low-rank and\narise in applications across the social, physical, biological, and ecological sciences.\nFor example, they can arise as discretizations of diﬀerential operators, where the\nextent to which the matrix is numerically low-rank depends on the details of the\noperator and the discretization; and they can arise as noisy corruptions of general\n(hypothesized) data matrices with low exact rank. When dealing with such matrices\none can iteratively build ˆA until a desired distance ∥A −ˆA∥is small. This section\ncovers a variety of eﬃcient and reliable low-rank approximation algorithms for both\nof these scenarios.\n4.1\nProblem classes\nLow-rank approximation is naturally formalized as an optimization problem; one\nchooses ˆA and its factors to minimize a loss function subject to some constraints.\nThe most common loss functions are distances ˆA 7→∥A −ˆA∥induced by the Frobe-\nnius or spectral norms. Alternatively, one can use the discontinuous loss function\nˆA 7→rank(ˆA) as a measure of the storage requirements for ˆA. Constraints depend\non the loss function in a complementary way. When minimizing a norm-induced\ndistance, one imposes rank constraints by limiting the dimensions of the factors.\nWhen minimizing the rank of ˆA (i.e., when seeking an approximation that admits\nthe smallest-possible representation) one constrains the approximation error ∥A−ˆA∥\nto be at most some speciﬁed value. One can also impose structural constraints on\nthe factors of ˆA, such as being orthogonal, diagonal, or a submatrix of the target.\nOur overview of randomized algorithms for low-rank matrix approximation is\norganized around such structural constraints. Accordingly, we use the term prob-\nlem class for loose groups of low-rank approximation problems wherein the factors\nfacilitate similar downstream tasks.\nCurrently, our two problem classes are the\nfollowing.\n• Spectral decompositions (§4.1.1): this consists of low-rank SVD and Hermi-\ntian eigendecomposition.\n• Submatrix-oriented decompositions, i.e., decompositions with factors based\non submatrices of the target matrix (§4.1.2): this consists of so-called CUR\nand interpolative decompositions.\nPage 64\narXiv Version 2\n\n\n4.1. Problem classes\nLow-rank Approximation\nOptimal decompositions in the ﬁrst class often serve as baselines in theoretical\nanalyses of randomized algorithms for low-rank decomposition in both classes. That\nis, such comparisons are made regardless of whether the approximation is spectral\nor submatrix-oriented. This fact can blur the distinction between the two problem\nclasses, and the distinctions can blur even further when one considers methods for\neﬃciently converting from one decomposition to another. Still, keeping the problem\nclasses separate is useful as an organizing principle for the most fundamental low-\nrank approximation problems in RandNLA.\nRemark 4.1.1. Low-rank approximations that impose no requirements on ˆA’s rep-\nresentation are brieﬂy addressed in Section 4.3.6 in the context of computational\nroutines. Methods for low-rank approximation with other representations (e.g., QR,\nUTV, LU, nonnegative factorization) are discussed in Section 4.4.\n4.1.1\nSpectral decompositions\nIn what follows we give an overview of the SVD and Hermitian eigendecomposition,\nwith emphasis on the roles of these decompositions in low-rank approximation.\nAfter covering these concepts we explain how they provide two perspectives on\nprincipal component analysis (PCA). We advise the reader to at least skim this\noverview material even if they are already familiar with the relevant concepts; low-\nrank approximation is much more prominent in RandNLA than it is in classical\nNLA.\nSingular value decomposition\nThe SVD is widely used to compute low-rank approximations and as a workhorse\nalgorithm for PCA. Given a m × n matrix A, where m ≥n (without loss of gener-\nality), its SVD is\nA\n=\nU\nΣ\nV∗\nm × n\nm × n\nn × n\nn × n ,\n(4.2)\nwhere U = [u1, . . . , un] and V = [v1, . . . , vn] are column-orthonormal matrices that\ncontain the left and right singular vectors of A. The matrix Σ = diag(σ1, . . . , σn)\ncontains the corresponding singular values; we use the convention that they appear\nin decreasing order σ1 ≥. . . ≥σn ≥0.\nWe can also think about the SVD as\nexpressing A as the sum of n rank-one matrices\nA =\nn\nX\ni=1\nσiuiv∗\ni .\n(4.3)\nIn applications it is common to encounter data matrices with low-rank structure,\ni.e., matrices for which r = rank(A) is smaller than the ambient dimensions m and\nn of A. In this case, the singular values {σi : i ≥r + 1} are zero, the corresponding\nsingular vectors span the left and right null spaces, and it is natural to consider the\ncompact SVD where the sum in (4.3) is truncated at i = r. For a matrix A with\napproximate low-rank structure, we can obtain approximations with low exact rank\nby truncating this sum even earlier, at some k < r:\nA ≈ˆAk :=\nk\nX\ni=1\nσiuiv∗\ni\n=[u1, . . . , uk] diag(σ1, . . . , σk)[v1, . . . , vk]∗= UkΣkV∗\nk,\n(4.4)\narXiv Version 2\nPage 65\n\n\nLow-rank Approximation\n4.1. Problem classes\nTruncating trailing singular values yields optimal rank-constrained approximations,\nin the sense of solving\nˆAk ∈arg min\nrank(ˆA\n′)=k\n∥A −ˆA\n′∥.\n(4.5)\nThis holds for every k ∈JrK. In other words, if A is approximated by a rank-k\nmatrix ˆAk given through its SVD, no further computation is needed to canonically\nobtain approximations of A with any rank k ≤r.\nThe optimality result of (4.5) holds for any unitarily invariant matrix norm, and\nit is known as the Eckart-Young-Mirsky Theorem when considered for the spectral\nnorm or Frobenius norm. The reconstruction errors according to these norms are\n∥A −ˆAk∥2 = σk+1(A)\nand\n∥A −ˆAk∥F =\nsX\nj>k\nσ2\nj (A).\n(4.6)\nThese facts are important in applications, where it is common to see rank(A) =\nmin{m, n} in exact arithmetic and yet many of the trailing singular values are so\nsmall that they can be presumed to be noise. That is, the truncation introduced in\n(4.4) is often used as a denoising technique.\nHermitian eigendecomposition\nA matrix is called Hermitian if it is equal to its adjoint, i.e., if A = A∗. For real\nmatrices, being Hermitian is the same as being symmetric. The eigendecomposition\nof a Hermitian matrix A is\nA\n=\nV\nΛ\nV∗\nn × n\nn × n\nn × n\nn × n ,\n(4.7)\nwhere V is an orthogonal matrix of eigenvectors and Λ = diag(λ1, . . . , λn) is a real\nmatrix containing the eigenvalues of A. A Hermitian matrix is further called positive\nsemideﬁnite (or “psd”) if λi ≥0 for all i.\nWe use the convention of sorting eigenvalues in decreasing order of absolute\nvalue: |λ1| ≥· · · ≥|λn|. This allows for a more direct comparison to the SVD,\nsince we obtain low-rank approximations\nA ≈ˆAk :=\nk\nX\ni=1\nλiviv∗\ni\n=[v1, . . . , vk] diag(λ1, . . . , λk)[v1, . . . , vk]∗= VkΛkV∗\nk\n(4.8)\nfor which the spectral and Frobenius-norm distances to A match those from (4.6).\nIndeed, a (truncated) eigendecomposition can be converted to a (truncated) SVD\nby taking the columns of V as the right singular vectors, setting the left singular\nvectors according to\nui =\n(\nvi\nif λi > 0\n−vi\notherwise\nand setting the singular values to σ = |λ| (elementwise).\nIf a matrix is Hermitian then it is better to compute (and work with) its eigen-\ndecomposition, rather than its SVD. The ﬁrst reason for this is that a rank-k eigen-\ndecomposition requires almost half the storage of a rank-k SVD. The second reason\nPage 66\narXiv Version 2\n\n\n4.1. Problem classes\nLow-rank Approximation\nis that algorithms for computing low-rank eigendecompositions are able to leverage\nstructure in the matrix for improved eﬃciency. These eﬃciency improvements can\nbe dramatic for psd matrices, where an eigendecomposition is technically also an\nSVD.\nConnections to principal component analysis\nPCA is a linear dimension reduction technique that is widely used in data science\napplications for extracting features, or for visualizing and summarizing complicated\ndatasets. The idea of PCA is to form k new variables (components) Z = [z1, . . . , zk]\nas linear combinations of the variables X = [x1, . . . , xn] ∈Rm×n (that are assumed\nto have been preprocessed to have column-wise zero empirical mean). Speciﬁcally,\ngiven the data matrix X, one forms the variables as Z = XW where the weights\nW = [w1, . . . , wk] ∈Rn×k are chosen so that the ﬁrst component z1 accounts\nfor most of the variability in the data, the second component z2 for most of the\nremaining variability, and so on.\nFormally, we can formulate this problem as a variance maximization problem\nw1 := arg max\n∥w∥2\n2=1\nVar(Xw)\n(4.9)\nwhere we deﬁne the variance operator as Var(Xw) :=\n1\nm−1∥Xw∥2\n2. Deﬁning the\nsample covariance matrix C :=\n1\nm−1X∗X, this problem can be stated as\nw1 := arg max\n∥w∥2\n2=1\nw∗Cw.\n(4.10)\nWe recognize (4.10) as the variational formulation of the dominant eigenvector of a\nHermitian matrix. That is, w1 satisﬁes\nCw1 = λ1(C)w1.\n(4.11)\nMore generally, PCA ﬁnds the weights w1, . . . , wk by diagonalizing the empirical\nsample covariance matrix C as C = WΛW∗, and retaining only the top k eigenvec-\ntors.\nHow one computes the PCA depends on how the data is presented and accessed.\nThere are two situations of interest. In situations where the covariance matrix C\nis given by the problem at hand—and thus where we access C directly, but do not\ndirectly access the data matrix X—one can directly employ a low-rank Hermitian\neigendecomposition to compute the dominant k eigenvectors.\nIf instead we are\npresented with the variables in form of a mean-centered data matrix X, then the\nlow-rank SVD becomes the preferable approach to computing the weights W. This\nis because we can relate the eigenvalue decomposition of the inner product X∗X to\nthe SVD of X = UΣV∗by\nX∗X = (VΣU∗)(UΣV∗) = VΣ2V∗.\n(4.12)\nHence, we obtain the k weights W = [w1, . . . , wk] by computing the top k right\nsingular vectors Vk = [v1, . . . , vk], and the eigenvalues of the sample covariance\nmatrix C are given by the diagonal elements\n1\nm−1Σ2.\nFast randomized algorithms for computing the SVD and eigenvalue decomposi-\ntion enable scaling PCA to especially large problems. However, one does not need a\nlarge problem to beneﬁt from these randomized algorithms. From 2016 until at least\ntime of writing, the pca function SciKit-Learn defaults to a randomized algorithm\nwhen d = min{m, n} ≥500 and k is less than 0.8d [Gri16].\narXiv Version 2\nPage 67\n\n\nLow-rank Approximation\n4.1. Problem classes\n4.1.2\nSubmatrix-oriented decompositions\nHere we describe four types of submatrix-oriented decompositions: a CUR decompo-\nsition and three types of interpolative decompositions. Historically, these have been\nused far less often than spectral decompositions. However, their value propositions\nhave become much more compelling in recent years:\n• They can oﬀer reduced storage requirements compared to spectral decom-\npositions, especially for sparse data matrices. This can be very valuable in\nprocessing massive data sets.\n• They provide for more transparent data interpretation. This is especially true\nwhen data is modeled as a matrix as a matter of convenience, rather than as\na statement about the data deﬁning a meaningful linear operator A 7→Av.\nRemark 4.1.2. The following material is dense. We encourage the reader to return\nto it as needed while reading later parts of this section.\nCUR decomposition\nDeﬁnition.\nA CUR decomposition is a low-rank approximation of the form\nA\n≈\nC\nU\nR,\nm × n\nm × k\nk × k\nk × n\n(4.13)\nwhere the factors C and R are formed by small subsets of actual columns and rows\nof A, and the linking matrix U is chosen so that some norm of A −CUR is small.\nMotivation.\nThe literature on CUR decomposition traces back to work by Gore˘ınov,\nZamarashkin, and Tyrtyshnikov, who proved existential results for CUR decompo-\nsitions with certain approximation error bounds [GZT95; GTZ97]. Gore˘ınov et al.\nmotivated their investigations by pointing out that CUR decompositions have far\nlower storage requirements than partial SVDs when A is sparse.More speciﬁcally,\nthey advocated for applying CUR decompositions for low-rank approximation of\noﬀ-diagonal blocks in block matrices.\nThe usage of CUR as a data-analysis tool was popularized by Mahoney and\nDrineas [MD09], following the development of eﬃcient randomized algorithms for\ncomputing CUR decompositions with good approximation guarantees [DMM08].\nThe argument of Mahoney and Drineas was that experts often have a clear under-\nstanding of the actual meaning of certain columns and rows in a matrix, and this\nmeaning is preserved by the CUR. In contrast, SVD (or PCA) forms linear combi-\nnations of the columns or rows of the input matrix; these linear combinations can\nprove diﬃcult to interpret and destroy structures such as sparsity or nonnegativity.\nWords of warning.\nDespite its simple deﬁnition, there are important subtleties in\nworking with and understanding CUR decompositions.\nFirst, CUR is unique among submatrix-oriented decompositions in that it in-\nvolves taking products of submatrices of A. Therefore if A has low numerical rank\nand its CUR decomposition is computed to high accuracy, then all three factors (C,\nU, and R) will be ill-conditioned. This can have detrimental eﬀects on numerical\nbehavior, particularly in the computation of U, which will behave qualitatively like\ninverting a matrix of low numerical rank.\nPage 68\narXiv Version 2\n\n\n4.1. Problem classes\nLow-rank Approximation\nSecond, out of all the submatrix-oriented decompositions, CUR is of the least\ninterest for providing exact decompositions of full-rank matrices. For example, if\nA = CUR is a tall matrix of rank n, then we would necessarily have C = AP\nand U = P∗R−1 for some permutation matrix P. Therefore the only real freedom\nin full-rank CUR of a tall matrix is in choosing the spanning set of rows that\ndeﬁne R.\nA similar conclusion applies when A is a wide matrix of rank m; an\nexact decomposition would necessarily have R = P∗A and U = C−1P for some\npermutation matrix P, and the only real freedom would be in choosing the spanning\nset of columns that deﬁne C.\nThe consequences of this second fact will be seen when we discuss randomized\nalgorithms for computing CUR decompositions.\nIn particular, we will see that\nrandomized algorithms for (low-rank) CUR generally do not involve computing\n“full-rank CUR decompositions” on smaller matrices. This will stand in contrast to\nrandomized algorithms for (low-rank) SVD and interpolative decompositions, which\nusually do involve computing the corresponding full-rank decomposition on smaller\nmatrices.\nInterpolative decompositions\nLow-rank interpolative decompositions (IDs) come in three diﬀerent ﬂavors that\nshare two common notes. The ﬁrst shared note of these ﬂavors is that exactly one\nof the factors that deﬁne ˆA is a submatrix of A. The second shared note concerns\nthe factors of ˆA that are not submatrices of A. These factors, called interpolation\nmatrices, are subject to certain regularity conditions.\nOur crash course on ID comes in three parts. First, we deﬁne versions of ID\nthat only involve one interpolation matrix, so-called one-sided IDs. After that, we\nexplain how accuracy guarantees of low-rank ID are aﬀected by regularity conditions\non interpolation matrices. This explanation is important; we reference it repeatedly\nwhen we cover randomized algorithms for one-sided ID (§4.3.4). We wrap up by\nintroducing the two-sided ID.\nThe one-sided IDs: column ID and row ID.\nIn the low-rank case, a column ID is an\napproximation of the form\nA\n≈\nC\nX\nm × n\nm × k\nk × n\n(4.14)\nwhere C is given by a small number of columns of A and X is a wide interpolation\nmatrix. Full-rank column IDs can be of interest to us when A is very wide, in which\ncase we have k = m ≪n and X = C−1A. Next, we consider row IDs. In the\nlow-rank case, these are approximations of the form\nA\n≈\nZ\nR.\nm × n\nm × k\nk × n\n(4.15)\nwhere R is given by a small number of rows of A and Z is a tall interpolation matrix.\nThe submatrices C and R can be represented by ordered column and row index sets,\nwhich we denote by J and I, that satisfy\nC = A[:, J]\nand\nR = A[I, :].\nThese ordered index sets are called skeleton indices.\narXiv Version 2\nPage 69\n\n\nLow-rank Approximation\n4.1. Problem classes\nOur deﬁnitions of row and column IDs are not complete until we specify the\nregularity conditions on X and Z. The skeleton indices (J, I) play a central role\nhere. Most importantly, we require that the interpolation matrices satisfy\nX[:, J] = Ik = Z[I, :].\nThese regularity conditions have two direct consequences, namely\nA[:, J] = A[:, J]X[:, J] , and\nA[I, :] = Z[I, :]A[I, :],\nIn addition to these conditions, the literature on ID typically requires that the\nentries of X and/or Z are bounded in modulus by a small constant M. While we do\nnot subscribe to this requirement in our deﬁnition of IDs, there is good motivation\nbehind it. We address this motivation next.\nQuality-of-approximation in low-rank IDs.\nSuppose that ˆA is a low-rank column ID\nof A. It is immediate from our deﬁnition of low-rank ID that there are at most\n\u0000n\nk\n\u0001\npossible values for the subspace range(ˆA). In general, it can happen that none of\nthese subspaces coincides with a dominant k-dimensional left singular subspace of\nA. When this happens, it will necessarily be the case that ∥A −ˆA∥2 > σk+1(A),\nwhereas a general rank-k approximation could achieve an error equal to σk+1(A).\nThis raises a question.\nHow much of a price do we pay by imposing that requirement that ˆA be\na low-rank ID?\nThe following proposition (which we prove in Appendix C.1.1 by standard tech-\nniques) gives a partial answer to this question. The answer is notable in that it\nreveals the value of bounding the interpolation matrices.\nProposition 4.1.3. Let ˜A be any rank-k approximation of A that satisﬁes the\nspectral norm error bound ∥A −˜A∥2 ≤ϵ. If ˜A = ˜A[:, J]X for some k × n matrix X\nand an index vector J, then ˆA = A[:, J]X is a low-rank column ID that satisﬁes\n∥A −ˆA∥2 ≤(1 + ∥X∥2)ϵ.\n(4.16)\nFurthermore, if |Xij| ≤M for all (i, j), then\n∥X∥2 ≤\np\n1 + M 2k(n −k).\n(4.17)\nWe note that the assumptions on the matrix ˜A from the proposition statement\nimply that ˜A[:, J] is full column-rank. This implies that (˜A[:, J])†(˜A[:, J]) = Ik and\nhence that X is completely determined from the index vector J. Since a rank-k\nmatrix will typically have multiple subsets of k columns that span its range, we\nﬁnally see that the matrix ˜A does not uniquely determine the interpolation matrix\nX used in the proposition. Indeed, the range of possibilities for the interpolation\nmatrix is rather remarkable. While it is known that there always exists an index set\nfor which maxij |Xij| = 1 [Pan00], it is NP-hard to compute this index set [C¸M09].\nAt the same time, algorithms such as strong rank-revealing QR can be applied to\n˜A with typical runtime O(mnk), while ensuring maxij |Xij| ≤2 [GE96].\nPage 70\narXiv Version 2\n\n\n4.1. Problem classes\nLow-rank Approximation\nAll-in-all, the main point of an interpolative decomposition is to provide a low-\nrank approximation that prominently features a submatrix of the target. Therefore\nwhile an upper bound M on the entries of an interpolation matrix gives the bound\n(4.16) indirectly by way of (4.17), such a bound should not the end of the story.\nThe spectral norm ∥X∥2 is ultimately more informative for this purpose, even if it\nis harder to compute.\nOf course, all of the points we have raised here for column IDs apply to row IDs\nwith minor modiﬁcations.\nTwo-sided ID.\nThe concept of a one-sided ID can be extended to a two-sided ID\nby considering simultaneous row and column IDs. In the low-rank case, a two-sided\nID is an approximation of the form\nA\n≈\nZ\nA[I, J]\nX.\nm × n\nm × k\nk × k\nk × n\n(4.18)\nMethods for full-rank one-sided ID can be useful in computing low-rank two-sided\nIDs. That is, if we ﬁrst compute a low-rank column ID ˆA = CX by some black-box\nmethod, then after obtaining a full-rank row ID of the tall matrix C = A[:, J] =\nZA[I, J] we have ˆA = ZA[I, J]X.\nOn relationships between submatrix-oriented decompositions\nThe landscape of methods for submatrix-oriented decompositions is very inter-\ntwined. As we have already indicated, algorithms for one-sided ID are the main\nbuilding blocks of algorithms for low-rank two-sided ID. Later in this section we also\nmention several algorithms for CUR decomposition which depend on algorithms for\none-sided ID. In view of this, we emphasize the following point.\nFor our purposes, it is helpful to introduce hierarchical relationships\namong submatrix-oriented decompositions. As such, we designate al-\ngorithms for one-sided ID as computational routines, while methods for\nCUR and two-sided ID are designated as drivers.\nNext, let us point out a sense in which CUR and two-sided ID are “dual” to one\nanother. Both are submatrix-oriented decompositions that have three factors. For\nCUR, the outer factors are submatrices of A and no requirements are placed on the\ninner factor (the linking matrix). In particular, if we specify the outer factors by\nordered index sets J and I, then a CUR decomposition is expressed as\nA\n≈\nA[:, J]\nU\nA[I, :].\nm × n\nm × k\nk × k\nk × n\n(4.19)\nThis can be contrasted with two-sided ID as deﬁned in (4.18). There, the inner\nfactor is a submatrix of A, and moderate requirements are placed on the outer\nfactors (the interpolation matrices).\nThe properties of “linking matrices” and “interpolation matrices” are diﬀerent\nenough to warrant their diﬀerent names. The problem of computing a low-rank\napproximation via two-sided ID can be better numerically behaved, compared to\nlow-rank approximation by CUR [MT20, §13]. However, CUR oﬀers far greater\npotential for storage reduction when dealing with sparse matrices. The diﬀerences\narXiv Version 2\nPage 71\n\n\nLow-rank Approximation\n4.1. Problem classes\nbetween two-sided ID and CUR can become less pronounced when one considers\nmethods for losslessly converting one such representation to the other, as we mention\nin Section 4.2.3.\n4.1.3\nOn accuracy metrics\nThe problems from Section 3 were mostly unconstrained minimization of convex\nquadratics. Such problems are very nice, since the gradient of the quadratic loss\nfunction constitutes a canonical error metric that can be driven to zero. Low-rank\napproximation problems can likewise be framed as optimization problems. However,\nthese formulations either involve constraints or a nonconvex objective function. This\ndistinction is important, since these structures rule out checking for a zero gradient\nas a cheap optimality condition.\nThe main error metrics in low-rank approximation are norm-induced distances.\nFor reasons that we give under the next two headings it is not appropriate to\nconsider distances from a computed approximation ˆA to some nominally “optimal”\napproximation. Instead, one measures the distance from the approximation to the\ntarget, most often in the spectral or Frobenius norms.\nDistance to optimal approximations\nNon-unique solutions and sensitivity to perturbations.\nRecall from Section 4.1.1 how\ntruncating A’s SVD at rank k gives an optimal rank-k approximation in any unitar-\nily invariant norm. Unfortunately, this truncation will be non-unique when A has\nmore than one singular value equal to σk. This is easiest to see when A = I is the\nidentity matrix, in which case every diagonal {0, 1}-matrix of rank k is an optimal\nrank-k approximation to A.\nMore generally, if A has multiple singular values that are close to σk, then\nextremely small perturbations to A can result in large changes to the singular vectors\ncorresponding to these singular values; see [Bha97, §6 – §8] for details. This has\na secondary complication: it is harder to estimate the dominant k singular vectors\nof a matrix than it is to ﬁnd a rank-k approximation that is “near optimal” in the\nsense of (4.5).\nIntractability of computing optimal approximations.\nWhen working with submatrix-\noriented decompositions, we do not even have the luxury of deﬁning “optimal”\napproximations in the manner of truncated SVDs. Indeed, the problem of ﬁnding\nan “optimal” ID necessitates specifying any regularity conditions such as the bound\nM in a constraint |Xij| ≤M. As we mentioned before, even when ˆA has exact rank\nk, a rank-k column ID with M = 1 always exists but is NP-hard to ﬁnd [C¸M09].\nGoing to another extreme, we could set aside the matter of M and simply set\nX = C†A for a matrix C containing k columns of A. In this case it is not known if\nthe columns can be chosen to minimize Frobenius- or spectral-norm error ∥ˆA −A∥\nin time less than O(nk). Still, there are theoretical guarantees for approximation\nquality by CUR relative to approximation quality achievable by SVD. We refer\nthe reader to [DMM08; BMD09], [VM16, §1 - §2], and Appendix C.1.1 for more\ninformation about CUR and ID in our context.\nPage 72\narXiv Version 2\n\n\n4.2. Drivers\nLow-rank Approximation\nDistance relative to that of a reference approximation\nIt is problematic to use a distance from ˆA to A as an error metric for ˆA. This is\nbecause there are situations when any such distance will be large even when ˆA is\nclose to an “optimal” approximation. The simplest example of this is PCA, in which\ncases the approximation rank is O(1), independent of the dimensions of the matrix\nor properties of its spectrum. More generally, it can be hard to obtain a low-rank\napproximation that is very close to A when A has slow spectral decay, in the sense\nthat the distribution of its singular values has a heavy tail. Accurate approximations\ncan also be hard to come by if the factors of ˆA are highly constrained.\nOne handles this situation by considering the distance between A and ˆA relative\nto that between A and some reference matrix Ar. Formally, we concern ourselves\nwith the smallest value of ϵ needed to achieve\n∥A −ˆA∥≤(1 + ϵ)∥A −Ar∥.\nThe reference matrices Ar used in RandNLA theory are not available to us when\nperforming computations. In fact, they are usually not optimal for the formal low-\nrank approximation problem at hand. The most common source of non-optimality\nis that the reference is subject to a more stringent rank constraint: rank(Ar) <\nrank(ˆA) ≤rank(A). Another source of non-optimality is that it may not be pos-\nsible to decompose the reference into factors with the required structure (e.g., the\nstructure required by low-rank CUR). For example, an approximation of A obtained\nby a rank-k truncated SVD cannot (in general) be converted into a rank-k CUR\ndecomposition using submatrices of A.\n4.2\nDrivers\nThere exist many randomized algorithms for computing low-rank approximations\nof matrices. This section focuses on low-rank approximation algorithms that take\nthe two-stage approach popularized by [HMT11], because this approach has been\ndemonstrated to be eﬃcient and highly reliable over the years.\nThe high-level\nidea of the two-stage approach is the following: ﬁrst one constructs a “simple”\nrepresentation of ˆA with the aid of randomization, and then one deterministically\nconverts that representation of ˆA into a more useful form.\nIn order to discuss these drivers for low-rank approximation, it is necessary to\nmention brieﬂy the following two concepts (these are handled by computational\nroutines, to be discussed in detail in Section 4.3):\n• A QB decomposition is a simple representation that is useful for SVD and\neigendecomposition. The representation takes the form ˆA = QB for a tall\nmatrix Q with orthonormal columns and B = Q∗A. The important point\nhere is that the QB decomposition involves explicit construction of and access\nto both Q and B. We discuss QB algorithms in Section 4.3.2.\n• Column subset selection (CSS) is the problem of selecting from a matrix a set\nof columns that is “good” in some sense. CSS algorithms largely characterize\nalgorithms for one-sided ID. We discuss methods for these two problems in\nSection 4.3.4. They are important here because a one-sided ID can be used\nfor the simple representation of ˆA when working toward an SVD, eigendecom-\nposition, two-sided ID, or CUR decomposition.\narXiv Version 2\nPage 73\n\n\nLow-rank Approximation\n4.2. Drivers\n4.2.1\nMethods for SVD\nThe are several families of randomized algorithms for computing low-rank SVDs.\nHere we describe a few families that give ˆA = UΣV∗through its compact SVD.\nA ﬂexible method\nWe begin with Algorithm 3.\nThis algorithm uses randomization to compute a\nQB decomposition of A, then deterministically computes QB’s compact SVD, and\nﬁnally truncates that SVD to a speciﬁed rank.\nThis algorithm assumes that the QBDecomposer is iterative in nature. It assumes\nthat each iteration adds some number of columns to Q and rows to B, and that\nthe algorithm can terminate once an implementation-dependent error metric for\nQB ≈A falls below ϵ or once QB reaches a rank limit. Here we have set the rank\nlimit to k + s where s is a nonnegative “oversampling parameter.”\nAlgorithm 3 SVD1 : QB-backed low-rank SVD (see [HMT11] and [RST10])\n1: function SVD1(A, k, ϵ, s)\nInputs:\nA is an m × n matrix. The returned approximation will have rank at\nmost k. The approximation produced by the randomized phase of the\nalgorithm will attempt to A to within ϵ error, but will not produce\nan approximation of rank greater than k + s.\nOutput:\nThe compact SVD of a low-rank approximation of A.\nAbstract subroutines:\nQBDecomposer generates a QB decomposition of a given matrix; it\ntries to reach a prescribed error tolerance but may stop early if it\nreaches a prescribed rank limit.\n2:\nQ, B = QBDecomposer(A, k + s, ϵ) # QB ≈A\n3:\nr = min{k, number of columns in Q}\n4:\nU, Σ, V∗= svd(B)\n5:\nU = U[: , : r]\n6:\nV = V[: , : r]\n7:\nΣ = Σ[: r , : r]\n8:\nU = QU\n9:\nreturn U, Σ, V∗\nThe literature recommends setting s to a small positive number (e.g., s = 5\nor s = 10) to account for the fact that the trailing singular vectors of QB may\nnot be good estimates for the corresponding singular vectors of A. However, using\nany positive oversampling parameter complicates the interpretation of the error\ntolerance ϵ. If a user deems this problematic then they can simply set k ←k + s\nand s ←0.\nSuch an approach can be reasonable if tuning parameters for the\nQB algorithm are chosen appropriately. Speciﬁcally, if techniques such as power\niteration are used (see Section 4.3.1) then the trailing singular vectors of QB can\nbe reasonably good approximations to the corresponding singular vectors of A.\nPage 74\narXiv Version 2\n\n\n4.2. Drivers\nLow-rank Approximation\nSacriﬁcing accuracy for speed\nConverting from an ID.\nSetting our sights beyond Algorithm 3, it is noteworthy\nthat if ˆA is given in any compact representation then it can be losslessly converted\ninto an SVD without ever accessing A. For example, given a column ID ˆA = CX,\nwe would compute a QR decomposition C = QR, set B = RX, and then proceed\nwith (Q, B) as in Algorithm 3. As another example, conversion from a row ID to\nan SVD is illustrated implicitly in [HMT11, Algorithm 5.2].\nSuch approaches are potentially useful because one-sided ID can easily be im-\nplemented in a way that accesses A with a single matrix-matrix multiplication and\nthen selects a row or column submatrix. However, this comes at a cost of a much\nless accurate solution compared to typical QB methods.\nSingle-pass algorithms.\nFor very large problems the main measure of an algorithm’s\ncomplexity is the number of times it moves A through fast memory. Besides the\nabove ID-based method, there are three algorithms for low-rank SVD which move\nA through fast memory only once. Each of them uses multi-sketching in the sense\nof Section 2.6. The ﬁrst and second options simply use Algorithm 3, but speciﬁcally\nwith single-pass QB methods based on type 1 or type 2 multi-sketching. Discussion\nof such QB algorithms is deferred to Section 4.3.2. The third option is the algorithm\ndescribed in [TYU+17b, §7.3.2], which relies on type 3 multi-sketching.\nRemark 4.2.1. Algorithms designed to minimize the number of views of a matrix\nare usually analyzed in the pass eﬃcient model for algorithm complexity [DKM06a].\nEarly work on randomized pass-eﬃcient and single-pass algorithms can be found in\n[FKV04; DKM06a; DKM06b].\nError estimation in sketched one-sided SVD.\nSingle-pass algorithms are unlikely to\nproduce highly accurate approximations of singular vectors or singular values. How-\never, their results may be accurate enough to be useful in certain applications. This\nmotivates methods for estimating the errors of approximations returned by these\nalgorithms. Appendix E.3 provides a bootstrap-based error estimator for a simple\nrandomized algorithm that recovers approximate singular vectors from one side of\nthe matrix.\n4.2.2\nMethods for Hermitian eigendecomposition\nEach randomized algorithm for low-rank SVD has a corresponding version that is\nspecialized to Hermitian matrices. We recount those specialized algorithms here,\nand we mention an additional algorithm that is unique to the approximation of psd\nmatrices. In general, we shall say that A is n × n and that the algorithms represent\nˆA = V diag(λ)V∗, where V is a tall column-orthonormal matrix and λ is a vector\nwith entries sorted in decreasing order of absolute value.\nA ﬂexible method for Hermitian indeﬁnite matrices\nAlgorithm 4 is a variation of [HMT11, Algorithm 5.3].\nIts parameters (k, ϵ, s)\nhave essentially the same interpretations as Algorithm 3. When s = 0, its output\nis simply is a compact eigendecomposition of QCQ∗, where C = Q∗AQ and Q is\nobtained from a black-box QBDecomposer. The main diﬀerence between this method\nand Algorithm 3 is that ϵ is scaled down by a factor 1/2 before being passed to\narXiv Version 2\nPage 75\n\n\nLow-rank Approximation\n4.2. Drivers\nQBDecomposer. This change is needed to so that if s = 0 and ∥QB −A∥≤ϵ then\nthe ﬁnal approximation satisﬁes ∥ˆA −A∥≤ϵ; see [HMT11, §5.3].\nAlgorithm 4 EVD1 : QB-backed low-rank eigendecomposition; see [HMT11]\n1: function EVD1(A, k, ϵ, s)\nInputs:\nA is an n × n Hermitian matrix. The returned approximation will\nhave rank at most k. The approximation produced by the randomized\nphase of the algorithm will attempt to A to within ϵ error, but will\nnot produce an approximation of rank greater than k + s.\nOutput:\nApproximations of the dominant eigenvectors and eigenvalues of A.\nAbstract subroutines:\nQBDecomposer generates a QB decomposition of a given matrix; it\ntries to reach a prescribed error tolerance but may stop early if it\nreaches a prescribed rank limit.\n2:\nQ, B = QBDecomposer(A, k + s, ϵ/2)\n3:\nC = BQ # since B = Q∗A, we have C = Q∗AQ\n4:\nU, λ = eigh(C) # full Hermitian eigendecomposition\n5:\nr = min{k, number of entries in λ}\n6:\nP = argsort(|λ|)[: r]\n7:\nU = U[: , P]\n8:\nλ = λ[P]\n9:\nV = QU\n10:\nreturn V, λ\nSacriﬁcing accuracy for speed with Hermitian indeﬁnite matrices\nConverting from an ID.\n[HMT11, Algorithm 5.4] is a second approach to Hermi-\ntian eigendecomposition, based on postprocessing a low-rank row ID of A. We do\nnot include pseudocode for this algorithm in this monograph. However, the ba-\nsic observation underlying the approach is that one can use the symmetry of A to\ncanonically approximate an initial row ID ˜A = ZA[I, :] ≈A by the Hermitian ma-\ntrix ˆˆA = ZA[I, I]Z∗. The compact representation of this Hermitian matrix makes\nit easy to compute its eigendecomposition by a lossless process.\nWhen should one use [HMT11, Algorithm 5.4] over Algorithm 4? Our answer\nis the same as for using [HMT11, Algorithm 5.2] over Algorithm 3. That is, the\nID approach is only of interest when it moves A through fast memory once, and it\nshould be considered alongside other low-rank eigendecomposition algorithms with\nsimilar data movement patterns.\nSingle-pass algorithms.\nJust as with fast algorithms for low-rank SVD, one can\nobtain fast algorithms for low-rank Hermitian eigendecomposition by using Algo-\nrithm 4 with the QB methods based on type 1 or type 2 multi-sketching.\nBesides those approaches, we make note of [HMT11, Algorithm 5.6], which ac-\ncesses A exclusively through a single sketch Y = AS and makes no assumptions\nPage 76\narXiv Version 2\n\n\n4.2. Drivers\nLow-rank Approximation\non the representation of A in-memory. This access pattern is possible because the\nalgorithm solves a least squares problem involving Y, orth(Y), and S to project a\nsmall k × k “core matrix” onto the set of Hermitian matrices.\nNystr¨om approximations for positive semideﬁnite matrices\nNow we suppose that the n × n matrix A is psd, in which case we can deﬁne the\nNystr¨om approximation of A with respect to a matrix X as\nˆA = (AX) (X∗AX)† (AX)∗.\n(4.20)\nWhen framed this way, the Nystr¨om approximation is deﬁned for any matrix X\nwith k ≤n columns. Indeed, it does not even presume that X is random. However,\nin RandNLA, we ultimately set X to a sketching operator and produce a compact\nspectral decomposition ˆA = V diag(λ)V∗. For any given type of sketching operator,\nlow-rank approximation of psd matrices by Nystr¨om approximations tend to be\nmore accurate than approximations produced by comparable algorithms for general\nHermitian eigendecomposition.\nWhat’s in a name?\nDisambiguating “Nystr¨om approximation.”\nThe literature on\nrandomized algorithms for Nystr¨om approximation heavily emphasizes using col-\numn selection operators [WS00; DM05; Pla05; KMT09a; KMT09b; LKL10; Bac13;\nGM16; RCR15; DKM20]. This stems from an analogy between sampling columns\nof kernel matrices in machine learning and the Nystr¨om method from integral equa-\ntion theory. See [DM05, §5] for a detailed discussion. In view of the prominence\nof column sampling in Nystr¨om approximations, part of Section 6 is dedicated to\nspecialized methods for sampling columns from psd matrices.\nWhen X is a general sketching operator, the approximation (4.20) has been called\na “projection-based SPSD approximation” [GM16]. The diﬀerent terminology for\ngeneral sketching operators X is helpful for distinguishing the resulting approxima-\ntions from those referred to as “Nystr¨om approximations” in the machine learning\nliterature. However, it is not in line with our philosophy that RandNLA concepts\nshould be described with minimal assumptions on the nature of the sketching dis-\ntribution. This philosophy, ﬁrst advocated for by Drineas and Mahoney [DMM+11;\nMD16], leads us to adopt the following convention.\nThe term “Nystr¨om approximation” shall be used for any approximation\nof the form (4.20), even when X is a general sketching operator.\nWe note that this also follows the convention used by El Alaoui and Mahoney in\ntheir work on kernel ridge regression (see [AM15, Theorem 1]).\nAlgorithms.\nAlgorithm 5 is a practical approach to low-rank eigendecomposition\nby Nystr¨om approximation. It includes a function call TallSketchOpGen(A, k + s)\nwhich returns a sketching operator S with n rows and k + s columns. Our reason\nfor specifying a sketching operator in this way is to provide ﬂexibility in whether\nthe sketching operator is data-oblivious or data-aware. In this context, the main\ntype of data-aware sketching operator would be based on so-called power iteration;\nsee Section 4.3.1 and [GM16].\narXiv Version 2\nPage 77\n\n\nLow-rank Approximation\n4.2. Drivers\nAlgorithm 5 EVD2 : for psd matrices only; adapts [TYU+17a, Algorithm 3]\n1: function EVD2(A, k, s)\nInputs:\nA is an n×n psd matrix. The returned approximation will have rank\nat most k, but the sketching operator used in the algorithm can have\nrank as high as k + s.\nOutput:\nApproximations of the dominant eigenvectors and eigenvalues of A.\nAbstract subroutines:\nTallSketchOpGen generates a sketching operator with a prescribed\nnumber of columns, for use in sketching a given matrix from the\nright.\n2:\nS = TallSketchOpGen(A, k + s)\n3:\nY = AS\n4:\nν = √n · ϵmach · ∥Y∥# ϵmach is machine epsilon for current numeric type\n5:\nY = Y + νS\n# regularize for numerical stability\n6:\nR = chol(S∗Y) # R is upper-triangular and R∗R = S∗Y = S∗(A + νI)S\n7:\nB = Y(R∗)−1\n# B has n rows and k + s columns\n8:\nV, Σ, W∗= svd(B) # can discard W\n9:\nλ = diag(Σ2)\n# extract the diagonal\n10:\nr = min{k, number of entries in λ that are greater than ν}\n11:\nλ = λ[:r] −ν\n# undo regularization\n12:\nV = V[:, :r].\n13:\nreturn V, λ\nThe role of the parameter s in Algorithm 5 is analogous to that in Algorithms 3\nand 4 – the algorithm eﬀectively computes data needed for a rank k + s eigen-\ndecomposition before truncating that approximation to rank k. However, unlike\nAlgorithms 3 and 4, Algorithm 5 has no control of approximation error. We re-\nfer the reader to [FTU21, Algorithm E.2] for a more sophisticated version of this\nalgorithm which can accept an error tolerance.\n4.2.3\nMethods for CUR and two-sided ID\nHere we describe two approaches to CUR and one approach to two-sided ID. The\ndescriptions are largely qualitative in that they are stated in terms of algorithms\nfor low-rank column ID and CSS (which are detailed in Section 4.3.4).\nCUR by falling back on CSS\nPerhaps the simplest approach to CUR computes the row and column indices (I, J)\nin one stage and then computes the linking matrix U in a second stage. The column\nindices J are obtained by a randomized algorithm for CSS on A, then the row indices\nI are obtained by some CSS algorithm on C∗= A[:, J]∗.1 Because the matrix C is\n1Of course, this process could be reversed to compute I and then J.\nPage 78\narXiv Version 2\n\n\n4.2. Drivers\nLow-rank Approximation\nso much smaller than A, it is often practical to use a deterministic algorithm when\nperforming CSS on C∗.\nThere are two canonical choices for the linking matrix in this context: one\nobtained by projection\nUproj = (A[:, J])† A (A[I, :])†\nand one obtained by submatrix inversion\nUsub = A[I, J]†.\nIt should be clear that the approximation error incurred by using the former matrix\nwill never be larger than when using the latter. Furthermore, the process of com-\nputing the former matrix is better conditioned than the process of computing the\nlatter. Therefore it is generally preferable to use Uproj as the linking matrix when\nimplementing CUR via randomized CSS.\nRemark 4.2.2. Randomized algorithms for CUR based on the pattern above were\nﬁrst proposed in [DMM08; BMD09], particularly with linking matrices closer to\nthe form Usub. Deterministic analyses of CUR approximation quality with various\nlinking matrices can be found in [GZT95; GTZ97].\nCUR by a combination of column ID and CSS\nSuppose we have access to data (X, J) from a column ID of an initial low-rank\napproximation of A. Given this data, we can recover the row index set I and U for\na CUR decomposition by running CSS on C∗= A[:, J]∗and setting U = X (A[I, :])†.\nThis approach only requires the application of one pseudo-inverse, which com-\npares favorably to the two applications of pseudo-inverses needed to compute Uproj\nin the ﬁrst approach to CUR. At the same time, if the randomized algorithm for\ncomputing (X, J) happens to return an interpolation matrix satisfying X = C†A,\nthen the resulting decomposition could have been obtained by the elementary CUR\nalgorithm with linking matrix Uproj. Therefore there is a sense in which this tem-\nplate algorithm generalizes the elementary approach to CUR.\nWe instantiate this template algorithm in Algorithm 6. The CSS step of the\nalgorithm calls a deterministic function for computing a QR decomposition with\ncolumn pivoting, with the semantics indicated in Table 1.1. Whether the algorithm\nstarts with a row ID or column ID depends on the aspect ratio of the data matrix;\n[MT20, §13.3] recommend starting with a row ID when A is wide in the related\ncontext of computing two-sided IDs.\nTwo-sided ID via one-sided ID\nTwo-sided IDs are canonically computed by a simple reduction to one-sided ID: ﬁrst\nobtain (X, J) by a column ID of A and then obtain (I, Z) by a row ID of A[:, J].\nThe initial column ID of A will be computed by a randomized algorithm and hence\nwill always be low-rank. However, it is not expensive to compute a full-rank row ID\nA[:, J] = ZA[I, J] by a deterministic method under the standard assumption that\n|J| ≪min{m, n}. Such an approach is described in [VM16, §2.4, §4].\nFinally, we note that a two-sided ID can be naturally repurposed for CUR\ndecomposition by either of the two qualitative approaches to CUR described above.\nIn the ﬁrst case one only needs the index sets (I, J) and computes the linking matrix\nby any desired method. In the second case one needs the index sets and one of the\narXiv Version 2\nPage 79\n\n\nLow-rank Approximation\n4.3. Computational routines\nAlgorithm 6 CURD1 : CUR by randomizing an initial ID [VM16; DM21b]\n1: function CURD1(A, k, s)\nInputs:\nA is an m × n matrix. The returned approximation will have rank at\nmost k. The ColumnID abstract subroutine can use sketching opera-\ntors of rank up to k + s in its internal calculations.\nOutput:\nA low-rank CUR decomposition of A.\nAbstract subroutines:\nColumnID produces a low-rank column ID of a given matrix, up to\nsome speciﬁed rank.\n2:\nif m ≥n then\n3:\nX, J = ColumnID(A, k, s) # |J| = k and A[:, J]X ≈A\n4:\nQ, T, I = qrcp(A[:, J]∗)\n# only care about the indices I\n5:\nI = I[: k]\n6:\nU = X (A[I, :])†\n7:\nelse\n8:\nZ∗, I = ColumnID(A∗, k, s) # |I| = k and ZA[I, :] ≈A.\n9:\nQ, T, J = qrcp(A[I, :])\n# only care about the indices J\n10:\nJ = J[: k]\n11:\nU = (A[:, J])† Z\n12:\nreturn J, U, I\ninterpolation matrices (i.e., one of Z or X).\nThe latter approach for converting\nfrom two-sided ID to CUR is used in [VM16, §3 and §4]. General discussion on\nconverting from two-sided ID to CUR can be found in [Mar18, §11.2], [MT20,\n§13.2], and [DM21b, §4.1].\n4.3\nComputational routines\nThe last section explained how randomized algorithms for low-rank approximation\nexhibit a great deal of modularity. Here we summarize the design spaces for the\nconstituent modules.\nSections 4.3.1 to 4.3.4 cumulatively cover QB, column ID, CSS, and building\nblocks for the same. We acknowledge up-front that we treat column ID and CSS\nin far detail than we do QB. This imbalance is not a statement about the impor-\ntance of column ID or CSS over QB. Rather, it stems from our desire to clarify\nbroader concepts surrounding column ID that can be diﬃcult to tease out from\nother literature.\nFrom there, Section 4.3.5 lists methods for norm estimation which are important\nfor solving low-rank approximation problems to ﬁxed accuracy. Our last topic in the\nrealm of computational routines for low-rank approximation is the notion of low-\nrank approximations from oblique projections (§4.3.6). This framework motivates\na type of low-rank approximation that is cheap to compute but that does not have\nmeaningfully-structured factors.\nPage 80\narXiv Version 2\n\n\n4.3. Computational routines\nLow-rank Approximation\nWe emphasize that this section mentions many algorithms for a wide variety\nof problems. Due to practical constraints we only address a handful of these al-\ngorithms in detail. Pseudocode for select algorithms can be found in Appendix C;\nthese algorithms have been selected based on some combination of their conceptual\nsigniﬁcance and their practicality.\n4.3.1\nPower iteration\nGiven a matrix A, suppose we sketch Y = AS using a very tall sketching operator S.\nIn a low-rank approximation context – regardless of whether we work with spectral\nor submatrix-oriented decompositions – it is generally preferable for range(Y) to be\nwell-aligned with the span of A’s dominant left singular vectors. This, in turn, is\nfacilitated by having range(S) be well-aligned with the span of A’s dominant right\nsingular vectors. To accomplish this, RandNLA libraries should include methods\nfor generating such sketching operators based on power iteration.\nA basic approach to power iteration makes alternating applications of A and A∗\nto an initial data-oblivious sketching operator So, to obtain a data-aware sketching\noperators such as\nS = (A∗A)q So\nor\nS = (A∗A)q A∗So,\n(4.21)\nfor some parameter q ≥0. Practical implementations need to incorporate some\nform of stabilization in between the successive applications of A and A∗).\nWe\ngive a general formulation of such a method in Appendix C.2.1 with Algorithm 8.\nNotably, this general method allows an arbitrary number of passes over the data\nmatrix (including zero passes, as an API convenience).\nRemark 4.3.1. The closest relative to Algorithm 8 in the literature is probably\n[ZM20, Algorithm 3.3]. However, the core idea behind this algorithm was explored\nearlier by Bjarkason [Bja19].\n4.3.2\nOrthogonal projections: QB and rangeﬁnders\nWe begin with two deﬁnitions.\nGiven a matrix A, a QB decomposition is given by a pair of matrices\n(Q, B) where Q is column-orthonormal and B = Q∗A. It is intended\nthat QB serve as an approximation of A. An algorithm that computes\nonly the factor Q from a QB decomposition is called a rangeﬁnder.\nThe value of QB decompositions stems from how they deﬁne approximations\nby orthogonal projection: ˆA = QB = QQ∗A. It is important to note that QB\nalgorithms do not necessarily ﬁrst compute Q in one phase and then set B = Q∗A\nin a second phase.\nIndeed, the beneﬁt of the rangeﬁnder abstraction is that it\nconsiders an equivalent problem while setting aside the potentially-nuanced matter\nof computing B.\nBefore proceeding to algorithms, we note that these concepts are not useful in\nthe “full-rank” setting. Consider, for example, when A has full row-rank. Here, any\northogonal matrix Q and accompanying B = Q∗A are valid outputs of rangeﬁnders\nand QB decomposition algorithms. Therefore full-rank QB decompositions can be\nentirely unstructured. Despite this caveat, QB decompositions are of fundamental\nimportance in randomized algorithms for low-rank approximation.\narXiv Version 2\nPage 81\n\n\nLow-rank Approximation\n4.3. Computational routines\nRangeﬁnder algorithms and basic QB\nThe simplest rangeﬁnders are based on power iteration.\nFor example, one can\nprepare a data-aware sketching operator S of the form (4.21), compute Y = AS,\nand then return Q = orth(Y); this is formalized as Algorithm 9 in Appendix C.2.2.\nMore advanced rangeﬁnders use block Krylov subspace methods. Speciﬁc examples\nof such rangeﬁnders can be found in [MM15], [Bja19, §7], and [MT20, §11.7].\nAlgorithm 10 in Appendix C.2.2 is the simplest approach to QB. It obtains Q\nby calling an abstract rangeﬁnder and obtains B by explicitly computing B = Q∗A.\nIterative QB algorithms\nThe most eﬀective QB algorithms work by building (Q, B) iteratively [MV16].\nGenerically, each iteration of such a QB method entails some number of matrix-\nmatrix multiplications with A (as a rangeﬁnder step), adds a speciﬁed number of\ncolumns to Q and rows to B, and makes a suitable in-place update to A. Iterations\nterminate once some metric of approximation error A ≈QB (e.g., ∥A−QB∥2 or an\napproximation thereof) falls below a certain level.\nIterative QB methods were improved by [YGL18]. In particular, Algorithm 2 of\n[YGL18] does not modify A, it uses a power-iteration-based rangeﬁnder to compute\nnew blocks for (Q, B), and it eﬃciently updates the Frobenius error ∥A −QB∥F\nas the iterations proceed. This method is useful because it has complete control\nover the Frobenius norm error of the returned approximation.\nAlgorithm 11 in\nAppendix C.2.2 generalizes this method by allowing an abstract rangeﬁnder in the\niterative loop that updates (Q, B).\nMeanwhile, Algorithm 4 of [YGL18] performs power iteration before entering\nits main iterative loop, it does not access A while in the iterative loop, and it can\nterminate early if a target accuracy is met before a pre-speciﬁed rank-limit. This\nalgorithm has the advantage of reducing the number of times A is moved through\nfast memory. In fact, when power iteration is omitted, it can be implemented as\na single-pass method based on Type 1 multi-sketching. The downside is that this\nalgorithm may waste a substantial amount of work if the rank limit is much higher\nthan necessary. This downside is compounded when power iteration is omitted. We\nreproduce this method with slight modiﬁcations in Appendix C.2.2 as Algorithm 12.\nStopping criteria for iterative QB algorithms\nThe Frobenius norm is easily computed for sparse matrices and dense matrices that\nare stored explicitly in memory. However, it can be diﬃcult to compute for abstract\nlinear operators when the matrix is accessed only via matrix-vector multiplies, and\nthis can pose problems in computing QB decompositions to speciﬁed accuracy. One\napproach to address this situation is by careful application of a well-known random-\nized Frobenius norm estimator as part of the QB decomposition [GCG+19, §3.4,\n§3.5, Eq. (3.26)]. We also note that those looking for high-quality approximations\noften prefer that error be bounded in spectral norm (and only use the Frobenius\nnorm because it is usually very cheap to compute). The problem of estimating spec-\ntral norms is well-studied in the NLA literature. Section 4.3.5 reviews randomized\nalgorithms for estimating matrix norms.\nPage 82\narXiv Version 2\n\n\n4.3. Computational routines\nLow-rank Approximation\nApproximate single-pass QB via Type 2 multi-sketching\nRecall that a Type 2 multi-sketch of A is a sketch of the form (Y1, Y2) = (AS1, S2A)\nfor independent sketching operators (S1, S2). This sketch can be used to compute a\nQB decomposition that is approximate, in the sense that QB ≈A holds for column-\northonormal Q, but we drop the hard requirement that B = Q∗A.\nPut simply, the method is to compute Q = orth(Y1) and then B = (S2Q)†Y2.\nThe intuition behind this approach is that if QQ∗A is a good approximation for A,\nthen we would have Y2 ≈S2QQ∗A, which would imply B ≈Q∗A. We refer the\nreader to [TYU+17b, §4.2] for a proper explanation of this method.\n4.3.3\nColumn-pivoted matrix decompositions\nThroughout this section we work with an r × c matrix G. As before, we begin with\nsome deﬁnitions.\nA column-pivoted decomposition of G is any decomposition of the form\nGP = FT\n(4.22)\nwhere P is a permutation matrix and T is upper-triangular. The permu-\ntation matrix is encoded by a vector of pivots, J, so that G[:, J] = GP.\nThere are many ways of producing such decompositions. We are only interested in\nthe ways where rank-k matrices obtained by truncation\nˆG := (F[:, :k])(T[:k, :])P∗\n(4.23)\nprovide reasonably good rank-k approximations of G. The meaning of “reasonably\ngood” is subjective. It depends on the computational cost of the algorithm, and\nit depends on how well ˆG approximates G compared to the best approximation\nobtained by a truncated column-pivoted matrix decomposition. Interestingly, some\nrandomized algorithms for CSS (see §4.3.4) do not use the factors that appear in\n(4.23); instead, these algorithms only care that the pivots J could make approxi-\nmations form (4.23) reasonably accurate.\nHow much do we truncate?\nWhen a randomized algorithm uses this primitive for\nlow-rank approximation, the matrix G is usually a sketch of the target data matrix\nA, and k is close to min{r, c}. It helps to consider diﬀerent situations when trying\nto build intuition for why these randomized algorithms work. Speciﬁcally, it helps\nto consider when G is equal to A, or a low-rank approximation thereof. In both\nsuch cases one should have k ≪min{r, c}.\nBasics of column-pivoted decomposition algorithms\nThere are two main families of algorithms for producing these decompositions: those\nbased on QR with column pivoting (QRCP) and those based on LU with partial\npivoting (LUPP).2 Algorithms in the former family deﬁne the decomposition (4.22)\nin the natural way, where F is column-orthonormal. For algorithms in the latter\nfamily, one must consider how pivoted LU traditionally uses row pivoting. Therefore,\n2The standard process of computing an LU decomposition with partial pivoting is called Gaus-\nsian elimination with partial pivoting and is abbreviated as GEPP.\narXiv Version 2\nPage 83\n\n\nLow-rank Approximation\n4.3. Computational routines\nto compute (4.22) via LUPP, one must compute the transposed factors (P∗, F∗, T∗)\nin a row-pivoted decomposition,\nP∗G∗= T∗F∗,\nwhere T∗is lower-triangular (with unit diagonal) and F∗is upper-triangular.\nRoughly speaking, QRCP-based algorithms prioritize accuracy, while LUPP-\nbased algorithms prioritize speed. The extent to which a speciﬁc algorithm does\nwell on these performance metrics depends on the algorithm’s pivoting rule. The\nmost widely used QRCP-based methods use the same pivoting rule as LAPACK’s\nGEQP3. Meanwhile, the most widely used LUPP-based methods use the pivoting\nrule from LAPACK’s GETRF. It is valid to rely on either of these functions for the\ncolumn-pivoted decomposition steps that arise in randomized algorithms. However,\none should be aware of two potential sources of error when using GETRF for a column-\npivoted decomposition rather than GEQP3.\nWhat can we expect of GETRF’s pivots?\nWhen GETRF is applied to G∗, the process of\ncomputing the pivots up to and including the ℓth pivot makes decisions based only\non the ﬁrst ℓrows of G. Therefore it is unwise to use GETRF unless one has reason\nto believe that the information in G’s trailing r −k rows would not drastically alter\nthe columns chosen as pivots based on the ﬁrst k rows. Similarly, it is unwise to use\nˆG as an approximation of G when k ≪min{r, c}, since this would suppose that G’s\nleading k rows are signiﬁcantly more important than all others. One is most likely\nto ﬁnd meaningful information in the column-pivoted LU decomposition when G is\nvery wide (r ≪c) and k is close to r.\nWhat isn’t in the pivots?\nSuppose F has w = min{r, c} columns. It is easy to verify\nthat for any nonsingular upper-triangular matrix U of order w, the decomposition\nproduced after a change-of-basis\n(F, T) ←(FU−1, UT)\nwill preserve (4.22) for the same permutation matrix P.\nIt is informative to consider how such changes of basis aﬀect ˆG. For example,\nin simplest case, it is easy to see that ˆG would not change if U were diagonal.\nThis simple case shows that conditioning of the factors (F, T) is unimportant in our\nformalism of column-pivoted decomposition.\nTo speak to a more interesting case, let us partition F and T into blocks [F1, F2]\nand [T1; T2] so that F1 has k columns and T1 has k rows. Straightforward calcula-\ntions show that\n∥G −F1T1P∗∥= ∥F2T2∥\n(4.24)\nholds in any unitarily-invariant norm. Meanwhile, less straightforward calculations3\nshow that there is always an upper-triangular nonsingular matrix U for which\n∥G −(FU−1)[:, :k](UT[:k, :])∥= ∥(I −F1F†\n1)F2T2∥≤∥F2T2∥.\n(4.25)\nNote that if there is substantial overlap between range(F1) and range(F2), then the\ninequality in (4.25) will be strict by a signiﬁcant margin. Therefore if accuracy of\nthe approximation (4.23) is important, then our decomposition should make sure\nthat the columns of F are orthogonal to one another. This is ensured by QR-based\nmethods, but it is not ensured by LU-based methods.\n3See Proposition C.1.3 in Appendix C.1.2.\nPage 84\narXiv Version 2\n\n\n4.3. Computational routines\nLow-rank Approximation\nPartial decompositions\nStandard algorithms for computing a column-pivoted decomposition of an r × c\nmatrix require Θ(min{rc2, cr2}) operations. One can get away with spending less\neﬀort when only a partial decomposition is needed. Formally, in a (k-step) partial\ncolumn-pivoted decomposition, we relax the requirement that T be triangular. In-\nstead, we require that T can be partitioned into a 2-by-2 block triangular matrix\nwhere the upper-left block is k × k and triangular in the proper sense.\nThe aforementioned standard algorithms for column-pivoted decomposition can\nbe modiﬁed to compute k-step partial decompositions of r × c matrices in Θ(rck)\noperations. There are plans for a version of LAPACK subsequent to 3.10 to support\nthis functionality as it pertains to QRCP.4 At a practical level, it is certainly worth\nusing this functionality when it is available.\nHowever, this functionality is not\ncritical, since randomized algorithms for low-rank approximation rarely need to\ncompute k-step partial decompositions with k ≪min{r, c}.\nMore details on column-pivoted decomposition algorithms\nThe pivots chosen by the strong rank-revealing QR (strong RRQR) algorithm from\n[GE96] lead to the best theoretical guarantees for low-rank approximation by a par-\ntial column-pivoted QR decomposition. In practice, it is more common to truncate\nthe output of LAPACK’s GEQP3, which is faster than strong RRQR.\nAlgorithms based on LUPP are typically faster than those based on pivoted\nQR. While the LUPP approach comes with weaker guarantees (as explained above),\nthese limitations are less signiﬁcant in a randomized context where we seek nearly\nfull-rank decompositions of wide sketches. Indeed, there is little practical diﬀer-\nence in solution quality between LUPP-based and QRCP-based versions of some\nrandomized algorithms for CSS and column ID [Mar22b; DM21b].\nOther possibilities for column-pivoted matrix decomposition include LU or QR\nwith tournament pivoting [GDX11; DGG+15]. Algorithms based on tournament\npivoting exhibit reduced communication and hence can be more eﬃcient without\nsigniﬁcant loss of accuracy.\nFinally, we note that Section 5.1.2 includes a randomized algorithm for full-rank\nQRCP. It is easy enough to modify that algorithm to support early termination.\nSome variants of this algorithm speciﬁcally focus on low-rank approximation (e.g.,\nthe SRQR algorithm from [XGL17]).\n4.3.4\nOne-sided ID and CSS\nColumn ID and CSS are nearly equivalent problems. That is, a method for CSS\ncan canonically be extended to a method for column ID by taking X = (A[:, J])†A.\nConversely, a method for column ID can be adapted to CSS by discarding any\ncalculations that are only needed to form X. This section covers deterministic and\nrandomized algorithms for both of these problems. Readers who are particularly\ninterested in theoretical aspects of these algorithms should consult [BMD09].5\n4See https://github.com/Reference-LAPACK/lapack/issues/661.\n5We frame all of our discussion of one-sided ID around column ID, rather than row ID.\narXiv Version 2\nPage 85\n\n\nLow-rank Approximation\n4.3. Computational routines\nTemplate deterministic algorithms\nSuppose we want to compute a rank-k column ID of an r × c matrix G. There is a\ntemplate deterministic algorithm for handling this problem based on the notion of\ncolumn-pivoted decompositions, as discussed in Section 4.3.3.\nThe template algorithm works in two phases.\nThe ﬁrst phase produces the\ndecomposition GP = FT where P is a permutation matrix and T is upper-triangular.\nThe second phase is a matter of simple postprocessing. The postprocessing begins\nby partitioning T into a 2-by-2 block triangular matrix\nT =\n\u0014\nT11\nT12\n0\nT22\n\u0015\n.\nwhere T11 is k × k and triangular in the usual sense.\nFrom here, one sets the\ninterpolation matrix to X = [Ik×k, T−1\n11 T12]P∗, and one sets the skeleton indices to\nthe vector J that provides X[:, J] = Ik.\nThe importance of this algorithm stems from how its output can equivalently be\nanalyzed as a truncated column-pivoted matrix decomposition. That is, the column\nID induced by (J, X) satisﬁes\nG[:, J]X = F[:, :k]T[:k, :]P∗.\nWe can therefore gain insights into the behavior of this column ID algorithm by\nappealing to results such as Proposition C.1.3, which we alluded to earlier in our\ndiscussion of LU and QR based pivoting methods.\nThis approach to column ID is illustrated more formally as Algorithm 13 in\nAppendix C.2.3. It is easy to see that the CSS version of this algorithm does not\nneed to compute X, since the deﬁnition of X implies that J can be determined from\nthe ﬁrst k rows of P.\nRandomized algorithms\nWe list ﬁve randomized algorithms for CSS and column ID below.\nWith some\nexception for the ﬁfth algorithm, we do not comment on the theoretical guarantees\nof these methods.\n1. For CSS, one can sample columns with probability proportional to their\nnorms, where column norms are updated by projecting out selected columns\nas a QRCP-like factorization proceeds [DV06]. Applying the standard post-\nprocessing scheme to the (partial) factorization yields the interpolation matrix\nX needed for a column ID.\n2. Also for CSS, one can sample columns according to a probability distribution\nrelated to so-called leverage scores of the matrix under consideration.\nWe\ndiscuss leverage score sampling in detail in Section 6. For now, we note that\nthis approach especially useful for computing Nystr¨om approximations.\n3. The algorithm in [BMD09] approaches CSS with a combination of leverage\nscore sampling and postprocessing by deterministic QRCP. The factorization\nproduced by this postprocessing can be processed further to produce the in-\nterpolation matrix for a column ID.\nPage 86\narXiv Version 2\n\n\n4.3. Computational routines\nLow-rank Approximation\n4. [XGL17, §V.D] suggests solving CSS by taking the pivots from a randomized\nalgorithm for QRCP. The output of the randomized algorithm for QRCP can,\nof course, be processed to recover the interpolation matrix for a column ID.\n5. [VM16, §5.1] approaches low-rank column ID by computing a (nearly) full-\nrank column ID of a sketch Y = SA. The unmodiﬁed data (X, J) is used to\ndeﬁne the low-rank column ID A[:, J]X ≈A.\nThe last of these methods is simple and practical. It appears with slight modi-\nﬁcations in Appendix C.2.3 as Algorithm 14, while the corresponding CSS version\nappears as Algorithm 15. For both the column ID and CSS versions, it is recom-\nmended that S be a data-aware sketching operator based on power iteration. To\ngain intuition for this method, one should ﬁrst verify that if (X, J) deﬁnes a full-\nrank column ID of Y, then it also deﬁnes a full-rank column ID of ˜A = (AY†)Y.\nWith that given, we can apply Proposition 4.1.3 to see that the induced low-rank\ncolumn ID satisﬁes an error bound\n∥A −A[:, J]X∥2 ≤(1 + ∥X∥2)∥A −˜A∥2.\nThis bound is noteworthy for the following reason: using power iteration to prepare\na data-aware sketching operator S will drive ˜A closer to a rank-k approximation of\nA obtained by a truncated SVD. That, in turn, would give ∥A −˜A∥2 ≈σk+1(A).\nRemark 4.3.2. The value of power iteration in the context of CSS / column ID and\nin the context of rangeﬁnders / QB is a key reason for considering power iteration\nas a basic primitive of RandNLA.\nOn ﬁxed-accuracy one-sided ID\nStandard implementations of deterministic QRCP-based algorithms for one-sided\nID can compute approximations to speciﬁed accuracy. Randomized algorithms for\nlow-rank one-sided ID do not possess this capability to the same extent. In some\nrespects, this is a principal disadvantage of low-rank approximation by ID compared\nto QB. However, there are partial workarounds.\nFor example, suppose we approximate A via a QB decomposition, ˜A = QB. If\nwe computed (X, J) by a full-rank column ID of B, then we would also have a full-\nrank column ID of ˜A. If X was obtained by the standard postprocessing of output\nfrom strong rank-revealing QR, then we would have |Xij| ≤2. A straightforward\napplication of Proposition 4.1.3 shows that if we had\n∥A −QB∥2 ≤\nϵ\n(1 +\np\n1 + 4k(n −k))\n(4.26)\nfor a rank-k QB decomposition, then we could be certain that ∥A −A[:, J]X∥2\nwas at most ϵ. Therefore, in principle, one could compute the QB decomposition\niteratively, and only compute the ID of B once (4.26) is satisﬁed.\nThe above approach is not without its shortcomings. For one thing, reducing\n∥A−QB∥2 entails increasing k, and so the termination criterion is a moving target.\nAs another issue, it needs to bound spectral norms of implicitly represented linear\noperators. We address the problem of estimating matrix norms next.\narXiv Version 2\nPage 87\n\n\nLow-rank Approximation\n4.3. Computational routines\n4.3.5\nEstimating matrix norms\nNorm estimation plays an important role in stopping criteria for iterative low-rank\napproximation algorithms, particularly for QB and Nystr¨om approximations. Here\nwe summarize methods that would be appropriate for expensive norms or norms of\nabstract linear operators that are only accessible by matrix-vector multiplications.\nRemark 4.3.3. The material presented here is covered in greater detail in [HMT11,\n§4.3 - §4.4] and [MT20, §5 - §6, §12.0 - §12.4].\nA cheap spectral norm bound.\nLet the vectors z1, . . . , zr ∈Rn be vectors with\ncomponents drawn iid from the standard normal distribution and let β > 1 be a\ntuning parameter. Then, for any A, it is known that the inequality\n∥A∥2 ≤β\nr\n2\nπ max\nj∈JrK ∥Azj∥2\n(4.27)\nholds with probability at least 1 −β−r [HMT11; WLR+08].\nFurthermore, this\nbound is easy to compute because the necessary vectors Azj can be formed with a\nsingle matrix-matrix product with A.\nA basic Frobenius norm estimator.\nLet Z ∈Rn×r be the matrix whose columns\nare the random vectors z1, . . . , zr mentioned above. Then, it turns out that the\nquantity 1\nr∥AZ∥2\nF is an unbiased estimate for the squared Frobenius norm, in the\nsense that\nE\nh\n1\nr∥AZ∥2\nF\ni\n= ∥A∥2\nF.\n(4.28)\nIn addition to being unbiased, the variance of the error estimate can also be con-\ntrolled according to\nvar\n\u0010\n1\nr∥AZ∥2\nF\n\u0011\n≤\n2\nr∥A∥2\n2∥A∥2\nF,\n(4.29)\nas shown in [Gir89]. Hence, as long as r is suﬃciently large, then the error estimate\n1\nr∥AZ∥2\nF is likely to be close to ∥A∥2\nF. From a computational standpoint, this error\nestimate is similar to the one described above for the spectral norm, insofar as it\nonly requires r matrix-vector products with A.\nA cheap Schatten p-norm estimator.\nLetting σ denote the vector of singular values\nof A, the Schatten 2p-norm of A is ∥A∥(S,2p) :=\n\u0010Pmin{m,n}\ni=1\nσ2p\ni\n\u00111/2p\n.\nTaking\np = 1 reduces to the Frobenius norm. The spectral norm is obtained in the limit\nas p →∞. In fact, deterministic bounds show that the spectral norm and Schatten\np-norm more or less coincide when p ≳log min{m, n}.\nThe Kong-Valiant estimator [KV17b] can be used to cheaply estimate these\nnorms. It only accesses A by multiplication with an n × k data-oblivious sketching\noperator, where k can be materially smaller than min{m, n}. See [MT20, §5.4] for\na statement of the algorithm and remarks on its theoretical guarantees.\nAccurate spectral norm estimators.\nThere is a large literature on deterministic and\nrandomized algorithms for estimating spectral norms. Much of this literature is\nbased on methods designed for estimating the largest eigenvalue of a positive deﬁnite\nmatrix (which can naively be applied since\np\n∥A∗A∥2 = ∥A∥2). Most notably, Dixon\nPage 88\narXiv Version 2\n\n\n4.4. Other low-rank approximations\nLow-rank Approximation\nwas the ﬁrst to study the randomized power method [Dix83], and Kuczy´nski and\nWo´zniakowski were the ﬁrst to study randomized Lanczos methods [KW92]. See\n[MT20, Algorithm 5] for a basic randomized Lanczos method and the subsequent\nremarks on block randomized Lanczos [MT20, §6.5].\n4.3.6\nOblique projections\nLow-rank approximations can be expressed in a manner resembling the triple-sketch\nfrom Section 2.6. For sketching operators S1 ∈Rn×k and S2 ∈Rd×m, we can deﬁne\nˆA = AS1(S2AS1)†S2A = Y1Y†\n3Y2,\nwhere\nY1 = AS1,\nY2 = S2A,\nand\nY3 = S2AS1.\nThis construction obtains each column of ˆA by projecting the corresponding column\nof A onto the range of Y1, where the projection is orthogonal with respect to\nthe possibly degenerate inner product (u, v) 7→⟨S2u, S2v⟩. We call ˆA an oblique\nprojection of A.\nThe simplest oblique projections use column and row selection operators for\n(S1, S2). This provides a CUR decomposition where Y†\n3 is the linking matrix U.\nThe connection to CUR foreshadows a more general fact: the sketching operators\nused in oblique projection are not necessarily independent of one another [DMM08].\nAn example in this regard is that Nystr¨om approximations amount to oblique pro-\njections that use S2 = S∗\n1.\nIt is natural to consider oblique projections where S1 and S2 are independent\n(e.g., independent Gaussian operators). Such approximations can entail extremely\nill-conditioned computations if one is not careful.\nThis ill-conditioning can be\navoided through the numerically stable approach described by Nakatsukasa [Nak20].\nThese approximations employ oversampling for S2 (relative to S1) and split Y3 (or\na regularized variant thereof) into two factors. The representation returned by this\napproach consists of four matrices.\nHistorical notes\nOblique projections for low-rank approximation are closely related to the rank re-\nduction formula described in [CFG95]. Drineas et al. ﬁrst used oblique projections\nfor low-rank approximation via CUR decomposition [DMM08], wherein S1, S2 are\ncolumn and row selection matrices respectively. Clarkson and Woodruﬀpioneered\nthe use of general oblique projections in randomized algorithms for low-rank ap-\nproximation [CW09, Theorem 4.7]. Oblique projections have since been discussed\nin the context of a generalized LU factorization [DGR19].\n4.4\nOther low-rank approximations\nHere we review a handful of other low-rank approximation problems and algorithms,\nparticularly speaking to our development plans for RandLAPACK.\narXiv Version 2\nPage 89\n\n\nLow-rank Approximation\n4.4. Other low-rank approximations\nDomain-speciﬁc representations.\nSeveral low-rank approximation problems of inter-\nest involve specialized factorizations. We plan for RandLAPACK to eventually sup-\nport nonnegative matrix factorization [EMW+18], dynamic mode decomposition\n(DMD) [EMK+19; EBK19], and possibly sparse PCA [EZM+20]. Among these\nmethods, we expect that DMD will have highest priority, since full-rank DMD is\nslated for inclusion into LAPACK in the near future [Drm22]. For a general intro-\nduction to DMD we refer the reader to [TRL+14].\nLow-rank Cholesky.\nAs a separate topic, there is also a longstanding algorithm\nfor “low-rank Cholesky” decompositions [XG16].\nWe are unsure of its eventual\nrole in RandLAPACK, since a representation of the form ˆA = LL∗for a very tall\nlower-triangular matrix L oﬀers almost no beneﬁt over L being dense.\nStill, it\nwill be considered in the near future alongside the recently proposed algorithm by\n[CET+22] for randomly pivoted partial Cholesky decomposition.\nLow-rank QR.\nSuppose A is a large full column-rank matrix with QR decompo-\nsition A = QR.\nThis decomposition has two especially prominent uses: (1) it\nfacilitates application of a pseudoinverse A†v = R−1Q∗v in O(mn) time, and (2) it\ncan be used as preprocessing for more complicated orthogonal decompositions such\nas SVD. Unfortunately, low-rank QR decomposition, which is simply the economic\nQR decomposition of a rank-k approximation of A, does not fully realize either of\nthese use-cases.\nThe trouble with low-rank QR is that a k × n upper-triangular matrix with\nk ≪n is eﬀectively a full matrix. That is, the mere representation of a low-rank\nmatrix by a QR decomposition is not much more useful than representation by QB\ndecomposition. Note also that unpivoted QR makes no eﬀort to produce a rank-\nrevealing representation, compared to pivoted QR. Therefore RandLAPACK will not\noﬀer methods for low-rank approximation by unpivoted QR.\nLow-rank UTV.\nA UTV decomposition ˆA = UTV∗uses column-orthogonal matri-\nces U, V and a triangular matrix T. UTV (also called QLP) can be thought of as a\ncheaper alternative to SVD. As we discuss in the next section, RandLAPACK might\ninclude algorithms for UTV when ˆA is full-rank [GM18; MQH19; KCL21]. Some of\nthose algorithms (e.g., that in [MQH19]) proceed iteratively and can be terminated\nearly. If RandLAPACK supports full-rank UTV by such an algorithm then it will\nexpose the low-rank variant.\nSeveral algorithms for producing low-rank approximations represented by UTV\nare given in [DG17; FXG19; WX20; RB20; KC21]. We would need a better under-\nstanding of those methods, particularly how they compare to our planned methods\nfor low-rank SVD, before making decisions on which of them to support.\nLow-rank LU.\nLU is central to solving systems of linear equations in the full-rank\ncase.\nThere is a small literature on low-rank LU within the ﬁeld of RandNLA:\n[SSA+18; DGR19; ZM20]. In RandLAPACK we anticipate restricting our attention\nto algorithms that are related to a Gaussian elimination process (that is, where the\nerror matrix can be expressed as a Schur complement of a block matrix), along the\nlines of [DGR19]. These algorithms are likely more useful for low-rank approxima-\ntion with a ﬁxed accuracy requirement rather than with a ﬁxed rank requirement.\nThey are based on an oblique projection with k = d, that is S2AS1 is square.\nPage 90\narXiv Version 2\n\n\n4.5. Existing libraries\nLow-rank Approximation\nRandLAPACK might include the LU algorithms from [SSA+18] if they can\nbe proven to be signiﬁcantly faster than high-quality implementations of QB al-\ngorithms.\nIf proven useful, we will consider in the future generalized LU-based\nlow-rank approximation, as introduced in [DGR19]. The algorithms for low-rank\nLU in [ZM20] are based on QB and so are unlikely to be included in RandLAPACK.\n4.5\nExisting libraries\nHere we review established numerical libraries that support randomized low-rank\napproximation. All of the libraries that focus on RandNLA (save for one) implement\nadvanced sketching operators such as SRFTs.\nNLA and data science packages that use RandNLA.\nThere are a few packages for\nNLA that include a method for low-rank SVD based on QB decomposition:\n• MLSVD RSI in the Tensorlab MATLAB toolbox\n• rsvd in SciKit-CUDA,\n• cusolverDnXgesvdr in NVIDIA’s cuSOLVE,\n• and randomized svd in SciKit-Learn.\nThe last of these functions warrants special emphasis. SciKit-Learn’s pca function\nactually defaults to randomized svd for suﬃciently large matrices [Gri16]. In this\nway, one of the most important functions in the most widely-used Python package\nfor data science already relies on RandNLA.\nID.\nID is a Fortran library for ID/CUR [MRS+14]. It is callable as part of the\nSciPy Python library. ID provides indirect support for SVD as part of its methods\nfor converting one low-rank factorization into another. It also includes routines for\nrank estimation and norm estimation. RandLAPACK will include many of these\nsame utilities as ID while expanding its scope of driver-level functions.\nRSVDPACK.\nRSVDPACK is a C and MATLAB library for low-rank SVD and\nID/CUR [VM15]. It is callable after building from source code which is provided on\nGitHub. Its SVD algorithms are based on a particular QB implementation [VM15,\n§3.4] and its ID/CUR algorithms follow [VM16]. RSVDPACK comes in diﬀerent\nimplementations which target diﬀerent architectures.\nBy comparison, RandLAPACK will take more general approaches to QB and\nID/CUR, and it will include methods for other factorizations such as eigendecompo-\nsition via Nystr¨om approximations. RandLAPACK will target diﬀerent architectures\nby building on LAPACK++ as a portability layer [GLA+17].6\nRistretto.\nRistretto is available on the Python Package Index. This library is based\non the rsvd package implemented in R [EVB+19].\nIt supports low rank SVD,\nID/CUR, LU, Nystr¨om, PCA, Hermitian eigendecomposition, nonnegative matrix\nfactorization [EMW+18], dynamic mode decomposition [EMK+19; EBK19; ED16],\n6This library is developed as part of SLATE [KWG+17; AAB+17].\narXiv Version 2\nPage 91\n\n\nLow-rank Approximation\n4.5. Existing libraries\nand sparse PCA [EZM+20]. One algorithm is provided for each distinct type of\nfactorization. Many of these algorithms are based on QB [EVB+19, §3.3], while its\nID/CUR algorithms also follow [VM16]. This library has also been demonstrated\nto be useful for ﬁnding patterns in large-scale climate data [VEK+19], and for\nproviding routines for randomized tensor decompositions [EMB+20].\nWe plan for RandLAPACK to eventually support the same range of factorizations\nas Ristretto (with the exception of low-rank LU). However, our priority is to focus\non the factorizations in Section 4.2, and to oﬀer a range of algorithms for computing\neach of these decompositions. Our longer-term plans include making RandLAPACK’s\nC++ implementation callable from Python.\nLibSkylark.\nLibSkylark [KAI+15] is written in C++ and callable after installing\nfrom source, which is available on GitHub.\nTo our knowledge, it is the only\nRandNLA library that supports both least squares and low-rank approximation.\nIts low-rank approximation functionality is restricted to SVD through a QB ap-\nproach. See Section 3.5 for its least squares functionality.\nLowRankApprox.jl.\nLowRankApprox.jl is a Julia library for low-rank SVD, QR, ID,\nCUR, and Hermitian eigendecomposition. It is callable after installation with the\nJulia package manager. Most of its algorithms are based on ﬁrst computing an ID,\nrather than a QB decomposition. Note that this is quite diﬀerent from the plans\nwe have outlined for RandLAPACK over Sections 4.2 and 4.3.\nOther implementations.\nThe many algorithms considered in [Bja19] are accom-\npanied by Python implementations hosted on GitHub.\nThe RandNLA tutorial\n[Wan15] covers a wide range of algorithms for low-rank approximation and hosts\nsome MATLAB implementations on GitHub.\nPage 92\narXiv Version 2\n\n\nSection 5\nFurther Possibilities for Drivers\n5.1 Multi-purpose matrix decompositions .........................\n94\n5.1.1 QR decomposition of tall matrices ...............................\n94\n5.1.2 QR decomposition with column pivoting ......................\n95\n5.1.3 UTV, URV, and QLP decompositions ..........................\n98\n5.2 Solving unstructured linear systems ........................... 100\n5.2.1 Direct methods ......................................................... 100\n5.2.2 Iterative methods ...................................................... 102\n5.3 Trace estimation ......................................................... 104\n5.3.1 Trace estimation by sampling ..................................... 104\n5.3.2 Trace estimation with help from low-rank approximation\n105\n5.3.3 Estimating the trace of f(B) via integral quadrature ...... 107\nThis section covers multi-purpose matrix decompositions, the solution of un-\nstructured linear systems, and trace estimation. These are the last problems we\ncover that might be handled by “drivers” in a high-level RandNLA library. We em-\nphasize that this monograph does not exhaust the set of prominent linear algebra\nproblems that are amenable to randomization. We make no eﬀort to cover ran-\ndomized algorithms for general eigenvalue problems, nor do we cover randomized\nalgorithms for computing the action of matrices produced from matrix functions\n(i.e., computing f(A)b for an analytic matrix function f), even though there are\neﬀective algorithms for both of these problems [NT21; GS22; CKN22].\nWe have chosen the topics of this section because they require comparatively\nlittle background material to state, and we believe our summary of the relevant\nalgorithms has some contribution to the literature. For example, the key contribu-\ntion from Section 5.1 is a novel algorithm for QR with column pivoting based on\nCholesky QR. The algorithm is notable for its ability to handle ill-conditioned or\neven outright rank-deﬁcient matrices. The contributions from Section 5.2 include\ndetailed introductions to recently-developed iterative methods for solving general\nlinear systems. Finally, our coverage of trace estimation in Section 5.3, includes\nstate-of-the-art algorithms and implementations that were not available when ear-\nlier RandNLA surveys were published.\n93\n\n\nFurther Possibilities for Drivers\n5.1. Multi-purpose matrix decompositions\n5.1\nMulti-purpose matrix decompositions\nEarly in the year 2000, the IEEE publication Computing in Science & Engineering\npublished a list of the top ten algorithms of the twentieth century. Among this list\nwas the decompositional approach to matrix computation, on which G. W. Stewart\ngave the following remark.\nThe underlying principle of the decompositional approach of matrix\ncomputation is that it is not the business of matrix algorithmists to\nsolve particular problems but to construct computational platforms from\nwhich a variety of problems can be solved. This approach, which was in\nfull swing by the mid-1960s, has revolutionized matrix computation.\nThis section covers three decompositions that provide broad platforms for prob-\nlem solving. They are addressed in an order where randomization oﬀers increasing\nbeneﬁts over purely deterministic algorithms. We note in advance that these ran-\ndomized algorithms do not aim for an asymptotic speedup over deterministic meth-\nods. Rather, the aim is to signiﬁcantly reduce time-to-solution by taking better\nadvantage of modern computing hardware.\n5.1.1\nQR decomposition of tall matrices\nAlgorithms for computing unpivoted QR decompositions are true workhorses of\nnumerical linear algebra.\nThey are the foundation for the preferred algorithms\nfor solving least squares problems with full-rank data matrices. They are also an\nimportant ingredient in preprocessing for more expensive algorithms.\nFor example, suppose we want to decompose a very tall m × n matrix A via QR\nwith column pivoting. The instinctive thing to do here is to reach for the LAPACK\nfunction GEQP3. However, on modern machines, it is much faster to compute an\nunpivoted decomposition A = QR, and then run GEQP3 on R. The ﬁnal decom-\nposition would be mathematically equivalent to calling GEQP3 directly on A, just\nrepresented in a diﬀerent format.\nWith this signiﬁcance of unpivoted QR in mind, we brieﬂy cover two types of\nrandomized algorithms for computing such decompositions.\nOrthogonality in the standard inner product\nCholesky QR is a method for computing unpivoted QR decompositions of matrices\nwith linearly independent columns. It is based on the following elementary obser-\nvation: given a QR decomposition A = QR of a full-column-rank matrix A, the\nfactor R is simply the upper-triangular Cholesky factor of the Gram matrix A∗A.\nTherefore in principle one can compute a QR decomposition as follows.\n1. Compute a Cholesky decomposition of the Gram matrix A∗A = R∗R.\n2. Perform a matrix-matrix triangular solve to obtain Q = AR−1.\nImplementing Cholesky QR only requires three functions: syrk from BLAS, potrf\nfrom LAPACK, and trsm from BLAS. Standard implementations of these functions\nparallelize extremely well. As a result, Cholesky QR can oﬀer substantial speedups\nover Householder QR (and even Tall-and-Skinny QR [DGG+15]) for very tall ma-\ntrices on modern machines.\nPage 94\narXiv Version 2\n\n\n5.1. Multi-purpose matrix decompositions\nFurther Possibilities for Drivers\nDespite the speed advantage of Cholesky QR, it is rarely used in practice, since\nit is unsuitable for even moderately ill-conditioned matrices. Recently it has been\nshown that randomization can overcome this limitation by preconditioning Cholesky\nQR to ensure stability [FGL21]. For detailed analysis of this method we refer the\nreader to the results in [Bal22b] on the algorithm called “RCholeskyQR2.”\nIn Section 5.1.2 we extend this methodology to rank-deﬁcient matrices, and we\nconnect it to an existing randomized algorithm for QRCP of general matrices.\nOrthogonality in a sketched inner product\nIn [BG22], Balabanov and Grigori propose a randomized Gram–Schmidt (RGS)\nprocess that orthogonalizes n vectors in Rm with respect to a sketched inner product\n⟨u, v⟩S = (Su)∗(Sv).\n(5.1)\nWe call such vectors S-orthogonal or sketch-orthogonal. When [BG22, Algorithm 2]\nis run on the columns of a matrix A, values computed during sketched projections\nare assembled in an upper-triangular matrix R so that A = QR and (SQ)∗(SQ) = In.\nOne can choose the distribution from which S is drawn so that Q will be nearly-\northonormal with respect to the standard inner product, with high probability.\nEmpirical and theoretical results show RGS is faster than classic Gram–Schmidt\nbut as stable as modiﬁed Gram–Schmidt.\nThe idea of computing QR decompositions where Q is sketch-orthogonal can be\ntaken in several directions. For example, a block version of RGS is proposed and\nanalyzed in [BG21]. Taking this approach to the extreme where the block size is the\nnumber of columns in the matrix, one can compute the factor R by Householder QR\non SA and then represent Q = AR−1 as a linear operator. Note that this procedure\nis the essence of sketch-and-precondition for least squares, as proposed in [RT08].\nA detailed numerical analysis of this last method can be found in [Bal22b], where\nthe algorithm is called RCholeskyQR.\n5.1.2\nQR decomposition with column pivoting\nWe recall the following reformulation of QR with column pivoting (QRCP) for the\nreader’s convenience.\nGiven a matrix A, produce a column-orthogonal matrix Q, an upper-\ntriangular matrix R, and a permutation vector J so that\nA[:, J] = QR.\nThe diagonal entries of R should approximate A’s singular values, and the columns\nof Q should approximate A’s left singular vectors. These stipulations reﬂect QRCP’s\nmain use cases:\nin low-rank approximation and in solving ill-conditioned least\nsquares problems. As usual, we say that our matrix A is m × n.\nIt’s all in the pivots.\nWe note that if m ≥n, then for any permutation vector\nJ, the economic QR decomposition of A[:, J] is unique.1 Therefore J completely\n1Technically, it is only unique up to sign ﬂips on the columns of Q and rows of R. But it is\nclear how signs must be chosen if the diagonal of R is to approximate the singular values of A.\narXiv Version 2\nPage 95\n\n\nFurther Possibilities for Drivers\n5.1. Multi-purpose matrix decompositions\ndetermines how well the columns of Q (resp., diagonal entries of R) approximate\nthe left singular vectors of A (resp., singular values of A).\nThe method of choosing pivots that sees the widest use today (a simple method\nbased on column norms) was ﬁrst described in [BG65]. The straightforward imple-\nmentation of this method can have subtle failure cases in ﬁnite-precision arithmetic,\nhowever, this can be resolved by carefully restructuring norm calculations [DB08].\nAn established randomized algorithm for general matrices\nHere, we outline a remarkable algorithm ﬁrst developed by Martinsson [Mar15] and\nDuersch and Gu [DG17], and then reﬁned by Martinsson, Quintana-Ort´ı, Heavner,\nand van de Geijn [MHG17]. This reﬁned algorithm was introduced with the name\nHouseholder QR with Randomization for Pivoting or HQRRP. As this name implies,\nthe factor Q from HQRRP is an m×m operator deﬁned by n Householder reﬂectors.\nThe algorithm can run much faster than standard QRCP methods by processing the\nmatrix in column blocks, which makes it possible to cast the overwhelming majority\nof its operations in terms of BLAS 3, instead of about half BLAS 2.\nWhile a full description of HQRRP is beyond our scope, we can outline its\nstructure. As input, it requires that the user provide a block size parameter b and\nan oversampling parameter s. Typical values for these parameters are b = 64 and\ns = 10. HQRRP starts by forming a thin (b + s) × n sketch Y = SA, and then it\nenters the following iterative loop.\n1. Use any QRCP method to ﬁnd Pblock: the ﬁrst b pivots for Y.\n2. Process the panel A[:, Pblock] by QRCP.\n3. Suitably update (A, Y) and return to Step 1.\nThe update to A at Step 3 can be handled by standard methods, such as those used\nin blocked unpivoted Householder QR. The update to Y is more subtle. If done\nappropriately (particularly, by Duersch and Gu’s method [DG17]) then the leading\nterm in the FLOP count for HQRRP is identical to that of unpivoted Householder\nQR. The one downside of this algorithm is that the diagonal entries of R are not\nguaranteed to decrease across block boundaries.\nImplementation notes.\nWe adapted the C implementation from [MHG17] into C++\ncode at\nhttps://github.com/rileyjmurray/hqrrp.\nOur main change was to access BLAS and LAPACK through BLAS++ and LA-\nPACK++. The modiﬁed code also allows for matrix dimensions to be speciﬁed with\neither 32-bit or 64-bit integers and includes a small test suite.\nWe brieﬂy point out two opportunities to improve the performance of this algo-\nrithm. The ﬁrst is to use mixed-precision arithmetic. Speciﬁcally, both the sketch\nof A and the call to deterministic QRCP on that sketch could use reduced precision.\nGiven that the real purpose of QRCP on the sketch is to select the block pivot in-\ndices for A, it might be that loss of accuracy in that phase does not compromise the\naccuracy of the larger algorithm. The second opportunity is to call unpivoted QR\non the very matrix Apanel in the second phase of processing a block; if pivoting is\nused in the second phase then the pivots can be determined by deterministic QRCP\non the R factor from the unpivoted QR of Apanel.\nPage 96\narXiv Version 2\n\n\n5.1. Multi-purpose matrix decompositions\nFurther Possibilities for Drivers\nA novel randomized algorithm for very tall matrices\nThe following algorithm overcomes the limitation of the preconditioned Cholesky\nQR methodology from [FGL21] of requiring full-rank data matrices. It does so by\nusing a randomized preconditioner based on QRCP.\nAlgorithm 7 QRCP via sketch-and-precondition and Cholesky QR.\n1: function [Q, R, J] = sap chol qrcp(A, d)\nInputs:\nA matrix A ∈Rm×n, an integer d satisfying n ≤d ≪m\nOutput:\nColumn-orthonormal Q ∈Rm×k, upper-triangular R ∈Rk×n, and a\npermutation vector J of length n.\nAbstract subroutines:\nSketchOpGen generates an oblivious sketching operator\n2:\nS = SketchOpGen(d, m)\n# S is d × m\n3:\n[Qsk, Rsk, J] = qrcp(SA) # SA[:, J] = QskRsk\n4:\nk = rank(Rsk)\n5:\nApre = A[:, J[:k]](Rsk[:k, :k])−1\n6:\n[Q, Rpre] = chol qr(Apre)\n7:\nR = RpreRsk[:k, :]\n8:\nreturn Q, R, J\nRemark 5.1.1. This monograph was released as a technical report in November\n2022. It has come to our attention that Algorithm 7 was discovered slightly earlier\nby Balabanov; it is termed RRRCholesyQR2 in arXiv:2210.09953:v2 [Bal22b].\nThe following proposition states that Algorithm 7 produces correct output in\nexact arithmetic, under mild assumptions on (S, A). We prove the proposition in\nAppendix D.\nProposition 5.1.2. Consider the context of Algorithm 7. If rank(SA) = rank(A)\nthen A[:, J] = QR.\nA practical implementation of Algorithm 7 would need to consider aspects of\nﬁnite-precision arithmetic. One such aspect is that we cannot use the exact rank\nfor Rsk on Line 4. Instead, some tolerance-based scheme would be needed.\nIn analyzing the behavior of this algorithm our main concern is the condition\nnumber of Apre. Indeed, if that matrix is not well-conditioned, then the factor Q\nfrom Cholesky QR may not be orthonormal to machine precision. More generally,\nif cond(Apre) ≥ϵ−1/2 (where ϵ is the working precision), then it is possible for\nCholesky QR to fail outright.\nOur next proposition says that if Apre is formed in exact arithmetic then its con-\ndition number depends on neither the conditioning of A nor that of Ask. Therefore\nif the distribution of the sketching operator is chosen judiciously, then the algorithm\nshould return an accurate decomposition with extremely high probability.\narXiv Version 2\nPage 97\n\n\nFurther Possibilities for Drivers\n5.1. Multi-purpose matrix decompositions\nProposition 5.1.3. Consider the context of Algorithm 7 and let U be an orthonor-\nmal basis for the range of A. If rank(SA) = rank(A), then the singular values of\nApre are the reciprocals of the singular values of SU.\nProposition 5.1.3 follows easily from Proposition 3.3.1; we omit a formal proof.\nApplication to matrices with any aspect ratio.\nAlthough Cholesky QR only applies\nto very tall matrices, one could apply it to any m × n matrix A (with m ≥n) by\nprocessing the matrix in blocks.\nIn fact, it would be natural to use Cholesky QR as the subroutine for processing a\nblock of columns of A in HQRRP. Since each iteration of HQRRP performs QRCP\non a sketch of A, the triangular factor from that run of QRCP can be used as\nthe preconditioner in processing the subsequent panel of A. However, there is a\ncomplication in this approach.\nHQRRP’s update rule for A requires that each panel’s orthogonal factor\nis represented as a composition of b Householder reﬂectors, where each\nreﬂector is m × m. By contrast, Cholesky QR only returns an explicit\nm × b column-orthonormal matrix Q.\nThis issue can be resolved by using a method to restore the full Householder rep-\nresentation of the explicit column-orthonormal matrix Q. In LAPACK, this is done\nwith sorhr col, which amounts to unpivoted LU factorization.\nWhile pairing\nCholesky QR with sorhr col will reduce its speed beneﬁt, it may still be faster\nthan Householder QR (GEQRF) and Tall-and-Skinny QR (GEQR) in certain settings.\nDetailed analysis of and benchmarks for this method are forthcoming.\n5.1.3\nUTV, URV, and QLP decompositions\nIf QRCP cannot be relied upon to provide an adequate surrogate for the SVD, then\none can consider decompositions of the form\nA = UTV∗,\nwhere U, V are column-orthogonal and T is square and triangular. This recovers\nthe SVD when T is the diagonal matrix of singular values of A. It also recovers\nQRCP when V is a permutation matrix. These decompositions were ﬁrst meaning-\nfully studied by Stewart [Ste92; Ste93; Ste99]. They are known by various names,\nincluding UTV, URV, and QLP. We have a slight preference for the name “UTV”\nfor aesthetic reasons.\nDeterministic algorithms\nStewart’s best-known algorithm for UTV (see [Ste99]) is as follows.\n1. Run QRCP on the original matrix: A = Q1R1(P1)∗.\n2. Run QRCP on (R1)∗, to obtain R1 = P2(R2)∗(Q2)∗.\n3. Grouping terms, we ﬁnd the factors\nA =\n\u0000Q1P2\n| {z }\nU\n\u0001\n(R2)∗\n| {z }\nT\n\u0000P1Q2\n| {z }\nV\n\u0001∗.\nNote in particular that T is lower triangular.\nPage 98\narXiv Version 2\n\n\n5.1. Multi-purpose matrix decompositions\nFurther Possibilities for Drivers\nAssuming the standard pivoting scheme is used in the second call to QRCP, one\ncan be certain that the diagonal entries of T are in decreasing order: Tii ≥Tjj for\nj ≤i. Numerical experiments show that the diagonal of T can track the singular\nvalues of A much better than the diagonal of R1 (see, e.g., [Ste99, §3]). One can\nﬁnd intuition for this by considering the similarities between the successive calls to\nQRCP with the successive calls to QR in the well-known QR iteration. In [FHH99],\nStewart’s UTV algorithm is even described as “half a QR iteration.” Remarkably,\nthis algorithm can be modiﬁed to interleave the computation of R1 with factoring\nR1 [Ste99, §5]. The resulting method, like QRCP, can be stopped early at a speciﬁed\nrank or once some accuracy metric is satisﬁed.\nComplete Orthogonal Decomposition\nThere is a notion of a UTV decomposition\nthat is not the SVD, not QRCP, and yet predates Stewart’s UTV by several decades.\nIt is called the complete orthogonal decomposition (COD), and it is computed by\none call to QRCP followed by one call to unpivoted QR [HL69]. The main use of a\nCOD is to facilitate the application of a pseudoinverse A† when A is rank-deﬁcient.\nWe note that this is only modestly in line with the “spirit” of UTV, which asks for\na decomposition that can be used as a surrogate for the SVD more generally. Still,\nthe COD does have some historical importance in the development of randomized\nUTV algorithms.\nRandomized algorithms\nThe ﬁrst randomized algorithm for UTV was described in [DDH07, §5]. It used a\nrandom orthogonal transformation as a preconditioner for computing a COD, which\nmade it safe to replace the usual call to QRCP with a call to unpivoted QR. This\napproach does not produce good surrogates for the SVD on its own, however, it has\nsince been extended with power iteration ideas through the PowerURV algorithm\n[GM18, §3].2 PowerURV is able to obtain better approximations of the SVD than\nStewart’s UTV without using any pivoted QR decompositions.\nMuch of the value in Stewart’s algorithm for UTV is its ability to compute\nthe decomposition incrementally. The earliest randomized algorithm for UTV that\nenjoys this capability is given in [MQH19, Figure 4]. Qualitatively, this algorithm\ncan be thought of as extending the ideas of HQRRP without relying on HQRRP\nas a black box. In a historical context, it is notable because it is the ﬁrst full-rank\nUTV algorithm to use sketching (i.e., random dimension reduction) rather than\nrandom rotations.\nAs we wrap up the discussion on this topic, we note that one can trivially\nincorporate randomization into Stewart’s UTV by using HQRRP for the requisite\nQRCP calls. There would be a downside to this approach in that the diagonal\nentries of T would not be guaranteed to decrease across block boundaries. However,\nthat downside could be circumvented by using HQRRP for the initial QRCP of\nA and then using a standard QRCP algorithm (e.g., LAPACK’s GEQP3) for the\nQRCP of (R1)∗. The speedup of such an approach over Stewart’s UTV would be\nfundamentally limited, but it should still be observable for n×n matrices even when\nn is as small as a few thousand.\n2We note that the authors of [DDH07] were not trying to develop a randomized algorithm for\nits own sake. Rather, they used randomization as a tool to reduce many linear algebra problems to\na format amenable to recursive unpivoted QR, which can be accelerated by black-box fast matrix\nmultiplication methods.\narXiv Version 2\nPage 99\n\n\nFurther Possibilities for Drivers\n5.2. Solving unstructured linear systems\n5.2\nSolving unstructured linear systems\nTwo broad methodologies have emerged for incorporating randomization into gen-\neral linear solvers. The ﬁrst aims to ameliorate the cost of common safeguards that\nare applied to fast but potentially unreliable direct methods. The second aims to\nrestructure computations in existing general-purpose iterative methods. There is\ngenerally more excitement in the community for methods of this second kind, but\nmethods of the ﬁrst kind remain a subject of practical interest.\n5.2.1\nDirect methods\nDirect methods for solving systems of linear equations center on ﬁnding a factored\nrepresentation of the system matrix. Most famously, we have the LU decomposition\nof a general n × n matrix, which takes the form\nA = LU\nfor a lower-triangular matrix L with unit diagonal (Lii = 1 for all i) and an upper-\ntriangular matrix U. For Hermitian matrices, there is the LDL decomposition\nA = LDL∗,\nwhere L is unit lower-triangular and D is block diagonal with blocks of size one\nand two.\nThese are some of the most fundamental matrix decompositions. Once in hand,\nthey can be used to solve linear systems involving A in O(n2) operations.\nThe\nstandard methods for their computation exhibit good data locality and are naturally\nadapted to parallel processing environments. However, these decompositions should\nbe used cautiously; there are some nonsingular matrices for which they do not\nexist, or for which they cannot be computed stably in ﬁnite precision arithmetic.\nTherefore these decompositions need to be carefully modiﬁed to ensure reliability\nwithout sacriﬁcing too much speed.\nStability through randomized pivoting\nPivoting is the standard paradigm to modify LU and LDL for improved numerical\nstability. For LU, we have partial pivoting and complete pivoting, which look like\nPA = LU\nand\nP1AP2 = LU\n(5.2)\nrespectively, where P, P1, P2 are permutation matrices.\nThe standard algorithms for computing these decompositions are Gaussian elim-\nination with partial pivoting (GEPP) and Gaussian elimination with complete piv-\noting (GECP). While GEPP is substantially faster than GECP, it has weaker the-\noretical guarantees than GECP when it comes to numerical behavior. In [MG15],\nMelgaard and Gu propose a randomized algorithm for partially pivoted LU that\nmakes pivoting decisions in a manner similar to HQRRP (see page 96). The random-\nized algorithm achieves eﬃciency comparable to that of GEPP, while also satisfying\nGECP-like element-growth bounds with high probability.\nFor LDL, pivoted decompositions take the form\nA = (PL)D(PL)∗,\n(5.3)\nPage 100\narXiv Version 2\n\n\n5.2. Solving unstructured linear systems\nFurther Possibilities for Drivers\nwhere (again) D is block-diagonal with blocks of size one and two. There are a va-\nriety of ways to introduce pivoting into LDL decompositions. The most notable are\nBunch–Kaufman [BK77] and bounded Bunch–Kaufman (which uses rook pivoting)\n[AGL98], both of which are available in LAPACK. In [FXG18], Feng, Xiao, and Gu\npropose a randomized algorithm for pivoted LDL that is as stable as GECP and\nyet only slightly slower than Bunch–Kaufman and bounded Bunch–Kaufman.\nStability through randomized rotations\nIn Section 5.1.3, we mentioned how the ﬁrst randomized algorithm for UTV used\nrandomized preconditioning to compute a COD-like factorization using only unpiv-\noted QR decompositions. This was not the ﬁrst use of randomization to remove\nthe need for pivoting in matrix decompositions. In fact, this idea was explored by\nParker in 1995 to remove the need for pivoting in Gaussian elimination [Par95].\nHere we summarize Parker’s approach.\nWe begin by introducing some terms. For an integer d ≥1, a butterﬂy matrix\nof size 2d × 2d is a two-by-two block matrix, with d × d diagonal matrices in each\nof the four blocks. Speaking loosely, a recursive butterﬂy transformation (RBT) is\na product of a chain of matrices, each with butterﬂy matrices as diagonal blocks.\nRBTs of order n (i.e., RBTs of size n × n) are usually analyzed when n is a power\nof two for the sake of simplicity. The recursive structure in RBTs makes it possible\nto apply them with FFT-like methods. In particular, an RBT of order n = 2ℓcan\nbe applied to an n-vector in O(nℓ) time. Detailed discussion on RBTs of general\norder can be found in [Pec21].\nWe are interested in RBTs that are orthogonal and random. The orthogonality\nis useful since it means the same FFT-like algorithms used to apply an RBT can\nbe used to apply its inverse. The randomness in orthogonal RBT stems from how\none chooses the entries in the diagonal matrices. While there are a variety of ways\nthat this can be done [Par95], we simply speak in terms of a distribution Dn over\northogonal RBTs of order n.\nOne of the major contributions of [Par95] was to prove that for any nonsingular\nmatrix A of order n, one can sample B1, B2 iid from a certain distribution Dn, so\nthat matrix B1AB2 has an unpivoted LU decomposition with high probability. Put\nanother way, the decomposition\nA = (B1)∗LU(B2)∗\nexists with high probability.\nThe high speed at which RBTs can be applied and the excellent data locality\nproperties of unpivoted matrix decompositions have led to substantial interest in\nRBTs from the HPC community. For example, implementation considerations for\nhybrid CPU/GPU machines were studied in [BDH+13] (in the single-node setting)\nand [LLD20] (in the distributed setting).\nThe idea of using RBTs to precondition an “unsafe” unpivoted method naturally\napplies to LDL. In this case, one obtains factorizations of the form\nA = (BL)D(BL)∗\nwhere B is the random RBT. Again, this methodology has received recent attention\nfrom the HPC community; see [BBB+14] for work in the multi-core distributed-\nmemory setting [BBB+14] and [BDR+17] for work in the setting of a single machine\nwith a hybrid CPU/GPU architecture.\narXiv Version 2\nPage 101\n\n\nFurther Possibilities for Drivers\n5.2. Solving unstructured linear systems\nRemarkably, although the idea of RBTs seems predicated on destroying sparsity\nstructure present in the matrix A, the random RBT methodology can be applied\nto sparse matrices without catastrophic ﬁll-in. See [BLR14] for work on this topic\nfor both general matrices and symmetric/Hermitian indeﬁnite matrices.\n5.2.2\nIterative methods\nBackground on GMRES\nGMRES is a well-known iterative method for solving linear systems of the form\nAx = b where A is n × n and nonsingular. The trajectory (xp)p≥1 it generates has\na simple variational characterization. Speciﬁcally, xp minimizes L(x) = ∥Ax −b∥2\n2\nover all vectors x in the p-dimensional Krylov subspace\nKp = span\n\b\nb, Ab, . . . , Ap−1b\n\t\n.\n(5.4)\nThe standard implementation of GMRES uses the Arnoldi process. This can be\nseen as a specialization of (modiﬁed) Gram–Schmidt to orthogonalize implicitly-\ndeﬁned matrices of the form Kp = [b, Ab, . . . , Ap−1b]. In particular, as iterations\nproceed, the Arnoldi process maintains a column-orthonormal matrix Vp where\nrange(Vp) = Kp. Optionally, it can also maintain an Arnoldi decomposition, which\nrepresents AVp = Vp+1Hp in terms of an n × (p + 1) column-orthonormal matrix\nVp+1 and a (p + 1) × p upper-Hessenberg matrix Hp.3\nLetting Tmv(A) denote the cost of a matrix-vector multiply with A, the Arnoldi\ndecomposition up to step p can be computed in time\nO(pTmv(A) + np2).\nIf we are given this decomposition, then the least squares problem deﬁning xp can\nbe solved in O(np) time by applying a suitable direct method. Strictly speaking,\none does not need to compute xp−1 to compute xp.\nWe summarize some ways to introduce randomness into GMRES below. They\nall work by relaxing the requirement that Vp be column-orthonormal while retaining\nthe requirement that range(Vp) = Kp. Some of them work by changing the loss\nfunction L(x) to be minimized by xp.\nThese methods are of interest when the\ncost of the matrix-vector multiplies is dwarfed by the complexity of maintaining\nthe Arnoldi decomposition. We note that this situation can only arise when A is a\nsparse or otherwise structured operator.\nRandomized GMRES: Arnoldi decompositions in a sketch-orthogonal basis\nThe method from [BG22, § 4.2] can be interpreted as using a “sketched Arnoldi\nprocess” based on sketched Gram–Schmidt. It works by building up Vp so that\nits columns are S-orthogonal in the sense of (5.1), where S is a d × n sketching\noperator (p ≲d ≪n). Along the way, it maintains an Arnoldi-like decomposition\nAVp = Vp+1Hp, where Vp+1 is likewise S-orthogonal. Access to this decomposition\nat step p makes it possible to minimize the loss function ∥S(Ax −b)∥2\n2 over all x in\nKp in only O(np) added time.\n3A matrix is called upper-Hessenberg if all entries below the ﬁrst subdiagonal are zero.\nPage 102\narXiv Version 2\n\n\n5.2. Solving unstructured linear systems\nFurther Possibilities for Drivers\nTo understand the quality of the solution obtained by this method it is helpful\nto consider the unconstrained formulation\nmin\nz ∥AVpz −b∥2\n2.\n(5.5)\nGMRES would return x⋆= Vpz⋆where z⋆solves (5.5) exactly.\nThe sketched\nArnoldi approach eﬀectively approximates this solution by applying sketch-and-\nsolve to (5.5).\nThis puts us in a position to draw from our coverage of sketch-\nand-solve in Section 3.2.1.\nIf δ is the eﬀective distortion of S for the subspace\nKp+1, then the solution xsk obtained by the sketched Arnoldi approach will satisfy\n∥Axsk −b∥2 ≤(1 + δ)∥Ax⋆−b∥2.\nThe big-O time complexity of the sketched Arnoldi process is unchanged relative\nto the classic Arnoldi process. However, the ﬂop count for the sketched process can\nbe up to a factor of two smaller. The sketched process also makes better use of\nBLAS 2 over BLAS 1, and it has fewer synchronization points compared to the\nclassic Arnoldi process based on modiﬁed Gram–Schmidt. Taken together, using\nthe sketched process can signiﬁcantly reduce the wallclock time needed to obtain\nthe decomposition of AVp while retaining the reliability of classic Arnoldi.\nWe note that a block version of this algorithm (for linear systems with multiple\nright-hand sides) is presented in the preprint [BG21]. For MATLAB implementa-\ntions of these methods, see [Bal22a].\nRandomized GMRES: handling general non-orthogonal bases\nBoth classic GMRES and the randomized variant given above maintain Arnoldi-like\ndecompositions of matrices AVp at a cost of O(np2) time complexity. Interestingly,\nthis cost cannot be asymptotically reduced by forgoing the decomposition of AVp.\nThe trouble is that building Vp with full orthogonalization – in the standard sense\nor the S-orthogonal sense – already takes O(np2) time.\nIn [NT21], Nakatsukasa and Tropp identiﬁed that (5.5) has precisely the form\nneeded to beneﬁt from randomized algorithms, independent from how Vp and AVp\nare generated.\nBased on this observation they called attention to longstanding\nclassical methods for computing non-orthogonal bases of Krylov subspaces.\nFor\nexample, one can compute Vp by a truncated k-step Arnoldi process for some k ≪p.\nThis can be done in O(npk) time and can easily be implemented to provide a dense\nrepresentation of AVp at no added cost. Alternatively, it may be practical to use\nthe Chebyshev method if one has knowledge of the spectrum of A.\n[NT21] primarily advocates for approximately solving (5.5) via sketch-and-solve,\nwhere the sketched subproblem is handled by factoring SAVp. Note that in exact\narithmetic the solutions obtained from this method would coincide with those of\nGMRES based on sketched Arnoldi. On the one hand this is very appealing, since\nthe cost of running this method for p iterations can easily undercut the O(np2) cost\nof sketched Arnoldi. On the other hand, the behavior of these methods can diﬀer\nin ﬁnite-precision arithmetic. If one is too lax in building the basis matrix Vp then\nthe condition number of AVp can explode as p increases.\nAll in all, the design space for this methodology is large and worth navigating\nwith care.\nValuable advice in this regard is given throughout [NT21, §3 – §5].\nOne particularly compelling comment is that one could simply solve (5.5) to high\naccuracy via a sketch-and-precondition method, such as Algorithm 1. The resulting\nsolution in this case would be very close to that produced by GMRES.\narXiv Version 2\nPage 103\n\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nNested randomization in block-projection and block-descent methods\nHaving discussed GMRES at length, we now speak to a family of iterative solvers\nthat do not use the Krylov subspace approach.\nThis family came into focus with the development of sketch-and-project – a\ntemplate iterative algorithm for solving linear systems of the form Fz = g, where\nF ∈RM×m has at least as many rows as columns (M ≥m) [GR15]. Its special cases\ninclude randomized Kaczmarz [SV08] and randomized block Kaczmarz [NT14]. It\nalso has variants that are speciﬁcally designed for overdetermined least squares\nproblems [GIG21].\nWithout getting into the mechanics of sketch-and-project in detail, we note that\nthese methods share a signiﬁcant weakness: their convergence rates worsen as one\nconsiders larger and larger problems. We think they are most likely to be useful\nwhen one cannot ﬁt an m×m matrix in memory. While such situations fall outside\nour primary data model, the subproblems encountered in sketch-and-project are\namenable to methods we have covered. Indeed, the subproblems are equivalent to\nproblems of the form\nmin\ny∈Rm{∥y −b∥2\n2 : A∗y = c},\n((3.4), revisited)\nwhere the number of columns n in A is a user-selected tuning parameter n ≪m ≤\nM. Such problems are clearly amenable to Algorithm 2.\nRecently, a general analysis framework for randomized linear system solvers\nbased on block projection or block descent has been proposed [PJM22]. We refer\nthe reader to Table 3 of [PJM22] (and appendices A.15 – A.26) for an extensive\nlist of new and old randomized linear system solvers that are amenable to their\nproposed analysis framework.\nSome of these methods are distinguished in their\napplicability to underdetermined problems. As with sketch-and-project, the sub-\nproblems encountered in essentially all of these methods can be chosen to have a\nstructure amenable to Algorithm 2.\n5.3\nTrace estimation\nMany scientiﬁc computing and machine learning applications require estimating\nthe trace of a square linear operator A that is represented implicitly. Randomized\nmethods are especially eﬀective for such problems.\n5.3.1\nTrace estimation by sampling\nLet A be n × n and {e1, . . . , en} be the standard basis vectors in Rn. Clearly, one\ncan compute the trace of A with n matrix-vector products by using the identity\ntr(A) =\nn\nX\ni=1\ne∗\ni Aei.\nRandomization creates opportunities to estimate this quantity using m ≪n matrix-\nvector multiplications. The most basic method uses the fact that if ω ∼D is a\nrandom vector satisfying E[ωω∗] = In, then\ntr(A) = E [ω∗Aω] .\nPage 104\narXiv Version 2\n\n\n5.3. Trace estimation\nFurther Possibilities for Drivers\nIt is natural to approximate the expected value by the empirical mean. That is,\nupon drawing m independent vectors ωi ∼D, we estimate\ntr(A) ≈1\nm\nm\nX\ni=1\nω∗\ni Aωi.\n(5.6)\nThe idea for this method goes back to 1987 with work by Girard [Gir87], who\nproposed that D be the uniform distribution over the ℓ2 hypersphere with radius\n√n. Shortly thereafter, Hutchinson proposed that one take D as a distribution\nover Rademacher random vectors [Hut90]. Hutchinson’s choice of D minimizes the\nvariance of the estimator when A is ﬁxed, while Girard’s choice minimizes the worst-\ncase variance over sets of matrices that are closed under conjugation by unitary\nmatrices; see [Epp23] for an explanation of this point.\nWe call the right-hand side of (5.6) a Girard–Hutchinson estimator. Such es-\ntimators require m ∈Ω(1/ϵ2) samples to approximate tr(A) to within ϵ error for\nsome constant failure probability.\n5.3.2\nTrace estimation with help from low-rank approximation\nCompress and trace\nIn [SAI17], Saibaba, Alexanderian, and Ipsen propose two randomized algorithms\nfor estimating the trace of a psd linear operator A.\nWhen A is accessible by matrix-vector products, the proposed method begins\nwith a rangeﬁnder step to ﬁnd a column-orthonormal n × m matrix Q where\nQQ∗AQQ∗≈A. The method then approximates\ntr(A) ≈tr(QQ∗AQQ∗) = tr(Q∗AQ).\nWhether or not this bound is accurate depends on the rate of A’s spectral decay\nand on how well-aligned Q is with the dominant eigenvectors of A. This method\ncan provide for better relative error bounds than a Girard–Hutchinson estimator if\nA’s spectral decay is suﬃciently fast and Q is obtained by power iteration.\nTrace estimation is especially challenging when matrix-vector products with A\nare expensive. This often happens when A is the image of another matrix B under\na matrix function, in the sense of [Hig08]. Saibaba et al. consider the case where\nA = log(I + B)\nfor a psd matrix B.4 The idea here is again to ﬁnd a tall n×m column-orthonormal\nQ so that QQ∗BQQ∗is a good low-rank approximation of B. Then we approximate\ntr(A) ≈\nm\nX\ni=1\nlog (1 + λi(Q∗BQ))\nwhere λi(·) returns the ith-largest eigenvalue of the given matrix. Error bounds can\nbe obtained for this estimate under suitable assumptions on the spectral decay of\nB. We note that some of the techniques used to prove these bounds extend to any\nmatrix function that is operator-monotone, such as the matrix square-root.\n4For any positive deﬁnite matrix M, we use log(M) to denote the Hermitian matrix with the\nsame eigenvectors as M and whose eigenvalues are the logs of the eigenvalues of M. One can verify\nthat tr(log(M)) = log det M holds for any positive deﬁnite M.\narXiv Version 2\nPage 105\n\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nSplit, trace, and approximate\nIn [MMM+21], Meyer et al. combined ideas from low-rank approximation with the\nGirard–Hutchinson estimator to obtain Hutch++. This estimator starts by sampling\na matrix Q uniformly at random from the set of n×m column-orthonormal matrices.\nIt then deﬁnes the low-rank approximation ˆA = QQ∗AQQ∗and computes the trace\nof this approximation by the formula\ntr(ˆA) = tr(Q∗AQ).\nThe last phase of Hutch++ applies Girard–Hutchinson to the deﬂated matrix\n∆= (I −QQ∗)A(I −QQ∗),\nand adds this estimate to tr(ˆA).\nThe basic validity of Hutch++ follows by splitting the trace of A into two parts:\ntr(A) = tr(ˆA) + tr(A −ˆA)\nand verifying that tr(A −ˆA) = tr(∆). As a splitting and deﬂation approach, this\nmethod is very eﬀective in reducing the variance of the Girard–Hutchinson estima-\ntor. Early results along these lines can be found in [GSO17], which investigated the\nuse of deﬂation in estimating the trace of an inverted matrix.\nThe initial results proven for Hutch++ applied only to psd matrices. In that\ncontext, Hutch++ can (with some small ﬁxed failure probability) compute tr(A) to\nwithin ϵ relative error using only O(1/ϵ) matrix-vector products. This is a sub-\nstantial improvement upon the O(1/ϵ2) matrix-vector products that are required\nby plain Girard–Hutchinson estimators. In fact, the sample complexity of Hutch++\ncannot be improved when considering a large class of algorithms [MMM+21, The-\norems 4.1 and 4.2].\nPersson, Cortinovis, and Kressner have since extended Hutch++ so that it can\nproceed adaptively, only terminating once some error tolerance has been achieved\n(up to a controllable failure probability) [PCK21]. The analysis of their modiﬁed\nHutch++ method notably accommodates symmetric indeﬁnite matrices A. We note\nthat the accuracy guarantees of trace estimators for indeﬁnite matrices cannot be as\nstrong as those for positive deﬁnite matrices. Indeed, relative error guarantees are\nessentially impossible when tr(A) = 0. Persson et al., therefore, provide additive\nerror guarantees in this setting.\nLeveraging the exchangability principle\nIn [ETW23], Epperly, Tropp, and Webber develop a trace estimator based on the\nexchangability principle. In the context of trace estimation, this principle stipulates\nthat if an algorithm computes its estimate based on m pairs {(ωi, Aωi)}m\ni=1 where\nωi are iid random vectors, then the minimum-variance unbiased estimator for tr(A)\nmust be invariant under relabelings {ωi}m\ni=1 ←{ωσ(i)}m\ni=1 for permutations σ.\nHutch++ does not respect the exchangability principle, since it uses randomness\nin two distinct stages: ﬁrst to compute the matrix Q and then to estimate the trace\nof the ∆by a a Girard–Hutchinson estimator.\nThe XTrace algorithm proposed in [ETW23] can be thought of as a symmetrized\nversion of Hutch++. Given m samples {(ωi, Aωi)}m\ni=1, its estimate is an average of m\nruns of Hutch++, where the jth run uses Qj = orth([Aωi]i̸=j) and estimates tr(∆j)\nPage 106\narXiv Version 2\n\n\n5.3. Trace estimation\nFurther Possibilities for Drivers\nby ω∗\nj ∆jωj.\nImplementing XTrace naively would be very expensive.\nHowever,\nas explained in [ETW23, §2.1], a careful implementation can achieve the same\nasymptotic complexity as Hutch++. XTrace also comes with adaptive-stopping and\nvariance estimation methods analogous to those developed in [PCK21].\n5.3.3\nEstimating the trace of f(B) via integral quadrature\nSection 5.3.2 touched on a method for estimating the trace of A = log(B + I),\nwhere B is psd and log(·) is the matrix logarithm. This section covers powerful\nmethods for a broader class of trace estimation problems. The original method,\nnow known as stochastic Lanczos quadrature (SLQ), was introduced to the linear\nalgebra community in [BFG96], was popularized by [UCS17], and has since extended\nin a few diﬀerent ways [CH22; PK22; CTU22].\nTo begin, consider how any function f : R →R can canonically be extended to\nact on a Hermitian matrix by acting separately on the eigenvalues of the matrix.\nThat is, if we expand B in its eigenbasis\nB =\nn\nX\ni=1\nλiuiu∗\ni ,\nthen we can deﬁne\nf(B) =\nn\nX\ni=1\nf(λi)uiu∗\ni .\nHere we cover quadrature-based methods for approximating the trace of such ma-\ntrices. The concepts behind these methods apply whenever f is suﬃciently smooth\nand B is Hermitian. Theoretical guarantees for these methods are usually obtained\nunder stronger assumptions, such as f being analytic on [λn, λ1], or B being psd.\nTechnical background\nThe concepts we summarize below are detailed in the book [GM10].\nRiemann-Stieltjes integrals.\nLet µ be a real-valued function on R. The expression\nZ\nR\nf(t)dµ(t)\n(5.7)\nis called the Riemann-Stieltjes integral of f against µ. We do not provide a formal\ndeﬁnition of this integral.\nRather, we oﬀer two footholds for understanding it.\nFirst, if µ is continuously diﬀerentiable, then (5.7) is simply the Riemann integral\nof t 7→f(t)( d\ndtµ(t)). Second, recall the interpretation of Riemann integration in\nwhich one identiﬁes dt ≈tℓ+1 −tℓ, where tℓ< tℓ+1 are consecutive points in a\npartition of the region of integration. If the analogous interpretation is applied to\n(5.7), then we would say that dµ(t) ≈µ(tℓ+1) −µ(tℓ).\nFor our purposes we can assume that µ is nondecreasing. We also assume that\nthere are constants L and U where µ(t) = µ(L) for all t ≤L and µ(t) = µ(U) for\nall t ≥U. Under these assumptions, (5.7) is well-deﬁned whenever f is continuous.\narXiv Version 2\nPage 107\n\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nQuadrature and orthogonal polynomials.\nAn s-point quadrature rule for (5.7) speci-\nﬁes s-vectors w and θ (of weights and nodes respectively) to deﬁne an approximation\nZ\nR\nf(t)dµ(t) ≈\ns\nX\nℓ=1\nwℓf(θℓ).\n(5.8)\nThe nodes and weights selected by any reliable quadrature method will depend\non µ. One prominent approach to deﬁning quadrature rules is to require that (5.8)\nholds with equality whenever f is a polynomial of degree d, where d is suitably\nbounded in terms of s. The idea behind this is that s increases, we should be able\nto accommodate polynomials of higher degree.\nGaussian quadrature achieves optimal sample complexity. This method is exact\nfor polynomials up to degree 2s−1, and there is no rule that can guarantee exactness\nfor polynomials of degree higher than 2s −1 with only s samples. There is a deep\nconnection between Gaussian quadrature and orthogonal polynomials that make\nthis quadrature rule viable in practice. The connection uses the fact that, under\nour assumptions on µ, it can be taken to deﬁne an inner product\n⟨p, q⟩µ =\nZ\nR\np(t)q(t)dµ(t).\nThis inner product can be used to deﬁne an orthonormal basis for the set of poly-\nnomials that have at most some prescribed degree. When this orthonormal basis is\nsorted by degree, the resulting sequence of polynomials must satisfy a three-term\nrecurrence relationship [GM10, §2]. The coeﬃcients of the recurrence relationship\nup to step s can be assembled in a tridiagonal matrix J, called the Jacobi matrix, of\nsize s×s. The nodes and weights of Gaussian quadrature against µ can be recovered\nfrom an eigendecomposition of the Jacobi matrix [GM10, Theorem 6.2].\nStochastic Lanczos quadrature : approximating Girard–Hutchinson\nA Girard–Hutchinson estimator for the trace of f(B) takes the form\nT = 1\nm\nm\nX\ni=1\nTi\nwhere\nTi = ω∗\ni f(B)ωi\nfor independent random vectors ωi drawn from a suitable distribution. The naive\nway to compute this estimator would be to call a black-box function that implements\nthe action of f(B); for each sample i one would compute vi = f(B)ωi and then\ntake a dot product Ti = ω∗\ni vi. Here we describe an alternative approach which\nbegins with an integral representation for Ti and then approximates that integral\nvia Gaussian quadrature [BFG96]. The resulting method for trace estimation is\nnow known as stochastic Lanczos quadrature (SLQ) [UCS17].\nIntegral representation of a single sample.\nLet u denote the piecewise constant func-\ntion that is zero for t ≤0 and one for t > 0. This function can be used to deﬁne a\nRiemann-Stieltjes integral that samples f at any prescribed point. Speciﬁcally, for\nany scalar z, we have f(z) =\nR\nR f(t)du(t −z). Therefore upon setting\nµi(t) =\nn\nX\nj=1\n|ω∗\ni uj|2 u(t −λj),\n(5.9)\nPage 108\narXiv Version 2\n\n\n5.3. Trace estimation\nFurther Possibilities for Drivers\nthe following identity is immediate from the deﬁnition of f(B):\nTi =\nZ\nR\nf(t)dµi(t).\n(5.10)\nWe note that this integral is written as being over all of R, but it would suﬃce to\nintegrate over the interval [λn, λ1].\nQuadrature of a sample’s integral representation.\nIntegration is often described as a\ncontinuous analog of summation. As such, one usually thinks of quadrature as an\nact of approximating a continuous operation by a discrete operation. Quadrature of\nRiemann-Stieltjes integrals does not always follow this pattern. Indeed, the integral\n(5.10) can already be expressed as a weighted sum of s = n point evaluations\nof f, with weights wiℓ= |ω∗\ni uℓ|2 and nodes θiℓ= λℓ.\nThe problem with this\nrepresentation is that we do not know the weights or nodes a-priori. Therefore in\nthe setting of Riemann-Stieltjes integration it is possible that quadrature acts as\na means of approximating an unknown integrator µi by a known integrator ˆµi for\nwhich we can eﬃciently compute\nR\nR f(t)dˆµi(t).\nEnter, Lanczos quadrature.\nThis is a method for computing the Gaussian\nquadrature rule (or variations thereof) of Riemann-Stieltjes integrals with integra-\ntors of the form (5.9) [GM10, §7]. It uses the fact that the polynomials that are\northogonal with respect to the integrator µi are none other than the Lanczos poly-\nnomials associated with (B, ωi) (see [GM10, Theorem 4.2]). Hence, the Lanczos\nalgorithm for computing an orthonormal basis for the s-dimensional Krylov sub-\nspace\nspan{ωi, Bωi, . . . , B(s−1)ωi}\ncan be used to compute the Jacobi matrix. Given that, standard tridiagonal eigen-\nsolvers can provide us with the nodes and weights needed for Gaussian quadrature.\nImplementation notes.\nSLQ entails approximating m samples of the form ω∗\ni f(B)ωi,\nwhere each ωi is an independent random vector drawn from some distribution D.\nThe quality of each approximate sample depends on the number of nodes allowed\nin the Gaussian quadrature rule, and hence on the number of steps in the Lanczos\nalgorithm.\nTaking s steps of the Lanczos algorithm will always require s −1 matrix-vector\nproducts with B. The arithmetic and storage complexity needed to compute each\nsample in SLQ depends on whether we run Lanczos proper or a version of Lanczos\nthat only computes the data needed for Gaussian quadrature. Indeed, in the latter\ncase we have a substantial amount of freedom to make tradeoﬀs between computa-\ntional complexity and numerical stability. At one end this tradeoﬀ, s iterations of\nLanczos with full orthogonalization costs O(ns) storage and O(ns2) arithmetic. At\nthe other end of the tradeoﬀ, performing no reorthogonalization reduces the costs\nto only O(n) storage and O(ns) arithmetic. (We emphasize that these costs do not\naccount for the s −1 matrix-vector products needed with B.)\nSLQ is a powerful tool, with important applications in Gaussian process regres-\nsion. For an implementation of this method that scales to petascale problems by\nrunning on GPU farms, we refer the reader to the IMATE Python package [Ame22].\narXiv Version 2\nPage 109\n\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nBeyond stochastic Lanczos quadrature\nAccelerated quadrature-based methods.\nLet A = f(B).\nThe convergence rate of\nSLQ for estimating tr(A) can only be as good as a Girard–Hutchinson estimator.\nAs such, one needs m ∈Ω(1/ϵ2) samples in order to estimate tr(A) to within ϵ\nerror for some constant failure probability. This leaves substantial improvement\nfor SLQ in the case when A is positive deﬁnite, where Hutch++ could make do\nwith m ∈Ω(1/ϵ) queries to A. Luckily, it is possible to extend SLQ to use similar\nsplitting techniques that Hutch++ employs for its variance reduction; see [CH22]\nand [PK22] for details.\nSpectral density estimation.\nOne of SLQ’s remarkable properties is that its quadra-\nture rule for approximating (5.10) does not depend on f. As such, if the quadrature\nnodes and weights are computed to estimate tr(f(B)) for one function f, then one\ncan use those same nodes and weights to compute an estimate for tr(g(B)) for\nanother function g. This gives some motivation for directly estimating the function\nφ(t) = Pn\nj=1 u(t −λi)\n(5.11)\nwhich satisﬁes\nR\nR f(t)dφ(t) = tr(f(B)) for all continuous functions f : R →R.\nNote that φ is a nonnegative nondecreasing function with limt→∞φ(t) = n. As\nsuch, φ/n is a cumulative probability distribution function that can be uniquely\nidentiﬁed with the spectrum of B.\nThe problem of estimating a function of the form (5.11) is a particular case\nof spectral density estimation.\nThis problem, which has broader applications in\nthe physical sciences than trace estimation, has been approached explicitly with\nrandomized algorithms [Lin16]. Approaching the linear algebraic problem of trace\nestimation with this in mind can lead to new insights on how to leverage prior knowl-\nedge on the structure of f or B for algorithmic purposes. In particular, [CTU22]\nprovides a systematic treatment of quadrature-based trace estimation algorithms\nbased on this perspective.\nPage 110\narXiv Version 2\n\n\nSection 6\nAdvanced Sketching:\nLeverage Score Sampling\n6.1 Deﬁnitions and background ........................................ 112\n6.1.1 Standard leverage scores ............................................ 112\n6.1.2 Subspace leverage scores ............................................ 115\n6.1.3 Ridge leverage scores ................................................. 116\n6.2 Approximation schemes .............................................. 117\n6.2.1 Standard leverage scores ............................................ 117\n6.2.2 Subspace leverage scores ............................................ 118\n6.2.3 Ridge leverage scores ................................................. 119\n6.3 Special topics and further reading .............................. 120\n6.3.1 Leverage score sparsiﬁed embeddings ........................... 120\n6.3.2 Determinantal point processes .................................... 121\n6.3.3 Further variations on leverage scores ............................ 122\nLeverage scores quantify the extent to which a low-dimensional subspace aligns\nwith coordinate subspaces. They are fundamental to RandNLA theory since they\ndetermine how well a matrix can be approximated through sketching by row or\ncolumn selection, and thus indirectly how well a matrix can be approximated by\nsparse data-oblivious sketching methods [DM16]. They have algorithmic uses in\nleast squares [DMM06; DMM+12] and low-rank approximation [DMM08; BMD09;\nMD16] among other topics. More broadly, they play a key role in statistical regres-\nsion diagnostics [CH88; MMY15].\nThe computational value of leverage scores stems from how they induce data-\naware probability distributions over the rows or columns of a matrix. Leverage score\nsampling refers to sketching by row or column sampling according to a leverage score\ndistribution (or an approximation thereof). The quality of sketches produced by\nleverage score sampling is relatively insensitive to numerical properties of the matrix\nto be sketched. This can be contrasted with sketching by uniform row or column\nsampling, which can perform very poorly on certain families of matrices.\n111\n\n\nLeverage Score Sampling\n6.1. Deﬁnitions and background\nLeverage score distributions can be computed exactly with standard determin-\nistic algorithms. However, exact computation is expensive except in very speciﬁc\ncases (see Section 7). Therefore in practice it is necessary to use randomized algo-\nrithms to approximate leverage score distributions. On the one hand, this point is\nsigniﬁcant since the costs of the approximation algorithms undermine the eﬃciency\ngains obtained from sketching by simple row or column selection, making the cost\ncomparable to implementing data-oblivious random projection methods. On the\nother hand, uniform sampling is clearly suboptimal in many cases, e.g., in that it\ncan miss important nonuniformity structures needed to obtain data-aware subspace\nembeddings. In general, the practical utility of leverage scores derives from when\nrow or column selection of a matrix is required by a particular application. Lever-\nage scores, therefore, compete with both uniform sampling and other methods for\ncolumn (or row) selection as discussed in Section 4.3.4.\nWe emphasize that we have made no concrete plans regarding RandLAPACK’s\nsupport for leverage score sampling methods. We review them here since they are\nprominent and sophisticated sketching methods, and they might be appropriate to\nsupport in RandLAPACK via a suite of computational routines.\nIn what follows we introduce three ﬂavors of leverage scores (§6.1) and meth-\nods for approximately computing them (§6.2). We also cover three special topics:\nSection 6.3.1 explains how leverage scores can be used to deﬁne long-axis-sparse\nsketching operators (in the sense of Section 2.4.2), and Sections 6.3.2 and 6.3.3\ndiscuss generalizations of leverage scores.\n6.1\nDeﬁnitions and background\nHere we cover three types of leverage scores and corresponding approaches to lever-\nage score sampling. The ﬁrst type of leverage score (which we mean by default) is\napplicable to sketching in the embedding regime. As such, it is applicable primarily\nto highly overdetermined least squares problems or other saddle point problems with\ntall data matrices. We spend more time on this ﬁrst type of leverage score since it\nhas theoretical value in understanding the behavior of RandNLA algorithms. The\nsecond type is used for sketching in the sampling regime and has applications in\na variety of low-rank approximation problems. The third type is speciﬁcally for\napproximating psd matrices (typically kernel matrices) in the presence of explicit\nregularization.\n6.1.1\nStandard leverage scores\nLet U be an n-dimensional linear subspace of Rm and PU be the orthogonal pro-\njector from Rm to U. The ith leverage score of U is\nℓi(U) = ∥PUδi∥2\n2 = PU[i, i].\n(6.1)\nwhere δi is the ith standard basis vector. Collectively, leverage scores describe how\nwell the subspace U aligns with the standard basis in Rm. They have algorithmic\nimplications when we consider induced leverage score distributions, deﬁned by\npi(U) =\nℓi(U)\nPm\nj=1 ℓj(U) = ℓi(U)\nn\n.\n(6.2)\nPage 112\narXiv Version 2\n\n\n6.1. Deﬁnitions and background\nLeverage Score Sampling\nGiven a matrix A, one can associate as many sets of leverage scores to that\nmatrix A, as one can associate subspaces to A. Two of the most important such\nsubspaces are U = range(A) and V = range(A∗). In these contexts we say that\nthe leverage score for the ith row of A is ℓi(U), while the leverage score for the\njth column is ℓj(V ). Such leverage scores provide leverage score distributions over\nthe rows and columns of A, respectively. Note that only one of these distributions\ncan be nonuniform if A is full-rank. Therefore when speaking of leverage scores we\ntypically assume the m × n matrix A is tall, which allows for the possibility that\np(U) is nonuniform.\nMoving forward, we routinely replace U by A in (6.1) and (6.2), with the un-\nderstanding that U = range(A).\nProbabilistic guarantees of sketching via row sampling\nSuppose S is a wide d×m sketching operator that implements row sampling accord-\ning to a probability distribution q. We are interested in evaluating the statistical\nquality of S as a row sampling operator for an m × n matrix A. Here, our measure\nof sketch quality the smallest ϵ ∈(0, 1) where y ∈range(A) implies\n(1 −ϵ)∥y∥2\n2 ≤∥Sy∥2\n2 ≤(1 + ϵ)∥y∥2\n2.\n(6.3)\nNote that this metric is very similar to subspace embedding distortion.\nIn this\nmonograph we have generally advocated for measuring sketch quality by a scale-\ninvariant metric called eﬀective distortion. Despite this, we care about (6.3) since\nit provides for the following standard result (which we prove in Appendix A.3).\nProposition 6.1.1. Suppose A is an m × n matrix of rank n. If\nr := min\nj∈JmK\nqj\npj(A)\nthen for all 0 < ϵ < 1, we have\nPr {(6.3) fails for (S, A, ϵ)} ≤2n\n\u0012\nexp(ϵ)\n(1 + ϵ)(1+ϵ)\n\u0013rd/n\n(6.4)\nand exp(ϵ) < (1 + ϵ)(1+ϵ).\nThe proposition’s basic message is that the probability of SA being a good\nsketch improves as q gets closer to the leverage score distribution p(A), where\n“closer” means that the value r becomes larger. This makes it desirable for q to\napproximate the leverage score distribution. In practice, such approximations would\nbe obtained by ﬁrst estimating leverage scores (e.g., via the method described in\nSection 6.2.1) and then normalizing according to the estimates. That is, we compute\nˆℓas an estimate of ℓ(A), then set\nqi =\nˆℓi\nPm\nj=1 ˆℓj\n.\nWith this in mind, we turn to our next question: how large should d be so that the\nfailure probability (6.4) tends to zero as n tends to inﬁnity?\narXiv Version 2\nPage 113\n\n\nLeverage Score Sampling\n6.1. Deﬁnitions and background\nAs a short answer, it can be shown that taking d ∈O\n\u0000n log n/rϵ2\u0001\nis suﬃcient\nfor (6.4) to tend to zero as n tends to inﬁnity.1 With (exact) leverage score sampling\nwe are fortunate to have r = 1, and so it suﬃces for the embedding dimension to\nsatisfy\ndlev ∈O\n\u0012n log n\nϵ2\n\u0013\n.\n(6.5)\nTo describe the bound with uniform sampling, we introduce the coherence of A as\nC (A) := m max\ni∈JmK ℓi(A).\nIt is easily be shown that coherence is bounded by n ≤C (A) ≤m and that uniform\nsampling leads to r = n/C (A). In view of these facts, the embedding dimension for\nuniform sampling should be on the order of\ndunif ∈O\n\u0012C (A) log n\nϵ2\n\u0013\n.\nThis is no better than leverage score sampling, and it can be much worse.\nAs a ﬁnal point on the eﬀectiveness of sketching by row selection methods,\nconsider the situation of using approximate leverage scores where we have a bound\nqj ≥βpj(A) for all j. In such a situation we would have β ≤r and setting d = dlev/β\nwould suﬃce to achieve the same guarantees as leverage score sampling.\nPreconditioned leverage score sampling, hidden in plain sight\nMany data-oblivious sketching operators can be described as applying a “rotation”\nand then performing coordinate subsampling. Here are two such examples.\n• A wide d × m Haar sketching operator S can be viewed as a composition of\nan m × m orthogonal matrix followed by a coordinate sampling operator.\n• The diagonal sign ﬂip and the fast trig transform in an SRFT amounts to a\nrotation, and the full action of the SRFT is just applying coordinate sampling\nto the rotated input.\nIn both cases, the rotation acts as a type of preconditioner for sampling, i.e., as\na transformation that converts a given problem into a related form that is more\nsuitable for sampling methods [DM16]. The example of SRFTs is especially infor-\nmative, since using an embedding dimension d ∈O(n log n) suﬃces for a d × m\nSRFT to be a subspace embedding with constant distortion (say, distortion 1/2)\nwith high probability [AMT10].\nFormulas for leverage scores\nThere are many concrete ways to express the leverage scores of a tall m × n matrix\nA. Here is an expression that emphasizes the matrix itself, without making explicit\nreference to its range:\nℓj(A) = A[j, :] (A∗A)† A[j, :]∗.\n(6.6)\n1Technically, this choice of d also gives an explicit rate at which the probability tends to zero,\nbut we do not dwell on that here.\nPage 114\narXiv Version 2\n\n\n6.1. Deﬁnitions and background\nLeverage Score Sampling\nWe can obtain other concrete expressions for the leverage scores by considering\nany matrix U whose columns form an orthonormal basis for U = range(A). For\nexample, this matrix U could be the Q from a QR decomposition or the U from\nthe SVD or any other such matrix. Any such matrix suﬃces since P = UU∗, as the\northogonal projector onto U, satisﬁes\nℓj(A) = ∥U[j, :]∥2\n2 = (UU∗)[j, j].\nThe subspace perspective is useful since it shows that leverage scores are unchanged\nif A is replaced by AA∗. More generally, if A = EF and F has full row-rank then\nthe leverage scores of E match those of A.\n6.1.2\nSubspace leverage scores\nThe standard leverage scores described in Section 6.1.1 are not suitable for low-\nrank approximation. The ﬁrst problem is that it is perfectly reasonable to ask for a\nlow-rank approximation of a matrix that is invertible but has many small singular\nvalues. In such situations both the row and column leverage scores will be uniform,\nand hence contain no information. The second problem is that the map from a\nmatrix to its leverage scores is not locally continuous at A whenever A is rank-\ndeﬁcient. (As a general rule, it is diﬃcult to solve linear algebra problems where\nthe map from problem data to the solution is discontinuous.)\nThese shortcomings can partially be addressed with the concept of subspace\nleverage scores, which are also called rank-k leverage scores and leverage scores\nrelative to the best rank-k approximation; see [DMM+12, §5] along with [DMM08]\nas an earlier conference version of the same.\nExpressing the m × n matrix A by its compact SVD, A = UΣV∗, the rank-k\nleverage scores for its range are\nℓk\nj (A) = ∥U[j, :k]∥2\n2.\nNote that the rank-k leverage scores can be nonuniform regardless of the aspect\nratio of the matrix. Indeed, so long as k < rank(A), the rank-k leverage scores of\nboth range(A) and range(A∗) can be nonuniform. The problem of discontinuity of\nthe map from a matrix to its rank-k subspace leverage scores can still persist. More\ngenerally, there is a problem that a matrix may admit multiple distinct “best rank-k\napproximations” for a given value of k. These problems are less troublesome if one\nassumes that the kth spectral gap σk(A) −σk+1(A) is bounded away from zero.\n(This assumption is perhaps more often made than well-justiﬁed.) Alternatively,\none can consider how well the computed scores approximate the leverage scores for\nsome “nearby” rank-k space [DMM+12].\nLet us turn to how subspace leverage scores are used. Continuing to focus on\nthe case of row sampling, we are interested in the rank-k leverage score distribution\npk\nj (A) =\nℓk\nj (A)\nPm\ni=1 ℓk\ni (A).\nIf S denotes a d × m row-sampling operator induced by pk(A), then the sketch\nY = SA leads naturally to the approximation ˆA = AY†Y. Letting Ak denote some\nbest-rank-k approximation of A in a unitarily invariant matrix norm “∥· ∥,” it is\npossible to choose d suﬃciently large so that\n∥A −ˆA∥≲∥A −Ak∥\n(6.7)\narXiv Version 2\nPage 115\n\n\nLeverage Score Sampling\n6.1. Deﬁnitions and background\nholds with high probability. Note that if Y were an arbitrary matrix then it would\nbe possible to choose Y so that the projection ˆA = AY†Y was equal to some best-\nrank-k approximation of A. However, the restriction that the rows of Y are scaled\nrows of A signiﬁcantly limits the projectors that could be used to deﬁne ˆA. Because\nof this limitation, one may need d ≫k to have any chance that (6.7) holds.\nRemark 6.1.2. One rarely samples according to an exact rank-k leverage score dis-\ntribution in practice. Rather, one uses randomized algorithms to approximate them.\nThe key fact that enables this approximation is that leverage scores (“standard” or\n“subspace”) are preserved if we replace A by AA∗. Moreover, as leverage scores\nquantify a notion of eigenvector localization, we should note that in many applica-\ntions one has domain knowledge that eigenvalues should be localized [SCS10], and\nthis could be used to construct approximations.\n6.1.3\nRidge leverage scores\nRidge leverage scores are used to approximate matrices in the presence of explicit\nregularization. That is, we are given an m × m psd matrix K and a positive reg-\nularization parameter λ, and we approximate K + λI by ˆK + λI where K is a psd\nmatrix of rank n ≪m. The low-rank structure in these approximations makes it\nmuch cheaper to apply (ˆK + λI)−1 compared to (K + λI)−1. This motivates the\nfollowing question.\nWhat rank n is needed for (ˆK + λI)−1 to approximate (K + λI)−1 up to\nsome ﬁxed accuracy?\nIt turns out that this is determined by quantity tr(K(K + λI)−1), which is called\nthe eﬀective rank of K. Using µi to denote the ith-largest eigenvalue of K, we can\nexpress the eﬀective rank as\ntr(K(K + λI)−1) =\nm\nX\ni=1\nµi\nµi + λ.\nSince we are working with psd matrices it is natural to deﬁne ˆK as a Nystr¨om\napproximation of K with respect to some sketching operator S (see Section 4.2.2).\nTaking that as given, this leaves the question of how to choose the distribution for S.\nHere it is worth considering how many numerically-low-rank psd matrices arising in\napplications are deﬁned implicitly through pairwise evaluations of a kernel function\non a given dataset. Taking S as a column-selection operator is especially appealing\nin these settings.\n[AM15] introduced ridge leverage scores as a framework for data-aware column\nsampling in this context. Formally, the ridge leverage scores of (K, λ) are\nℓi(K; λ) =\n\u0010\nK (K + λI)−1\u0011\n[i, i].\n(6.8)\nIn certain cases – particularly for estimating ridge leverage scores – it can be conve-\nnient to express these quantities in terms of a matrix B that satisﬁes K = BB∗and\nthat has at least as many rows as columns. Speciﬁcally, by expressing B in terms\nof its compact SVD, one can show that\nℓi(K; λ) = b∗\ni (B∗B + λI)−1 bi\n(6.9)\nwhere b∗\ni is the ith row of B. We note that the identity matrix appearing in (6.9)\nwill be smaller than that from (6.8) if B is not square.\nPage 116\narXiv Version 2\n\n\n6.2. Approximation schemes\nLeverage Score Sampling\n6.2\nApproximation schemes\nComputing leverage scores exactly is an expensive proposition. If A is a tall m × n\nmatrix, then it takes O(mn2) time to compute the standard leverage scores exactly.2\nIf one is interested in subspace leverage scores and k is small, then one can in\nprinciple use Krylov methods to approximate the dominant k singular vectors in\nfar less than O(mn2) time. Such methods are not very reliable for producing good\napproximations of the truncated SVD, but they might suﬃce for estimating leverage\nscores. If we want to compute the ridge leverage scores of an m×m matrix K exactly,\nthen the straightforward implementation takes O(m3) time.\nThese facts necessitate the development of eﬃcient and reliable methods for\nleverage score estimation, which we discuss below. While these methods are gener-\nally too sophisticated for the RandBLAS, they may be appropriate for higher-level\nlibraries such as RandLAPACK.\n6.2.1\nStandard leverage scores\nSuppose the m×n matrix A is very tall, i.e., m ≫n. Here we summarize a method\nby Drineas et al. that can compute approximate leverage scores, to within a constant\nmultiplicative error factor, in O(mn log m) time, i.e., in roughly the time it takes\nto implement a random projection, with some constant failure probability bounded\naway from one [DMM+12]. This can oﬀer improved eﬃciency over straightforward\nO(mn2) approaches when m ≫n and yet m ∈o(2n).\nWe set the stage for this method by expressing leverage scores as follows\nℓj(A) = ∥δ∗\nj U∥2\n2 = ∥δ∗\nj UU∗∥2\n2 = ∥δ∗\nj AA†∥2\n2\n(6.10)\nwhere we note that the second equality in the above display follows from unitary\ninvariance of the spectral norm. The method proceeds by approximating two opera-\ntions in the right-most expression in (6.10). First we approximate the pseudoinverse\nof A and then we approximate the matrix-matrix product AA†. It is important to\nnote that using approximations in both steps is essential for asymptotic complexity\nimprovements, since traditional methods would take O(mn2) for the ﬁrst step and\nO(m2n) time for the second step. (In extreme situations, depending on the hard-\nware that would be used, it may be worth performing the matrix-matrix product\nof the second step explicitly.)\nThe pseudoinverse computation is approximated by applying a wide d1 × m\nSRFT S1 to the left of A. Letting U1Σ1V∗\n1 be an SVD of this d1 × n sketched\nmatrix S1A, we approximate\nℓj(A) ≈ˆℓj(A) = ∥δ∗\nj A(S1A)†∥2\n2\n= ∥δ∗\nj AV1Σ−1\n1 U∗\n1∥2\n2\n= ∥δ∗\nj AV1Σ−1\n1 ∥2\n2\nat a cost of O(d1n2). However, we are not out of the woods yet, since multiplying A\nwith V1Σ−1\n1\nwould still cost O(mn2). This is addressed by applying a tall sketching\n2The preferred way to do this would be to take the row norms of the factor Q from a thin QR\ndecomposition of A.\narXiv Version 2\nPage 117\n\n\nLeverage Score Sampling\n6.2. Approximation schemes\noperator S2 of size n × d2 to the right of V1Σ−1\n1\nbefore multiplying it by A. That\nis, we further approximate\nˆℓj(A) ≈ˆˆℓj(A) = ∥δ∗\nj A(V1Σ−1\n1 S2)∥2\n2.\n(6.11)\nThis reduces the cost of the matrix multiplication to O(mnd2) and hence the cost of\nthe overall procedure to O(d1n2 + d2mn). [DMM+12] gives details on how large d1\nand d2 must be to ensure useful accuracy guarantees for the approximate leverage\nscores; see also [MMY15, §5.2] for a related evaluation.\nThis estimation method can be adapted to eﬃciently compute “cross-leverage\nscores,” as well as subspace leverage scores; see [DMM+12] for details. It also has\nnatural adjustments to make it faster. For example, [CW17] suggest replacing the\nSRFT S1 by ˜S1 = FC where C is a CountSketch and F is an SRFT that further\ncompresses the output of C; [NN13] propose replacing S1 by a SASO (recall from\nSection 2.4.1 that a SASO is generalized CountSketch), which yields a similar speed-\nup as that achieved in [CW17].\n6.2.2\nSubspace leverage scores\nThere is a wide range of possibilities for estimating subspace leverage scores. We\ndescribe two such methods here (slightly adapted) from [DMM+12]. Let us say that\nwe want to estimate the rank-k leverage scores of A for some k ≪min{m, n}. Both\nof the algorithms below work by ﬁnding the exact leverage scores of an implicit\nrank-k matrix ˆA, for which a distance ∥ˆA −A∥is near-optimal among all rank-k\napproximations.\nAn adaptation of [DMM+12, Algorithm 5]\nThe original goal of this algorithm was to return the leverage scores of a rank-k\napproximation of A that was near-optimal in Frobenius norm. Framing things more\nabstractly, the approach requires that the user specify an oversampling parameter\ns ∈O(k). Its ﬁrst step is to compute a rank-(k + s) QB decomposition of A (e.g.,\nby some method from Section 4.3.2) A ≈QB. Next, it computes the top k left\nsingular vectors of B by some traditional method. Letting Uk denote the (k +s)×k\nmatrix of such leading left singular vectors, the algorithm takes the columns of QUk\nto deﬁne approximations of the leading k left singular vectors of A. The row-norms\nof this matrix deﬁne the approximate rank-k leverage scores.\nIn context, [DMM+12, Algorithm 5] used an elementary QB decomposition with\nQ = orth(AS) for an n×(k+s) Gaussian operator S. The analysis of this algorithm\npresumed that s ≥⌈k/ϵ + 1⌉for some tolerance parameter ϵ. The meaning of ϵ was\nas follows: when viewed as random variables, the returned leverage scores coincide\nwith those of a rank-k approximation ˆA where\nE∥ˆA −A∥2\nF ≤(1 + ϵ)\nX\nj>k\nσj(A)2.\nLooking back at this error bound from our present perspective, it is clear that\na huge variety of similar bounds can be obtained by using diﬀerent methods for\nthe QB decomposition. One possibility on this front would be to use adaptive QB\nalgorithms that approximate A to some prescribed accuracy.\nSubspace leverage\nscores obtained in this way may be well-suited for approximating A by a low-rank\nsubmatrix-oriented decomposition up to prescribed accuracy.\nPage 118\narXiv Version 2\n\n\n6.2. Approximation schemes\nLeverage Score Sampling\nA description of [DMM+12, Algorithm 4]\nThis is a two-stage method to ﬁnd the leverage scores of a rank-k approximation to\nA that is near-optimal in spectral norm.\nTo understand the ﬁrst stage, recall that some of the simplest QB algorithms\nmake use of power iteration as described in Section 4.3.1. That is, rather than\nsetting Q = orth(AS) for Gaussian S, they set S = (A∗A)qS0 for Gaussian S0.\nPractical implementations of QB based on power iteration introduce stabilization\nbetween successive applications of A and A∗. Such stabilization preserves the range\nof AS, but it may change its singular vectors. If such stabilization is not used,\nthen the left singular vectors of A(A∗A)qS0 for Gaussian S0 would be reasonable\napproximations to the leading left singular vectors of A (modulo numerical problems\nthat are sure to arise for moderate q).\nThe observation above is the basis for [DMM+12, Algorithm 4]. In context, its\nﬁrst stage is to compute Sq+1 = (AA∗)qAS0 from an n × 2k Gaussian operator S0.\nIn a second stage, approximate leverage scores of Sq+1 – call them ˆℓi – are obtained\nfrom any method that ensures\n|ˆℓi −ℓi(Sq+1)| ≤ϵ ℓi(Sq+1).\nThese approximations are the estimates for the rank-k leverage scores of A.\n[DMM+12, Lemma 15 and Theorem 16] prescribe a value for q (as a function\nof m, n, k, and ϵ) that ensures an approximation guarantee for the leverage score\nestimates given above.\nSpeciﬁcally, for the prescribed q, the estimated leverage\nscores are within a factor\n1−ϵ\n2(1+ϵ) of the leverage scores of a rank-k matrix ˆA that\nsatisﬁes\nE∥ˆA −A∥2 ≤(1 + ϵ/10)σk+1(A).\nAs before, the randomness in this expectation is over the randomness used to esti-\nmate the leverage scores.\n6.2.3\nRidge leverage scores\nA wide variety of algorithms have been devised to estimate ridge leverage scores or\ncarry out approximate ridge leverage score sampling. The simplest such algorithm,\nproposed in [AM15] alongside the deﬁnition of ridge leverage scores, proceeds as\nfollows:\n• Start with a distribution p = (pi)i∈JmK over the column index set of K.\n• Construct a column selection operator S with n columns, where each column\nis independently set to δi ∈Rm with probability pi.\n• Compute the Nystr¨om approximation of K with respect to S. Suppose the\napproximation is represented as ˆK = BB∗for an m × n matrix B.\n• Using bi ∈Rn for the ith row of B, take ˜ℓi := b∗\ni (B∗B + λI)−1bi as an\napproximation for the ith ridge leverage score of K with regularization λ.\nOne can of course start with p = (1/m)i∈JmK as the uniform distribution over\ncolumns of K. An alternative starting point is the distribution p = diag(K)/ tr(K).\nWhile the latter distribution can lead to useful theoretical guarantees (see [AM15,\nTheorem 4]) it is not suitable for computing very accurate approximations.\narXiv Version 2\nPage 119\n\n\nLeverage Score Sampling\n6.3. Special topics and further reading\nIterative methods should be used if accurate approximations to ridge leverage\nscores are desired. Notably, most of the iterative methods in the literature simul-\ntaneously estimate the ridge leverage scores and sample columns from K according\nto the estimates [MM17; CLV17; RCC+18]. This algorithmic structure blurs the\ndistinction between approximating ridge leverage scores and producing a Nystr¨om\napproximation of K via column selection. This precise nature of the blurring can\nalso vary substantially from one algorithm to another. For example, [MM17, Al-\ngorithms 2 and 3] are very diﬀerent from [CLV17, Algorithm 1], which in turn is\nmaterially diﬀerent from [RCC+18, Algorithms 1 and 2].\nThe abundance and sophistication of these methods make it impractical for us\nto summarize them here.\nWe instead settle for stating their general qualitative\nconclusions. Letting d = tr(K(K + λI)−1) denote the eﬀective rank of K, one can\nconstruct an approximation ˆK of rank n ∈O(d log d) for which ∥K −ˆK∥2 ≤λ holds\nwith high probability. Furthermore, these approximations can be constructed in\ntime O(mn2) using only n column samples from K. We refer the reader to the cited\nworks above for details on speciﬁc algorithms.\n6.3\nSpecial topics and further reading\nHere, we mention a handful of generalizations and variants of leverage score sam-\npling that, while not part of our immediate plans, may be of longer-term inter-\nest. The interested reader should consult the source material for details of what\nwe describe below.\nIn addition to those source materials, the interested reader\nis referred to Sobczyk and Gallopoulos’ paper [SG21], which is accompanied by\na carefully developed C++ and Python library called pylspack [SG22]. We also\nrecommend Larsen and Kolda’s recent work [LK20, §4 and Appendix A] – which\nincludes practical advice on leverage score sampling and theoretical results with\nexplicit constant factors.\n6.3.1\nLeverage score sparsiﬁed embeddings\nOur concept of long-axis-sparse operators from Section 2.4.2 is based on the Leverage\nScore Sparsiﬁed or LESS embeddings of Derezi´nski et al.\n[DLD+21].\nHere we\nexplain the role of leverage scores when using these sketching operators.\nLet S be a a random d × m long-axis-sparse operator (d ≪m) with sparsity\nparameter k and sampling distribution p = (p1, . . . , pm). The idea of LESS embed-\ndings is that varying k should provide a way to interpolate between the low cost\nof sketching by row sampling and the high cost of sketching by Gaussian opera-\ntors, while still obtaining a sketch that is meaningfully Gaussian-like. Indeed, if\nk ≈n = rank(A), then [Der22a] showed that, with high probability, the resulting\nsketching operator is nearly indistinguishable from a dense sub-gaussian operator\n(such as Gaussian or Rademacher), despite the reduction from O(dmn) time to\nO(dn2).\nThis performance comparison was demonstrated for several estimation\ntasks involving the inverse covariance matrix A∗A [DLD+21], as well for the New-\nton Sketch optimization method [DLP+21].\nAs with other uses of leverage scores, approximate leverage scores suﬃce for\nLESS embeddings; and the computational cost of a LESS embedding is typically\ndominated by the cost of estimating the leverage scores of A. The use of leverage\nscores in the sparsiﬁcation pattern is essential for theoretically showing that a LESS\nPage 120\narXiv Version 2\n\n\n6.3. Special topics and further reading\nLeverage Score Sampling\nembedding exhibits nearly identical performance to a Gaussian operator for all\nmatrices A. Good empirical performance observed in practice, to a varying degree,\nalso when p is the uniform distribution and k ≪rank(A) [DLP+21, §5].\n6.3.2\nDeterminantal point processes\nIn many data analysis applications, submatrix-oriented decompositions such as\nNystr¨om approximation via column selection are desirable for their interpretability.\nIn this context, we may wish to produce a very small but high-quality sketch of the\nmatrix A, using a method more reﬁned (albeit slower) than leverage score sampling.\nHere we discuss Determinantal Point Processes (DPPs; [DM21a]) as one of many\nsuch methods from the literature.\nLet A be an m×m psd matrix. A Determinantal Point Process is a distribution\nover index subsets J ⊆JmK such that:\nP(J = S) = det(A[S, S])\ndet(A + I) .\nThe above DPP formulation is known as an L-ensemble, and it is also sometimes\ncalled volume sampling [DRV+06; DM10]. Unlike leverage score sampling, individ-\nual indices sampled in a DPP are not drawn independently, but rather jointly, to\nminimize redundancies in the sampling process. In fact, a DPP can be viewed as\nan extension of leverage score sampling that incorporates dependencies between the\nsamples, inducing diversity in the selected subset [KT12].\nDPP sampling can be used to construct improved Nystr¨om approximations\nˆA = (AS)(S∗AS)†(AS)∗where the selection matrix S corresponds to the random\nsubset J. In particular, [DRV+06; GS12; DKM20] established strong guarantees\nfor this approach in terms of the nuclear norm error relative to the best rank k\napproximation: ∥ˆA −A∥∗≤(1 + ϵ)∥Ak −A∥∗, where k is the target rank and\nthe subset size |J| is chosen to be equal or slightly larger than k. DPPs have also\nfound applications in machine learning [KT12; DKM20; DM21a] as a method for\nconstructing diverse and interpretable data representations.\nIt is challenging to implement eﬃcient methods for sampling from a DPP, and\nthis is an area of ongoing work. One promising method has recently been proposed\nby Poulson [Pou20]. Two other classes of methods can be obtained by exploiting\nthe connection between DPPs and ridge leverage scores.\n1. One can use intermediate sampling with ridge leverage scores to produce a\nlarger index set T, which is then trimmed down to produce a smaller DPP\nsample J ⊆T [Der19; DCV19; CDV20].\n2. One can use iterative reﬁnement on a Markov chain, where we start with\nan initial subset J1, and then we gradually update it by swapping out one\nindex at a time, producing a sequence of subsets J1, J2, J3, ..., which rapidly\nconverges to a DPP distribution [AGR16; AD20; ADV+22].\nThe computational cost of these procedures is usually dominated by the cost of\nestimating the ridge leverage scores (recall that there are methods for doing this that\ndo not need to access all of A). However, these procedures carry additional overhead\nsince some of the ridge leverage score samples must be discarded to produce the\nﬁnal DPP sample.\narXiv Version 2\nPage 121\n\n\nLeverage Score Sampling\n6.3. Special topics and further reading\n6.3.3\nFurther variations on leverage scores\nIn the case of tall data matrices A, leverage scores are useful for ﬁnding data-aware\nsketching operators S so that the Euclidean norms of vectors in the range of SA\nare comparable to the Euclidean norms of vectors in the range of A. A related\nconcept called Lewis weights can be used for matrix approximation where we want\nS to approximately preserve the p-norm of vectors in the range of A for some p ̸= 2\n[CP15]. These are improved versions of leverage-like scores used by Dasgupta et al.\nfor ℓp regression [DDH+09]. A more recently proposed concept samples according\nto the probabilities\npi =\n\r\r\r(A∗A)−1 A[i, :]\n\r\r\r\n2\nPm\nj=1\n\r\r\r(A∗A)−1 A[j, :]\n\r\r\r\n2\nin order to estimate the variability of sketch-and-solve solutions to overdetermined\nleast squares problems [MZX+22]; see also [MMY15] for related importance sam-\npling probabilities that come with useful statistical properties. Similar probabilities\n(where all norms in the above expression were squared) were studied in [DCM+19].\nPage 122\narXiv Version 2\n\n\nSection 7\nAdvanced Sketching:\nTensor Product Structures\n7.1 The Kronecker and Khatri–Rao products ................... 124\n7.2 Sketching operators .................................................... 125\n7.2.1 Row-structured tensor sketching operators .................... 125\n7.2.2 The Kronecker SRFT ................................................ 126\n7.2.3 TensorSketch ............................................................ 127\n7.2.4 Recursive sketching ................................................... 127\n7.2.5 Leverage score sampling for implicit matrices with tensor\nproduct structures .................................................... 128\n7.3 Partial updates to Kronecker product sketches .......... 130\n7.3.1 Background on the CP decomposition .......................... 131\n7.3.2 Sketching for the CP decomposition ............................. 132\n7.3.3 Background on the Tucker decomposition ..................... 133\n7.3.4 Sketching for the Tucker decomposition ........................ 134\n7.3.5 Implementation considerations .................................... 134\nThis section considers eﬃcient sketching of data with tensor product structure.\nWe speciﬁcally focus on implicit matrices with Kronecker and Khatri–Rao product\nstructure. These structures are of interest in RandNLA due to their prominent role\nin certain randomized algorithms for tensor decomposition. A secondary point of\ninterest is that the operators discussed in this section can also be used for sketching\nunstructured matrices. They may, for example, be used as alternatives to unstruc-\ntured test vectors in norm and trace estimation [BK21]. In this case, the main\nbeneﬁt would not be improved speed but reduced storage requirements for storing\nthe sketching operator.\nIn Section 7.1 we deﬁne the Kronecker and Khatri–Rao matrix products. Sec-\ntion 7.2 presents four families of sketching operators that can be applied eﬃciently\nto matrices that are stored implicitly with these product structures. Section 7.3 dis-\ncusses implementation considerations for the structured sketching operators in this\nsection, with a focus on how they can be used in tensor decomposition algorithms.\n123\n\n\nTensor Product Structures\n7.1. The Kronecker and Khatri–Rao products\nA note on scope\nWe should emphasize that algorithms for general tensor computations are out-of-\nscope for RandLAPACK. The functionality described here would only be made avail-\nable as utility functions (i.e., computational routines) for facilitating certain tensor\ncomputations. This is part of a broader idea that RandLAPACK should facilitate\nadvanced sketching operations of interest in RandNLA that are outside the scope\nof the RandBLAS.\n7.1\nThe Kronecker and Khatri–Rao products\nSuppose that B is an m×n matrix and C is a p×q matrix. The Kronecker product\nof B and C is the mp × nq matrix\nB ⊗C =\n\n\nB[1, 1] · C\nB[1, 2] · C\n· · ·\nB[1, n] · C\nB[2, 1] · C\nB[2, 2] · C\n· · ·\nB[2, n] · C\n...\n...\n...\nB[m, 1] · C\nB[m, 2] · C\n· · ·\nB[m, n] · C\n\n.\nIf B and C have the same number of columns (i.e., if n = q), then their Khatri–Rao\nproduct is the mp × n matrix\nB ⊙C =\n\u0002B[:, 1] ⊗C[:, 1]\nB[:, 2] ⊗C[:, 2]\n· · ·\nB[:, n] ⊗C[:, n]\u0003\n.\nThe Khatri–Rao product is sometimes also referred to as the matching columnwise\nKronecker product for transparent reasons. The Kronecker and Khatri–Rao prod-\nucts for more than two matrices are deﬁned in the obvious way. Note that for two\nvectors x and y we have that\nx ⊗y = x ⊙y = vec(x ◦y)\nwhere ◦denotes the outer product and vec(·) is an operator that turns a matrix\ninto a vector by vertically concatenating its columns. We also use ⊛to denote the\nelementwise (Hadamard) product.\nMatrices with Kronecker and Khatri–Rao product structure tend to be very\nlarge. For example, consider matrices B1, . . . , BL, all of size m × n. Their Kro-\nnecker product B1 ⊗· · · ⊗BL is an mL × nL matrix and their Khatri–Rao product\nB1 ⊙· · · ⊙BL is an mL × n matrix.\nThe exponential dependence on L means\nthat these products can become very large even if the matrices B1, . . . , BL are not\nespecially large. Even just forming and storing these products may therefore be\nprohibitively expensive.\nKronecker and Khatri–Rao product matrices feature prominently in algorithms\nfor tensor decomposition (i.e., decomposition of multidimensional arrays into sums\nand products of more elementary objects, see Section 7.3). They also appear in a\nvariety of other contexts when sketching techniques are helpful, such as for repre-\nsentation of polynomial kernels [PP13; ANW14; WZ20; WZ22], when ﬁtting poly-\nnomial chaos expansion models in surrogate modeling [TNX15; SNM17; CMX+22],\nmulti-dimensional spline ﬁtting [DSS+18], and in PDE inverse problems [CLN+20].\nPage 124\narXiv Version 2\n\n\n7.2. Sketching operators\nTensor Product Structures\n7.2\nSketching operators\nSection 7.2.1 introduces sketching operators that are distinguished by having rows\nwith particular structures. Section 7.2.2 discusses a variant of the SRFT with an ad-\nditional tensor-produce structure. Section 7.2.3 discusses TensorSketch operators,\nwhich are analogous to CountSketch operators from Section 2.4.1. In Section 7.2.4\nwe describe sketching operators that are recursive and have multi-stage structure.\nThese incorporate some of the sketching operators discussed in the previous subsec-\ntions as stepping stones. Section 7.2.5 covers row sampling methods for tall matrices\nwith tensor product structure.\nWe note that the sketching operators in Sections 7.2.1–7.2.4 are all oblivious,\nwhereas the sampling-based methods in Section 7.2.5 are not. We also note that\nall of the oblivious sketching operators we discuss could be applied to unstructured\nmatrices. This would yield no speed beneﬁt compared to using their unstructured\ncounterparts, but it would reduce the storage requirement compared to traditional\ndense sketching operators of the kind supported by the RandBLAS.\n7.2.1\nRow-structured tensor sketching operators\nHere we describe three types of sketching operators whose rows can be applied to\nKronecker and Khatri–Rao product matrices very eﬃciently. The second of these\nmethods requires notions of tensor representations such as the CP format, which\nwe will revisit in Section 7.3.\nKhatri–Rao products of elementary sketching operators\nThe most basic row-structured sketching operator takes the form\nS = (S1 ⊙S2 ⊙· · · ⊙SL)∗,\n(7.1)\nwhere each Sk is an appropriate random matrix of size mk × d for k ∈JLK. Such\nan operator maps (m1 · · · mL)-vectors to d-vectors. It can be eﬃciently applied to\nKronecker product vectors, which in turn means that it can be applied eﬃciently\n(column-wise) to both Kronecker and Khatri–Rao product matrices. Consider vec-\ntors x1, . . . , xL where xk is a length-mk vector. The operator in (7.1) is then applied\nto a vector v = x1 ⊗· · · ⊗xL via the formula\nSv = (S∗\n1x1) ⊛(S∗\n2x2) ⊛· · · ⊛(S∗\nLxL).\nTo the best of our knowledge, [BBB15] were the ﬁrst to use random matrices\nof the form (7.1) to accelerate tensor computations in the spirit of RandNLA.1\nThey suggest drawing the entries of each Sk independently from a distribution with\nmean zero and unit variance, but they do not provide any theoretical guarantees for\nthe performance of such sketching operators. Sun et al. [SGT+18] independently\npropose using operators of the form (7.1) where the submatrices Sk are chosen\nto be either Gaussian or sparse operators. They also propose a variance-reduced\nmodiﬁcation which is an appropriate rescaling of the sum of several maps of the\nform (7.1). They provide theoretical guarantees for sketching operators in (7.1)\nwith L = 2 (and its variance-reduced modiﬁcation) when S1 and S2 have entries\nthat are drawn independently from an appropriately scaled mean-zero sub-Gaussian\ndistribution, leaving analysis for the case when L > 2 open for future work.\n1Similar ideas were used earlier for applications in diﬀerential privacy; see [KRS+10; Rud12].\narXiv Version 2\nPage 125\n\n\nTensor Product Structures\n7.2. Sketching operators\nRow-wise vectorized tensors\nRakhshan and Rabusseau [RR20] propose a distribution of sketching operators for\nwhich the ith row is given by S[i, :] = vec(Xi)∗, where Xi is a tensor in some\nfactorized format and vec is a function that returns a vectorized version of its input\nas a column vector (vec(X) = X(:) in Matlab notation). More speciﬁcally, they\nconsider two cases: In the ﬁrst case, Xi is in CP format and deﬁned elementwise\nvia\nXi[j1, j2, . . . , jL] =\nR\nX\nr=1\na(i,1)\nr\n[j1] · a(i,2)\nr\n[j2] · · · a(i,L)\nr\n[jL]\n(7.2)\nwhere the vector entries a(i,n)\nr\n[jn] are drawn independently from an appropriately\nscaled Gaussian distribution.\nIn the second case, Xi is in so-called tensor train\nformat and deﬁned elementwise via\nXi[j1, j2, . . . , jL] = A(i,1)\nj1\nA(i,2)\nj2\n· · · A(i,L)\njL\n,\n(7.3)\nwhere L is the number of tensor modes, and each matrix A(i,n)\njn\nis of size Rn ×Rn+1\nwhere R1 = RL+1 = 1 to ensure that the product is a scalar. The entries of A(i,n)\njn\nare drawn independently from an appropriately scaled Gaussian distribution.\nFor both of the constructs described above, the inner product of vec(Xi) and\nKronecker product vectors can be computed eﬃciently due to the special structure\nof the CP and tensor train formats. This makes eﬃcient application of the operator\nto Kronecker and Khatri–Rao product matrices possible. Theoretical guarantees are\nprovided for these vectorized tensor sketching operators in [RR20]. The follow-up\nwork [RR21] shows that the results for the tensor train-based sketching operators\nalso extend to the case when the cores are drawn from a Rademacher distribution.\nTwo-stage operators\nIwen et al. [INR+20] propose a two-stage sketching procedure for mapping (m1 · · · mL)-\nvectors to d-vectors. The ﬁrst step consists of applying a row-structured matrix\n(S1 ⊗· · · ⊗SL), where each Sk is a sketching operator of size pk × mk. This maps\nthe (m1 · · · mL)-vector to an intermediate embedding space of dimension (p1 · · · pL).\nThis is then followed by another sketching operator T of size d × (p1 · · · pL) which\nmaps the intermediate representation to the ﬁnal d-dimensional space.\n7.2.2\nThe Kronecker SRFT\nKronecker SRFTs are a variant of the SRFTs discussed in Section 2.5. They can be\napplied very eﬃciently to a Kronecker product vector without forming the vector\nexplicitly. They were ﬁrst proposed by [BBK18] for eﬃcient sketching of the Khatri–\nRao product matrices that arise in tensor CP decomposition. Theoretical analysis\nof these sketching operators can be found in [JKW20; MB20; BKW21].\nThe Kronecker SRFT that maps (m1 · · · mL)-vectors to d-vectors takes the form\nS =\nrm1 · · · mL\nd\nR\n\u0010\nL\nO\nk=1\nFk\n\u0011\u0010\nL\nO\nk=1\nDk\n\u0011\n,\n(7.4)\nwhere each Dk is a diagonal mk × mk matrix of independent Rademachers, each Fk\nis an mk ×mk fast trigonometric transform, and R randomly samples d components\nPage 126\narXiv Version 2\n\n\n7.2. Sketching operators\nTensor Product Structures\nfrom an (m1 · · · mL)-vector. The Kronecker SRFT replaces the F and D operators\nin the standard SRFT by Kronecker products of smaller operators of the same form.\nWith x1, . . . , xL deﬁned as in Section 7.2.1, the operator in (7.4) can be applied\neﬃciently to x1 ⊗· · · ⊗xL via the formula\nrm1 · · · mL\nd\nR\n\u0010\nL\nO\nk=1\nFk\n\u0011\u0010\nL\nO\nk=1\nDk\n\u0011\u0010\nL\nO\nk=1\nxk\n\u0011\n=\nrm1 · · · mL\nd\nR\n\u0010\nL\nO\nk=1\nFkDkxk\n\u0011\n.\nThe formula shows that only those entries in N\nk FkDkxk that are sampled by R\nneed to be computed. From this, we can back out the indices of each vector FkDkxk\nthat need to be computed. Given these indices one could compute the relevant\nentries of these vectors using subsampled FFT methods of the kind alluded to in\nSection 2.5. We note that this formula is straightforwardly extended to Kronecker\nand Khatri–Rao product matrices.\n7.2.3\nTensorSketch\nA TensorSketch operator is a kind of structured CountSketch that can be applied\nvery eﬃciently to Kronecker product matrices.2 The improved computational eﬃ-\nciency of TensorSketch comes at the cost of needing a larger embedding dimension\nthan CountSketch. TensorSketch was ﬁrst proposed in [Pag13] for fast approximate\nmatrix multiplication. It was further developed in [PP13; ANW14; DSS+18] where\nit is used for low-rank approximation, regression, and other tasks.\nLet x1, . . . , xL be deﬁned as in Section 7.2.1. A TensorSketch, which we denote\nby S below, maps an (m1 · · · mL)-vector v = x1 ⊗· · · ⊗xL to a d-vector via the\nformula\nSv = DFT−1 \u0010\nL⊛\nk=1\nDFT(Skxk)\n\u0011\n,\n(7.5)\nwhere each Sk is an independent CountSketch that maps mk-vectors to d-vectors.\nHere, DFT denotes the discrete Fourier transform which can be eﬃciently applied\nusing fast Fourier transform (FFT) methods.\nTensorSketches use the fact that\npolynomials can be multiplied using the DFT, which is why DFT and its inverse\nappear in the formula above; see [Pag13] for details.\nRemark 7.2.1. We have not investigated whether fast trig transforms other than\nthe discrete Fourier transform (e.g., the discrete cosine transform) can be used for\nthis type of sketching operator.\n7.2.4\nRecursive sketching\nIn order to achieve theoretical guarantees, the sketching operators discussed so far\nrequire an embedding dimension d which scales exponentially with L when embed-\nding a vector of the form x1 ⊗· · · ⊗xL. Ahle et al. [AKK+20] propose sketching\noperators that are computed recursively and have the remarkable property that\ntheir requisite embedding dimensions scale polynomially with L. Since [AKK+20]\nare concerned with oblivious subspace embedding of polynomial kernels, they con-\nsider the case when all x1, . . . , xL are of the same length. However, their results\n2Recall that a CountSketch is a SASO in the sense of Section 2.4.1. Each short-axis vector in\na CountSketch has a single nonzero entry, sampled from the Rademacher distribution.\narXiv Version 2\nPage 127\n\n\nTensor Product Structures\n7.2. Sketching operators\nshould extend to the general case when the vectors are of diﬀerent lengths (for\nexample, see [Mal22, Corollary 18]).\nSuppose x1, . . . , xL are m-vectors and that L = 2q for a positive integer q. The\nrecursive sketching operator ﬁrst computes\ny(0)\nk\n= Tkxk\nfor\nk ∈JLK\nwhere T1, . . . , TL are independent SASOs (e.g., CountSketches, see Section 2.4.1)\nthat map m-vectors to d-vectors. The d-vectors y(0)\n1 , . . . , y(0)\nL\nare now combined\npairwise into L/2 = 2q−1 vectors. This is done by computing\ny(1)\nk\n= Sk(y(0)\n2k−1 ⊗y(0)\n2k )\nfor\nk ∈JL/2K\nwhere S1, . . . , SL/2 are independent sketching operators that map d2-vectors to d-\nvectors. If the initial T1, . . . , TL are CountSketches then the Si are canonically Ten-\nsorSketches. If instead T1, . . . , TL are more general SASOs then the Si are canoni-\ncally Kronecker SRFTs. Regardless of which conﬁguration we use, the pairwise com-\nbination of vectors is repeated for a total of q = log2(L) steps until a single d-vector\nremains, which is the embedding of x1 ⊗· · · ⊗xL. The case when L is not a power\nof two is handled by adding additional vectors xk = e1 for k = L + 1, . . . , 2⌈log2(L)⌉\nwhere e1 is the ﬁrst standard basis vector in Rm. Recursive sketching operators are\nlinear despite their somewhat complicated description.\nSong et al. [SWY+21] develop a similar recursive sketching operator which takes\ninspiration from the one discussed above and applies it to the sketching of polyno-\nmial kernels. For the degree-L polynomial kernel, this involves sketching of matrices\nof the form A⊙L = A ⊙· · · ⊙A, where the matrix A appears L times in the right-\nhand side.\nThe recursive sketching operator by [AKK+20] can be described by a binary\ntree, with each node corresponding to an appropriate sketching operator. Ma and\nSolomonik [MS22] generalize this idea by allowing for other graph structures, but\nlimit nodes in these graphs to be associated with Gaussian sketching operators.\nUnder this framework, they develop a structured sketching operator whose embed-\nding dimension only scales linearly with L. These operators can be adapted for\neﬃcient application to vectors with general tensor network structure which includes\nKronecker products of vectors as a special case.\n7.2.5\nLeverage score sampling for implicit matrices with tensor\nproduct structures\nConsider the problem of sketching and solving a least squares problem\nmin\nx\n∥AX −Y∥F\n(7.6)\nwhen the columns of A have tensor product structure and Y is a thin unstructured\nmatrix. The sketching operators discussed so far in this section can be eﬃciently\napplied to A. However, since Y lacks structure, these sketching operators require\naccessing all nonzero elements of Y. This can be prohibitively expensive in appli-\ncations such as the following.\n• In iterative methods for tensor decomposition, one typically solves a sequence\nof least squares problems for which A is structured and Y contains all the\nPage 128\narXiv Version 2\n\n\n7.2. Sketching operators\nTensor Product Structures\nentries of the tensor being decomposed [KB09]. When Y has a ﬁxed proportion\nof nonzero entries, the cost will therefore scale exponentially with the number\nof tensor indices—a manifestation of the curse-of-dimensionality.\n• When ﬁtting polynomial chaos expansion functions in surrogate modeling\n[TNX15; SNM17; CMX+22], A contains evaluations of a multivariate poly-\nnomial on a structured quadrature grid and Y (which will now be a column\nvector) contains the outputs of an expensive data generation process (e.g., an\nexperiment or high-ﬁdelity PDE simulation).\nIn both example applications, it is clearly desirable to avoid using all entries of\nY when solving (7.6). As discussed in Section 6, leverage score sampling can be\nused to sketch-and-solve least squares problems without accessing all entries of the\nright-hand side Y while still providing performance guarantees. Here we discuss\nhow to take advantage of the structure of A to speed up leverage score sampling.\nKronecker product structure\nConsider a Kronecker product A = A1 ⊗· · · ⊗AL of mk × nk matrices Ak. It is\npossible to perform exact leverage score sampling on A without even forming it.\nCheng et al. [CPL+16] used this fact to approximately solve least squares problems\nwith Kronecker product design matrices, which has applications in algorithms for\nTucker tensor decomposition. Formal statements and proofs of these results later\nappeared in [DJS+19].\nTo see how the sampling works, let (pi) be the leverage score sampling distribu-\ntion of A and let (pik) be the leverage score sampling distribution of Ak for k ∈JLK.\nFor any i ∈JQL\nk=1 mkK and corresponding multi-index (i1, . . . , iL) satisfying\nA[i, :] = A1[i1, :] ⊗· · · ⊗AL[iL, :],\n(7.7)\nit holds that\npi = p(1)\ni1 p(2)\ni2 · · · p(L)\niL .\n(7.8)\nTherefore, instead of drawing an index i according to (pi), one can draw the index\nik according to (p(k)\nik ) for each k ∈JLK. Due to (7.7), the row corresponding to the\ndrawn index can be computed and rescaled without constructing A. This process\ncan be easily adapted to drawing multiple samples.\nFahrbach et al. [FFG22] discuss how the sampling approach above can be adapted\nfor use in ridge regression when the design matrix is a Kronecker product. Malik\net al. [MXC+22] show an approach for eﬃcient sampling according to the exact\nleverage scores of matrices of the form A[:, J] when A is a Kronecker product and\nJ is an index vector that satisﬁes certain monotonicity properties.\nKhatri–Rao product structure\nSampling according to the leverage scores of a Khatri–Rao product matrix A =\nA1 ⊙· · · ⊙AL is more challenging than it is for a Kronecker product matrix. Still,\nseveral approaches for doing so have been proposed.\nWe divide them into two\ncategories. The methods in the ﬁrst category sample according to the leverage scores\nof the Kronecker product of A1, . . . , AL instead of the Khatri–Rao product since this\nallows for simple and eﬃcient sampling. This can be viewed as sampling from a\ncoarse approximation of the Khatri–Rao product leverage scores. The methods in\narXiv Version 2\nPage 129\n\n\nTensor Product Structures\n7.3. Partial updates to Kronecker product sketches\nthe second category sample according to exact or high-accuracy approximations of\nthe Khatri–Rao product leverage score distribution.\nSampling according to Kronecker product leverage scores\nAs noted by [CPL+16;\nBBK18], the leverage scores of A can be upper bounded by\nℓi(A) ≤\nL\nY\nk=1\nℓik(Ak),\n(7.9)\nwhere (i1, . . . , iL) is the multi-index corresponding to i. The two papers [CPL+16;\nLK20] use the expression on the right-hand side of (7.9) as an approximation to the\nexact leverage scores on the left-hand side. By using the bound (7.9), they are able\nto prove theoretical performance guarantees when this approach is used for sketch-\nand-solve in least squares problems. More precisely, Cheng et al. [CPL+16] sample\naccording to a mixture of the distribution in (7.8) and a distribution which depends\non the magnitude of the dependent variables (i.e., the entries in the “right-hand\nsides” in the least squares problem). Larsen and Kolda [LK20] sample with respect\nto only the distribution in (7.8).\nBharadwaj et al. [BMM+22] extend the work\nby [CPL+16; LK20] to a distributed-memory setting and provide high-performance\nparallel implementations. Ideas similar to those in [LK20] are developed for the more\ncomplicated design matrices that arise in algorithms for tensor ring decomposition\nin [MB21]. Those matrices have columns that are sums of vectors with Kronecker\nproduct structure.\nSampling according to exact or high-quality approximations of leverage scores\nMalik\n[Mal22] proposes a diﬀerent approach for the Khatri–Rao product least squares\nproblem. By combining some of the ideas for fast leverage score estimation (see\n§6.2) and recursive sketching (see §7.2.4) with a sequential sampling approach,\nhe improves the sampling and computational complexities of [LK20].\nMalik et\nal. [MBM22] simplify and generalize the method by [Mal22] to a wider family of\nstructured matrices. An upshot of this work is a method for eﬃciently sampling\na Khatri–Rao product matrix according to its exact leverage score distribution\nwithout forming the matrix.\nMotivated by applications in kernel methods, [WZ20] develop a recursive lever-\nage score sampling method for sketching of matrices of the form A⊙L = A⊙· · ·⊙A.\nTheir method starts by sampling from a coarse approximation to the leverage score\nsampling distribution and then iteratively reﬁning it. These ideas are further re-\nﬁned in [WZ22] where the method is also extended to general Khatri–Rao products\nof matrices that can all be distinct.\n7.3\nPartial updates to Kronecker product sketches\nThe structured sketching operators discussed in Section 7.2 are notable in that\nthey are deﬁned in terms of multiple smaller sketching operators. Here we discuss\nsituations when it is advantageous to reuse some of these smaller sketches across\nmultiple calls to the structured sketching operator. The examples we discuss come\nfrom works that use sketching in tensor decomposition algorithms. Our goal with\nthis discussion is to bring to light some of the functionality we think is important\nPage 130\narXiv Version 2\n\n\n7.3. Partial updates to Kronecker product sketches\nTensor Product Structures\nfor structured sketches to have in order to best support potential usage in the\ntensor community.\nBy tensor, we mean multi-index arrays containing real numbers. A tensor with\nL indices is called an L-way tensor. Vectors and matrices are one- and two-way\ntensors, respectively. Calligraphic capital letters (e.g., X) are used to denote tensors\nwith three or more indices. Much like matrix decomposition, the purpose of tensor\ndecomposition is to decompose a tensor into some number of simpler components.\nWe only give minimal background material on tensor decomposition here; see the\nreview papers [KB09; CMD+15; SDF+17] for further details.\n7.3.1\nBackground on the CP decomposition\nWe ﬁrst consider the CANDECOMP/PARAFAC (CP) decomposition which is also\nknown as the canonical polyadic decomposition [KB09, §3]. It decomposes an L-way\ntensor X of size m1 × m2 × · · · × mL into a sum of R rank-1 tensors:\nX =\nR\nX\nr=1\na(1)\nr\n◦a(2)\nr\n◦· · · ◦a(L)\nr\n,\n(7.10)\nwhere ◦denotes the outer product. The mn ×R matrices A(n) =\nh\na(n)\n1\n· · ·\na(n)\nR\ni\nfor n ∈JLK are called factor matrices. When R is suﬃciently large, we can express\nthe factor matrices as the solution to\narg min\nA(1),...,A(L)\n\r\r\rX −\nR\nX\nr=1\na(1)\nr\n◦a(2)\nr\n◦· · · ◦a(L)\nr\n\r\r\r\nF,\n(7.11)\nwhere ∥· ∥F denotes the Frobenius norm as generalized to tensors in the obvious\nway. The broader problem of tensor decomposition is concerned with approximately\nsolving (7.11). In particular, it is common to seek locally optimal solutions to this\nproblem even when R is too small for an identity of the form (7.10) to hold for X.\nIt is computationally intractable to solve (7.11) in the general case. However,\nthe problem admits several heuristics that are eﬀective in practice. One of the most\npopular heuristics is alternating minimization, wherein one solves for only one factor\nmatrix at a time while keeping the others ﬁxed. That is, one solves a sequence of\nproblems of the form\nA(n) = arg min\nA\n\r\r\rX −\nR\nX\nr=1\na(1)\nr\n◦· · · ◦a(n−1)\nr\n◦ar ◦a(n+1)\nr\n◦· · · ◦a(L)\nr\n\r\r\r\nF\n(7.12)\nfor n ∈JLK. If we adopt appropriate notation then (7.12) can be stated as a familiar\nlinear least squares problem. Accordingly, this alternating minimization approach\nis called alternating least squares (ALS). The ALS approach cycles through the\nindices n ∈JLK multiple times until some termination criteria is met.\nTypical\ntermination criteria include reaching a maximum number of iterations or seeing\nthat the improvement in the objective falls below some threshold.\nFormulating and solving the least squares problem\nWe begin by introducing ﬂattened representations of X. Speciﬁcally, for n ∈JLK,\nthe mn×\n\u0010Q\nj̸=n mj\n\u0011\nmatrix X(n) is given by horizontally concatenating the mode-n\narXiv Version 2\nPage 131\n\n\nTensor Product Structures\n7.3. Partial updates to Kronecker product sketches\nﬁbers X[i1, . . . , in−1, :, in+1, . . . , iN] as columns. Such a matrix can be expressed in\nMatlab notation as follows\nX(n) = reshape\n\npermute(X, [n, 1, . . . , n −1, n + 1, . . . , L]), mn,\nY\nj̸=n\nmj\n\n.\n(7.13)\nNext, we introduce the following ﬂattened tensorizations of the factor matrices:\nA̸=n := A(L) ⊙· · · ⊙A(n+1) ⊙A(n−1) ⊙· · · ⊙A(1) =:\n1\nK\nj=L\nj̸=n\nA(j).\n(7.14)\nWhere we emphasize that the order of the matrices in the above product is im-\nportant; our notation for the Khatri–Rao product at right reﬂects how the order\nproceeds from j = L to j = 1, skipping j = n.\nIn terms of these matrices, the ALS update rule for the nth factor matrix is\nA(n) = arg min\nA\n\r\rA̸=nA∗−X∗\n(n)\n\r\r\nF.\n(7.15)\nWe note that the Gram matrix for this least squares problem can be computed\neﬃciently by the formula\nA̸=n∗A̸=n = (A(L)∗A(L)) ⊛· · · ⊛(A(n+1)∗A(n+1))\n⊛(A(n−1)∗A(n−1)) ⊛· · · ⊛(A(1)∗A(1)).\n(7.16)\nTherefore solving the least squares problem in (7.15) via the normal equations can\nbe very eﬃcient [KB09, §3.4]. Indeed, the ALS update rule for the nth factor matrix\nbecomes\nA(n) = X(n)A̸=n(A̸=n∗A̸=n)†.\n(7.17)\nThe most expensive part of this update is actually computing X(n)A̸=n [BBK18,\n§3.1.1], which is analogous to the vector F∗h in the normal equations for an overde-\ntermined least squares problem minz ∥Fz −h∥2\n2. Therefore, the fact that comput-\ning this matrix is the computational bottleneck in solving (7.15) is the opposite of\nwhat one would expect when not working with tensors. This phenomenon is why\nrow-sampling sketching operators have been successful in ALS algorithms that use\nsketch-and-solve for the least squares subproblems [LK20].\nRemark 7.3.1. Although it is cheap to form the Gram matrix (7.16), there is poten-\ntial for very bad conditioning even when L is small. We do not know how seriously\nthe poor conditioning aﬀects ALS approaches to CP decomposition in practice.\n7.3.2\nSketching for the CP decomposition\nBattaglino et al. [BBK18] apply the Kronecker SRFT from Section 7.2.2 to the least\nsquares problem in (7.15). Letting Tj and Fj be of size mj × mj, the sketching\noperator used before solving for the nth factor matrix is\nSn =\nv\nu\nu\nt\nQL\nj=1\nj̸=n\nmj\nd\nR\n\u0010\n1\nO\nj=L\nj̸=n\nTj\n\u0011\u0010\n1\nO\nj=L\nj̸=n\nFj\n\u0011\n.\n(7.18)\nPage 132\narXiv Version 2\n\n\n7.3. Partial updates to Kronecker product sketches\nTensor Product Structures\nOur notation for the Kronecker product operator indexes from j = L to j = 1 so as\nto mimic our earlier notation for the Khatri–Rao product (see (7.14)).\nA by-the-book application of this operator would require drawing new R and\n(Fj)j̸=n every time it is applied in (7.15), i.e., L times for every execution of the for\nloop. Battaglino et al. [BBK18, Alg. 4] instead propose drawing F1, . . . , FL once and\nthen reusing them throughout the algorithm, only drawing R anew for each least\nsquares problem. This reduces the computational cost considerably since it allows\nfor greater reuse of various computed quantities. More speciﬁcally, the expensive\napplication of the full Kronecker SRFT to the unstructured matrix X∗\n(n) does not\nhave to be computed for every least squares problem.\nLarsen and Kolda [LK20] also sketch the least squares problems in (7.15).\nThey use the eﬃcient leverage score sampling scheme for Khatri–Rao products\ndiscussed in Section 7.2.5.\nThis approach also allows for some reuse between\nsubsequent sketches.\nWhen solving for A(n) in (7.15), a row with multi-index\n(i1, . . . , in−1, in+1, . . . , iL) is sampled with probability p(1)\ni1 · · · p(n−1)\nin−1 p(n+1)\nin+1 · · · p(L)\niL ,\nwhere (p(k)\nik ) is the leverage score sampling distribution for A(k). Since each A(k)\nonly change for every Lth least squares problem, the probability distribution (p(k)\nik )\ncan be used in L −1 least squares problems before it needs to be recomputed.\n7.3.3\nBackground on the Tucker decomposition\nThe Tucker decomposition [KB09, §4] is another popular method that decomposes\nan L-way tensor X of size m1 × m2 × · · · × mL into\nR1\nX\nr1=1\nR2\nX\nr2=1\n· · ·\nRL\nX\nrL=1\nG[r1, r2, . . . , rL] a(1)\nr1 ◦a(2)\nr2 ◦· · · ◦a(L)\nrL ,\n(7.19)\nwhere the so-called core tensor G is of size R1×R2×· · ·×RL. The mn×Rn matrices\nA(n) =\nh\na(n)\n1\n· · ·\na(n)\nRn\ni\nfor n ∈JLK are called factor matrices. Similarly to the\nCP decomposition, the core tensor and factor matrices can be initialized randomly\nand then updated iteratively via ALS:3\nFor n in JLK :\nA(n) = arg min\nA\n∥B̸=nG∗\n(n)A∗−X∗\n(n)∥F,\n(7.20)\nG = arg min\nZ\n∥B vec(Z) −vec(X)∥F,\n(7.21)\nwhere\nB̸=n = A(L) ⊗· · · ⊗A(n+1) ⊗A(n−1) ⊗· · · ⊗A(1),\nB = A(L) ⊗· · · ⊗A(1),\nand the unfolding G(n) is deﬁned analogously to X(n) in (7.13). The steps in (7.20)\nand (7.21) are then repeated until some convergence criterion is met.\nWe note\nthat the least squares problems (7.20) and (7.21) are highly overdetermined when\n(Rn)n∈JLK are small compared to (mn)n∈JLK.\n3The update rules in (7.20) and (7.21) have been formulated as least squares problems in order\nto show where sketching can be applied in the ALS algorithm. A more standard formulation of\nthe update rules can be found in [KB09, §4.2].\narXiv Version 2\nPage 133\n\n\nTensor Product Structures\n7.3. Partial updates to Kronecker product sketches\n7.3.4\nSketching for the Tucker decomposition\nMalik and Becker [MB18] apply the TensorSketch discussed in Section 7.2.3 to these\nproblems. From a straightforward adaption of (7.5) to matrix Kronecker products,\nwe have that the TensorSketch of the design matrix B̸=n is computed via the formula\nDFT−1\n\u0012\u0010\n1\nK\nj=L\nj̸=n\n\u0000DFT(SjA(j))\n\u0001⊤\u0011⊤\u0013\n,\nwhere Sj is a d×mj CountSketch, and where ⊤denotes transpose without complex\nconjugation. The formula for sketching B is the same except for that it also includes\nthe nth term in the Khatri–Rao product.\nInstead of drawing new CountSketches for every application of TensorSketch,\n[MB18, Alg. 2] draw two sets of independent CountSketches at the start of the\nalgorithm: (S(1)\nj )L\nj=1 where S(1)\nj\nis of size d1 × mj, and (S(2)\nj )L\nj=1 where S(2)\nj\nis of\nsize d2 × mj. These two sets of sketches are then reused throughout the algorithm:\n(S(1)\nj ) are used for sketching (7.20) and (S(2)\nj ) are used for sketching (7.21). The\nlatter least squares problems are much larger than the former.\nUsing two sets\nof sketching operators makes it possible to choose a larger embedding dimension\nfor the latter problem, i.e., choosing d2 > d1. By reusing CountSketches in this\nfashion, considerable improvement in run time is achieved. Moreover, it is possible\nto compute all relevant sketches of unfoldings of X at the start of the algorithm,\nleading to an algorithm that requires only a single pass of X in order to decompose it.\n7.3.5\nImplementation considerations\nWe deem it most appropriate to implement the structured sketches discussed in\nSection 7.2 in RandLAPACK rather than RandBLAS. In order to facilitate the appli-\ncations discussed in Section 7.3, it should be possible to update or redraw speciﬁc\ncomponents of the sketching operator after it has been created. For example, when\napplying the operator in (7.18) in an ALS algorithm for CP decomposition as in\n[BBK18, Alg. 4], we want to keep the random diagonal matrices F1, . . . , FL ﬁxed\nbut draw a new sampling operator R before each application of Sn.\nIn the applications above, components are shared across the L diﬀerent sketching\noperators that are used when updating the L diﬀerent factor matrices. Rather than\ndeﬁning L diﬀerent sketching operators with shared components, it is better to\ndeﬁne a single operator that contains all components and which allows “leaving\none component out” when applied to a matrix or vector. For example, consider\na Kronecker SRFT from (7.4) but with reversed order in the Kronecker products.\nIt contains the components R and F1, . . . , FL. A user should be able to supply a\nparameter n which indicates that the nth term in the Kronecker products should\nbe left out when computing the sketch, resulting in a sketch of the form (7.18).\nPage 134\narXiv Version 2\n\n\nAppendix A\nDetails on Basic Sketching\nA.1 Subspace embeddings and eﬀective distortion ............ 135\nA.1.1 Eﬀective distortion of Gaussian operators ..................... 137\nA.2 Short-axis-sparse sketching operators ......................... 137\nA.2.1 Implementation notes\n............................................... 137\nA.2.2 Theory and practical usage ......................................... 139\nA.3 Theory for sketching by row selection ......................... 140\nThis appendix covers sketching theory and implementation of sketching opera-\ntors. Its contents are relevant to Sections 2, 3 and 6.\nA.1\nSubspace embeddings and eﬀective distortion\nLet S be a wide d × m sketching operator and L be a linear subspace of Rm. Recall\nfrom Section 2.2.2 that S embeds L into Rd with distortion δ ∈[0, 1] if\n(1 −δ)∥x∥2 ≤∥Sx∥2 ≤(1 + δ)∥x∥2\nholds for all x in L. We often use the term δ-embedding for such an operator. Note\nthat if S is a δ-embedding and δ′ is greater than δ, then S is also a δ′-embedding.\nIt can be useful to speak of the smallest distortion δ for which S is a δ-embedding\nfor L; we call this the distortion of S for L, and denote it by\nD(S; L) = inf{ δ : 0 ≤δ ≤1\nS is a δ-embedding for L}.\nIn this notation, we have D(S; L) ≥1 when ker S∩L is nontrivial. If there is a unit\nvector x in L where ∥Sx∥> 2, then D(S; L) = +∞.\nSubspace embedding distortion has a signiﬁcant limitation in that it depends on\nthe scale of S, while many RandNLA algorithms have no such dependence. This\nshortcoming leads us to deﬁne the eﬀective distortion of S for L as\nDe(S; L) = inf\nt>0 D(tS; L).\n(A.1)\nHere, the inﬁmum is over t > 0 rather than t ̸= 0 since D(S; L) = D(−S; L).\n135\n\n\nDetails on Basic Sketching\nA.1. Subspace embeddings and eﬀective distortion\nThere is a convenient formula for eﬀective distortion using concepts of restricted\nsingular values and restricted condition numbers. Restricted singular values are a\nfairly general concept of use in random matrix theory; see, e.g., [OT17]. They are\nmeasures an operator’s “size” when considered from diﬀerent vantage points within\na set of interest. Formally, we deﬁne the largest and smallest restricted singular\nvalues of a sketching operator S for a subspace L as\nσmax(S; L) = max\nx∈L {∥Sx∥2 : ∥x∥2 = 1}\nand\nσmin(S; L) = min\nx∈L {∥Sx∥2 : ∥x∥2 = 1}\nGiven these concepts, we deﬁne the restricted condition number of S on L as\ncond(S; L) = σmax(S; L)\nσmin(S; L) ,\nwhere we take c/0 = +∞for any c ≥0.\nWe have formulated the concepts of restricted singular values and condition\nnumbers in a way that reﬂects their geometric meaning. More concrete descriptions\ncan be obtained by considering any matrix U whose columns provide an orthonormal\nbasis for L. With this one can see that σmin(S; L) and σmax(S; L) coincide with the\nextreme singular values of SU, and that cond(S; L) = cond(SU).\nNext, we provide the connection between restricted condition numbers and ef-\nfective distortion. Appendix B.1.1 explores this connection more in the context of\nsketch-and-precondition algorithms for saddle point problems.\nProposition A.1.1. Let L be a linear subspace and S be a sketching operator on\nL. The eﬀective distortion of S for L is\nDe(S; L) = κ −1\nκ + 1\nwhere we take (∞−1)/(∞+ 1) = 1.\nProof. The scaled sketching operator tS is a δ-embedding for L if and only if\n(1 −δ)∥x∥2 ≤t∥Sx∥2 ≤(1 + δ)∥x∥2\nfor all x in L.\nThis is equivalent to\n1 −δ\nt\n≤∥Sx∥2\n∥x∥2\nand\n∥Sx∥2\n∥x∥2\n≤1 + δ\nt\nfor all x in L \\ {0}.\nTo simplify these bounds, we optimize over x. Abbreviating σ1 := σmax(S; L) and\nσn := σmax(S; L) for n = dim(L), we ﬁnd that tS is a δ-embedding if and only if\n1 −δ\nt\n≤σn\nand\n1 + δ\nt\n≤σ1.\nThese identities can be rearranged to ﬁnd the following constraints on δ:\n1 −σ1t ≤δ\nand\ntσn −1 ≤δ.\nPage 136\narXiv Version 2\n\n\nA.2. Short-axis-sparse sketching operators\nDetails on Basic Sketching\nThe value of t which permits minimum δ is that which makes the lower bounds\ncoincide. That is, the optimal t is t⋆= 2/(σ1 + σn). Plugging this into the bounds\nabove, our constraints on δ reduce to\nδ ≥1 −σ1t⋆= t⋆σn −1 = σ1 −σn\nσ1 + σn\n= κ −1\nκ + 1,\nas desired.\nA.1.1\nEﬀective distortion of Gaussian operators\nIt is informative to consider the concepts of restricted condition numbers and ef-\nfective distortion for Gaussian sketching operators. Therefore, let us suppose that\nour d×m sketching operator S has iid mean-zero Gaussian entries, and consider an\nn-dimensional subspace L in Rm. By rotational invariance of Gaussian distribution,\nwe can infer that the distribution of cond(S; L) coincides with that of cond(˜S) for a\nd × n Gaussian matrix ˜S. Strong concentration results are available to understand\nthe distribution of cond(˜S).\nSpeciﬁcally, letting d = sn for a constant s > 1, results by Silverstein [Sil85] and\nGeman [Gem80] imply\ncond(S; L) →\n√s + 1\n√s −1\nalmost surely as\nn →∞.\n(A.2)\nThis can be turned around using Proposition A.1.1 to obtain\nDe(S; L) →1\n√s\nalmost surely as\nn →∞.\n(A.3)\nWe emphasize that (A.2) and (A.3) hold for any ﬁxed n-dimensional subspace L.\nThese facts justify aggressively small choices of embedding dimension when using\nGaussian sketching operators in randomized algorithms for least squares. Meng,\nSaunders, and Mahoney come to the same conclusion in their work on LSRN\n[MSM14, Theorem 4.4].\nA.2\nShort-axis-sparse sketching operators\nIn this appendix we make liberal use of the abbreviation SASO (for “short-axis-\nsparse sketching operator”) introduced in Section 2.4.1. Without loss of generality,\nwe describe SASOs in the wide case, i.e., when S is d × m with d ≪m.\nA.2.1\nImplementation notes\nConstructing SASOs column-wise\nIt is extremely cheap to construct and store a wide SASO. The precise storage\nformat depends on how one wants to apply the SASO later on, which can vary\ndepending on context. However, the construction is embarrassingly parallel across\ncolumns provided one uses CBRNGs (counter-based random number generators;\nsee §2.1.1), and this structure leads to canonical methods for generating SASOs.\nWe ﬁrst consider the SASO construction where row indices are partitioned into\nindex sets I1, . . . , Ik of roughly equal size. Given such a partition, the indices of\narXiv Version 2\nPage 137\n\n\nDetails on Basic Sketching\nA.2. Short-axis-sparse sketching operators\nnonzeros for a given column are chosen by taking one element (independently and\nuniformly) from each of the index sets Ij. The naive implementation can sample\nthese row indices with k parallel calls to the CBRNG.\nNow consider the construction where the row indices for a column are chosen by\nselecting k elements from JdK uniformly without replacement. This can be done in\nO(km) time by using Fisher-Yates sampling and carefully re-using workspace. For\na concrete implementation, we refer the reader to\nhttps://github.com/BallisticLA/RandBLAS/blob/19sept22/src/sjlts.\ncc#L14-L78.1\nWhile the implementation above is sequential, it is easy to parallelize. Given T\nthreads, the natural modiﬁcation to the algorithm takes O(mk/T) time and requires\nO(Td) workspace. The constants in the big-O notation are small.\nRemarks on storage formats\nIt is reasonable for a standard library to restrict SASOs to only the most common\nsparse matrix formats. We believe both compressed sparse row (CSR) and com-\npressed sparse column (CSC) are worth considering. CSC is the more natural of the\ntwo since (wide) SASOs are constructed columnwise. If CSR format is preferred for\nsome reason, then we recommend constructing S columnwise while retaining extra\ndata to facilitate conversion to CSR.\nIn principle, if the nonzero entries of S are ±1 and CSC is used as the storage\nformat, then one could do away with storing the nonzero values explicitly; one could\ninstead store the sign information using signed integers for the row indices. We do\nnot favor this approach since it precludes working with SASOs with more than two\ndistinct nonzero values.\nFor the matrices A and Ask, we must consider whether they are in column-major\nor row-major format. Indeed, both formats need to be supported since Section 3\nframed all least squares problems with tall data matrices. While this was without\nloss of generality from a mathematical perspective, a user with an underdetermined\nleast squares problem involving a wide column-major data matrix B is eﬀectively\nneeding to sketch the tall row-major matrix A = B∗.\nApplying a wide SASO\nThere are four combinations of storage formats we need to consider for (S, A).\nS is CSC, A is row-major.\nSuppose we have P processors. Our suggested approach\nis to partition the row index set JdK into I1, . . . , IP and to have each processor be\nresponsible for its own block of rows. The pth processor computes its row block by\nstreaming over the columns of S and rows of A, accumulating outer products as\nindicated below\nAsk[Ip, :] =\nX\nℓ∈JmK\nS[Ip, ℓ]A[ℓ, :].\nAn individual term S[Ip, ℓ]A[ℓ, :] can cheaply be accumulated into Ask[Ip, :] by using\nthe fact that S[Ip, ℓ] is extremely sparse.\nIf R denotes the number of nonzeros\nin S[Ip, ℓ], then the outer-product accumulation can be computed with R axpy\n1This code was written when we used the term “SJLT” for what we now call a “SASO.”\nPage 138\narXiv Version 2\n\n\nA.2. Short-axis-sparse sketching operators\nDetails on Basic Sketching\noperations involving the row A[ℓ, :]. Note that since S has k nonzeros per column\n(with row indices distributed uniformly at random), this value R is a sum of |Ip|\niid Bernoulli random variables with mean k/d. Therefore the expected number of\naxpy’s performed by processor p for term ℓis |Ip|k/d.\nS is CSR, A is row-major.\nHere, we suggest that the d rows of Ask be computed\nseparately from one another.\nAn individual row is given by Ask[i, :] = S[i, :]A.\nEvaluating the product of this sparse vector and dense matrix can be done by\ntaking a linear combination of a small number of rows of A. Speciﬁcally, if R is\nthe number of nonzeros in S[i, :] then computing Ask[i, :] only requires R rows from\nA. Since the columns of S are independent, R is a sum of m iid Bernoulli random\nvariables with mean k/d. Therefore we expect to access mk/d rows of A in order\nto compute Ask[i, :].\nS is CSC, A is column-major.\nHere, we suggest that the n columns of Ask be com-\nputed separately from one another. An individual column is given by Ask[:, j] =\nSA[:, j]. We evaluate this product by taking a linear combination of the columns of\nS, according to\nAsk[:, j] =\nX\nℓ∈JmK\nS[:, ℓ]A[ℓ, j].\nNote that each of the ℓterms in this sum is a sparse vector with k nonzero entries.\nBased on our preliminary experiments, this method has mediocre single-thread per-\nformance, but it has excellent scaling properties for many-core machines.\nS is CSR, A is column-major.\nWe were unable to determine a method that paral-\nlelizes well for this pair of data formats. The most eﬃcient algorithm may be to\nconvert S to CSC and then to apply the preferred method when S is CSC and A is\ncolumn-major.\nA.2.2\nTheory and practical usage\nSASO theory\nA precursor to the SASOs we consider is described in [DKS10], which sampled row\nindices for nonzero entries from JdK with replacement. The ﬁrst theoretical analysis\nof the SASOs we consider was conducted in [KN12] and concerned the distributional\nJohnson-Lindenstrauss property. Shortly thereafter, [CW13] and [MM13] studied\nOSE properties for SASOs with a single nonzero per column; the latter referred to\nthe construction as “CountSketch.”\nTheoretical analyses for OSE properties of general SASOs (i.e., those with more\nthan one nonzero per column) were ﬁrst carried out by [NN13; KN14] and sub-\nsequently improved by [BDN15; Coh16].\nMuch of the SASO analysis has been\nthrough the lens of “hashing functions,” and does not require that the columns of\nthe sketching operator are fully independent.\nWe do not know of any practical\nadvantage to SASOs with partial dependence across the columns.\nRemark A.2.1 (Navigating the literature). [CW17] is a longer journal version of\n[CW13]. [KN14] and [KN12] have the same title, and the former is considered a\nmore developed journal version of the latter.\narXiv Version 2\nPage 139\n\n\nDetails on Basic Sketching\nA.3. Theory for sketching by row selection\nSelecting parameters for SASOs\nWe are in the process of developing recommendations for how to set the parameters\nd and k for a distribution over SASOs. So far we have observed that when d is\nﬁxed the sketch quality increases rapidly with k before reaching a plateau.\nAs\none point of reference, we have observed that there is no real beneﬁt in k being\nlarger than eight when embedding the range of a 100, 000 × 2, 000 matrix into a\nspace with ambient dimension d = 6, 000. Furthermore, for the data matrices we\ntested, the restricted condition numbers of those sketching operators were tightly\nconcentrated at O(1). Extensive experiments with parameter selection for SASOs\nin a least squares context are given in [Ura13].\nA.3\nTheory for sketching by row selection\nHere we prove Proposition 6.1.1. Our proof is inspired by [Tro20, Problem 5.13],\nwhich begins with the following adaptation of [Tro15, Theorem 5.1.1].\nTheorem A.3.1. Consider an independent family {X1, . . . , Xs} ⊂Hn of random\npsd matrices that satisfy λmax(Xi) ≤L almost surely. Let Y = Ps\ni=1 Xi, and deﬁne\nthe mean parameters\nµmax = λmax(EY)\nand\nµmin = λmin(EY).\nOne has that\nPr {λmax(Y −(1 + t)EY) ≥0} ≤n\n\u0014\nexp(t)\n(1 + t)(1+t)\n\u0015µmax/L\nfor t > 0,\nand\nPr {λmax((1 −t)EY −Y) ≥0} ≤n\n\u0014\nexp(−t)\n(1 −t)(1−t)\n\u0015µmin/L\nfor t ∈(0, 1).\nHere, we restate the result we aim to prove.\nProposition A.3.2 (Adaptation of Proposition 6.1.1). Suppose A is an m × n\nmatrix of rank n, q is a distribution over JmK, and t is in (0, 1). Let S be a d × m\nsketching operator with rows that are distributed iid as\nS[i, :] =\nδj\np\ndqj\nwith probability qj,\nand let E(t, S) denote the event that\n(1 −t)∥y∥2\n2 ≤∥Sy∥2\n2 ≤(1 + t)∥y∥2\n2 ∀y ∈range A.\nUsing r := minj∈JmK\nqj\npj(A), we have\nPr {E(t, S) fails} ≤2n\n\u0012\nexp(t)\n(1 + t)(1+t)\n\u0013rd/n\n.\nProof. The way that we use Theorem A.3.1 is along the lines of the hint in [Tro20,\nProblem 5.13, Part 3]. We begin by considering the Gram matrices G = A∗A and\nGsk = A∗S∗SA. The event E(t, S) is equivalent to\n(1 −t)In ⪯G−1/2GskG−1/2 ⪯(1 + t)In.\nPage 140\narXiv Version 2\n\n\nA.3. Theory for sketching by row selection\nDetails on Basic Sketching\nThe sketched Gram matrix can be expressed as a sum of d outer products of\nrows of SA. Each of the d outer products is conjugated by G−1/2 to obtain our\nmatrices {X1, . . . , Xd}. That is, we set\nXi = G−1/2 ((SA) [i, :])∗((SA) [i, :]) G−1/2\n(A.4)\nso that Y = Pd\ni=1 Xi satisﬁes EY = In. A union bound provides\nPr{E(t, S) fails} ≤Pr{λmax(Y) ≥1 + t} + Pr{1 −t ≥λmin(Y)}.\nNote that the claim of this proposition only invokes Theorem A.3.1 in the special\ncase when t is between zero and one. Moreover, our particular choice of Y leads to\nµmin = µmax = 1. Given these restrictions, it can be shown that the larger of the\ntwo probability bounds in the theorem is that involving the term exp(t)/(1+t)(1+t).\nTherefore we have\nPr{E(t, S) fails} ≤2n\n\u0010\nexp(t)/(1 + t)(1+t)\u00111/L\n.\nNext, we turn to ﬁnding the smallest possible L given this construction, so as to\nmaximize 1/L.\nLet i be an arbitrary index in JdK. By the deﬁnition of S, the following must\nhold for some k ∈JmK:\nXi = 1\nd\n\u0012 1\nqk\nG−1/2A[k, :]∗A[k, :]G−1/2\n\u0013\n.\nOur next step is to use the fact that the trace of a rank-1 psd matrix is equal to its\nlargest eigenvalue. Cycling the trace shows that\nλmax\n\u0010\nG−1/2A[k, :]∗A[k, :]G−1/2\u0011\n= A[k, :]G−1A[k, :]∗= ℓk(A),\nand hence\nL = 1\nd max\nj∈JmK\n\u001aℓj(A)\nqj\n\u001b\nis the smallest value that guarantees λmax(Xi) ≤L.\nTo complete the proof we use the assumption that A is of full rank n to express\nthe leverage score ℓj(A) as ℓj(A) = npj(A). This shows that\nL = n\nd max\nj∈JmK\npj(A)\nqj\n,\nand the proposition’s claim follows from just a little algebra.\narXiv Version 2\nPage 141\n\n\nDetails on Basic Sketching\nA.3. Theory for sketching by row selection\nPage 142\narXiv Version 2\n\n\nAppendix B\nDetails on Least Squares\nand Optimization\nB.1 Quality of preconditioners ........................................... 143\nB.1.1 Eﬀective distortion in sketch-and-precondition ............... 145\nB.2 Basic error analysis ..................................................... 146\nB.2.1 Concepts: forward and backward error ......................... 146\nB.2.2 Basic sensitivity analysis for least squares problems ........ 147\nB.2.3 Sharper sensitivity analysis for overdetermined problems . 149\nB.2.4 Simple constructions to bound backward error ............... 149\nB.2.5 More advanced concepts ............................................ 151\nB.3 Ill-posed saddle point problems\n................................. 152\nB.4 Minimizing regularized quadratics .............................. 153\nB.4.1 A primer on kernel ridge regression .............................. 153\nB.4.2 Eﬃcient sketch-and-solve for regularized quadratics ........ 155\nThis appendix covers a few distinct topics. Appendix B.1 proves a novel result\nrelevant to sketch-and-precondition algorithms for saddle point problems, and con-\nnects this result to the idea of eﬀective distortion. In Appendix B.2, we provide\nbackground from classical NLA on what it means to compute an “accurate” solution\nto a least squares problem (overdetermined or underdetermined). Appendix B.3\nderives limiting solutions of saddle point problems as the regularization parame-\nter tends to zero from above. These limiting solutions are important for treating\nsaddle point problems as linear algebra problems even when their optimization for-\nmulations are ill-posed. Finally, Appendix B.4 gives background on kernel ridge\nregression and details a new approach to sketch-and-solve of regularized quadratics.\nB.1\nQuality of preconditioners\nHere we consider preconditioners of the kind described in Section 3.3.2. These are\nobtained by sketching a tall m×n data matrix A in the embedding regime, factoring\nthe sketch, and using the factorization to construct an orthogonalizer.\n143\n\n\nLeast Squares and Optimization\nB.1. Quality of preconditioners\nProposition B.1.1 (Adaptation of Proposition 3.3.1). Consider a sketch Ask =\nSA and a matrix U whose columns are an orthonormal basis for range(A).\nIf\nrank(Ask) = rank(A) and the columns of AskM are an orthonormal basis for the\nrange of Ask, then singular values of AM are the reciprocals of the singular values\nof SU.\nObserve that this proposition is a linear algebraic result, i.e., there is no ran-\ndomness. When applied to randomized algorithms, the randomness enters only via\nthe construction of the sketch.\nThis result can be applied to problems with ridge regularization by working\nwith augmented matrices in the vein of Section 3.3.2. In that context it is necessary\nto not only replace (A, Ask) by (ˆA, ˆAsk), but also to replace S by the augmented\nsketching operator ˆS that takes ˆA to ˆAsk. The augmented sketching operator in\nquestion was already visualized in Algorithm 2.\nOur proof of Proposition B.1.1 requires ﬁnding an explicit expression for M.\nTowards this end, we prove the following lemma.\nLemma B.1.2. Suppose Ask is a tall d×n matrix and that M is a full-column-rank\nmatrix for which the columns of AskM form an orthonormal basis for range(Ask).\nIf B is a full-row-rank matrix for which Ask = AskMB, then we have M = B†.\nProof of Lemma B.1.2. Let k = rank(Ask) = rank(AskM). Since the columns of\nAskM are orthonormal we can infer that it has dimensions d × k. Similarly, since\nM is full column-rank we can infer that it is n × k. We prove that B = M†, which\namounts to showing four properties:\n1. MBM = M,\n2. BMB = B,\n3. BM is an orthogonal projector, and\n4. MB is an orthogonal projector.\nBy the lemma’s assumption we have the identity Ask = AskMB. Left multiply this\nexpression through by (AskM)∗to see that\nM∗A∗\nskAsk = B.\n(B.1)\nNext, we right multiply both sides of (B.1) by M and use column orthonormality\nof AskM to obtain BM = Ik — this is suﬃcient to show the ﬁrst three conditions\nfor the pseudoinverse. Showing the fourth and ﬁnal condition takes more work. For\nthat we left multiply (B.1) by M so as to express\nMM∗A∗\nskAsk = MB.\nTherefore our task is to show that MM∗A∗\nskAsk is an orthogonal projector.\nConsider the compact SVD Ask = UskΣskVsk. Since Ask is rank-k we have that\nUsk has k columns and Σ is a k × k invertible matrix. Since the columns of AskM\nform an orthonormal basis for the range of Ask, it must be that AskM = UskW\nfor some k × k orthogonal matrix W. Furthermore, this orthogonal matrix can be\nexpressed as ΣskV∗\nskM = W, which implies\nVskV∗\nskM = VskΣ−1\nsk W.\n(B.2)\nPage 144\narXiv Version 2\n\n\nB.1. Quality of preconditioners\nLeast Squares and Optimization\nWe have reached a checkpoint in the proof. Our next task is to obtain an expression\nfor M by simplifying (B.2).\nConsider the subspaces X = range Vsk, Y = ker Ask, and Z = range M, all\ncontained in Rn. We know that Y ∩Z is trivial since rank(AskM) = rank(M). At\nthe same time, since Y and Z are of dimensions n −k and k respectively, it must\nbe that Z = Y ⊥. This fact can be combined with Y = X⊥(from the fundamental\ntheorem of linear algebra) to obtain Z = X, which in turn implies VskV∗\nskM = M.\nTherefore (B.2) simpliﬁes to\nM = VskΣ−1\nsk W.\nThis expression is precisely what we need; when the dust settles, it tells us that\nMM∗A∗\nskAsk = VskV∗\nsk.\nProof of Proposition B.1.1. Let k = rank(A).\nIt suﬃces to prove the statement\nwhere U is the m × k matrix containing the left singular vectors of A. Our proof\ninvolves working with the compact SVD A = UΣV∗, where V is n × k and Σ is\ninvertible. Noting that Ask = SUΣV∗holds by deﬁnition of Ask, we can replace SU\nby its economic QR factorization SU = QR to see\nAsk = QRΣV∗.\n(B.3)\nFurthermore, since rank(Ask) = k it must be that rank(SU) = k. This tells us that\nR is invertible and that Q provides an orthonormal basis for the range of Ask.\nBy assumption on M, the matrix AskM is also an orthonormal basis for the range\nof Ask. Therefore there exists a k × k orthogonal matrix P where QP = AskM. We\ncan rewrite (B.3) as\nAsk = (QP) (P∗RΣV∗) .\nSince the left factor in the above display is simply AskM, we have\nAsk = AskM (P∗RΣV∗) .\n(B.4)\nThe next step is to abbreviate B = P∗RΣV∗and apply Lemma B.1.2 to infer that\nB = M†. Invoking the column-orthonormality of V and invertibility of (Σ, R, P) we\nfurther have B† = M = VΣ−1R−1P. Plug in this expression for M to see that\nAM = (UΣV∗)\n\u0000VΣ−1R−1P\n\u0001\n= UR−1P.\n(B.5)\nThe proof is completed by noting that the singular values of R−1 are the reciprocals\nof the singular values of QR = SU.\nB.1.1\nEﬀective distortion in sketch-and-precondition\nRecall from Appendix A.1 that if the columns of U are an orthonormal basis for a\nlinear subspace L, then the restricted condition number of S on L is\ncond(S; L) = cond(SU).\nThis identity combines with Proposition B.1.1 to make for a remarkable fact.\nNamely, if L = range(A) and M is an orthogonalizer of a sketch SA, then\ncond(S; L) = cond(AM).\n(B.6)\nLet us contextualize this fact algorithmically.\narXiv Version 2\nPage 145\n\n\nLeast Squares and Optimization\nB.2. Basic error analysis\nIf A is an m × n matrix (m ≫n) in a saddle point problem, and if\nthat problem is approached by the sketch-and-precondition methodology\nfrom Section 3.2.2, then the condition number of the preconditioned\nmatrix handed to the iterative solver is equal to the restricted condition\nnumber of S on range(A).\nBut we can take this one step further. By invoking Proposition A.1.1 and applying\n(B.6), we obtain the following expression for the eﬀective distortion of S for L:\nDe(S; L) = cond(AM) −1\ncond(AM) + 1.\n(B.7)\nAlarm bells should be going oﬀin some readers’ heads.\nThe right-hand-side of\n(B.7) is none other than the convergence rate of LSQR (or CGLS) for a least\nsquares problem with data matrix AM! This shows a deep connection between our\nproposed concept of eﬀective distortion and the venerated sketch-and-precondition\nparadigm in RandNLA.\nB.2\nBasic error analysis for least squares problems\nWhen solving a computational problem numerically it is inevitable that the com-\nputed solutions deviate from the problem’s exact solution. This is a simple con-\nsequence of working in ﬁnite-precision arithmetic, and it remains true even when\nusing very reliable algorithms. Furthermore, for large-scale computations it is of-\nten of interest to trade oﬀcomputational complexity with solution accuracy; this\nhas led to algorithms that produce approximate solutions even when run in exact\narithmetic.\nThese facts were encountered in the earliest days of NLA. Their consequence\nin applications has motivated the development of a vast literature on quantifying\nand bounding the error of approximate solutions to computational problems. Since\nseveral of the randomized algorithms from Section 3.2 purport to solve problems to\nany desired accuracy, it is prudent for us to summarize key points from this vast\nliterature here. The material from Appendices B.2.1 to B.2.4 is typically covered in\nan introductory course on numerical analysis. Appendix B.2.5 mentions important\ntopics which might not be covered in such a course.\nRemark B.2.1. We have focused this appendix strongly on basic least squares prob-\nlems (overdetermined and underdetermined) to keep it a reasonable length.\nB.2.1\nConcepts: forward and backward error\nThe forward error of an approximate solution to a computational problem is its\ndistance to the problem’s exact solution. Forward error is easy to interpret, but it\nis not without its limitations. First, it can rarely be computed in practice, since\nit is presumed that we do not have access to the problem’s exact solution. This\nmeans that substantial eﬀort is needed to approximate or bound forward error in\ndiﬀerent contexts. Second, even if one algorithm’s behavior with respect to forward\nerror has been analyzed, it may not be feasible to repurpose the analysis for another\nalgorithm. These shortcomings motivate the ideas of backward error and sensitivity\nanalysis, wherein one asks the following questions, respectively.\nPage 146\narXiv Version 2\n\n\nB.2. Basic error analysis\nLeast Squares and Optimization\n• How much do we need to perturb the problem data so that the computed\nsolution exactly solves the perturbed problem?\n• How does a small perturbation to a given problem change that problem’s\nexact solution?\nThe connection between the two concepts is clear: any bound on backward error\ncan be combined with sensitivity analysis to obtain an estimate of forward error.\nThe idea of sensitivity analysis is especially powerful since it is agnostic to the source\nof the problem’s perturbation; the perturbations might be due to rounding errors\nfrom ﬁnite-precision arithmetic, uncertainty in data (as might arise from experi-\nmental observations), or deliberate choices to only compute approximate solutions.\nIn any of these cases one can combine knowledge of an algorithm’s backward-error\nguarantees to obtain forward error estimates.\nThis reasoning can be carried further to arrive at two major beneﬁts of the\n“backward error plus sensitivity analysis” approach.\n• A large portion of algorithm-speciﬁc error analysis can be accomplished purely\nby understanding the algorithm’s behavior with respect to backward error.\n• For many problems one can cheaply compute upper bounds on a solution’s\nbackward error at runtime.\nThe combination of backward error and sensitivity analysis can therefore be used\nto establish a priori guarantees on algorithm numerical behavior and a posteriori\nguarantees on the quality of an approximate solution. However, we do note that\nsensitivity analysis results require knowledge of problem data that may not be\navailable, such as extreme singular values of a data matrix in a least squares problem.\nTherefore it is still diﬃcult to compute forward error bounds at runtime.\nB.2.2\nBasic sensitivity analysis for unregularized least squares\nproblems\nHere we paraphrase facts from [GV13, §5.3 and §5.6] on sensitivity analysis of\nbasic least squares problems.\nOur restatements adopt the notation we used for\nsaddle point problems, wherein both overdetermined and underdetermined problems\ninvolve a tall m × n matrix A. The overdetermined problem induced by A and an\nm-vector b is\nmin\nx∈Rn ∥Ax −b∥2\n2,\nwhile the underdetermined problem induced by A and an n-vector c is\nmin\ny∈Rm\n\b\n∥y∥2\n2 : A∗y = c\n\t\n.\nIn the following theorem statements, the reader should bear in mind that a per-\nturbation δA can only satisfy ∥δA∥2 < σn(A) if σn(A) is positive. Therefore these\ntheorem statements only apply when A is full-rank.\nTheorem B.2.2. Suppose b is neither in the range of A nor the kernel of A∗, and\nlet x = A†b be the optimal solution of the overdetermined least squares problem with\ndata (A, b). Consider perturbations δb and δA where ∥δA∥2 < σn(A). Deﬁne\nϵ = max\n\u001a∥δA∥2\n∥A∥2\n, ∥δb∥2\n∥b∥2\n\u001b\n(B.8)\narXiv Version 2\nPage 147\n\n\nLeast Squares and Optimization\nB.2. Basic error analysis\ntogether with some auxiliary quantities\nsin θ = ∥b −Ax∥2\n∥b∥2\nand\nν =\n∥Ax∥2\nσn(A)∥x∥2\n.\n(B.9)\nThe perturbation δx necessary for x + δx to solve the least squares problem with\ndata (A + δA, b + δb) satisﬁes\n∥δx∥2\n∥x∥2\n≤ϵ\nn\nν\ncos θ + κ(A)(1 + ν tan θ)\no\n+ O(ϵ2).\n(B.10)\nTheorem B.2.2 restates part of [GV13, Theorem 5.3.1]; following the proof of\nthis result, the source material presents some simpliﬁed estimates for these bounds.\nThe ﬁrst step in producing the simpliﬁed estimate is to note that ν ≤κ(A) holds\nfor all nonzero x. Then, under the modest geometric assumption that θ is bounded\naway from π/2, (B.10) suggests that\n∥δx∥2\n∥x∥2\n≲ϵ\n\u001a\nκ(A) + ∥b −Ax∥2\n∥b∥2\nκ(A)2\n\u001b\n.\n(B.11)\nThe signiﬁcance of this bound is that it shows the dependence of ∥δx∥2 on the square\nof the condition number of A. This dependence is a fundamental obstacle to solving\nleast squares problems to a high degree of accuracy when measured by forward error.\nThe situation is diﬀerent if we try to bound the perturbation ∥A(δx)∥. We provide\nthe following result (which completes the restatement of [GV13, Theorem 5.3.1]) as\na step towards explaining why.\nTheorem B.2.3. Under the hypothesis and notation of Theorem B.2.2, we have\n∥A(δx)∥2\n∥b −Ax∥2\n≤ϵ\n\u001a\n1\nsin θ + κ(A)\n\u0012\n1\nν tan θ + 1\n\u0013\u001b\n+ O(ϵ2).\n(B.12)\nTheorem B.2.3 can be seen as a sensitivity analysis result for a very speciﬁc\nclass of dual saddle point problems. Speciﬁcally, since we have assumed that A is\nfull rank, y solves the dual problem if and only if y = b −Ax where x solves the\nprimal problem. In the same vein, if x + δx solves a perturbed primal problem and\nwe set δy = −A(δx), then y + δy solves the perturbed dual problem.\nAs with the bound for δx, (B.12) can be estimated under mild geometric as-\nsumptions; [GV13, pg. 267] points out that if θ is suﬃciently bounded away from\n0 and π/2, then we should have\n∥δy∥2\n∥y∥2\n≲ϵ κ(A).\n(B.13)\nThis shows there is more hope for solving dual saddle point problems to a high\ndegree of forward error accuracy, at least by comparison to primal saddle point\nproblems. Indeed, the following adaptation of [GV13, Theorem 5.6.1] provides an\neven more favorable sensitivity analysis result for underdetermined least squares.\nTheorem B.2.4. Let y = (A∗)†c solve the underdetermined least squares problem\nwith data (A, c) for a nonzero vector c. Consider perturbations δc and δA where\nϵ = max\n\u001a∥δc∥2\n∥c∥2\n, ∥δA∥2\n∥A∥2\n\u001b\n< σn(A).\nPage 148\narXiv Version 2\n\n\nB.2. Basic error analysis\nLeast Squares and Optimization\nThe perturbation δy needed for y + δy to solve the underdetermined least squares\nproblem with data (A + δA, c + δc) satisﬁes\n∥δy∥2\n∥y∥2\n≤3 ϵ κ(A) + O(ϵ2).\n(B.14)\nB.2.3\nSharper sensitivity analysis for overdetermined problems\nThe analysis results in Appendix B.2.2 have notable limitations: they hide constants\nin O(ϵ2) terms. Luckily there are a wealth of more precise results in the literature\nthat work with diﬀerent notions of error. One good example along these lines for\noverdetermined least squares is given in [Ips09, Fact 5.14], which obtains a relative\nerror bound normalized by the solution of the perturbed problem rather than the\noriginal problem. We paraphrase this fact below.\nTheorem B.2.5. Consider an overdetermined least squares problem with data\n(Ao, bo) that is solved by xo = (Ao)†(bo); consider also perturbed problem data\nA = Ao + δAo and b = bo + δbo together with solution x = A†b.\nIf we have\nrank(Ao) = rank(A) = n and deﬁne\nϵA = ∥δAo∥2\n∥Ao∥2\nand\nϵb =\n∥δbo∥\n∥Ao∥2∥x∥2\n,\nthen we have\n∥xo −x∥2\n∥x∥2\n≤(ϵA + ϵb)κ(Ao) + ϵA\n[κ(Ao)]2 ∥y∥2\n∥Ao∥2∥x∥2\n(B.15)\nfor y = b −Ax.\nBounds of similar character are given for y on [Ips09, pg. 101]. These bounds are\nuseful for understanding how solutions exhibit diﬀerent sensitivity for perturbations\nto the data matrix compared to perturbations to the right-hand-side. Even better\nbounds can be obtained by assuming structured perturbations.\nFor example, if\nrange(Ao) = range(A), then the sensitivity of the overdetermined least squares\nsolution depends only linearly on κ(Ao) [Ips09, Exercise 5.1].\nOur discussion of sensitivity analysis has only considered normwise perturba-\ntions to the problem data. More informative bounds can be had by considering\ncomponentwise perturbations. For example, one can measure a perturbation of an\ninitial matrix A by the smallest α for which |δAij| ≤α|Aij| for all i, j. We re-\nfer the reader to [Hig02, §20.1] for a componentwise sensitivity analysis result on\noverdetermined least squares.\nB.2.4\nSimple constructions to bound backward error\nHere we describe two methods for constructing perturbations to problem data that\nrender an approximate solution exact. By computing the norms of these perturba-\ntions, we can obtain upper bounds on (normwise) backward error. Such bounds are\nuseful as termination criteria for iterative solvers.\nThe notation here matches that of Theorem B.2.5. That is, we say our original\nleast squares problem has data (Ao, bo) and that x is an approximate solution to\nthis problem.\nRemark B.2.6. For discussion on componentwise backward error bounds for overde-\ntermined least squares we again refer the reader to [Hig02, §20.1].\narXiv Version 2\nPage 149\n\n\nLeast Squares and Optimization\nB.2. Basic error analysis\nInconsistent overdetermined problems\nLetting r = bo −Aox, we deﬁne\nδAo = rr∗Ao\n∥r∥2\n2\nand\nA = Ao + δAo,\n(B.16)\nand subsequently\nδbo = −(δAo)x\nand\nb = bo + δbo.\n(B.17)\nSome simple algebra shows that x satisﬁes the normal equations\nA∗(b −Ax) = 0,\ntherefore it solves the overdetermined least squares problem with data (A, b).\nThis construction was ﬁrst given in [Ste77, Theorem 3.2]. It is especially nice\nsince the perturbation is rank-1, and so its spectral norm\n∥δAo∥2 = ∥A∗\nor∥2\n∥r∥2\nis easily computed at runtime by an iterative solver for overdetermined least squares.\nFurthermore, if the iterative solver in question is LSQR, and if we assume exact\narithmetic, then the perturbation will satisfy δAox = 0 [PS82, §6.2]. Therefore\nthe vector δbo in (B.17) is zero when running LSQR (or any equivalent method) in\nexact arithmetic.\nConsistent overdetermined problems: a word of warning\nThe perturbations given in (B.16) – (B.17) are not suitable for least squares prob-\nlems where the optimal residual, (I −AoA†\no)bo, is zero or nearly zero.\nIn these\nsituations one should use a perturbation designed for consistent linear systems. We\ndescribe such a construction here based on termination criteria used in LSQR.\nLet δbo be an arbitrary m-vector. Suppose we set δAo as a function of δbo in\nthe following way:\nδAo = (δbo + bo −Aox) x∗\n∥x∥2\n2\n.\nIt can be seen that Ax = b upon taking b = bo + δbo and A = Ao + δAo, and so x\ntrivially solves the perturbed least squares problem with data (A, b).\nOne can obtain many backward-error constructions by considering diﬀerent\nchoices for δbo as a function of x, the problem data (Ao, bo), and desired error toler-\nances. The construction for LSQR considers two tolerance parameters ϵA, ϵb ∈[0, 1),\nand sets δbo as follows [PS82, §6.1]:\nδbo =\n\u0012\nϵb∥bo∥2\nϵb∥bo∥2 + ϵA∥Ao∥∥x∥2\n\u0013\n(Aox −bo).\n(B.18)\nThe parameters ϵA and ϵb indicate the (relative) sizes of perturbations to (Ao, bo)\nthat a user deems allowable. The authors of [PS82] suggest that “allowable” be\nbased on the extent to which (Ao, bo) are not known exactly in applications.\nIt is natural to want to reduce the two tolerance parameters (ϵA, ϵb) to a single\ntolerance parameter. For example, one might take ϵA = ϵb. Unfortunately, our\nPage 150\narXiv Version 2\n\n\nB.2. Basic error analysis\nLeast Squares and Optimization\nexperience is that taking ϵA = ϵb can produce unreliable algorithm behavior for\noverdetermined problems. Therefore we recommend that one sets ϵA = 0 if one\nwants to think only in terms of a single tolerance for consistent overdetermined\nproblems.\nWhile this may seem like a blunt solution, it ensures that δAo = 0,\nwhich is useful in applying Theorem B.2.5. If setting ϵA = 0 still feels too extreme\nthen one might consider setting ϵA = (ϵb)2 ≪ϵb.\nRemark B.2.7. As a minor detail, we point out that the norm of Ao in (B.18) is de-\nliberately ambiguous. While the spectral norm would probably be most natural, the\nformal LSQR algorithm replaces ∥Ao∥by an estimate of ∥Ao∥F that monotonically\nincreases from one iteration to the next; see [PS82, §5.3].\nB.2.5\nMore advanced concepts\nSome of the earliest work on backward-error analysis for solutions to linear systems\nfocused on componentwise backward error for direct methods [OP64]. A principal\nshortcoming of componentwise error metrics is that they are expensive to compute,\nespecially as stopping criteria for iterative solvers. [ADR92] investigates metrics for\ncomponentwise backward error suitable for iterative solvers.\nThe “backward error plus sensitivity analysis” approach may overestimate for-\nward error. Alternative estimates are available for some Krylov subspace methods\nsuch as PCG, wherein one uses algorithm-speciﬁc recurrences to estimate forward\nerror in the Euclidean norm or the norm induced by Aµ := [A; √µ]. See, for ex-\nample, [AK01; ST02; ST05].\nThese error bounds are more accurate when used\nwith a good preconditioner, which we can generally expect to have when using the\nrandomized algorithms described herein.\nIt is not easy to apply sensitivity analysis results to compute forward error\nbounds at runtime. A primary obstacle in this regard is the need to have accurate\nestimates for the extreme singular values of Ao or the perturbation A (depending\non the sensitivity analysis result in question).\nOn this topic we note that if M\nis an SVD-based preconditioner then we will have computed the singular values\nand right singular vectors of a sketch SAo. Those singular values can be used as\napproximations to the reciprocals of the singular values of Ao. It is conceivable\nthat more accurate approximations could be obtained by applying iterative pre-\nconditioned eigenvalue estimation methods for (Ao)∗Ao. Such iterative methods\ntypically require initialization with an approximate eigenvector. On this front one\ncan use the leading (resp. trailing) left singular vector of M to approximate the\ntrailing (resp. leading) right singular vector of Ao. One should not expect too much\nof such estimates, however.1\nFinally, we note that some Krylov-subspace iterative methods can estimate con-\ndition numbers. For example, when LSQR is applied to a problem with data ma-\ntrix L, it can estimate the Frobenius condition number ∥L∥F∥L†∥F. Bear in mind\nthat in our context we call LSQR with the preconditioned augmented data matrix,\nL = AµM. It would be useful to embed estimators for componentwise condition\nnumbers (which are known to be computable in polynomial time [Dem92]) into\nKrylov subspace solvers.\n1Any “cheap” method for estimating the smallest singular value even of triangular matrices\ncan return substantial overestimates and underestimates [DDM01].\narXiv Version 2\nPage 151\n\n\nLeast Squares and Optimization\nB.3. Ill-posed saddle point problems\nB.3\nIll-posed saddle point problems\nOur saddle point formulations of least squares problems can be problematic when A\nis rank-deﬁcient and µ is zero, in which case our problems can actually be infeasible\nor unbounded below. This appendix uses a limiting analysis to deﬁne canonical\nsolutions to saddle point problems in these settings.\nWe begin by recalling\nmin\nx∈Rn ∥Ax −b∥2\n2 + µ∥x∥2\n2 + 2c∗x,\n((3.2), revisited)\nmin\ny∈Rm ∥A∗y −c∥2\n2 + µ∥y −b∥2\n2,\n((3.3), revisited)\nand\nmin\ny∈Rm{∥y −b∥2\n2 : A∗y = c}.\n((3.4), revisited)\nWe also note the following form of solutions to (3.3), when µ is positive\ny(µ) = (AA∗+ µI)−1 (Ac + µb) .\n(B.19)\nProposition B.3.1. For any tall m×n matrix A, any m-vector b, and any n-vector\nc, we have\nlim\nµ↓0 y(µ) = (A∗)†c + (I −AA†)b.\n(B.20)\nProof. Let k = rank(A). If k = 0 then the claim is trivial since (B.19) reduces to\ny(µ) = b for all µ > 0. Henceforth, we assume k > 1. To establish the claim,\nconsider how the compact SVD A = UΣV∗lets us express\nAA∗+ µIm = Hµ + Gµ\nin terms of the Hermitian matrices\nHµ = U\n\u0000Σ2 + µIk\n\u0001\nU∗\nand\nGµ = µ(Im −UU∗).\nSince HµGµ = GµHµ = 0, the following identity holds for all positive µ:\n(AA∗+ µI)−1 = H†\nµ + G†\nµ.\nFurthermore, by expressing\nH†\nµAc = U\n\u0000Σ2 + µIk\n\u0001−1 ΣV∗c\nG†\nµAc = 0\nµH†\nµb = U\n\u0000Σ2/µ + Ik\n\u0001−1 U∗b\nµG†\nµb = (Im −UU∗)b\nwe ﬁnd that\ny(µ) = H†\nµ(Ac + µb) + G†\nµ(Ac + µb)\n→UΣ−1V∗c + (Im −UU∗)b.\nThis is equivalent to the desired claim since (A∗)† = UΣ−1V∗and AA† = UU∗.\nPage 152\narXiv Version 2\n\n\nB.4. Minimizing regularized quadratics\nLeast Squares and Optimization\nIn light of the above proposition, we take y(0) = (A∗)†c + (I −AA†)b as our\ncanonical solution to the dual problem when µ = 0.\nNow we let x(µ) denote the solution to (3.2) parameterized by µ > 0. It is clear\nthat this is given by\nx(µ) = (A∗A + µI)−1 (A∗b −c).\nIt is easy to show that if c is not orthogonal to the kernel of A, then the norms\n∥x(µ)∥will diverge to inﬁnity as µ tends to zero. However, if c is orthogonal to the\nkernel of A, then we have\nlim\nµ↓0 x(µ) = (A∗A)†(A∗b −c) =: x(0).\n(B.21)\nWe actually take the limit above as our canonical solution to the primal problem\n(3.2) regardless of whether or not c is orthogonal to the kernel of A. Our reasons\nfor this are two-fold. First, the values x(0), y(0) given above are unchanged when\nc is replaced by its orthogonal projection onto range of A∗. Second, the value y(0)\nis always the limiting solution to the dual problem. Meanwhile, the proposed value\nfor x(0) relates to y(0) by y(0) = b −Ax(0).\nB.4\nMinimizing regularized quadratics\nAppendix B.4.1 provides a brief introduction to kernel ridge regression (KRR).\nIt covers the ﬁnite-dimensional linear algebraic formulation and the Hilbert space\nformulation of this regression model, and it explains how ridge regression can be\nunderstood in the KRR framework. Appendix B.4.2 presents a novel preconditioner-\ngeneration procedure for solving a sketch of the regularized quadratic minimization\nproblem (3.1).\nB.4.1\nA primer on kernel ridge regression\nKernel ridge regression (KRR) is a type of nonparametric regression for learning\nreal-valued nonlinear functions f : X →R. It can be formulated as a linear algebra\nproblem as follows: we are given λ > 0, an m × m psd “kernel matrix” K, and a\nvector of observations h in Rm; we want to solve\nargmin\nα∈Rm\n1\nm∥Kα −h∥2\n2 + λ α∗Kα.\n(B.22)\nEquivalently, we want to solve the KRR normal equations (K + mλ I)α = h. The\nnormal equations formulation makes it clear that KRR is an instance of (3.1).\nA standard library for RandNLA would be well-served to not dwell on how\nK is deﬁned; it should instead only focus on how K can be accessed. However,\nstrictly speaking, (B.22) only encodes a KRR problem when the entries of K are\ngiven by pairwise evaluations of a suitable two-argument kernel function on some\ndatapoints {xi}m\ni=1 ⊂X.\nLetting k : X × X →R denote this kernel function,\nthe user will take α that approximately solves (B.22) to deﬁne the learned model\ng(z) = Pm\ni=1 αik(xi, z).\narXiv Version 2\nPage 153\n\n\nLeast Squares and Optimization\nB.4. Minimizing regularized quadratics\nA more technical description\nThe kernel function k induces a reproducing kernel Hilbert space, H, of real-valued\nfunctions on X. This space is (up to closure) equal to the set of real-linear combi-\nnations of functions y 7→ku(y) := k(y, u) parameterized by u ∈X. Additionally,\nif the function\ny 7→f(y) =\nm\nX\ni=1\nαik(y, xi)\nis parameterized by α ∈Rm and {xi}m\ni=1 ⊂X, then its squared norm is given by\n∥f∥2\nH =\nm\nX\ni=1\nm\nX\nj=1\nαiαjk(xi, xj).\nUsing the kernel matrix K with entries Kij = k(xi, xj), we can express that squared\nnorm as ∥f∥2\nH = α∗Kα. Furthermore, for any u ∈X and any f ∈H we have\nf(u) = ⟨f, ku⟩H.\nFor details on reproducing kernel Hilbert spaces we refer the\nreader to [Aro50].\nKRR problem data consists of observations {(xi, hi)}m\ni=1 ⊂X ×R and a positive\nregularization parameter λ.\nWe presume there are functions g in H for which\ng(xi) ≈hi, and we try to obtain such a function by solving\nmin\ng∈H\n1\nm\nm\nX\ni=1\n(g(xi) −hi)2 + λ∥g∥2\nH.\n(B.23)\nIt follows from [KW70] that the solution to (B.23) is in the span of the functions\n{kxi}m\ni=1. Speciﬁcally, the solution is g⋆= Pm\ni=1 αikxi where α solves (B.22).\nWhy is ridge regression a special case of kernel ridge regression?\nSuppose we have an m × n matrix X = [x1, . . . , xn] with linearly independent\ncolumns, and that we want to estimate a linear functional ˆg : Rm →R given access\nto the n point evaluations (xi, ˆg(xi))n\ni=1.\nGiven a regularization parameter λ > 0, ridge regression concerns ﬁnding the\nlinear function g : Rm →R that minimizes\nL(g) =\n\r\r\r\r\r\r\r\n\n\ng(x1)\n...\ng(xn)\n\n−\n\n\nˆg(x1)\n...\nˆg(xn)\n\n\n\r\r\r\r\r\r\r\n2\n2\n+ nλ∥g∥2.\nTo make this concrete, let us represent ˆg and g by m-vectors ˆg and g respectively,\nand set h = X∗ˆg. We also adopt a slight abuse of notation to write L(g) = L(g),\nso that the task of ridge regression can be framed as minimizing\nL(g) = ∥X∗g −h∥2\n2 + λn∥g∥2\n2.\nRemark B.4.1. We pause to emphasize that this is a KRR problem with n datapoints\nthat deﬁne functions on X = Rm. The parameter “m” here has nothing to do with\nthe number of datapoints in the problem; our notational choices for (m, n) here are\nfor consistency with Section 3.\nPage 154\narXiv Version 2\n\n\nB.4. Minimizing regularized quadratics\nLeast Squares and Optimization\nThe essential part of framing ridge regression as a type of KRR is showing that\nthe optimal estimate g is in the range of X. To see why this is the case, let P denote\nthe orthogonal projector onto the range of X. Using X∗Pg = X∗g, we have that\nL(g) = ∥X∗Pg −h∥2\n2 + λn\n\u0000∥Pg∥2\n2 + ∥(I −P)g∥2\n2\n\u0001\n≥∥X∗Pg −h∥2\n2 + λn∥Pg∥2\n2\n= L(Pg),\nand so g minimizes L only if L(Pg) = L(g). Since L(Pg) = L(g) holds if and only\nif Pg = g, we have that g = Xα for some α in Rn. Therefore, under our stated\nassumption that the columns of X are linearly independent, the following problems\nare equivalent\narg min{L(g) : g is a linear functional on Rm},\narg min{∥X∗Xα −h∥2\n2 + λn∥Xα∥2\n2 : α ∈Rn}, and\narg min{∥Xα −ˆg∥2\n2 + λn∥α∥2\n2 : α ∈Rn}.\nThe second of these problems is KRR with a scaled objective and the n × n kernel\nmatrix K = X∗X. The last of these problems is ridge regression in the familiar form.\nGiven this description of ridge regression, one obtains KRR by applying the\nso-called “kernel trick” (see, e.g., [Mur12, §14]). That is, one replaces hj = x∗\nj ˆg by\nhj = ⟨kxj, ˆg⟩H = ˆg(xj)\nand expresses the point evaluation of g = Pn\ni=1 αikxi at xj by\ng(xj) =\nn\nX\ni=1\nαi⟨kxi, kxj⟩H.\nWe note that within the KRR formalism it is allowed for K to be singular, so long\nas it is psd. This is because if β is any vector in the kernel of K then the function\nu 7→Pn\ni=1 βik(xi, u) is identically equal to zero.\nB.4.2\nEﬃcient sketch-and-solve for regularized quadratics\nLet G be an m × m psd matrix and µ be a positive regularization parameter. The\nsketch-and-solve approach to KRR from [AM15] can be considered generically as a\nsketch-and-solve approach to the regularized quadratic minimization problem (3.1).\nThe generic formulation is to approximate G ≈AA∗with an m × n matrix A\n(m ≫n) and then solve\n(AA∗+ µI) z = h.\n(B.24)\nIdentifying b = h/µ, c = 0, and y = z shows that this amounts to a dual saddle\npoint problem of the form (3.3). Here we explain how the sketch-and-precondition\nparadigm can eﬃciently be applied to solve (B.24) under the assumption that AA∗\ndeﬁnes a Nystr¨om approximation of G.\nLet So be an initial m × n sketching operator. The resulting sketch Y = GSo\nand factor R = chol(S∗\noY) together deﬁne A = YR−1. This deﬁnes a Nystr¨om\napproximation since\nAA∗= (KSo) (S∗\noKSo)† (KSo)∗.\narXiv Version 2\nPage 155\n\n\nLeast Squares and Optimization\nB.4. Minimizing regularized quadratics\nRecall that the problem of preconditioner generation entails ﬁnding an orthogonal-\nizer of a sketch of Aµ = [A; √µI]. The fact that A is only represented implicitly\nmakes this delicate, but it remains doable, as we explain below.\nFor the sketching phase of preconditioner generation, we sample a d×m operator\nS (with d ≳n) and set\nAsk\nµ =\n\u0014\nS\n0\n0\nI\n\u0015\nAµ =\n\u0014\nSY\n√µR\n\u0015\nR−1.\nWe then compute the SVD of the augmented matrix\n\u0014 SY\n√µR\n\u0015\n= UΣV∗\nand ﬁnd that setting M = RVΣ−1 satisﬁes Ask\nµ M = U. The preconditioned linear\noperator AµM (and its adjoint) should be applied in the iterative solver by noting\nthe identity\n\u0014 A\n√µI\n\u0015\nM =\n\u0014 Y\n√µR\n\u0015\nVΣ−1.\nThis identity is important since it shows that R−1 need never be applied at any\npoint in the sketch-and-precondition approach to (B.24).\nPage 156\narXiv Version 2\n\n\nAppendix C\nDetails on Low-Rank\nApproximation\nC.1 Theory for submatrix-oriented decompositions ........... 157\nC.1.1 Approximation quality in low-rank ID .......................... 157\nC.1.2 Truncation in column-pivoted matrix decompositions ..... 158\nC.2 Computational routines .............................................. 160\nC.2.1 Power iteration for data-aware sketching ....................... 161\nC.2.2 RangeFinders and QB decompositions .......................... 162\nC.2.3 ID and subset selection .............................................. 166\nC.1\nTheory for submatrix-oriented decompositions\nC.1.1\nApproximation quality in low-rank ID\nProposition C.1.1 (Restatement of Proposition 4.1.3). Let ˜A be any rank-k ap-\nproximation of A that satisﬁes the spectral norm error bound ∥A −˜A∥2 ≤ϵ. If\n˜A = ˜A[:, J]X for some k × n matrix X and an index vector J, then ˆA = A[:, J]X is\na low-rank column ID that satisﬁes\n∥A −ˆA∥2 ≤(1 + ∥X∥2)ϵ.\n((4.16), restated)\nFurthermore, if |Xij| ≤M for all (i, j), then\n∥X∥2 ≤\np\n1 + M 2k(n −k).\n((4.17), restated)\nProof. Proceeding in the grand tradition of adding zero and applying the triangle\ninequality, we have the bound\n∥A −ˆA∥2 = ∥(A −˜A) + (˜A −ˆA)∥2\n≤∥A −˜A∥2 + ∥˜A −ˆA∥2.\n(C.1)\nWe prove (4.16) by bounding the two terms in (C.1). The ﬁrst term is trivial since\nwe have already assumed ∥A −˜A∥2 ≤ϵ. We bound the second term by using the\n157\n\n\nLow-rank Approximation\nC.1. Theory for submatrix-oriented decompositions\nidentity ˜A −ˆA = (˜A[:, J] −A[:, J])X and then invoking submultiplicivity of the\nspectral norm:\n∥˜A −ˆA∥2 ≤∥˜A[:, J] −A[:, J]∥2∥X∥2.\n(C.2)\nFinally, since the spectral norm of a matrix is at least as large as the spectral norm\nof any of its submatrices, we obtain ∥˜A[:, J] −A[:, J]∥2 ≤∥˜A −A∥2 ≤ϵ. Combining\nthis with (C.2) shows that (C.1) implies (4.16).\nNow we address (4.17). For this, consider the m×k matrix C = ˜A[:, J]. Because\nwe have assumed ˜A = CX has rank k and that X is k × n, we can infer that C is\nfull column-rank. We can also extract the columns from both sides of ˜A = CX at\nindices J to ﬁnd the identity C = CX[:, J]. Multiplying this identity through on the\nleft by C†, we can use C†C = Ik to obtain X[:, J] = Ik.\nNow if |Xij| ≤M for all (i, j) in addition to X[:, J] = Ik, then there exists a\npermutation P of the column index set JnK where X[:, P] = [Ik, V] for a k × (n −k)\nmatrix V satisfying |Vij| ≤M. Since permuting the columns of X does not change\nits spectral norm, it suﬃces to bound ∥[Ik, V]∥2. Towards this end, we claim without\nproof that for any block matrix W = [U, V], one has\n∥W∥2 ≤\nq\n∥U∥2\n2 + ∥V∥2\nF.\nThis, combined with ∥Ik∥2 = 1 and ∥V∥2\nF ≤M 2k(n −k), gives (4.17).\nRemark C.1.2. The bound (4.17) is not the best possible. Indeed, looking at the\nﬁnal steps in the proposition’s proof, we see that it suﬃces for M to bound the\nentries of X that are not part of the identity submatrix.\nC.1.2\nTruncation in column-pivoted matrix decompositions\nThis part of the appendix follows up on Section 4.3.3. It examines how changing\nbasis of a column-pivoted decomposition can aﬀect approximation quality when\ntruncating these decompositions. To begin, we set forth some deﬁnitions.\nOur matrix of interest, G, is r × c and given through a decomposition GP = FT\nfor a permutation matrix P and upper-triangular T. Let us say that F has w =\nmin{c, r} columns (and hence that T has as many rows). We use U to denote the set\ninvertible upper-triangular matrices of order w. For a positive integer k < rank(G),\nwe consider the matrix-valued function gk that is deﬁned on U according to\ngk(U) = F(U−1)[:, :k]U[:k, :]TP∗.\nNote that for every diagonal D ∈U we have gk(D) = (F[:, :k])(T[:k, :])P∗.\nProposition C.1.3. Partition the factors F and T into blocks [F1, F2] and [T1; T2]\nso that F1 has k columns and T1 has k rows. If U⋆is an optimal solution to\nmin\nU∈U ∥G −gk(U)∥F.\n(C.3)\nthen the following identity holds in any unitarily invariant norm:\n∥G −gk(U⋆)∥=\n\r\r\r\n\u0010\nI −F1F†\n1\n\u0011\nF2T2\n\r\r\r .\nFurthermore, we have ∥G−gk(Iw×w)∥= ∥F2T2∥, and the identity matrix is optimal\nfor (C.3) if and only if range(F2) and range(F1) are orthogonal.\nPage 158\narXiv Version 2\n\n\nC.1. Theory for submatrix-oriented decompositions\nLow-rank Approximation\nProof. Our proof requires working with several block matrices. First, the matrices\nT1 and T2 are further partitioned so that\nT =\n\u0014T1\nT2\n\u0015\n=\n\u0014T11\nT12\n0\nT22\n\u0015\nwhere T11 is k × k and T22 is (w −k) × (c −k). Next, we introduce U ∈U and\npartition it twice:\nU =\n\u0014U1\nU2\n\u0015\n=\n\u0014U11\nU12\n0\nU22\n\u0015\n.\nIn the expressions above, U1 is a wide matrix of shape k×w, U11 is a square matrix\nof order k, and U22 a square matrix of order w −k. Note that since U is upper-\ntriangular, the same is true of V = U−1. We partition V = [V1, V2] into a leading\nblock of k columns and a trailing block of w −k columns.\nThe point of all this notation is to help us ﬁnd useful expressions for gk(U). Our\nﬁrst such expression is\ngk(U) = FV1U1TP∗.\n(C.4)\nAs a step towards ﬁnding the next expression, we apply block a matrix-inversion\nidentity to compute\nFV1 = F1U−1\n11 .\nMeanwhile, a simple block matrix multiply gives\nU1T = U11T1 + U12T2.\nWe plug these two identities into (C.4) to obtain\ngk(U) = F1\n\u0000T1 + U−1\n11 U12T2\n\u0001\nP∗.\nThis expression is just what we need. By combining it with the identity G = FTP∗,\nwe easily compute the diﬀerence\nG −gk(U) = (F2T2 −F1U−1\n11 U12T2)P∗.\n(C.5)\nHaving access to (C.5) marks a checkpoint in our proof. With it, we obtain the\nfollowing identity for the distance from G to gk(U) in any unitarily invariant norm:\n∥G −gk(U)∥= ∥F2T2 −F1U−1\n11 U12T2∥.\nThis implies our claim about gk(Iw×w), since if U is diagonal, then U12 = 0. There-\nfore all that remains is our claim about matrices that solve (C.3). The truth of this\nclaim is easier to see after a change variables. Upon replacing U−1\n11 U12 by a general\nk × (w −k) matrix, we can express\nmin\nU∈U ∥G −gk(U)∥F = min\nn\n∥F2T2 −F1BT2∥F\n\f\f M ∈Rk×(w−k)o\n,\nand it is easily shown that the matrix M⋆= F†\n1F2 is an optimal solution to the\nproblem on the right-hand side of this equation.\narXiv Version 2\nPage 159\n\n\nLow-rank Approximation\nC.2. Computational routines\nC.2\nComputational routine interfaces and implemen-\ntations\nAs we explained in Section 4, the design space for low-rank approximation algo-\nrithms is quite large. Here we illustrate the breadth and depth of that design space\nwith pseudocode for computational routines needed for four drivers: SVD1, EVD1,\nEVD2, and CURD1 (Algorithms 3 through 6, respectively). All pseudocode here uses\nPython-style zero-based indexing.\nThe dependency structure of these drivers and their supporting functions is\ngiven in Figure C.1. From the ﬁgure we see that the following three interfaces are\ncentral to low-rank approximation.\n• Y = Orth(X) returns an orthonormal basis for the range of a tall input matrix;\nthe number of columns in Y will never be larger than that of X and may be\nsmaller. The simplest implementation of Orth is to return the orthogonal\nfactor from an economic QR decomposition of X.\n• S = SketchOpGen(ℓ, k) returns an ℓ× k oblivious sketching operator sam-\npled from some predetermined distribution. The most common distributions\nused for low-rank approximation were covered in Section 2.3. In actual im-\nplementations, this function would accept an input representing the state of\nthe random number generator.\n• Y = Stabilizer(X) has similar semantics Orth. It diﬀers in that it only\nrequires Y to be better-conditioned than X while preserving its range. The\nrelaxed semantics open up the possibility of methods that are less expensive\nthan computing an orthonormal basis, such as taking the lower-triangular\nfactor from an LU decomposition with column pivoting.\nWe explain the remaining interfaces as they arise in our implementations.\nPage 160\narXiv Version 2\n\n\nC.2. Computational routines\nLow-rank Approximation\nFigure C.1:\nDependency illustration for low-rank approximation functionality.\nLighter gray boxes correspond to abstract interfaces which specify semantics. Any\ninterface can have many diﬀerent implementations. To keep things at a reasonable\nlength the only interface with multiple implementations is QBDecomposer.\nSec-\ntion 4.3 describes many algorithms that could be used for any such interface.\nThe computational routines represented in Figure C.1 include Algorithms 9\nthrough 14. This appendix provides pseudocode for one additional function that is\nnot reﬂected in the ﬁgure: Algorithm 15 shows one way to perform row or column\nsubset selection. We note that while Algorithm 15 is not used in the drivers men-\ntioned above, it could easily have been used in a diﬀerent implementation of CURD1.\nC.2.1\nPower iteration for data-aware sketching\nWhen a TallSketchOpGen is called with parameters (A, k), it produces an n × k\nsketching operator where range(S) is reasonably well-aligned with the subspace\nspanned by the k leading right singular vectors of A. Here, “reasonably” is assessed\nwith respect to the computational cost incurred by running TallSketchOpGen. One\nextreme case of interest is to return an oblivious sketching operator without reading\nany entries of A.\nThis method uses a p-step power iteration technique. When p = 0, the method\nreturns an oblivious sketching operator. It is recommended that one use p > 0 (e.g.,\np ∈{2, 3}) when the singular values of A exhibit “slow” decay.\narXiv Version 2\nPage 161\n\n\nLow-rank Approximation\nC.2. Computational routines\nAlgorithm 8 TSOG1 : a TallSketchOpGen based on a power method, conceptually\nfollowing [ZM20]. The returned sketching operator is suitable for sketching A from\nthe right for purposes of low-rank approximation.\n1: function TSOG1(A, k)\nInputs:\nA is m × n, and k ≪min{m, n} is a positive integer.\nOutput:\nS is n × k, intended for later use in computing Y = AS.\nAbstract subroutines:\nSketchOpGen and Stabilizer\nTuning parameters:\np ≥0 controls the number of steps in the power method. It is equal\nto the total number of matrix-matrix multiplications that will involve\neither A or A∗.\nIf p = 0 then this function returns an oblivious\nsketching operator.\nq ≥1 is the number of matrix-matrix multiplications with A or A∗\nthat accumulate before the stabilizer is called.\n2:\npdone = 0\n3:\nif p is even then\n4:\nS = SketchOpGen(n, k)\n5:\nelse\n6:\nS = A∗SketchOpGen(m, k)\n7:\npdone = pdone + 1\n8:\nif pdone mod q = 0 then\n9:\nS = stabilizer(S)\n10:\nwhile p −pdone ≥2 do\n11:\nS = AS\n12:\npdone = pdone + 1\n13:\nif pdone mod q = 0 then\n14:\nS = stabilizer(S)\n15:\nS = A∗S\n16:\npdone = pdone + 1\n17:\nif pdone mod q = 0 then\n18:\nS = stabilizer(S)\n19:\nreturn S\nC.2.2\nRangeFinders and QB decompositions\nA general RangeFinder takes in a matrix A and a target rank parameter k, and\nreturns a matrix Q of rank d = min{k, rank(A)} such that the range of Q is an\napproximation to the space spanned by A’s top d left singular vectors.\nThe rangeﬁnder problem may also be viewed in the following way: given a\nmatrix A ∈Rm×n and a target rank k ≪min(m, n), ﬁnd a matrix Q with k\ncolumns such that the error ∥A −QQ∗A∥is “reasonably” small. Some RangeFinder\nimplementations are iterative and can accept a target accuracy as a third argument.\nPage 162\narXiv Version 2\n\n\nC.2. Computational routines\nLow-rank Approximation\nThe RangeFinder below, RF1, is very simple. It relies on an implementation of\nthe TallSketchOpGen interface (e.g., TSOG1) as well as the Orth interface.\nAlgorithm 9 RF1 : a RangeFinder that orthogonalizes a single row sketch\n1: function RF1(A, k)\nInputs:\nA is m × n, and k ≪min{m, n} is a positive integer\nOutput:\nQ is a column-orthonormal matrix with d = min{k, rank A} columns.\nWe have range(Q) ⊂range(A); it is intended that range(Q) is an\napproximation to the space spanned by A’s top d left singular vectors.\nAbstract subroutines and tuning parameters:\nTallSketchOpGen\n2:\nS = TallSketchOpGen(A, k)\n# S is n × k\n3:\nY = AS\n4:\nQ = orth(Y)\n5:\nreturn Q\nThe conceptual goal of QB decomposition algorithms is to produce an approx-\nimation ∥A −QB∥≤ϵ (for some unitarily-invariant norm), where rank(QB) ≤\nmin{k, rank(A)}. Our next three algorithms are diﬀerent implementations of the\nQBDecomposer interface. The ﬁrst two of these algorithms require an implementa-\ntion of the RangeFinder interface. The ability of the implementation QB1 to control\naccuracy is completely dependent on that of the underlying rangeﬁnder.\nAlgorithm 10 QB1 : a QBDecomposer that falls back on an abstract rangeﬁnder\n1: function QB1(A, k, ϵ)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\nϵ is a target for the relative error ∥A −QB∥/∥A∥measured in some\nunitarily-invariant norm.\nThis parameter is passed directly to the\nRangeFinder, which determines its precise interpretation.\nOutput:\nQ an m × d matrix returned by the underlying RangeFinder and\nB = Q∗A is d × n; we can be certain that d ≤min{k, rank(A)}. The\nmatrix QB is a low-rank approximation of A.\nAbstract subroutines and tuning parameters:\nRangeFinder\n2:\nQ = RangeFinder(A, k, ϵ)\n3:\nB = Q∗A\n4:\nreturn Q, B\nThe following algorithm builds up a QB decomposition incrementally. It’s said to\nbe fully-adaptive because it has ﬁne-grained control over the error ∥A−QB∥F. If the\nalgorithm is called with k = min{m, n}, then its output will satisfy ∥A−QB∥F ≤ϵ.\narXiv Version 2\nPage 163\n\n\nLow-rank Approximation\nC.2. Computational routines\nAlgorithm 11 QB2 : a QBDecomposer that’s fully-adaptive\n(see [YGL18, Algorithm 2])\n1: function QB2(A, k, ϵ)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\nϵ is a target for the relative error ∥A −QB∥F/∥A∥F. This parameter\nis used as a termination criterion upon reaching the desired accuracy.\nOutput:\nQ an m×d matrix combined of successive outputs from the underlying\nRangeFinder and B = Q∗A is d × n; we can be certain that d ≤\nmin{k, rank(A)}. The matrix QB is a low-rank approximation of A.\nAbstract subroutines:\nRangeFinder\nTuning parameters:\nblock size ≥1 - at every iteration (except possibly for the ﬁnal\niteration), block size columns are added to the matrix Q.\n2:\nd = 0\n3:\nQ = [ ] ∈Rm×d # Preallocation is dangerous; k = min{m, n} is allowed.\n4:\nB = [ ] ∈Rd×n\n5:\nsquared error = ∥A∥2\nF\n6:\nwhile k > d do\n7:\nblock size = min{block size, k −d}\n8:\nQi = RangeFinder(A, block size)\n9:\nQi = orth(Qi −Q (Q∗Qi)) # for numerical stability\n10:\nBi = Q∗\ni A # original matrix A is valid here\n11:\nB =\n\u0014 B\nBi\n\u0015\n12:\nQ =\n\u0002\nQ\nQi\n\u0003\n13:\nd = d + block size\n14:\nA = A −QiBi # modiﬁcation can be implicit, but is required by Line 8\n15:\nsquared error = squared error −∥Bi∥2\nF # compute by a stable method\n16:\nif squared error ≤ϵ2 then\n17:\nbreak\n18:\nreturn Q, B\nOur third and ﬁnal QB algorithm also builds up its approximation incrementally.\nIt is called pass-eﬃcient because it does not access the data matrix A within its\nmain loop (see [DKM06a] for the original deﬁnition of the pass-eﬃcient model).\nThe algorithm can use a requested error tolerance as an early-stopping criterion.\nThis function should never be called with k = min{m, n}. We note that it takes a\nfair amount of algebra to prove that this algorithm produces a correct result.\nPage 164\narXiv Version 2\n\n\nC.2. Computational routines\nLow-rank Approximation\nAlgorithm 12 QB3 : a QBDecomposer that’s pass-eﬃcient and partially adaptive\n(based on [YGL18, Algorithm 4])\n1: function QB3(A, k, ϵ)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\nϵ is a target for the relative error ∥A −QB∥F/∥A∥F. This parameter\nis used as a termination criterion upon reaching the desired accuracy.\nOutput:\nQ an m × d matrix combined of successively-computed orthonor-\nmal bases Qi and B = Q∗A is d × n; we can be certain that\nd ≤min{k, rank(A)}. The matrix QB is a low-rank approximation\nof A.\nAbstract subroutines:\nTallSketchOpGen\nTuning parameters:\nblock size is a positive integer; at every iteration (except possibly for\nthe last), we add block size columns to Q.\n2:\nQ = [ ] ∈Rm×0 # It would be preferable to preallocate.\n3:\nB = [ ] ∈R0×n\n4:\nsquared error = ∥A∥2\nF\n5:\nS = TallSketchOpGen(A, k)\n6:\nG = AS, H = A∗G # Can be done in one pass over A\n7:\nmax blocks = ⌈k/block size⌉\n8:\ni = 0\n9:\nwhile i < max blocks do\n10:\nbstart = i · block size + 1\n11:\nbend = min{(i + 1) · block size, k}\n12:\nSi = S[: , bstart : bend]\n13:\nYi = G[: , bstart : bend] −Q(BSi)\n14:\nQi, Ri = qr(Yi) # the next three lines are for numerical stability\n15:\nQi = Qi −Q(Q∗Qi)\n16:\nQi, ˆRi = qr(Qi)\n17:\nRi = ˆRiRi\n18:\nBi = (H[: , bstart : bend])∗−(YiQ)B −(BSi)∗B\n19:\nBi = (R∗\ni )−1 Bi # in-place triangular solve\n20:\nB =\n\u0014 B\nBi\n\u0015\n21:\nQ =\n\u0002\nQ\nQi\n\u0003\n22:\nsquared error = squared error −∥Bi∥2\nF # compute by a stable method\n23:\ni = i + 1\n24:\nif squared error ≤ϵ2 then\n25:\nbreak\n26:\nreturn Q, B\narXiv Version 2\nPage 165\n\n\nLow-rank Approximation\nC.2. Computational routines\nC.2.3\nID and subset selection\nAs we indicated in Sections 4.2.3 and 4.3.3, the collective design space of algorithms\nfor ID, subset selection, and CUR is very large. This appendix presents one ran-\ndomized algorithm for one-sided ID (Algorithm 14) and an analogous randomized\nalgorithm for subset selection (Algorithm 15). These algorithms are implemented in\nour Python prototype. The Python prototype has two more randomized algorithms\nwhich are not reproduced here (one for one-sided and one for two-sided ID).\nWe need two deterministic functions in order to state these algorithms. The\nﬁrst deterministic function – called as Q, R, J = qrcp(F, k) – returns data for an\neconomic QR decomposition with column pivoting, where the decomposition is re-\nstricted to rank k and may be incomplete. The second deterministic function (Al-\ngorithm 13, below) is the canonical way to use QRCP for one-sided ID. It produces\na column ID when the ﬁnal argument “axis” is set to one; otherwise, it produces a\nrow ID. When used for column ID, it’s typical for Y ∈Rℓ×w to be (very) wide and\nfor k to be only slightly smaller than ℓ(say, ℓ/2 ≤k ≤ℓ).\nAlgorithm 13 deterministic one-sided ID based on QRCP\n1: function osid qrcp(Y, k, axis)\nInputs:\nY is an ℓ× w matrix, typically a sketch of some larger matrix.\nk is an integer, typically close to min{ℓ, w}.\naxis is an integer, equals 1 for row ID and 2 for column ID.\nOutputs:\nWhen axis = 1:\nZ is ℓ× k and I is a length-k index vector.\nTogether, they satisfy Y[I, :] = (ZY[I, :])[I, :].\nWhen axis = 2:\nX is k × w and J is a length-k index vector.\nTogether, they satisfy Y[:, J] = (Y[:, J]X)[:, J].\nAbstract subroutines:\nqrcp\n2:\nif axis == 2 then\n3:\n(ℓ, w) = the number of (rows, columns) in Y\n4:\nassert k ≤min{ℓ, w}\n5:\nQ, R, J = qrcp(Y, k)\n6:\nT = (R[:k, :k])−1 R[:k, k + 1:] # use trsm from BLAS 3\n7:\nX = zeros(k, w)\n8:\nX[:, J] = [Ik×k, T]\n9:\nJ = J[:k]\n10:\nreturn X, J\n11:\nelse\n12:\nX, I = osid qrcp(Y∗, k, axis = 1)\n13:\nZ = X∗\n14:\nreturn Z, I\nPage 166\narXiv Version 2\n\n\nC.2. Computational routines\nLow-rank Approximation\nThe one-sided ID interface is\nM, P = OneSidedID(A, k, s, axis).\nThe output value M is the interpolation matrix and P is the length-k vector of\nskeleton indices. When axis = 1 we are considering a row ID and so obtain the\napproximation ˆA = MA[P, :] to A. When axis = 2, we are considering the low-\nrank column ID ˆA = A[:, P]M. Implementations of this interface perform internal\ncalculations with sketches of rank k + s.\nAlgorithm 14 OSID1 : implements OneSidedID by re-purposing an ID of a sketch.\nBesides the original source [VM16, §5.1], more information on this algorithm can\nbe found in [Mar18, §10.4] and [MT20, §13.4].\n1: function OSID1(A, k, axis)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\naxis is an integer, equal to 1 for row ID or 2 for column ID.\nOutput:\nA matrix Z and vector I satisfying Y[I, :] = (ZY[I, :])[I, :]\nor\na matrix X and vector J satisfying Y[:, J] = (Y[:, J]X)[:, J].\nAbstract subroutines:\nTallSketchOpGen and osid qrcp\nTuning parameters:\ns is a nonnegative integer. The algorithm internally works with a\nsketch of rank k + s.\n2:\nif axis == 1 then # row ID\n3:\nS = TallSketchOpGen(A, k + s)\n4:\nY = AS\n5:\nZ, I = osid qrcp(Y, k, axis = 0)\n6:\nreturn Z, I\n7:\nelse\n8:\nS = TallSketchOpGen(A∗, k + s)∗\n9:\nY = SA\n10:\nX, J = osid qrcp(Y, k, axis = 1)\n11:\nreturn X, J\nConsider the following interface for (randomized) row and column subset selec-\ntion algorithms\nP = RowOrColSelection(A, k, s, axis).\nThe index vector P and oversampling parameter is understood in the same way as\nthe OneSidedID interface. That is, P is a partial permutation of the row index set\nJmK (when axis = 1) or the column index set JnK (when axis = 2). Implementations\nare supposed to perform internal calculations with sketches of rank k + s.\narXiv Version 2\nPage 167\n\n\nLow-rank Approximation\nC.2. Computational routines\nAlgorithm 15 ROCS1 : implements RowOrColSelection by QRCP on a sketch\n1: function ROCS1(A, k, s, axis)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\naxis is an integer, equal to 1 for row selection or 2 for column selection.\nOutput:\nI: a row selection vector of length k\nor\nJ: a column selection vector of length k.\nAbstract subroutines:\nTallSketchOpGen\nTuning parameters:\ns is a nonnegative integer. The algorithm internally works with a\nsketch of rank k + s.\n2:\nif axis == 1 then\n3:\nS = TallSketchOpGen(A, k + s)\n4:\nY = AS\n5:\nQ, R, I = qrcp(Y∗)\n6:\nreturn I[: k]\n7:\nelse\n8:\nS = TallSketchOpGen(A∗, k + s)\n9:\nY = SA\n10:\nQ, R, J = qrcp(Y)\n11:\nreturn J[: k]\nPage 168\narXiv Version 2\n\n\nAppendix D\nCorrectness of Preconditioned\nCholesky QRCP\nIn this appendix we prove Proposition 5.1.2. Since this would involve a fair amount\nof bookkeeping if we used the notation of Algorithm 7, we begin with a more detailed\nstatement of the algorithm.\nLet A be m × n and S be d × m with n ≤d ≪m.\n1. Compute the sketch Ask = SA\n2. Decompose [Qsk, Rsk, J] = qrcp(Ask)\n(a) J is a permutation vector for the index set JnK.\n(b) Abbreviating Ask\nJ = Ask[:, J], we have Ask\nJ = QskRsk.\n(c) Let k = rank(Ask).\n(d) Qsk is m × k and column-orthonormal.\n(e) Rsk = [Rsk\n1 , Rsk\n2 ] is k × n upper-triangular.\n(f) Rsk\n1 is k × k and nonsingular.\n3. Abbreviate AJ = A[:, J] and explicitly from Apre = AJ[:, :k](Rsk\n1 )−1.\n4. Compute an unpivoted QR decomposition Apre = QRpre.\n(a) If rank(A) = k then Q is an orthonormal basis for the range of A.\n(b) For the purposes of this appendix, it does not matter what algorithm\nwe use to compute this decomposition. We assume the decomposition\nis exact.\n5. Explicitly form R = RpreRsk\nThe goal of this proof is to show that the equality A[:, J] = QR holds under the\nassumption that rank(SA) = rank(A). Let us ﬁrst establish some useful identities.\nBy steps 3 and 4 of the algorithm description above, we know that\nRpre = Q∗AJ[:, :k](Rsk\n1 )−1.\n169\n\n\nCorrectness of Preconditioned Cholesky QRCP\nCombining this with the characterization of R from Steps 2e and 5, we have\nR = Q∗AJ[:, :k](Rsk\n1 )−1[Rsk\n1 , Rsk\n2 ].\nWe may further expand this expression as such:\nR = Q∗AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ].\nSince Q is an orthonormal basis for the range of A and, consequently, AJ, we\nhave that\nQR = AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ].\n(D.1)\nWe use (D.1) to establish the claim by a columnwise argument. That is, we show\nthat QR[:, ℓ] = AJ[:, ℓ] for all 1 ≤ℓ≤n.\nFirst, consider the case when ℓ≤k. Let δn\nℓbe the ℓth standard basis vector in\nRn. Then, consider the following series of identities:\nQR[:, ℓ] = QRδn\nℓ\n= AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ]δn\nℓ\n= AJ[:, :k]δk\nℓ= AJ[:, ℓ],\nhence the desired statement holds for ℓ≤k.\nIt remains to show that QR[:, ℓ] = AJ[:, ℓ] for ℓ> k. Note that\nQR[:, ℓ] = AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ]δn\nℓ\n= AJ[:, :k]((Rsk\n1 )−1Rsk\n2 )[:, ℓ−k].\nLet γ = ((Rsk\n1 )−1Rsk\n2 )[:, ℓ−k]. Therefore, in order to obtain the desired identity for\nℓ> k, we will need to show that\nAJ[:, k]γ = AJ[:, ℓ].\nProposition D.0.1. If Ask\nJ [:, ℓ] = Ask\nJ [:, :k]u for some u ∈Rk, then\nAJ[:, ℓ] = AJ[:, :k]u.\nProof. To simplify notation, deﬁne the m×k matrix X = AJ[:, :k] and the m-vector\ny = AJ[:, ℓ].\nSuppose to the contrary that y ̸= Xu and Sy = SXu. Then, SXu −Sy = 0.\nDeﬁne U = ker(S[X, y]) and V = ker([X, y]). Clearly, U contains V . Additionally,\nif U contains a nonzero vector that is not in V , then dim(U) > dim(V ). This would\nfurther imply that rank(S[X, y]) < rank([X, y]).\nIf SXu−Sy = 0, then (u, −1) is a nonzero vector in U that is not in V . However,\nby our assumption, the sketch does not drop rank. Consequently, no such vector\n(u, −1) can exist, and we must have y = Xu.\nWe now prove that Ask\nJ [:, :k]γ = Ask\nJ [:, ℓ].\nTo do this, start by noting that\nAsk\nJ [:, :k] = QskRsk\n1 . Plugging in the deﬁnition of γ, we have\nAsk\nJ [:, :k]γ = QskRsk\n1 (Rsk\n1 )−1(Rsk\n2 )[:, ℓ−k] = Qsk(Rsk\n2 )[:, ℓ−k].\nThe next step is to use the simple observation that Rsk\n2 [:, ℓ−k] = Rsk[:, ℓ] to ﬁnd\nAsk\nJ [:, :k]γ = (QskRsk)[:, ℓ] = Ask\nJ [:, ℓ].\nCombining the above results and Proposition D.0.1 proves Proposition 5.1.2.\nPage 170\narXiv Version 2\n\n\nAppendix E\nBootstrap Methods\nfor Error Estimation\nE.1 Bootstrap methods in a nutshell ................................. 172\nE.2 Sketch-and-solve least squares .................................... 173\nE.3 Sketch-and-solve one-sided SVD ................................. 174\nWhenever a randomized algorithm produces a solution, a question immediately\narises: Is the solution suﬃciently accurate? In many situations, it is possible to\nestimate numerically the error of the solution using the available problem data —\na process that is often referred to as (a posteriori) error estimation.1 In addition\nto resolving uncertainty about the quality of a solution, another key beneﬁt of er-\nror estimation is that it enables computations to be done more adaptively. For\ninstance, error estimates can be used to determine if additional iterations should be\nperformed, or if tuning parameters should be modiﬁed. In this way, error estimates\ncan help to incrementally reﬁne a rough initial solution so that “just enough” work\nis done to reach a desired level of accuracy.\nIn this appendix, we provide a brief overview of bootstrap methods for error\nestimation in RandNLA. Up to now, these tools (which are common in statistics\nand statistical data analysis) have been designed for a handful of sketch-and-solve\ntype algorithms, and the development of bootstrap methods for a wider range of\nrandomized algorithms is an open direction of research. Our main purpose in writing\nthis appendix is to record the consideration we have given to bootstrap methods.\nOur secondary purpose is to provide a starting point for non-experts to survey this\nliterature as it evolves.\n1This should be contrasted with (a priori) error bounds often used in theoretical development\nof RandNLA algorithms, in which one bounds rather than estimates the error, and does so in a\nworst-case way that does not depend on the problem data.\n171\n\n\nBootstrap Methods for Error Estimation\nE.1. Bootstrap methods in a nutshell\nE.1\nBootstrap methods in a nutshell\nBootstrap methods have been studied extensively in the statistics literature for\nmore than four decades, and they comprise a very general framework for quantify-\ning uncertainty [ET94; ST12]. One of the most common uses of these methods in\nstatistics is to assess the accuracy of parameter estimates. This use-case provides\nthe connection between bootstrap methods and error estimation in RandNLA. In-\ndeed, an exact solution to a linear algebra problem can be viewed as an “unknown\nparameter,” and a randomized algorithm can be viewed as providing an “estimate”\nof that parameter. Taking the analogy a step further, a random sketch of a ma-\ntrix can also be viewed as a “dataset” from which the estimate of the “population”\nquantity is computed. Likewise, when bootstrap methods are applied in RandNLA,\nthe rows or columns of a sketched matrix often play the role of “data vectors”.\nWe now formulate the task of error estimation in a way that is convenient for\ndiscussion of bootstrap methods. First, suppose the existence of some ﬁxed but\nunknown “true parameter” θ ∈R. Suppose we estimate this parameter by a value\nˆθ depending on random samples from some probability distribution. The error of ˆθ\nis deﬁned as ˆϵ = |ˆθ −θ|, which we emphasize is both random and unknown. From\nthis standpoint, it is natural to seek the tightest upper bound on ˆϵ that holds with\na speciﬁed probability, say 1−α. This ideal bound is known as the (1−α)-quantile\nof ˆϵ, and is deﬁned more formally as\nq1−α = inf{t ∈[0, ∞) | P(ˆϵ ≤t) ≥1 −α}.\nAn error estimation problem is considered solved if it is possible to construct a\nquantile estimate ˆq1−α such that the inequality ˆϵ ≤ˆq1−α holds with probability\nthat is close to 1 −α.\nThe bootstrap approach to estimating q1−α is based on imagining a scenario\nwhere it is possible to generate many independent samples ˆˆϵ1, . . . ,ˆˆϵN of the random\nvariable ˆϵ. Of course, this is not possible in practice, but if it were, then an estimate\nof q1−α could be easily obtained using the empirical (1−α)-quantile of the samples\nˆˆϵ1, . . . ,ˆˆϵN. The key idea that bootstrap methods use to circumvent the diﬃculty is\nto generate “approximate samples” of ˆϵ, which can be done in practice.\nTo illustrate how approximate samples of ˆϵ can be constructed, consider a generic\nsituation where the estimate ˆθ is computed as a function of a dataset X1, . . . , Xn.\nThat is, suppose ˆθ = f(X1, . . . , Xn) for some function f. Then, a bootstrap sample\nof ˆϵ, denoted ˆˆϵ, is computed as follows:\n• Sample n points { ˆˆXi}n\ni=1 with replacement from the original dataset {Xi}n\ni=1.\n• Compute ˆˆθ := f( ˆˆX1, . . . , ˆˆXn)\n• Compute ˆˆϵ := |ˆˆθ −ˆθ|.\nBy performing N independent iterations of this process, a collection of bootstrap\nsamples ˆˆϵ1, . . . ,ˆˆϵN can be generated. Then, the desired quantile estimate ˆq1−α can\nbe computed as the smallest number t ≥0 for which the inequality\n1\nN\nN\nX\ni=1\nI{ˆˆϵi ≤t} ≥1 −α\nPage 172\narXiv Version 2\n\n\nE.2. Sketch-and-solve least squares\nBootstrap Methods for Error Estimation\nis satisﬁed, where I{·} refers to the {0, 1}-valued indicator function. This quantity\nis also known as the empirical (1 −α)-quantile of ˆˆϵ1, . . . ,ˆˆϵn. We will sometimes\ndenote it by quantile[ˆˆϵ1, . . . ,ˆˆϵn; 1 −α].\nTo provide some intuition for the bootstrap, the random variable ˆˆθ can be viewed\nas a “perturbed version” of ˆθ, where the perturbing mechanism is designed so that\nthe deviations of ˆˆθ around ˆθ are statistically similar to the deviations of ˆθ around\nθ [ET94]. Equivalently, this means that the histogram of ˆˆϵ1, . . . ,ˆˆϵN will serve as\na good approximation to the distribution of the actual random error variable ˆϵ.\nFurthermore, it turns out that this approximation is asymptotically valid (i.e., n →\n∞) and supported by quantitative guarantees in a broad range of situations [ST12].\nE.2\nSketch-and-solve least squares\nThere is a direct analogy between the discussion above and the setting of sketch-\nand-solve algorithms for least squares. First, the “true parameter” θ is the exact\nsolution x⋆= argminx∈Rn∥Ax −b∥2\n2. Second, the dataset X1, . . . , Xn corresponds\nto the sketches [ˆA, ˆb] = S[A, b]. Third, the estimate ˆθ corresponds to the sketch-\nand-solve solution ˆx = argminx∈Rn∥ˆAx −ˆb∥2\n2. Fourth, the error variable can be\ndeﬁned as ˆϵ = ρ(ˆx, x⋆), for a preferred metric ρ, such as that induced by the ℓ2 or\nℓ∞norms.\nOnce these correspondences are recognized, the previous bootstrap sampling\nscheme can be applied. For further background, as well as extensions to error es-\ntimation for iterative randomized algorithms for least squares, we refer to [LWM18].\nMethod 1 (Bootstrap error estimation for sketch-and-solve least squares).\nInput: A positive integer B, the sketches ˆA ∈Rd×n, ˆb ∈Rd, and ˆx ∈Rn.\nFor ℓ∈JBK do in parallel\n1. Draw a vector I := (i1, . . . , id) by sampling d numbers with replacement from\nJdK.\n2. Form the matrix ˆˆA := ˆA[I, :], and vector ˆˆb := ˆb[I].\n3. Compute the following vector and scalar,\nˆˆx := arg min\nx∈Rn\n∥ˆˆAx −ˆˆb∥2\nand\nˆˆϵℓ:= ∥ˆˆx −ˆx∥.\n(E.1)\nReturn: The estimate quantile[ˆˆϵ1, . . . ,ˆˆϵB; 1−α] for the (1−α)-quantile of ∥ˆx−x⋆∥.\nTo brieﬂy comment on some of the computational characteristics of this method,\nit should be emphasized that the for loop can be implemented in an embarrassingly\nparallel manner, which is typical of most bootstrap methods. Second, the method\nonly relies on access to sketched quantities, and hence does not require any access\nto the full matrix A. Likewise, the computational cost of the method is independent\nof the number of rows of A.\narXiv Version 2\nPage 173\n\n\nBootstrap Methods for Error Estimation\nE.3. Sketch-and-solve one-sided SVD\nE.3\nSketch-and-solve one-sided SVD\nWe call the problem of computing the singular values and right singular vectors of\na matrix a “one-sided SVD.” We further use the term “sketch-and-solve one-sided\nSVD” for an algorithm that approximates the top k singular values and singular\nvectors of A by those of a sketch ˆA = SA. Here we consider estimating the error\nincurred by such an algorithm. As matters of notation, we let {(σj, vj)}k\nj=1 denote\nthe top k singular values and right singular vectors of A and {(ˆσj, ˆvj)}k\nj=1 the\ncorresponding quantities for ˆA. We suppose that error is measured uniformly over\nj ∈JkK, which leads us to consider error variables of the form\nϵΣ := max\nj∈JkK |ˆσj −σj|\nand\nϵV := max\nj∈JkK ρ(ˆvj, vj).\nThe following bootstrap method, developed in [LEM20], provides estimates for the\n(1 −α)-quantiles of ϵΣ and ϵV .\nMethod 2 (Bootstrap error estimation for sketch-and-solve SVD).\nInput: The sketch ˆA ∈Rd×n and its top k singular values and right singular\nvectors (ˆσ1, ˆv1), . . . , (ˆσk, ˆvk), a number of samples B, a parameter α ∈(0, 1).\n• For ℓ∈JBK do in parallel\n1. Form ˆˆA ∈Rd×n by sampling d rows from ˆA with replacement.\n2. Compute the top k singular values and right singular vectors of ˆˆA, denoted\nas ˆˆσ1, . . . , ˆˆσk and ˆˆv1, . . . , ˆˆvk. Then, compute the bootstrap samples\nˆˆϵΣ,ℓ:= max\nj∈JkK |ˆˆσj −ˆσj|\n(E.2)\nˆˆϵV,ℓ:= max\nj∈JkK ρ(ˆˆvj, ˆvj).\n(E.3)\nReturn: The estimates quantile[ˆˆϵΣ,1, . . . ,ˆˆϵΣ,B; 1−α] and quantile[ˆˆϵV,1, . . . ,ˆˆϵV,B; 1−α]\nfor the (1 −α)-quantiles of ϵΣ and ϵV .\nAlthough this method is only presented with regard to singular values and right\nsingular vectors, it is also possible to apply a variant of it to estimate the errors\nof approximate left singular vectors.\nHowever, a few extra technical details are\ninvolved, which may be found in [LEM20].\nAnother technique to estimate error in the setting of sketch-and-solve one-sided\nSVD is through the spectral norm ∥ˆA\n∗ˆA−A∗A∥2. Due to the Weyl and Davis-Kahan\ninequalities, an upper bound on ∥ˆA\n∗ˆA −A∗A∥2 directly implies upper bounds on\nthe errors of all the sketched singular values ˆσ1, . . . , ˆσn and sketched right singular\nvectors ˆv1, . . . , ˆvn. Furthermore, the quantiles of the error variable ∥ˆA\n∗ˆA −A∗A∥2\ncan be estimated via the bootstrap, as shown in [LEM23].\nPage 174\narXiv Version 2\n\n\nBibliography\n[AAB+17]\nA. Abdelfattah, H. Anzt, A. Bouteiller, A. Danalis, J. Dongarra, M. Gates,\nA. Haidar, J. Kurzak, P. Luszczek, S. Tomov, S. Wood, P. Wu, I. Yamazaki,\nand A. YarKhan. Roadmap for the Development of a Linear Algebra Library\nfor Exascale Computing: SLATE: Software for Linear Algebra Targeting\nExascale. SLATE Working Notes 01, ICL-UT-17-02. June 2017.\n[ABB+99]\nE. Anderson, Z. Bai, C. Bischof, L. S. Blackford, J. Demmel, J. Dongarra,\nJ. D. Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen.\nLAPACK Users’ Guide. Society for Industrial and Applied Mathematics,\nJan. 1999.\n[AC06]\nN. Ailon and B. Chazelle. “Approximate nearest neighbors and the Fast\nJohnson-Lindenstrauss Transform”. In: Proceedings of the Thirty-Eighth\nAnnual ACM Symposium on Theory of Computing (STOC). STOC ’06.\nSeattle, WA, USA: Association for Computing Machinery, 2006, pp. 557–\n563. isbn: 1595931341.\n[AC09]\nN. Ailon and B. Chazelle. “The Fast Johnson–Lindenstrauss Transform\nand approximate nearest neighbors”. In: SIAM Journal on Computing 39.1\n(Jan. 2009), pp. 302–322.\n[Ach03]\nD. Achlioptas. “Database-friendly random projections: Johnson-Lindenstrauss\nwith binary coins”. In: Journal of Computer and System Sciences 66.4\n(2003), pp. 671–687.\n[ACW17a]\nH. Avron, K. L. Clarkson, and D. P. Woodruﬀ. “Sharper bounds for regular-\nized data ﬁtting”. In: Approximation, Randomization, and Combinatorial\nOptimization. Algorithms and Techniques (2017).\n[ACW17b]\nH. Avron, K. L. Clarkson, and D. P. Woodruﬀ. “Faster kernel ridge regres-\nsion using sketching and preconditioning”. In: SIAM Journal on Matrix\nAnalysis and Applications 38.4 (2017), pp. 1116–1138.\n[AD20]\nN. Anari and M. Derezi´nski. “Isotropy and log-concave polynomials: ac-\ncelerated sampling and high-precision counting of matroid bases”. In: 2020\nIEEE 61st Annual Symposium on Foundations of Computer Science (FOCS).\nIEEE. 2020, pp. 1331–1344.\n[ADD+09]\nE. Agullo, J. Demmel, J. Dongarra, B. Hadri, J. Kurzak, J. Langou, H.\nLtaief, P. Luszczek, and S. Tomov. “Numerical linear algebra on emerging\narchitectures: the PLASMA and MAGMA projects”. In: Journal of Physics:\nConference Series 180 (July 2009), p. 012037.\n[ADN20]\nP. Ahren, J. Demmel, and H.-D. Nguyen. “Algorithms for eﬃcient repro-\nducible ﬂoating point summation”. In: ACM Transactions on Mathematical\nSoftware 46.3 (2020).\n175\n\n\nBibliography\n[ADR92]\nM. Arioli, I. Duﬀ, and D. Ruiz. “Stopping criteria for iterative solvers”.\nIn: SIAM Journal on Matrix Analysis and Applications 13.1 (Jan. 1992),\npp. 138–144.\n[ADV+22]\nN. Anari, M. Derezi´nski, T.-D. Vuong, and E. Yang. “Domain sparsiﬁcation\nof discrete distributions using entropic independence”. In: ACM Symposium\non Discrete Algorithms (SODA). 2022.\n[AGL98]\nC. Ashcraft, R. G. Grimes, and J. G. Lewis. “Accurate symmetric indef-\ninite linear equation solvers”. In: SIAM Journal on Matrix Analysis and\nApplications 20.2 (Jan. 1998), pp. 513–561.\n[AGR16]\nN. Anari, S. O. Gharan, and A. Rezaei. “Monte carlo markov chain al-\ngorithms for sampling strongly rayleigh distributions and determinantal\npoint processes”. In: Conference on Learning Theory (COLT). PMLR. 2016,\npp. 103–115.\n[AK01]\nO. Axelsson and I. Kaporin. “Error norm estimation and stopping criteria in\npreconditioned conjugate gradient iterations”. In: Numerical Linear Algebra\nwith Applications 8.4 (2001), pp. 265–286.\n[AKK+20]\nT. Ahle, M. Kapralov, J. Knudsen, R. Pagh, A. Velingker, D. Woodruﬀ, and\nA. Zandieh. “Oblivious sketching of high-degree polynomial kernels”. In:\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete\nAlgorithms. SIAM, 2020, pp. 141–160.\n[AM15]\nA. E. Alaoui and M. W. Mahoney. “Fast randomized kernel methods with\nstatistical guarantees”. In: Annual Advances in Neural Information Pro-\ncessing Systems. 2015, pp. 775–783.\n[Ame22]\nS. Ameli. IMATE, a high-performance python package for implicit matrix\ntrace estimation. https://pypi.org/project/imate/. 2022.\n[AMT10]\nH. Avron, P. Maymounkov, and S. Toledo. “Blendenpik: Supercharging LA-\nPACK’s Least-Squares Solver”. In: SIAM Journal on Scientiﬁc Computing\n32.3 (Jan. 2010), pp. 1217–1236.\n[ANW14]\nH. Avron, H. Nguyen, and D. Woodruﬀ. “Subspace embeddings for the\npolynomial kernel”. In: Advances in Neural Information Processing Sys-\ntems. Vol. 27. Curran Associates, Inc., 2014.\n[Aro50]\nN. Aronszajn. “Theory of reproducing kernels”. In: Transactions of the\nAmerican mathematical society 68.3 (1950), pp. 337–404.\n[Bac13]\nF. Bach. “Sharp analysis of low-rank kernel matrix approximations”. In:\nProceedings of the 26th Annual Conference on Learning Theory (COLT).\n2013, pp. 185–209.\n[Bal22a]\nO. Balabanov. randKrylov, a MATLAB library for linear systems and eigen-\nvalue problems. https://github.com/obalabanov/randKrylov. 2022.\n[Bal22b]\nO. Balabanov. Randomized Cholesky QR factorizations. 2022. arXiv: 2210.\n09953.\n[BBB+14]\nM. Baboulin, D. Becker, G. Bosilca, A. Danalis, and J. Dongarra. “An ef-\nﬁcient distributed randomized algorithm for solving large dense symmetric\nindeﬁnite linear systems”. In: Parallel Computing 40.7 (July 2014), pp. 213–\n223.\n[BBB15]\nD. J. Biagioni, D. Beylkin, and G. Beylkin. “Randomized interpolative\ndecomposition of separated representations”. In: Journal of Computational\nPhysics 281 (2015), pp. 116–134.\nPage 176\narXiv Version 2\n\n\nBibliography\n[BBG+22]\nO. Balabanov, M. Beaupere, L. Grigori, and V. Lederer. Block subsampled\nrandomized Hadamard transform for low-rank approximation on distributed\narchitectures. 2022. arXiv: 2210.11295.\n[BBK18]\nC. Battaglino, G. Ballard, and T. G. Kolda. “A practical randomized CP\ntensor decomposition”. In: SIAM Journal on Matrix Analysis and Applica-\ntions 39.2 (2018), pp. 876–901.\n[BDH+13]\nM. Baboulin, J. Dongarra, J. Herrmann, and S. Tomov. “Accelerating linear\nsystem solutions using randomization techniques”. In: ACM Trans. Math.\nSoftw. 39.2 (Feb. 2013).\n[BDN15]\nJ. Bourgain, S. Dirksen, and J. Nelson. “Toward a uniﬁed theory of sparse\ndimensionality reduction in Euclidean space”. In: Geometric and Functional\nAnalysis 25.4 (July 2015), pp. 1009–1088.\n[BDR+17]\nM. Baboulin, J. Dongarra, A. R´emy, S. Tomov, and I. Yamazaki. “Solv-\ning dense symmetric indeﬁnite systems using GPUs”. In: Concurrency and\nComputation: Practice and Experience 29.9 (2017). e4055 cpe.4055, e4055.\n[BFG96]\nZ. Bai, G. Fahey, and G. Golub. “Some large-scale matrix computation\nproblems”. In: Journal of Computational and Applied Mathematics 74.1\n(1996), pp. 71–89.\n[BG13]\nC. Boutsidis and A. Gittens. “Improved matrix algorithms via the sub-\nsampled randomized Hadamard transform”. In: SIAM Journal on Matrix\nAnalysis and Applications 34.3 (Jan. 2013), pp. 1301–1340.\n[BG21]\nO. Balabanov and L. Grigori. Randomized block Gram-Schmidt process for\nsolution of linear systems and eigenvalue problems. 2021. arXiv: 2111 .\n14641.\n[BG22]\nO. Balabanov and L. Grigori. “Randomized Gram–Schmidt process with\napplication to GMRES”. In: SIAM Journal on Scientiﬁc Computing 44.3\n(2022), A1450–A1474.\n[BG65]\nP. Businger and G. H. Golub. “Linear least squares solutions by householder\ntransformations”. In: Numerische Mathematik 7.3 (June 1965), pp. 269–\n276.\n[BGL05]\nM. Benzi, G. H. Golub, and J. Liesen. “Numerical solution of saddle point\nproblems”. In: Acta Numerica 14 (2005), pp. 1–137.\n[Bha97]\nR. Bhatia. Matrix Analysis. Springer New York, 1997.\n[Bja19]\nE. K. Bjarkason. “Pass-eﬃcient randomized algorithms for low-rank matrix\napproximation using any number of views”. In: SIAM Journal on Scientiﬁc\nComputing 41.4 (Jan. 2019), A2355–A2383.\n[Bj¨o15]\n˚A. Bj¨orck. Numerical Methods in Matrix Computations. Vol. 59. 2015. isbn:\n978-3-319-05088-1.\n[Bj¨o96]\n˚A. Bj¨orck. Numerical Methods for Least Squares Problems. Society for In-\ndustrial and Applied Mathematics, Jan. 1996.\n[BK21]\nZ. Bujanovic and D. Kressner. “Norm and trace estimation with random\nrank-one vectors”. In: SIAM Journal on Matrix Analysis and Applications\n42.1 (2021), pp. 202–223.\n[BK77]\nJ. R. Bunch and L. Kaufman. “Some stable methods for calculating inertia\nand solving symmetric linear systems”. In: Mathematics of Computation\n31.137 (1977), pp. 163–179.\n[BKW21]\nS. Bamberger, F. Krahmer, and R. Ward. Johnson–Lindenstrauss Embed-\ndings with Kronecker Structure. 2021. arXiv: 2106.13349.\narXiv Version 2\nPage 177\n\n\nBibliography\n[BLR14]\nM. Baboulin, X. S. Li, and F. Rouet. “Using random butterﬂy transfor-\nmations to avoid pivoting in sparse direct methods”. In: High Performance\nComputing for Computational Science - VECPAR. Ed. by M. J. Dayd´e, O.\nMarques, and K. Nakajima. Vol. 8969. Lecture Notes in Computer Science.\nSpringer, 2014, pp. 135–144.\n[BM58]\nG. E. P. Box and M. E. Muller. “A note on the generation of random\nnormal deviates”. In: The Annals of Mathematical Statistics 29.2 (1958),\npp. 610–611.\n[BMD09]\nC. Boutsidis, M. W. Mahoney, and P. Drineas. “An improved approxima-\ntion algorithm for the column subset selection problem”. In: Proceedings of\nthe 20th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA).\n2009, pp. 968–977.\n[BMM+22]\nV. Bharadwaj, O. A. Malik, R. Murray, A. Bulu¸c, and J. Demmel. Distributed-\nMemory Randomized Algorithms for Sparse Tensor CP Decomposition. 2022.\narXiv: 2210.05105.\n[Boh07]\nM. Bohr. “A 30 year retrospective on Dennard’s MOSFET scaling paper”.\nIn: Solid-State Circuits Newsletter, IEEE 12.1 (2007), pp. 11–13.\n[BV21]\nD. Blackman and S. Vigna. “Scrambled linear pseudorandom number gen-\nerators”. In: ACM Trans. Math. Softw. 47.4 (Sept. 2021). Software available\nat https://prng.di.unimi.it/.\n[CDD+96]\nJ. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet,\nK. Stanley, D. Walker, and R. C. Whaley. “ScaLAPACK: a portable lin-\near algebra library for distributed memory computers—design issues and\nperformance”. In: Computer Physics Communications 97.1-2 (1996), pp. 1–\n15.\n[CDO+95]\nJ. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. W. Walker, and R. C.\nWhaley. “A proposal for a set of parallel basic linear algebra subprograms”.\nIn: Proceedings of the Second International Workshop on Applied Parallel\nComputing, Computations in Physics, Chemistry and Engineering Science.\nPARA ’95. Berlin, Heidelberg: Springer-Verlag, 1995, pp. 107–114. isbn:\n3540609024.\n[CDV20]\nD. Calandriello, M. Derezi´nski, and M. Valko. “Sampling from a k-DPP\nwithout looking at all items”. In: Advances in Neural Information Process-\ning Systems 33 (2020), pp. 6889–6899.\n[CET+22]\nY. Chen, E. N. Epperly, J. A. Tropp, and R. J. Webber. Randomly piv-\noted Cholesky: Practical approximation of a kernel matrix with few entry\nevaluations. 2022. arXiv: 2207.06503.\n[CFG95]\nM. T. Chu, R. E. Funderlic, and G. H. Golub. “A rank–one reduction\nformula and its applications to matrix factorizations”. In: SIAM Review\n37.4 (Dec. 1995), pp. 512–530.\n[CFS21]\nC. Cartis, J. Fiala, and Z. Shao. Hashing embeddings of optimal dimension,\nwith applications to linear least squares. 2021. arXiv: 2105.11815.\n[CH22]\nT. Chen and E. Hallman. Krylov-aware stochastic trace estimation. 2022.\narXiv: 2205.01736.\n[CH88]\nS. Chatterjee and A. Hadi. Sensitivity Analysis in Linear Regression. New\nYork: John Wiley & Sons, 1988.\n[CKN22]\nA. Cortinovis, D. Kressner, and Y. Nakatsukasa. Speeding up Krylov sub-\nspace methods for computing f(A)b via randomization. 2212.12758. 2022.\nPage 178\narXiv Version 2\n\n\nBibliography\n[CLA+20]\nA. Chowdhury, P. London, H. Avron, and P. Drineas. “Faster randomized\ninfeasible interior point methods for tall/wide linear programs”. In: Ad-\nvances in Neural Information Processing Systems. Ed. by H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin. Vol. 33. Curran Asso-\nciates, Inc., 2020, pp. 8704–8715.\n[CLN+20]\nK. Chen, Q. Li, K. Newton, and S. J. Wright. “Structured random sketch-\ning for PDE inverse problems”. In: SIAM Journal on Matrix Analysis and\nApplications 41.4 (2020), pp. 1742–1770.\n[CLV17]\nD. Calandriello, A. Lazaric, and M. Valko. “Distributed adaptive sampling\nfor kernel matrix approximation”. In: Artiﬁcial Intelligence and Statistics.\nPMLR. 2017, pp. 1421–1429.\n[C¸M09]\nA. C¸ivril and M. Magdon-Ismail. “On selecting a maximum volume sub-\nmatrix of a matrix and related problems”. In: Theoretical Computer Science\n410.47-49 (Nov. 2009), pp. 4801–4811.\n[CMD+15]\nA. Cichocki, D. Mandic, L. De Lathauwer, G. Zhou, Q. Zhao, C. Caiafa,\nand H. A. Phan. “Tensor decompositions for signal processing applications:\nfrom two-way to multiway component analysis”. In: IEEE Signal Processing\nMagazine 32.2 (2015), pp. 145–163.\n[CMX+22]\nN. Cheng, O. A. Malik, Y. Xu, S. Becker, A. Doostan, and A. Narayan.\nQuadrature Sampling of Parametric Models with Bi-ﬁdelity Boosting. 2022.\narXiv: 2209.05705.\n[Coh16]\nM. B. Cohen. “Nearly tight oblivious subspace embeddings by trace inequal-\nities”. In: Proceedings of the Twenty-Seventh Annual ACM-SIAM Sympo-\nsium on Discrete Algorithms (SODA). Society for Industrial and Applied\nMathematics, Dec. 2016.\n[CP15]\nM. B. Cohen and R. Peng. “Lp row sampling by lewis weights”. In: Proceed-\nings of the forty-seventh annual ACM Symposium on Theory of Computing\n(STOC). 2015, pp. 183–192.\n[CPL+16]\nD. Cheng, R. Peng, Y. Liu, and I. Perros. “SPALS: fast alternating least\nsquares via implicit leverage scores sampling”. In: Advances in Neural In-\nformation Processing Systems. 2016, pp. 721–729.\n[CTU22]\nT. Chen, T. Trogdon, and S. Ubaru. Randomized matrix-free quadrature\nfor spectrum and spectral sum approximation. 2022. arXiv: 2204.01941.\n[CW09]\nK. L. Clarkson and D. P. Woodruﬀ. “Numerical linear algebra in the\nstreaming model”. In: Proceedings of the Forty-First Annual ACM Sym-\nposium on Theory of Computing (STOC). STOC ’09. Bethesda, MD, USA:\nAssociation for Computing Machinery, 2009, pp. 205–214. isbn: 9781605585062.\n[CW13]\nK. L. Clarkson and D. P. Woodruﬀ. “Low rank approximation and re-\ngression in input sparsity time”. In: Proceedings of the Forty-Fifth Annual\nACM Symposium on Theory of Computing (STOC). Palo Alto, Califor-\nnia, USA: Association for Computing Machinery, 2013, pp. 81–90. isbn:\n9781450320290.\n[CW17]\nK. L. Clarkson and D. P. Woodruﬀ. “Low-rank approximation and regres-\nsion in input sparsity time”. In: J. ACM 63.6 (Jan. 2017). This is the\njournal version of a 2013 STOC article by the same name.\n[DB08]\nZ. Drmaˇc and Z. Bujanovi´c. “On the failure of rank-revealing qr factor-\nization software – a case study”. In: ACM Trans. Math. Softw. 35.2 (July\n2008).\narXiv Version 2\nPage 179\n\n\nBibliography\n[DCM+19]\nM. Derezi´nski, K. L. Clarkson, M. W. Mahoney, and M. K. Warmuth. “Min-\nimax experimental design: bridging the gap between statistical and worst-\ncase approaches to least squares regression”. In: Conference on Learning\nTheory (COLT). PMLR. 2019, pp. 1050–1069.\n[DCV19]\nM. Derezi´nski, D. Calandriello, and M. Valko. “Exact sampling of determi-\nnantal point processes with sublinear time preprocessing”. In: Advances in\nNeural Information Processing Systems 32 (2019).\n[DDD+87]\nJ. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, and\nD. Sorensen. Prospectus for the development of a linear algebra library for\nhigh-performance computers. https://netlib.org/lapack/lawns/. LAPACK\nWorking Note 01. Sept. 1987.\n[DDG+22]\nJ. Demmel, J. Dongarra, M. Gates, G. Henry, J. Langou, X. Li, P. Luszczek,\nW. Pereira, J. Riedy, and C. Rubio-Gonz´alez. Proposed Consistent Excep-\ntion Handling for the BLAS and LAPACK. 2022. arXiv: 2207.09281.\n[DDH+09]\nA. Dasgupta, P. Drineas, B. Harb, R. Kumar, and M. W. Mahoney. “Sam-\npling algorithms and coresets for ℓp regression”. In: SIAM Journal on Com-\nputing 38 (2009), pp. 2060–2078.\n[DDH+88]\nJ. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. “An ex-\ntended set of fortran basic linear algebra subprograms”. In: ACM Trans.\nMath. Softw. 14.1 (Mar. 1988), pp. 1–17.\n[DDH+90]\nJ. J. Dongarra, J. Du Croz, S. Hammarling, and I. S. Duﬀ. “A set of level\n3 basic linear algebra subprograms”. In: ACM Trans. Math. Softw. 16.1\n(Mar. 1990), pp. 1–17.\n[DDH07]\nJ. Demmel, I. Dumitriu, and O. Holtz. “Fast linear algebra is stable”. In:\nNumerische Mathematik 108.1 (Oct. 2007), pp. 59–91.\n[DDL+20]\nJ. Demmel, J. Dongarra, J. Langou, J. Langou, P. Luszczek, and M. W.\nMahoney. Prospectus for the Next LAPACK and ScaLAPACK Libraries:\nBasic ALgebra LIbraries for Sustainable Technology with Interdisciplinary\nCollaboration (BALLISTIC). http://www.netlib.org/lapack/lawnspdf/\nlawn297.pdf. July 2020.\n[DDM01]\nJ. Demmel, B. Diament, and G. Malajovich. “On the complexity of com-\nputing error bounds”. In: Foundations of Computational Mathematics 1.1\n(Jan. 2001), pp. 101–125.\n[Dem92]\nJ. Demmel. “The componentwise distance to the nearest singular matrix”.\nIn: SIAM Journal on Matrix Analysis and Applications 13.1 (Jan. 1992),\npp. 10–19.\n[Der19]\nM. Derezi´nski. “Fast determinantal point processes via distortion-free in-\ntermediate sampling”. In: Conference on Learning Theory (COLT). PMLR.\n2019, pp. 1029–1049.\n[Der22a]\nM. Derezi´nski. Algorithmic Gaussianization through Sketching: Converting\nData into Sub-gaussian Random Designs. 2022. eprint: 2206.10291.\n[Der22b]\nM. Derezi´nski. Stochastic Variance-Reduced Newton: Accelerating Finite-\nSum Minimization with Large Batches. 2022. arXiv: 2206.02702.\n[DG03]\nS. Dasgupta and A. Gupta. “An elementary proof of a theorem of Johnson\nand Lindenstrauss”. In: Random Structures and Algorithms 22.1 (2003),\npp. 60–65.\n[DG17]\nJ. A. Duersch and M. Gu. “Randomized QR with column pivoting”. In:\nSIAM Journal on Scientiﬁc Computing 39.4 (Jan. 2017), pp. C263–C291.\nPage 180\narXiv Version 2\n\n\nBibliography\n[DGG+15]\nJ. Demmel, L. Grigori, M. Gu, and H. Xiang. “Communication-avoiding\nrank-revealing QR decomposition”. In: SIAM Journal on Matrix Analysis\nand its Applications 36.1 (2015), pp. 55–89.\n[DGH+19]\nJ. Dongarra, M. Gates, A. Haidar, J. Kurzak, P. Luszczek, P. Wu, I. Ya-\nmazaki, A. Yarkhan, M. Abalenkovs, N. Bagherpour, S. Hammarling, J.\nˇS´ıstek, D. Stevens, M. Zounon, and S. D. Relton. “PLASMA”. In: ACM\nTransactions on Mathematical Software 45.2 (June 2019), pp. 1–35.\n[DGR+74]\nR. Dennard, F. Gaensslen, V. Rideout, E. Bassous, and A. LeBlanc. “Design\nof ion-implanted mosfet’s with very small physical dimensions”. In: Solid-\nState Circuits, IEEE Journal of 9.5 (Oct. 1974), pp. 256–268.\n[DGR19]\nJ. Demmel, L. Grigori, and A. Rusciano. An improved analysis and uniﬁed\nperspective on deterministic and randomized low rank matrix approxima-\ntions. 2019. arXiv: 1910.00223.\n[DHK+06]\nJ. Demmel, Y. Hida, W. Kahan, X. S. Li, S. Mukherjee, and E. J. Riedy.\n“Error bounds from extra-precise iterative reﬁnement”. In: ACM Trans.\nMath. Softw. 32.2 (June 2006), pp. 325–351.\n[Dix83]\nJ. D. Dixon. “Estimating extremal eigenvalues and condition numbers of\nmatrices”. In: SIAM Journal on Numerical Analysis 20.4 (1983), pp. 812–\n814.\n[DJS+19]\nH. Diao, R. Jayaram, Z. Song, W. Sun, and D. P. Woodruﬀ. Optimal Sketch-\ning for Kronecker Product Regression and Low Rank Approximation. 2019.\narXiv: 1909.13384.\n[DKM06a]\nP. Drineas, R. Kannan, and M. W. Mahoney. “Fast Monte Carlo algorithms\nfor matrices I: approximating matrix multiplication”. In: SIAM Journal on\nComputing 36 (2006), pp. 132–157.\n[DKM06b]\nP. Drineas, R. Kannan, and M. W. Mahoney. “Fast Monte Carlo algorithms\nfor matrices II: computing a low-rank approximation to a matrix”. In: SIAM\nJournal on Computing 36 (2006), pp. 158–183.\n[DKM20]\nM. Derezi´nski, R. Khanna, and M. W. Mahoney. “Improved guarantees\nand a multiple-descent curve for Column Subset Selection and the Nystr¨om\nmethod”. In: Annual Advances in Neural Information Processing Systems.\n2020, pp. 4953–4964.\n[DKS10]\nA. Dasgupta, R. Kumar, and T. Sarlos. “A sparse Johnson-Lindenstrauss\ntransform”. In: Proceedings of the Forty-Second ACM Symposium on The-\nory of Computing (STOC). STOC ’10. Cambridge, Massachusetts, USA:\nAssociation for Computing Machinery, 2010, pp. 341–350. isbn: 9781450300506.\n[DLD+21]\nM. Derezi´nski, Z. Liao, E. Dobriban, and M. Mahoney. “Sparse sketches\nwith small inversion bias”. In: Conference on Learning Theory (COLT).\nPMLR. 2021, pp. 1467–1510.\n[DLL+20]\nM. Derezi´nski, F. T. Liang, Z. Liao, and M. W. Mahoney. “Precise ex-\npressions for random projections: low-rank approximation and random-\nized Newton”. In: Advances in Neural Information Processing Systems 33\n(2020), pp. 18272–18283.\n[DLP+21]\nM. Derezi´nski, J. Lacotte, M. Pilanci, and M. W. Mahoney. “Newton-LESS:\nsparsiﬁcation without trade-oﬀs for the sketched Newton update”. In: Ad-\nvances in Neural Information Processing Systems 34 (2021).\n[DM05]\nP. Drineas and M. W. Mahoney. “On the Nystr¨om method for approxi-\nmating a Gram matrix for improved kernel-based learning”. In: Journal of\nMachine Learning Research 6 (2005), pp. 2153–2175.\narXiv Version 2\nPage 181\n\n\nBibliography\n[DM10]\nP. Drineas and M. Mahoney. Eﬀective Resistances, Statistical Leverage, and\nApplications to Linear Equation Solving. 2010. arXiv: 1005.3097.\n[DM16]\nP. Drineas and M. W. Mahoney. “RandNLA: randomized numerical linear\nalgebra”. In: Communications of the ACM 59 (2016), pp. 80–90.\n[DM18]\nP. Drineas and M. W. Mahoney. “Lectures on randomized numerical linear\nalgebra”. In: The Mathematics of Data. Ed. by M. W. Mahoney, J. C.\nDuchi, and A. C. Gilbert. IAS/Park City Mathematics Series. Available at\nhttps://arxiv.org/abs/1712.08880. AMS/IAS/SIAM, 2018, pp. 1–48.\n[DM21a]\nM. Derezi´nski and M. W. Mahoney. “Determinantal point processes in\nrandomized numerical linear algebra”. In: Notices of the AMS 68.1 (2021),\npp. 34–45.\n[DM21b]\nY. Dong and P.-G. Martinsson. Simpler is better: A comparative study of\nrandomized algorithms for computing the CUR decomposition. 2021. arXiv:\n2104.05877.\n[DMB+79]\nJ. J. Dongarra, C. B. Moler, J. R. Bunch, and G. W. Stewart. LINPACK\nusers guide. SIAM, 1979.\n[DMM+11]\nP. Drineas, M. W. Mahoney, S. Muthukrishnan, and T. Sarl´os. “Faster\nleast squares approximation”. In: Numerische Mathematik 117.2 (2011),\npp. 219–249.\n[DMM+12]\nP. Drineas, M. Magdon-Ismail, M. W. Mahoney, and D. P. Woodruﬀ. “Fast\napproximation of matrix coherence and statistical leverage”. In: Journal of\nMachine Learning Research 13 (2012), pp. 3475–3506.\n[DMM06]\nP. Drineas, M. W. Mahoney, and S. Muthukrishnan. “Sampling algorithms\nfor ℓ2 regression and applications”. In: Proceedings of the 17th Annual\nACM-SIAM Symposium on Discrete Algorithms (SODA). 2006, pp. 1127–\n1136.\n[DMM08]\nP. Drineas, M. W. Mahoney, and S. Muthukrishnan. “Relative-error CUR\nmatrix decompositions”. In: SIAM Journal on Matrix Analysis and Appli-\ncations 30.2 (Jan. 2008). This is a longer journal version of two conference\npapers from 2006., pp. 844–881.\n[Drm22]\nZ. Drmac. A LAPACK implementation of the Dynamic Mode Decomposi-\ntion I. https://netlib.org/lapack/lawns/. LAPACK Working Note 298. Oct.\n2022.\n[DRV+06]\nA. Deshpande, L. Rademacher, S. S. Vempala, and G. Wang. “Matrix ap-\nproximation and projective clustering via volume sampling”. In: Theory of\nComputing 2.1 (2006), pp. 225–247.\n[DSS+18]\nH. Diao, Z. Song, W. Sun, and D. Woodruﬀ. “Sketching for Kronecker\nproduct regression and P-splines”. In: Proceedings of the 21st International\nConference on Artiﬁcial Intelligence and Statistics. 2018, pp. 1299–1308.\n[DV06]\nA. Deshpande and S. Vempala. “Adaptive sampling and fast low-rank ma-\ntrix approximation”. In: Approximation, Randomization, and Combinato-\nrial Optimization. Algorithms and Techniques. Ed. by J. D´ıaz, K. Jansen,\nJ. D. P. Rolim, and U. Zwick. Berlin, Heidelberg: Springer Berlin Heidel-\nberg, 2006, pp. 292–303. isbn: 978-3-540-38045-0.\n[EBK19]\nN. B. Erichson, S. L. Brunton, and J. N. Kutz. “Compressed dynamic mode\ndecomposition for background modeling”. In: Journal of Real-Time Image\nProcessing 16.5 (2019), pp. 1479–1492.\n[ED16]\nN. B. Erichson and C. Donovan. “Randomized low-rank dynamic mode\ndecomposition for motion detection”. In: Computer Vision and Image Un-\nderstanding 146 (2016), pp. 40–50.\nPage 182\narXiv Version 2\n\n\nBibliography\n[EMB+20]\nN. B. Erichson, K. Manohar, S. L. Brunton, and J. N. Kutz. “Randomized\nCP tensor decomposition”. In: Machine Learning: Science and Technology\n1.2 (2020), p. 025012.\n[EMK+19]\nN. B. Erichson, L. Mathelin, J. N. Kutz, and S. L. Brunton. “Randomized\ndynamic mode decomposition”. In: SIAM Journal on Applied Dynamical\nSystems 18.4 (2019), pp. 1867–1891.\n[EMW+18]\nN. B. Erichson, A. Mendible, S. Wihlborn, and N. J. Kutz. “Random-\nized nonnegative matrix factorization”. In: Pattern Recognition Letters 104\n(2018), pp. 1–7.\n[Epp23]\nE. Epperly. Stochastic Trace Estimation. https://www.ethanepperly.\ncom/index.php/2023/01/26/stochastic-trace-estimation/. Accessed:\n2023-03-27. Jan. 2023.\n[ET94]\nB. Efron and R. J. Tibshirani. An introduction to the bootstrap. CRC press,\n1994.\n[ETW23]\nE. N. Epperly, J. A. Tropp, and R. J. Webber. XTrace: Making the most\nof every sample in stochastic trace estimation. 2023. arXiv: 2301.07825.\n[EVB+19]\nN. B. Erichson, S. Voronin, S. L. Brunton, and J. N. Kutz. “Randomized\nmatrix decompositions using R”. In: Journal of Statistical Software 89.11\n(2019).\n[EZM+20]\nN. B. Erichson, P. Zheng, K. Manohar, S. L. Brunton, J. N. Kutz, and A. Y.\nAravkin. “Sparse principal component analysis via variable projection”. In:\nSIAM Journal on Applied Mathematics 80.2 (2020), pp. 977–1002.\n[FFG22]\nM. Fahrbach, T. Fu, and M. Ghadiri. Subquadratic Kronecker Regression\nwith Applications to Tensor Decomposition. 2022. arXiv: 2209.04876.\n[FGL21]\nY. Fan, Y. Guo, and T. Lin. A Novel Randomized XR-Based Preconditioned\nCholeskyQR Algorithm. 2021. arXiv: 2111.11148.\n[FHH99]\nR. D. Fierro, P. C. Hansen, and P. S. K. Hansen. “UTV tools: Matlab tem-\nplates for rank-revealing UTV decompositions”. In: Numerical Algorithms\n20.2 (1999), pp. 165–194.\n[FKV04]\nA. Frieze, R. Kannan, and S. Vempala. “Fast Monte-Carlo algorithms for\nﬁnding low-rank approximations”. In: Journal of the ACM 51.6 (2004),\npp. 1025–1041.\n[FS11]\nD. C.-L. Fong and M. Saunders. “LSMR: an iterative algorithm for sparse\nleast-squares problems”. In: 33.5 (Jan. 2011), pp. 2950–2971.\n[FTU21]\nZ. Frangella, J. A. Tropp, and M. Udell. Randomized Nystr¨om Precondi-\ntioning. 2021. arXiv: 2110.02820 [math.NA].\n[FXG18]\nY. Feng, J. Xiao, and M. Gu. “Randomized complete pivoting for solving\nsymmetric indeﬁnite linear systems”. In: SIAM Journal on Matrix Analysis\nand Applications 39.4 (Jan. 2018), pp. 1616–1641.\n[FXG19]\nY. Feng, J. Xiao, and M. Gu. “Flip-ﬂop spectrum-revealing QR factor-\nization and its applications to singular value decomposition”. In: ETNA -\nElectronic Transactions on Numerical Analysis 51 (2019), pp. 469–494.\n[GCG+19]\nC. Gorman, G. Ch´avez, P. Ghysels, T. Mary, F.-H. Rouet, and X. S. Li.\n“Robust and accurate stopping criteria for adaptive randomized sampling\nin matrix-free hierarchically semiseparable construction”. In: SIAM Journal\non Scientiﬁc Computing 41.5 (2019), S61–S85.\n[GDX11]\nL. Grigori, J. Demmel, and H. Xiang. “CALU: a communication optimal\nLU factorization algorithm”. In: SIAM Journal on Matrix Analysis and\nApplications 32 (2011), pp. 1317–1350.\narXiv Version 2\nPage 183\n\n\nBibliography\n[GE95]\nM. Gu and S. C. Eisenstat. “A divide-and-conquer algorithm for the bidi-\nagonal SVD”. In: SIAM Journal on Matrix Analysis and Applications 16.1\n(Jan. 1995), pp. 79–92.\n[GE96]\nM. Gu and S. C. Eisenstat. “Eﬃcient algorithms for computing a strong\nrank-revealing QR factorization”. In: SIAM Journal on Scientiﬁc Comput-\ning 17.4 (July 1996), pp. 848–869.\n[Gem80]\nS. Geman. “A limit theorem for the norm of random matrices”. In: The\nAnnals of Probability 8.2 (1980), pp. 252–261.\n[GIG21]\nN. Gazagnadou, M. Ibrahim, and R. M. Gower. RidgeSketch: A Fast\nsketching based solver for large scale ridge regression. 2021. arXiv: 2105.\n05565 [math.OC].\n[Gir87]\nD. Girard. Un algorithme simple et rapid pour la validation croisee g´een´eralis´ee\nsur des probl´ems de grande taille. 1987.\n[Gir89]\nA. Girard. “A fast ‘Monte-Carlo cross-validation’ procedure for large least\nsquares problems with noisy data”. In: Numerische Mathematik 56.1 (1989),\npp. 1–23.\n[GLA+17]\nM. Gates, P. Luszczek, A. Abdelfattah, J. Kurzak, J. Dongarra, K. Arturov,\nC. Cecka, and C. Freitag. C++ API for BLAS and LAPACK. Tech. rep.\n02, ICL-UT-17-03. Revision 02-21-2018. June 2017.\n[GM10]\nG. H. Golub and G. Meurant. Matrices, Moments and Quadrature with\nApplications. Princeton University Press, 2010. isbn: 9780691143415.\n[GM16]\nA. Gittens and M. W. Mahoney. “Revisiting the Nystr¨om method for im-\nproved large-scale machine learning”. In: Journal of Machine Learning Re-\nsearch 17.117 (2016), pp. 1–65.\n[GM18]\nA. Gopal and P.-G. Martinsson. The PowerURV algorithm for computing\nrank-revealing full factorizations. 2018. arXiv: 1812.06007.\n[GR15]\nR. M. Gower and P. Richt´arik. “Randomized iterative methods for lin-\near systems”. In: SIAM Journal on Matrix Analysis and Applications 36.4\n(2015), pp. 1660–1690.\n[Gri16]\nO. Grisel. SciKit-Learn PR #5299: [MRG+3] Collapsing PCA and Ran-\ndomizedPCA. https://github.com/scikit-learn/scikit-learn/pull/\n5299. Released in SciPy 0.18.0. Website accessed: 2023-04-03. 2016.\n[GS12]\nV. Guruswami and A. K. Sinop. “Optimal column-based low-rank ma-\ntrix reconstruction”. In: Proceedings of the twenty-third annual ACM-SIAM\nSymposium on Discrete Algorithms (SODA). SIAM. 2012, pp. 1207–1214.\n[GS22]\nS. G¨uttel and M. Schweitzer. Randomized sketching for Krylov approxima-\ntions of large-scale matrix functions. 2022. arXiv: 2208.11447.\n[GSO17]\nA. S. Gambhir, A. Stathopoulos, and K. Orginos. “Deﬂation as a method of\nvariance reduction for estimating the trace of a matrix inverse”. In: SIAM\nJournal on Scientiﬁc Computing 39.2 (Jan. 2017), A532–A558.\n[GTZ97]\nS. Goreinov, E. Tyrtyshnikov, and N. Zamarashkin. “A theory of pseu-\ndoskeleton approximations”. In: Linear Algebra and its Applications 261.1-3\n(Aug. 1997), pp. 1–21.\n[GV13]\nG. H. Golub and C. F. Van Loan. Matrix Computations. en. 4th ed. Johns\nHopkins Studies in the Mathematical Sciences. Baltimore, MD: Johns Hop-\nkins University Press, Feb. 2013.\n[GV61]\nG. H. Golub and R. S. Varga. “Chebyshev semi-iterative methods, succes-\nsive overrelaxation iterative methods, and second order Richardson iterative\nmethods”. In: Numerische Mathematik 3.1 (Dec. 1961), pp. 157–168.\nPage 184\narXiv Version 2\n\n\nBibliography\n[GZT95]\nS. A. Gore˘ınov, N. L. Zamarashkin, and E. E. Tyrtyshnikov. “Pseudo-\nskeleton approximations of matrices”. In: Dokl. Akad. Nauk 343.2 (1995),\npp. 151–152.\n[Hig02]\nN. J. Higham. Accuracy and Stability of Numerical Algorithms. Second.\nPhiladelphia, PA, USA: Society for Industrial and Applied Mathematics,\n2002, pp. xxx+680. isbn: 0-89871-521-0.\n[Hig08]\nN. J. Higham. Functions of Matrices. Society for Industrial and Applied\nMathematics, Jan. 2008.\n[Hig97]\nN. J. Higham. “Iterative reﬁnement for linear systems and LAPACK”. In:\nIMA Journal of Numerical Analysis 17.4 (1997), pp. 495–509.\n[HL69]\nR. J. Hanson and C. L. Lawson. “Extensions and applications of the house-\nholder algorithm for solving linear least squares problems”. In: Mathematics\nof Computation 23.108 (1969), pp. 787–812.\n[HMT11]\nN. Halko, P. G. Martinsson, and J. A. Tropp. “Finding structure with\nrandomness: probabilistic algorithms for constructing approximate matrix\ndecompositions”. In: SIAM Review 53.2 (Jan. 2011), pp. 217–288.\n[HS52]\nM. R. Hestenes and E. Stiefel. “Methods of conjugate gradients for solving\nlinear systems”. In: Journal of Research of the National Bureau of Stan-\ndards 49.1 (1952).\n[Hut90]\nM. Hutchinson. “A stochastic estimator of the trace of the inﬂuence ma-\ntrix for Laplacian smoothing splines”. In: Communications in Statistics -\nSimulation and Computation 19.2 (Jan. 1990), pp. 433–450.\n[IEE19]\nIEEE. “IEEE Standard for Floating-Point Arithmetic”. In: IEEE Std 754-\n2019 (Revision of IEEE 754-2008) (2019), pp. 1–84.\n[IM98]\nP. Indyk and R. Motwani. “Approximate nearest neighbors: towards re-\nmoving the curse of dimensionality”. In: Proceedings of the 30th Annual\nACM Symposium on Theory of Computing (STOC). 1998, pp. 604–613.\n[INR+20]\nM. A. Iwen, D. Needell, E. Rebrova, and A. Zare. Lower Memory Obliv-\nious (Tensor) Subspace Embeddings with Fewer Random Bits: Modewise\nMethods for Least Squares. 2020. arXiv: 1912.08294.\n[Int19]\nIntel. Notes for oneMKL Vector Statistics. Tech. rep. Intel Corporation,\n2019, p. 120.\n[Ips09]\nI. C. F. Ipsen. Numerical Matrix Analysis: Linear Systems and Least Squares.\nSociety for Industrial and Applied Mathematics (SIAM), Philadelphia, PA,\n2009, pp. xiv+128. isbn: 978-0-898716-76-4.\n[JKW20]\nR. Jin, T. G. Kolda, and R. Ward. “Faster Johnson–Lindenstrauss trans-\nforms via Kronecker products”. In: Information and Inference: A Journal\nof the IMA 10.4 (Oct. 2020), pp. 1533–1562.\n[JL84]\nW. Johnson and J. Lindenstrauss. “Extensions of Lipshitz mapping into\nHilbert space”. In: Contemporary Mathematics 26 (1984), pp. 189–206.\n[JZ13]\nR. Johnson and T. Zhang. “Accelerating stochastic gradient descent using\npredictive variance reduction”. In: Advances in Neural Information Pro-\ncessing Systems 26 (2013).\n[KAI+15]\nG. Kollias, H. Avron, Y. Ineichen, C. Bekas, A. Curioni, V. Sindhwani,\nand K. Clarkson. libSkylark: A Framework for High-Performance Matrix\nSketching for Statistical Computing. http://sc15.supercomputing.org/\nsites/all/themes/SC15images/tech_poster/poster_files/post213s2-\nfile3.pdf. 2015.\narXiv Version 2\nPage 185\n\n\nBibliography\n[KB09]\nT. G. Kolda and B. W. Bader. “Tensor decompositions and applications”.\nIn: SIAM Review 51.3 (Aug. 2009), pp. 455–500.\n[KC21]\nM. F. Kaloorazi and J. Chen. “Projection-based QLP algorithm for eﬃ-\nciently computing low-rank approximation of matrices”. In: IEEE Trans-\nactions on Signal Processing 69 (2021), pp. 2218–2232.\n[KCL21]\nM. F. Kaloorazi, J. Chen, and R. C. de Lamare. A QLP Decomposition via\nRandomization. 2021. arXiv: 2110.01011.\n[KMT09a]\nS. Kumar, M. Mohri, and A. Talwalkar. “Ensemble Nystr¨om method”. In:\nAnnual Advances in Neural Information Processing Systems. 2009.\n[KMT09b]\nS. Kumar, M. Mohri, and A. Talwalkar. “Sampling techniques for the\nNystr¨om method”. In: Proceedings of the 12th Tenth International Work-\nshop on Artiﬁcial Intelligence and Statistics. 2009, pp. 304–311.\n[KN12]\nD. M. Kane and J. Nelson. “Sparser Johnson-Lindenstrauss Transforms”.\nIn: Proceedings of the 2012 Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA). 2012, pp. 1195–1206.\n[KN14]\nD. M. Kane and J. Nelson. “Sparser Johnson-Lindenstrauss Transforms”.\nIn: J. ACM 61.1 (Jan. 2014). Notes: journal version of a 2012 SODA paper\nby the same name; called “OSNAPs” in a related 2013 paper.\n[KRS+10]\nS. P. Kasiviswanathan, M. Rudelson, A. Smith, and J. Ullman. “The price\nof privately releasing contingency tables and the spectra of random matrices\nwith correlated rows”. In: Proceedings of the Forty-Second ACM Symposium\non Theory of Computing. 2010, pp. 775–784.\n[KT12]\nA. Kulesza and B. Taskar. “Determinantal point processes for machine\nlearning”. In: Foundations and Trends® in Machine Learning 5.2–3 (2012),\npp. 123–286.\n[KV17a]\nR. Kannan and S. Vempala. “Randomized algorithms in numerical linear\nalgebra”. In: Acta Numerica 26 (2017), pp. 95–135.\n[KV17b]\nW. Kong and G. Valiant. “Spectrum estimation from samples”. In: The\nAnnals of Statistics 45.5 (2017), pp. 2218–2247.\n[KW70]\nG. S. Kimeldorf and G. Wahba. “A correspondence between Bayesian esti-\nmation on stochastic processes and smoothing by splines”. In: The Annals\nof Mathematical Statistics 41.2 (1970), pp. 495–502.\n[KW92]\nJ. Kuczy´nski and H. Wo´zniakowski. “Estimating the largest eigenvalue by\nthe power and Lanczos algorithms with a random start”. In: SIAM Journal\non Matrix Analysis and Applications 13.4 (1992), pp. 1094–1122.\n[KWG+17]\nJ. Kurzak, P. Wu, M. Gates, I. Yamazaki, P. Luszczek, G. Ragghianti,\nand J. Dongarra. Designing SLATE: Software for Linear Algebra Targeting\nExascale. SLATE Working Notes 03, ICL-UT-17-06. Oct. 2017.\n[LEM20]\nM. E. Lopes, N. B. Erichson, and M. Mahoney. “Error estimation for\nsketched SVD via the bootstrap”. In: Proceedings of the 37thInternational\nConference on Machine Learning (ICML). Ed. by H. D. III and A. Singh.\nVol. 119. Proceedings of Machine Learning Research. PMLR, July 2020,\npp. 6382–6392.\n[LEM23]\nM. E. Lopes, N. B. Erichson, and M. W. Mahoney. “Bootstrapping the\noperator norm in high dimensions: Error estimation for covariance matrices\nand sketching”. In: Bernoulli 29.1 (2023), pp. 428–450.\n[LHK+79]\nC. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. “Basic linear\nalgebra subprograms for fortran usage”. In: ACM Trans. Math. Softw. 5.3\n(Sept. 1979), pp. 308–323.\nPage 186\narXiv Version 2\n\n\nBibliography\n[Li92]\nK.-h. Li. “Generation of random matrices with orthonormal columns and\nmultivariate normal variates with given sample mean and covariance”.\nIn: Journal of Statistical Computation and Simulation 43.1-2 (Oct. 1992),\npp. 11–18.\n[Lib09]\nE. Liberty. “Accelerated dense random projections”. PhD thesis. Yale Uni-\nversity, May 2009.\n[Lin16]\nL. Lin. “Randomized estimation of spectral densities of large matrices made\naccurate”. In: Numerische Mathematik 136.1 (Aug. 2016), pp. 183–213.\n[LK20]\nB. W. Larsen and T. G. Kolda. Practical Leverage-Based Sampling for Low-\nRank Tensor Decomposition. v3 released in 2022. 2020. arXiv: 2006.16438.\n[LKL10]\nM. Li, J. Kwok, and B.-L. Lu. “Making large-scale Nystr¨om approximation\npossible”. In: Proceedings of the 27th International Conference on Machine\nLearning (ICML). 2010, pp. 631–638.\n[LLD20]\nN. Lindquist, P. Luszczek, and J. Dongarra. “Replacing pivoting in dis-\ntributed Gaussian elimination with randomized techniques”. In: 2020 IEEE/ACM\n11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale\nSystems (ScalA). 2020, pp. 35–43.\n[LLS+17]\nH. Li, G. C. Linderman, A. Szlam, K. P. Stanton, Y. Kluger, and M. Tygert.\n“Algorithm 971: an implementation of a randomized algorithm for principal\ncomponent analysis”. In: ACM Trans. Math. Softw. 43.3 (Jan. 2017).\n[LP19]\nJ. Lacotte and M. Pilanci. Faster least squares optimization. 2019.\n[LSS13]\nQ. Le, T. Sarl´os, and A. Smola. “Fastfood-computing Hilbert space expan-\nsions in loglinear time”. In: International Conference on Machine Learning.\nPMLR. 2013, pp. 244–252.\n[LWM+07]\nE. Liberty, F. Woolfe, P. G. Martinsson, V. Rokhlin, and M. Tygert. “Ran-\ndomized algorithms for the low-rank approximation of matrices”. In: Pro-\nceedings of the National Academy of Sciences 104.51 (2007), pp. 20167–\n20172.\n[LWM18]\nM. E. Lopes, S. Wang, and M. Mahoney. “Error estimation for randomized\nleast-squares algorithms via the bootstrap”. In: Proceedings of the 35th\nInternational Conference on Machine Learning (ICML). Ed. by J. Dy and\nA. Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR,\nJuly 2018, pp. 3217–3226.\n[Mah11]\nM. W. Mahoney. Randomized algorithms for matrices and data. Founda-\ntions and Trends in Machine Learning. Boston: NOW Publishers, 2011.\n[Mah16]\nM. W. Mahoney. Lecture Notes on Randomized Linear Algebra. 2016.\n[Mal22]\nO. A. Malik. “More eﬃcient sampling for tensor decomposition with worst-\ncase guarantees”. In: Proceedings of the 39th International Conference on\nMachine Learning. Vol. 162. Proceedings of Machine Learning Research.\nPMLR, 2022, pp. 14887–14917.\n[Mar15]\nP. G. Martinsson. Blocked rank-revealing QR factorizations: How random-\nized sampling can be used to avoid single-vector pivoting. 2015. arXiv: 1505.\n08115.\n[Mar18]\nP.-G. Martinsson. “Randomized methods for matrix computations”. In:\nThe Mathematics of Data 25.4 (2018). Note: preprint arXiv:1607.01649\npublished in 2016, updated in 2019., pp. 187–239.\n[Mar22a]\nP. G. Martinsson. A remark on the precision of random number generation\nfor RandNLA. Personal communication. 2022.\narXiv Version 2\nPage 187\n\n\nBibliography\n[Mar22b]\nP. G. Martinsson. A remark on pivoting methods in randomized algorithms\nfor low-rank interpolative decomposition. Personal communication. 2022.\n[MB18]\nO. A. Malik and S. Becker. “Low-rank Tucker decomposition of large ten-\nsors using TensorSketch”. In: Advances in Neural Information Processing\nSystems. Vol. 31. Curran Associates, Inc., 2018.\n[MB20]\nO. A. Malik and S. Becker. “Guarantees for the Kronecker fast Johnson–\nLindenstrauss transform using a coherence and sampling argument”. In:\nLinear Algebra and its Applications 602 (Oct. 2020), pp. 120–137.\n[MB21]\nO. A. Malik and S. Becker. “A sampling-based method for tensor ring de-\ncomposition”. In: International Conference on Machine Learning. PMLR,\n2021, pp. 7400–7411.\n[MBM22]\nO. A. Malik, V. Bharadwaj, and R. Murray. Sampling-Based Decomposition\nAlgorithms for Arbitrary Tensor Networks. 2022. arXiv: 2210.03828.\n[MCD+22]\nG. Meanti, L. Carratino, E. De Vito, and L. Rosasco. “Eﬃcient hyperpa-\nrameter tuning for large scale kernel ridge regression”. In: (to appear in)\nProceedings of The 25th International Conference on Artiﬁcial Intelligence\nand Statistics. 2022.\n[MCR+20]\nG. Meanti, L. Carratino, L. Rosasco, and A. Rudi. “Kernel methods through\nthe roof: handling billions of points eﬃciently”. In: Advances in Neural\nInformation Processing Systems. Ed. by H. Larochelle, M. Ranzato, R.\nHadsell, M. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020,\npp. 14410–14422.\n[MD09]\nM. W. Mahoney and P. Drineas. “CUR matrix decompositions for improved\ndata analysis”. In: Proceedings of the National Academy of Sciences 106.3\n(2009), pp. 697–702.\n[MD16]\nM. W. Mahoney and P. Drineas. “Structural properties underlying high-\nquality randomized numerical linear algebra algorithms”. In: Handbook of\nBig Data. Ed. by P. B¨uhlmann, P. Drineas, M. Kane, and M. van de Laan.\nCRC Press, 2016, pp. 137–154.\n[Mez07]\nF. Mezzadri. “How to generate random matrices from the classical compact\ngroups”. In: Notices of the AMS 54.5 (2007), pp. 592–604.\n[MG15]\nC. Melgaard and M. Gu. Gaussian Elimination with Randomized Complete\nPivoting. 2015.\n[MHG17]\nP.-G. Martinsson, G. Q. O. N. Heavner, and R. van de Geijn. “Householder\nQR factorization with randomization for column pivoting (HQRRP)”. In:\nSIAM Journal on Scientiﬁc Computing 39.2 (Jan. 2017), pp. C96–C115.\n[MM13]\nX. Meng and M. W. Mahoney. “Low-distortion subspace embeddings in\ninput-sparsity time and applications to robust linear regression”. In: Pro-\nceedings of the 45th Annual ACM Symposium on Theory of Computing.\n2013, pp. 91–100.\n[MM15]\nC. Musco and C. Musco. “Randomized block Krylov methods for stronger\nand faster approximate singular value decomposition”. In: Neural Informa-\ntion Processing Systems. 2015, pp. 1396–1404.\n[MM17]\nC. Musco and C. Musco. “Recursive sampling for the Nystr¨om method”.\nIn: Advances in Neural Information Processing Systems. Ed. by I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett. Vol. 30. Curran Associates, Inc., 2017.\nPage 188\narXiv Version 2\n\n\nBibliography\n[MMM+21]\nR. A. Meyer, C. Musco, C. Musco, and D. P. Woodruﬀ. “Hutch++: op-\ntimal stochastic trace estimation”. In: Symposium on Simplicity in Algo-\nrithms (SOSA). Society for Industrial and Applied Mathematics, Jan. 2021,\npp. 142–155.\n[MMY15]\nP. Ma, M. W. Mahoney, and B. Yu. “A statistical perspective on algo-\nrithmic leveraging”. In: Journal of Machine Learning Research 16 (2015),\npp. 861–911.\n[Moo65]\nG. E. Moore. “Cramming more components onto integrated circuits”. In:\nElectronics 38.8 (1965), pp. 114–117.\n[MQH19]\nP. G. Martinsson, G. Quintana-Ort´ı, and N. Heavner. “RandUTV: A blocked\nrandomized algorithm for computing a rank-revealing UTV factorization”.\nIn: ACM Trans. Math. Softw. 45.1 (Mar. 2019).\n[MRS+14]\nP.-G. Martinsson, V. Rokhlin, Y. Shkolinsky, and M. Tygert. ID: A software\npackage for low-rank approximation of matrices via interpolative decompo-\nsitions, Version 0.4. http://www.tygert.com/id_doc.4.pdf. Available in\nSciPy. See also https://github.com/klho/PyMatrixID. Mar. 2014.\n[MS22]\nL. Ma and E. Solomonik. Cost-eﬃcient Gaussian Tensor Network Embed-\ndings for Tensor-structured Inputs. 2022. arXiv: 2205.13163.\n[MSM14]\nX. Meng, M. A. Saunders, and M. W. Mahoney. “LSRN: a parallel iterative\nsolver for strongly over- or underdetermined systems”. In: SIAM Journal on\nScientiﬁc Computing 36.2 (Jan. 2014). Software at https://web.stanford.\nedu/group/SOL/software/lsrn/, pp. C95–C118.\n[MT00]\nG. Marsaglia and W. W. Tsang. “The ziggurat method for generating ran-\ndom variables”. In: Journal of Statistical Software 5.8 (2000), pp. 1–7.\n[MT20]\nP.-G. Martinsson and J. A. Tropp. “Randomized numerical linear algebra:\nFoundations and Algorithms”. In: Acta Numerica 29 (2020), pp. 403–572.\n[Mur12]\nK. P. Murphy. Machine Learning: A Probabilistic Perspective (Adaptive\nComputation and Machine Learning series). English. Hardcover. The MIT\nPress, Aug. 24, 2012, p. 1104.\n[MV16]\nP.-G. Martinsson and S. Voronin. “A randomized blocked algorithm for\neﬃciently computing rank-revealing factorizations of matrices”. In: SIAM\nJournal on Scientiﬁc Computing 38.5 (Jan. 2016), S485–S507.\n[MXC+22]\nO. A. Malik, Y. Xu, N. Cheng, S. Becker, A. Doostan, and A. Narayan.\nFast Algorithms for Monotone Lower Subsets of Kronecker Least Squares\nProblems. 2022. arXiv: 2209.05662.\n[MZX+22]\nP. Ma, X. Zhang, X. Xing, J. Ma, and M. W. Mahoney. “Asymptotic anal-\nysis of sampling estimators for randomized numerical linear algebra algo-\nrithms”. In: Journal of Machine Learning Research 23.177 (2022). Journal\nversion of a 2020 PMLR paper of the same name., pp. 1–45.\n[Nak20]\nY. Nakatsukasa. Fast and stable randomized low-rank matrix approxima-\ntion. 2020. arXiv: 2009.11392.\n[NDM22]\nS. Na, M. Derezi´nski, and M. W. Mahoney. Hessian Averaging in Stochastic\nNewton Methods Achieves Superlinear Convergence. 2022. arXiv: 2204 .\n09266.\n[NDT09]\nN. H. Nguyen, T. T. Do, and T. D. Tran. “A fast and eﬃcient algorithm\nfor low-rank approximation of a matrix”. In: Proceedings of the Forty-First\nAnnual ACM Symposium on Theory of Computing (STOC). STOC ’09.\nBethesda, MD, USA: Association for Computing Machinery, 2009, pp. 215–\n224. isbn: 9781605585062.\narXiv Version 2\nPage 189\n\n\nBibliography\n[Ngu07]\nH. Nguyen, ed. GPU Gems 3. First. Addison-Wesley Professional, 2007.\nisbn: 9780321545428.\n[NN13]\nJ. Nelson and H. L. Nguyen. “OSNAP: faster numerical linear algebra al-\ngorithms via sparser subspace embeddings”. In: 2013 IEEE 54th Annual\nSymposium on Foundations of Computer Science. IEEE, Oct. 2013.\n[NT14]\nD. Needell and J. A. Tropp. “Paved with good intentions: Analysis of a ran-\ndomized block Kaczmarz method”. In: Linear Algebra and its Applications\n441 (Jan. 2014), pp. 199–221.\n[NT21]\nY. Nakatsukasa and J. A. Tropp. Fast & Accurate Randomized Algorithms\nfor Linear Systems and Eigenvalue Problems. 2021. arXiv: 2111 . 00113\n[math.NA].\n[NTD10]\nR. Nath, S. Tomov, and J. Dongarra. “Accelerating GPU kernels for dense\nlinear algebra”. In: Proceedings of the 2009 International Meeting on High\nPerformance Computing for Computational Science, VECPAR’10. Berke-\nley, CA: Springer, June 2010.\n[OA17]\nD. Orban and M. Arioli. Iterative Solution of Symmetric Quasi-Deﬁnite\nLinear Systems. Society for Industrial and Applied Mathematics, Apr. 2017.\n[OP64]\nW. Oettli and W. Prager. “Compatibility of approximate solution of linear\nequations with given error bounds for coeﬃcients and right-hand sides”. In:\nNumerische Mathematik 6.1 (Dec. 1964), pp. 405–409.\n[OPA19]\nI. K. Ozaslan, M. Pilanci, and O. Arikan. “Iterative Hessian sketch with\nmomentum”. In: 2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). 2019, pp. 7470–7474.\n[OT17]\nS. Oymak and J. A. Tropp. “Universality laws for randomized dimension\nreduction, with applications”. In: Information and Inference: A Journal of\nthe IMA 7.3 (2017), pp. 337–446.\n[Pag13]\nR. Pagh. “Compressed matrix multiplication”. In: ACM Transactions on\nComputation Theory 5.3 (Aug. 2013), 9:1–9:17.\n[Pan00]\nC.-T. Pan. “On the existence and computation of rank-revealing lu fac-\ntorizations”. In: Linear Algebra and its Applications 316.1 (2000). Special\nIssue: Conference celebrating the 60th birthday of Robert J. Plemmons,\npp. 199–222.\n[Par95]\nD. S. Parker. Random Butterﬂy Transformations with Applications in Com-\nputational Linear Algebra. Tech. rep. University of California, Los Angeles,\n1995.\n[PCK21]\nD. Persson, A. Cortinovis, and D. Kressner. Improved variants of the Hutch++\nalgorithm for trace estimation. 2021. arXiv: 2109.10659.\n[Pec21]\nJ. Peca-Medlin. “Numerical, spectral, and group properties of random but-\nterﬂy matrices”. PhD thesis. University of California, Irvine, 2021.\n[PJM22]\nV. Patel, M. Jahangoshahi, and D. A. Maldonado. Randomized Block Adap-\ntive Linear System Solvers. 2022. arXiv: 2204.01653.\n[PK22]\nD. Persson and D. Kressner. Randomized low-rank approximation of mono-\ntone matrix functions. 2022. arXiv: 2209.11023.\n[Pla05]\nJ. Platt. “FastMap, MetricMap, and Landmark MDS are all Nystr¨om al-\ngorithms”. In: Proceedings of the 10th International Workshop on Artiﬁcial\nIntelligence and Statistics. 2005, pp. 261–268.\n[PMG+13]\nJ. Poulson, B. Marker, R. A. van de Geijn, J. R. Hammond, and N. A.\nRomero. “Elemental”. In: 39.2 (Feb. 2013). https://github.com/elemental/\nElemental, pp. 1–24.\nPage 190\narXiv Version 2\n\n\nBibliography\n[Pou20]\nJ. Poulson. “High-performance sampling of generic determinantal point\nprocesses”. In: Philosophical Transactions of the Royal Society A: Mathe-\nmatical, Physical and Engineering Sciences 378.2166 (Jan. 2020), p. 20190059.\n[PP13]\nN. Pham and R. Pagh. “Fast and scalable polynomial kernels via explicit\nfeature maps”. In: Proceedings of the 19th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining. KDD ’13. New York,\nNY, USA: ACM, 2013, pp. 239–247. isbn: 978-1-4503-2174-7.\n[PS82]\nC. C. Paige and M. A. Saunders. “LSQR: an algorithm for sparse linear\nequations and sparse least squares”. In: ACM Trans. Math. Softw. 8.1 (Mar.\n1982), pp. 43–71.\n[PW16]\nM. Pilanci and M. J. Wainwright. “Iterative Hessian Sketch: fast and ac-\ncurate solution approximation for constrained least-squares”. In: J. Mach.\nLearn. Res. 17.1 (Jan. 2016), pp. 1842–1879.\n[PW17]\nM. Pilanci and M. J. Wainwright. “Newton Sketch: a near linear-time opti-\nmization algorithm with linear-quadratic convergence”. In: SIAM Journal\non Optimization 27.1 (Jan. 2017), pp. 205–245.\n[RB20]\nH. Ren and Z.-J. Bai. Single-pass randomized QLP decomposition for low-\nrank approximation. 2020. arXiv: 2011.06855.\n[RCC+18]\nA. Rudi, D. Calandriello, L. Carratino, and L. Rosasco. “On fast leverage\nscore sampling and optimal learning”. In: Advances in Neural Information\nProcessing Systems 31 (2018).\n[RCR15]\nA. Rudi, R. Camoriano, and L. Rosasco. Less is More: Nystr¨om Computa-\ntional Regularization. 2015. arXiv: 1507.04717.\n[RDA18]\nJ. Riedy, J. Demmel, and P. Ahrens. “Reproducible BLAS: Make Addition\nAssociative Again!” In: SIAM News (Oct. 2018).\n[RM19]\nF. Roosta-Khorasani and M. W. Mahoney. “Sub-sampled Newton meth-\nods”. In: Mathematical Programming 174.1-2 (2019), pp. 293–326.\n[RR07]\nA. Rahimi and B. Recht. “Random features for large-scale kernel machines”.\nIn: Advances in Neural Information Processing Systems. Ed. by J. Platt,\nD. Koller, Y. Singer, and S. Roweis. Vol. 20. Curran Associates, Inc., 2007.\n[RR20]\nB. Rakhshan and G. Rabusseau. “Tensorized random projections”. In: Pro-\nceedings of the Twenty Third International Conference on Artiﬁcial Intel-\nligence and Statistics. Ed. by S. Chiappa and R. Calandra. Vol. 108. Pro-\nceedings of Machine Learning Research. PMLR, Aug. 2020, pp. 3306–3316.\n[RR21]\nB. T. Rakhshan and G. Rabusseau. “Rademacher random projections with\ntensor networks”. In: NeurIPS Workshop on Quantum Tensor Networks in\nMachine Learning. 2021.\n[RST10]\nV. Rokhlin, A. Szlam, and M. Tygert. “A randomized algorithm for prin-\ncipal component analysis”. In: SIAM Journal on Matrix Analysis and Ap-\nplications 31.3 (Jan. 2010), pp. 1100–1124.\n[RT08]\nV. Rokhlin and M. Tygert. “A fast randomized algorithm for overdeter-\nmined linear least-squares regression”. In: Proceedings of the National Academy\nof Sciences 105.36 (Sept. 2008), pp. 13212–13217.\n[Rud12]\nM. Rudelson. “Row products of random matrices”. In: Advances in Math-\nematics 231.6 (2012), pp. 3199–3231.\n[SAI17]\nA. K. Saibaba, A. Alexanderian, and I. C. F. Ipsen. “Randomized matrix-\nfree trace and log-determinant estimators”. In: Numerische Mathematik\n137.2 (Apr. 2017), pp. 353–395.\narXiv Version 2\nPage 191\n\n\nBibliography\n[Sar06]\nT. Sarlos. “Improved approximation algorithms for large matrices via ran-\ndom projections”. In: Proceedings of the 47th Annual IEEE Symposium on\nFoundations of Computer Science (FOCS). FOCS ’06. USA: IEEE Com-\nputer Society, 2006, pp. 143–152. isbn: 0769527205.\n[SCS10]\nY. Saad, J. Chelikowsky, and S. Shontz. “Numerical methods for electronic\nstructure calculations of materials”. In: SIAM Review 52.1 (2010), pp. 3–\n54.\n[SDF+17]\nN. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalexakis,\nand C. Faloutsos. “Tensor decomposition for signal processing and ma-\nchine learning”. In: IEEE Transactions on Signal Processing 65.13 (2017),\npp. 3551–3582.\n[SG21]\nA. Sobczyk and E. Gallopoulos. “Estimating leverage scores via rank re-\nvealing methods and randomization”. In: SIAM Journal on Matrix Analysis\nand Applications 42.3 (2021), pp. 1199–1228.\n[SG22]\nA. Sobczyk and E. Gallopoulos. pylspack: Parallel algorithms and data\nstructures for sketching, column subset selection, regression and leverage\nscores. 2022. arXiv: 2203.02798.\n[SGT+18]\nY. Sun, Y. Guo, J. A. Tropp, and M. Udell. “Tensor random projection for\nlow memory dimension reduction”. In: NeurIPS Workshop on Relational\nRepresentation Learning. 2018.\n[Sil85]\nJ. W. Silverstein. “The smallest eigenvalue of a large dimensional Wishart\nmatrix”. In: The Annals of Probability 13.4 (1985), pp. 1364–1368.\n[SMD+11]\nJ. K. Salmon, M. A. Moraes, R. O. Dror, and D. E. Shaw. “Parallel ran-\ndom numbers: as easy as 1, 2, 3”. In: SC ’11: Proceedings of 2011 Interna-\ntional Conference for High Performance Computing, Networking, Storage\nand Analysis. 2011, pp. 1–12.\n[SNM17]\nP. Seshadri, A. Narayan, and S. Mahadevan. “Eﬀectively subsampled quadra-\ntures for least squares polynomial approximations”. In: SIAM/ASA Journal\non Uncertainty Quantiﬁcation 5.1 (2017), pp. 1003–1023.\n[SSA+18]\nG. Shabat, Y. Shmueli, Y. Aizenbud, and A. Averbuch. “Randomized LU\ndecomposition”. In: Applied and Computational Harmonic Analysis 44.2\n(Mar. 2018). Available on arXiv in 2013., pp. 246–272.\n[ST02]\nZ. Strakoˇs and P. Tich´y. “On error estimation in the conjugate gradient\nmethod and why it works in ﬁnite precision computations”. In: Electron.\nTrans. Numer. Anal. 13 (2002), pp. 56–80.\n[ST05]\nZ. Strakoˇs and P. Tich´y. “Error estimation in preconditioned conjugate\ngradients”. In: BIT Numerical Mathematics 45.4 (Dec. 2005). Extends a\nrelated 2002 paper by the same authors., pp. 789–817.\n[ST12]\nJ. Shao and D. Tu. The jackknife and bootstrap. Springer Science & Business\nMedia, 2012.\n[Ste77]\nG. Stewart. “Research, development, and LINPACK”. In: Mathematical\nSoftware. Elsevier, 1977, pp. 1–14.\n[Ste80]\nG. W. Stewart. “The eﬃcient generation of random orthogonal matrices\nwith an application to condition estimators”. In: SIAM Journal on Numer-\nical Analysis 17.3 (1980), pp. 403–409.\n[Ste92]\nG. Stewart. “An updating algorithm for subspace tracking”. In: IEEE\nTransactions on Signal Processing 40.6 (June 1992), pp. 1535–1541.\nPage 192\narXiv Version 2\n\n\nBibliography\n[Ste93]\nG. Stewart. “Updating a rank-revealing ULV decomposition”. In: SIAM\nJournal on Matrix Analysis and Applications 14.2 (Apr. 1993), pp. 494–\n499.\n[Ste99]\nG. W. Stewart. “The QLP approximation to the singular value decompo-\nsition”. In: SIAM J. Sci. Comput. 20.4 (Jan. 1999), pp. 1336–1348.\n[SV08]\nT. Strohmer and R. Vershynin. “A randomized Kaczmarz algorithm with\nexponential convergence”. In: Journal of Fourier Analysis and Applications\n15.2 (Apr. 2008), pp. 262–278.\n[SWY+21]\nZ. Song, D. Woodruﬀ, Z. Yu, and L. Zhang. “Fast sketching of polynomial\nkernels of polynomial degree”. In: Proceedings of the 38th International\nConference on Machine Learning (ICML). Vol. 139. Proceedings of Ma-\nchine Learning Research. PMLR, 2021, pp. 9812–9823.\n[TDB10]\nS. Tomov, J. Dongarra, and M. Baboulin. “Towards dense linear algebra for\nhybrid GPU accelerated manycore systems”. In: Parallel Computing 36.5-6\n(June 2010), pp. 232–240.\n[TNX15]\nTao, A. Narayan, and D. Xiu. “Weighted discrete least-squares polynomial\napproximation using randomized quadratures”. In: Journal of Computa-\ntional Physics 298 (2015), pp. 787–800.\n[TRL+14]\nJ. H. Tu, C. W. Rowley, D. M. Luchtenburg, S. L. Brunton, and J. N. Kutz.\n“On dynamic mode decomposition: theory and applications”. In: Journal\nof Computational Dynamics 1.2 (2014), pp. 391–421.\n[Tro11]\nJ. A. Tropp. “Improved analysis of the subsampled randomized Hadamard\ntransform”. In: Advances in Adaptive Data Analysis 03.01n02 (Apr. 2011),\npp. 115–126.\n[Tro15]\nJ. A. Tropp. “An introduction to matrix concentration inequalities”. In:\nFoundations and Trends® in Machine Learning 8.1-2 (2015), pp. 1–230.\n[Tro19]\nJ. A. Tropp. Matrix Concentration & Computational Linear Algebra. Lec-\nture notes for a course at ´Ecole Normale Sup´erieure, Paris. July 2019.\n[Tro20]\nJ. A. Tropp. Randomized Algorithms for Matrix Computations. Lecture\nnotes (available online in April 2021). Mar. 2020.\n[Tyg22]\nM. Tygert. A suggestion for sparse sketching operators. Personal commu-\nnication. 2022.\n[TYU+17a]\nJ. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher. “Fixed-rank approxi-\nmation of a positive-semideﬁnite matrix from streaming data”. In: Advances\nin Neural Information Processing Systems. Ed. by I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30.\nCurran Associates, Inc., 2017.\n[TYU+17b]\nJ. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher. “Practical sketch-\ning algorithms for low-rank matrix approximation”. In: SIAM Journal on\nMatrix Analysis and Applications 38.4 (Jan. 2017), pp. 1454–1485.\n[UCS17]\nS. Ubaru, J. Chen, and Y. Saad. “Fast estimation of tr(f(A)) via stochastic\nLanczos quadrature”. In: SIAM Journal on Matrix Analysis and Applica-\ntions 38.4 (Jan. 2017), pp. 1075–1099.\n[Ura13]\nY. Urano. “A fast randomized algorithm for linear least-squares regression\nvia sparse transforms”. MA thesis. New York University, Jan. 2013.\narXiv Version 2\nPage 193\n\n\nBibliography\n[VBG+18]\nJ. S. Vetter, R. Brightwell, M. Gokhale, P. McCormick, R. Ross, J. Shalf,\nK. Antypas, D. Donofrio, T. Humble, C. Schuman, B. Van Essen, S. Yoo,\nA. Aiken, D. Bernholdt, S. Byna, K. Cameron, F. Cappello, B. Chapman,\nA. Chien, M. Hall, R. Hartman-Baker, Z. Lan, M. Lang, J. Leidel, S. Li,\nR. Lucas, J. Mellor-Crummey, P. Peltz Jr., T. Peterka, M. Strout, and\nJ. Wilke. Extreme Heterogeneity 2018 – Productive Computational Science\nin the Era of Extreme Heterogeneity: Report for DOE ASCR Workshop\non Extreme Heterogeneity. English. Tech. rep. https://www.osti.gov/\nservlets/purl/1473756. US DOE Oﬃce of Science (SC), Washington,\nD.C. (United States), Dec. 2018.\n[VEK+19]\nM. Velegar, N. B. Erichson, C. A. Keller, and J. N. Kutz. “Scalable di-\nagnostics for global atmospheric chemistry using ristretto library (version\n1.0)”. In: Geoscientiﬁc Model Development 12.4 (2019), pp. 1525–1539.\n[Ver18]\nR. Vershynin. High-dimensional probability: An introduction with applica-\ntions in data science. Cambridge, United Kingdom New York, NY: Cam-\nbridge University Press, 2018. isbn: 9781108231596.\n[VM15]\nS. Voronin and P.-G. Martinsson. RSVDPACK: An implementation of ran-\ndomized algorithms for computing the singular value, interpolative, and\nCUR decompositions of matrices on multi-core and GPU architectures.\n2015.\n[VM16]\nS. Voronin and P.-G. Martinsson. “Eﬃcient algorithms for CUR and in-\nterpolative matrix decompositions”. In: Advances in Computational Math-\nematics 43.3 (Nov. 2016), pp. 495–516.\n[Wan15]\nS. Wang. A Practical Guide to Randomized Matrix Computations with\nMATLAB Implementations. 2015. arXiv: 1505.07570.\n[WGM18]\nS. Wang, A. Gittens, and M. W. Mahoney. “Sketched ridge regression:\noptimization perspective, statistical perspective, and model averaging”. In:\nJournal of Machine Learning Research 18 (2018), pp. 1–50.\n[WLR+08]\nF. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. “A fast randomized al-\ngorithm for the approximation of matrices”. In: Applied and Computational\nHarmonic Analysis 25.3 (2008), pp. 335–366.\n[Woo14]\nD. P. Woodruﬀ. “Sketching as a tool for numerical linear algebra”. In:\nFound. Trends Theor. Comput. Sci. 10.1–2 (Oct. 2014), pp. 1–157.\n[WS00]\nC. Williams and M. Seeger. “Using the nystr¨om method to speed up kernel\nmachines”. In: Advances in Neural Information Processing Systems. Ed. by\nT. Leen, T. Dietterich, and V. Tresp. Vol. 13. MIT Press, 2000.\n[WX20]\nN. Wu and H. Xiang. “Randomized QLP decomposition”. In: Linear Alge-\nbra and its Applications 599 (Aug. 2020), pp. 18–35.\n[WZ20]\nD. Woodruﬀand A. Zandieh. “Near input sparsity time kernel embeddings\nvia adaptive sampling”. In: Proceedings of the 37th International Confer-\nence on Machine Learning. Vol. 119. Proceedings of Machine Learning Re-\nsearch. PMLR, 2020, pp. 10324–10333.\n[WZ22]\nD. Woodruﬀand A. Zandieh. “Leverage score sampling for tensor product\nmatrices in input sparsity time”. In: Proceedings of the 39th International\nConference on Machine Learning. Vol. 162. Proceedings of Machine Learn-\ning Research. PMLR, 2022, pp. 23933–23964.\n[XG16]\nJ. Xiao and M. Gu. “Spectrum-revealing Cholesky factorization for kernel\nmethods”. In: 2016 IEEE 16th International Conference on Data Mining\n(ICDM). IEEE, Dec. 2016.\nPage 194\narXiv Version 2\n\n\nBibliography\n[XGL17]\nJ. Xiao, M. Gu, and J. Langou. “Fast parallel randomized QR with col-\numn pivoting algorithms for reliable low-rank matrix approximations”. In:\n2017 IEEE 24th International Conference on High Performance Computing\n(HiPC). 2017, pp. 233–242.\n[XRM17]\nP. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Newton-Type Methods\nfor Non-Convex Optimization Under Inexact Hessian Information. 2017.\narXiv: 1708.07164.\n[YCR+18]\nJ. Yang, Y.-L. Chow, C. Re, and M. W. Mahoney. “Weighted SGD for\nLp regression with randomized preconditioning”. In: Journal of Machine\nLearning Research 18.211 (2018), pp. 1–43.\n[YGL+17]\nW. Yu, Y. Gu, J. Li, S. Liu, and Y. Li. “Single-pass PCA of large high-\ndimensional data”. In: Proceedings of the Twenty-Sixth International Joint\nConference on Artiﬁcial Intelligence, IJCAI-17. 2017, pp. 3350–3356.\n[YGL18]\nW. Yu, Y. Gu, and Y. Li. “Eﬃcient randomized algorithms for the ﬁxed-\nprecision low-rank matrix approximation”. In: SIAM Journal on Matrix\nAnalysis and Applications 39.3 (Jan. 2018), pp. 1339–1359.\n[YMM16]\nJ. Yang, X. Meng, and M. W. Mahoney. “Implementing randomized matrix\nalgorithms in parallel and distributed environments”. In: Proceedings of the\nIEEE 104.1 (2016), pp. 58–92.\n[YPW17]\nY. Yang, M. Pilanci, and M. J. Wainwright. “Randomized sketches for ker-\nnels: fast and optimal nonparametric regression”. In: The Annals of Statis-\ntics 45.3 (2017), pp. 991–1023.\n[YXR+18]\nZ. Yao, P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Inexact Non-\nConvex Newton-Type Methods. 2018. arXiv: 1802.06925.\n[ZM20]\nB. Zhang and M. Mascagni. Pass-Eﬃcient Randomized LU Algorithms for\nComputing Low-Rank Matrix Approximation. 2020. arXiv: 2002.07138.\narXiv Version 2\nPage 195\n",
  "normalized_text": "Randomized Numerical Linear Algebra\nA Perspective on the Field With an Eye to Software\nApril 12, 2023\narXiv:2302.11474v2 [math.NA] 12 Apr 2023\n\nPreface\nRandomized numerical linear algebra – RandNLA, for short – concerns the use of\nrandomization as a resource to develop improved algorithms for large-scale linear\nalgebra computations. The origins of contemporary RandNLA lay in theoretical\ncomputer science, where it blossomed from a simple idea: randomization provides\nan avenue for computing approximate solutions to linear algebra problems more\neﬃciently than deterministic algorithms. This idea proved fruitful in and was largely\ndriven by the development of scalable algorithms for machine learning and statistical\ndata analysis applications. However, the true potential of RandNLA only came into\nfocus once it began to integrate with the ﬁelds of numerical analysis and “classical”\nnumerical linear algebra.\nThrough the eﬀorts of many individuals, randomized\nalgorithms have been developed that provide full control over the accuracy of their\nsolutions and that can be every bit as reliable as algorithms that might be found in\nlibraries such as LAPACK.\nThe spectrum of possibilities oﬀered by RandNLA has created a virtuous cycle\nof contributions by numerical analysts, statisticians, theoretical computer scientists,\nand the machine learning community. Recent years have even seen the incorporation of certain RandNLA methods into MATLAB, the NAG Library, NVIDIA’s\ncuSOLVER, and SciKit-Learn. In view of these developments, we believe the time\nis right to accelerate the adoption of RandNLA in the scientiﬁc community. In\nparticular, we believe the community stands to beneﬁt signiﬁcantly from a suitably deﬁned “RandBLAS” and “RandLAPACK,” to serve as standard libraries for\nRandNLA, in much the same way that BLAS and LAPACK serve as standards for\ndeterministic linear algebra.\nThis monograph surveys the ﬁeld of RandNLA as a step toward building meaningful RandBLAS and RandLAPACK libraries.\nSection 1 primes the reader for a\ndive into the ﬁeld and summarizes this monograph’s content at multiple levels of\ndetail. Section 2 focuses on RandBLAS, which is to be responsible for sketching.\nDetails of functionality suitable for RandLAPACK are covered in the ﬁve sections\nthat follow. Speciﬁcally, Sections 3 to 5 cover least squares and optimization, lowrank approximation, and other select problems that are well-understood in how\nthey beneﬁt from randomized algorithms. The remaining sections – on statistical\nleverage scores (Section 6) and tensor computations (Section 7) – read more like\ntraditional surveys. The diﬀerent ﬂavor of these latter sections reﬂects how, in our\nassessment, the literature on these topics is still maturing.\nWe provide a substantial amount of pseudo-code and supplementary material\nover the course of ﬁve appendices. Much of the pseudo-code has been tested via\npublicly available Matlab and Python implementations.\n\nAuthors\nRiley Murray, ICSI, LBNL, and University of California, Berkeley\nrjmurray@berkeley.edu\nJames Demmel, University of California, Berkeley\ndemmel@berkeley.edu\nMichael W. Mahoney, ICSI, LBNL, and University of California, Berkeley\nmmahoney@stat.berkeley.edu\nN. Benjamin Erichson, ICSI and Lawrence Berkeley National Laboratory\nerichson@icsi.berkeley.edu\nMaksim Melnichenko, University of Tennessee, Knoxville\nmmelnic1@vols.utk.edu\nOsman Asif Malik, Lawrence Berkeley National Laboratory\noamalik@lbl.gov\nLaura Grigori, INRIA Paris and J.L. Lions Laboratory, Sorbonne University\nlaura.grigori@inria.fr\nPiotr Luszczek, University of Tennessee, Knoxville\nluszczek@icl.utk.edu\nMicha l Derezi´nski, University of Michigan\nderezin@umich.edu\nMiles E. Lopes, University of California, Davis\nmelopes@ucdavis.edu\nTianyu Liang, University of California, Berkeley\ntianyul@berkeley.edu\nHengrui Luo, Lawrence Berkeley National Laboratory\nhrluo@lbl.gov\nJack Dongarra, University of Tennessee, Knoxville\ndongarra@icl.utk.edu\ni\n\nAcknowledgements\nMany individuals from the community gave detailed feedback on earlier versions of\nthis monograph that were not circulated publicly. These individuals include Mark\nTygert, Cameron Musco, Joel Tropp, Per-Gunnar Martinsson, Alex Townsend,\nDaniel Kressner, Alice Cortinovis, Ilse Ipsen, Sergey Voronin, Vivak Patel, Daniel\nMaldonado, Tammy Kolda, Florian Schaefer, Ramki Kannan, and Piyush Sao –\neach of them has our sincere gratitude for their assistance.\nIn addition, we thank the following people for providing input on the earliest\nstages of this project: Vivek Bharadwaj, Younghyun Cho, Jelani Nelson, Mark\nGates, Weslley da Silva Pereira, Julie Langou, and Julien Langou.\nThis work was partially funded by an NSF Collaborative Research Framework:\nBasic ALgebra LIbraries for Sustainable Technology with Interdisciplinary Collaboration (BALLISTIC), a project of the International Computer Science Institute,\nthe University of Tennessee’s ICL, the University of California at Berkeley, and\nthe University of Colorado at Denver (NSF Grant Nos. 2004235, 2004541, 2004763,\n2004850, respectively) [DDL+20]. Any opinions, ﬁndings, and conclusions or recommendations expressed in this material are those of the author(s) and do not\nnecessarily reﬂect the views of the National Science Foundation. MWM would also\nlike to thank the Oﬃce of Naval Research, which provided partial funding via a\nBasic Research Challenge on Randomized Numerical Linear Algebra.\nRelease History\n• 11/13/2022. The ﬁrst publicly-available draft of this monograph was circulated as a technical report at SC22.\n• 02/22/2023: arXiv V1. Improvements were made to Section 3.2.2 based on\ncomments from Joel Tropp.\nHelpful feedback from Robert Webber led to\nimprovements throughout Sections 4 and 5.2. Clariﬁcations and greater detail\nwere added to Appendix B.2 following comments from Ilse Ipsen.\n• 04/12/2023: arXiv V2. Section 5.1 was slightly revised based on valuable\ncomments from Oleg Balabanov.\nSection 5.3 was rewritten and expanded\nfollowing helpful discussions with Tyler Chen.\nSections 4.1.1 and 4.5 now\nmention an important piece of software that Mark Tygert brought to our\nattention. Revisions were made to Section 5.2.2 to more clearly and accurately\ncharacterize methods from the literature.\nii\n\nContents\n1\nIntroduction\n1\n1.1\nOur world . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nThis monograph, from an astronaut’s-eye view\n. . . . . . . . . . . .\n7\n1.3\nThis monograph, from a bird’s-eye view . . . . . . . . . . . . . . . .\n9\n1.4\nRecommended reading . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n1.5\nNotation and terminology . . . . . . . . . . . . . . . . . . . . . . . .\n17\n2\nBasic Sketching\n21\n2.1\nA high-level plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2.2\nHelpful things to know about sketching\n. . . . . . . . . . . . . . . .\n24\n2.3\nDense sketching operators . . . . . . . . . . . . . . . . . . . . . . . .\n30\n2.4\nSparse sketching operators . . . . . . . . . . . . . . . . . . . . . . . .\n32\n2.5\nSubsampled fast trigonometric transforms . . . . . . . . . . . . . . .\n35\n2.6\nMulti-sketch and quadratic-sketch routines . . . . . . . . . . . . . . .\n36\n3\nLeast Squares and Optimization\n39\n3.1\nProblem classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n3.2\nDrivers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\n3.3\nComputational routines\n. . . . . . . . . . . . . . . . . . . . . . . . .\n51\n3.4\nOther optimization functionality\n. . . . . . . . . . . . . . . . . . . .\n58\n3.5\nExisting libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n60\n4\nLow-rank Approximation\n63\n4.1\nProblem classes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n64\n4.2\nDrivers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n73\n4.3\nComputational routines\n. . . . . . . . . . . . . . . . . . . . . . . . .\n80\n4.4\nOther low-rank approximations . . . . . . . . . . . . . . . . . . . . .\n89\n4.5\nExisting libraries . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n91\n5\nFurther Possibilities for Drivers\n93\n5.1\nMulti-purpose matrix decompositions . . . . . . . . . . . . . . . . . .\n94\n5.2\nSolving unstructured linear systems . . . . . . . . . . . . . . . . . . . 100\n5.3\nTrace estimation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6\nAdvanced Sketching: Leverage Score Sampling\n111\n6.1\nDeﬁnitions and background . . . . . . . . . . . . . . . . . . . . . . . 112\n6.2\nApproximation schemes\n. . . . . . . . . . . . . . . . . . . . . . . . . 117\n6.3\nSpecial topics and further reading . . . . . . . . . . . . . . . . . . . . 120\niii\n\n7\nAdvanced Sketching: Tensor Product Structures\n123\n7.1\nThe Kronecker and Khatri–Rao products\n. . . . . . . . . . . . . . . 124\n7.2\nSketching operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\n7.3\nPartial updates to Kronecker product sketches\n. . . . . . . . . . . . 130\nA Details on Basic Sketching\n135\nA.1 Subspace embeddings and eﬀective distortion . . . . . . . . . . . . . 135\nA.2 Short-axis-sparse sketching operators . . . . . . . . . . . . . . . . . . 137\nA.3 Theory for sketching by row selection . . . . . . . . . . . . . . . . . . 140\nB Details on Least Squares and Optimization\n143\nB.1\nQuality of preconditioners . . . . . . . . . . . . . . . . . . . . . . . . 143\nB.2\nBasic error analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\nB.3\nIll-posed saddle point problems . . . . . . . . . . . . . . . . . . . . . 152\nB.4\nMinimizing regularized quadratics\n. . . . . . . . . . . . . . . . . . . 153\nC Details on Low-Rank Approximation\n157\nC.1 Theory for submatrix-oriented decompositions . . . . . . . . . . . . . 157\nC.2 Computational routines\n. . . . . . . . . . . . . . . . . . . . . . . . . 160\nD Correctness of Preconditioned Cholesky QRCP\n169\nE Bootstrap Methods for Error Estimation\n171\nE.1\nBootstrap methods in a nutshell\n. . . . . . . . . . . . . . . . . . . . 172\nE.2\nSketch-and-solve least squares . . . . . . . . . . . . . . . . . . . . . . 173\nE.3\nSketch-and-solve one-sided SVD . . . . . . . . . . . . . . . . . . . . . 174\nBibliography\n195\niv\n\nSection 1\nIntroduction\n1.1 Our world ...................................................................\n1\n1.1.1 Four value propositions of randomization ......................\n4\n1.1.2 What is, and isn’t, subject to randomness .....................\n6\n1.2 This monograph, from an astronaut’s-eye view\n.........\n7\n1.3 This monograph, from a bird’s-eye view .....................\n9\n1.4 Recommended reading ................................................\n13\n1.4.1 Tutorials, light on prerequisites ...................................\n13\n1.4.2 Broad and proof-heavy resources .................................\n14\n1.4.3 Perspectives on theory, light on proofs .........................\n14\n1.4.4 Deep investigations of speciﬁc topics ............................\n15\n1.4.5 Randomized numerical linear algebra: Foundations and Algorithms, by Martisson and Tropp ...............................\n15\n1.5 Notation and terminology ...........................................\n17\nThis introductory section has three principal goals: to motivate our subject and\nclarify common misconceptions that surround it (§1.1); to explain this monograph’s\nscope and overarching structure (§1.2); and to help direct the reader’s attention\nthrough section-by-section summaries (§1.3). Many readers may beneﬁt from our\n“survey of surveys” (§1.4), and all should at least brieﬂy consult the section on\nnotation and deﬁnitions (§1.5).\n1.1\nOur world\nNumerical linear algebra (NLA) concerns algorithms for computations on matrices\nwith numerical entries. Originally driven by applications in the physical sciences,\nit now provides the foundation for vast swaths of applied and computational mathematics. The cultural norms in this ﬁeld developed many years ago, in large part\nfrom recurring themes in problem formulations and algorithmically-useful structures\nin matrices. However, more recently, NLA has also been motivated by developments\nin machine learning and data science. Applications in these ﬁelds also have their\nown themes of problem formulations and structures in data, often of a very diﬀerent\nnature than those in more classical applications.\n1\n\nIntroduction\n1.1. Our world\nA dire situation.\nWhile communities that rely on NLA now vary widely, they share\none essential property: a ravenous appetite for solving larger and larger problems.\nFor decades, this hunger was satiated by complementary innovations in hardware\nand software. However, this progress should not be taken for granted. In particular,\nthere are two factors that increasingly present obstacles to scaling linear algebra\ncomputations to the next level.\n• Space and power constraints in hardware. Chips today have billions of transistors, and these transistors are packed into a very small amount of space.\nIt takes power to run these transistors at gigahertz frequencies, and power\ngenerates heat. It is hard for one hot thing to dissipate heat when surrounded\nby millions of other hot things. Too much heat can fry a chip.\nThese constraints are known to industry and research community alike, and\noften referred to as the breakdown of the Dennard’s Law [DGR+74; Boh07]\nand the sunsetting of Moore’s Law [Moo65]. The former represents the infamous power wall due to the inability of dissipating the heat produced by the\nprocessors leading to ﬂattened curve of clock frequency increases. The latter\nintroduced the post-Moore era of heterogeneous computing [VBG+18] leading\nto plethora specialized hardware targeting individual application spaces.\nThe end result of all this?\nA situation where “more powerful processors”\nare just scaled-out versions of “less powerful processors,” for both commodity and server-tier hardware. Any algorithm that does not parallelize well\nis fundamentally limited in its ability to leverage these advances.\nIf one’s\npockets are deep enough, then one can try to get around this with purposebuilt accelerators. But even then, there remains the matter of programming\nthose accelerators, and high-performance implementations of classical NLA\nalgorithms are anything but simple.\n• NLA’s maturity as a ﬁeld. Software can only improve so much without algorithmic innovations. At the same time, linear algebra is a very well-studied\ntopic, and most algorithmic breakthroughs in recent years have required carefully exploiting structures present in speciﬁc problems. Identifying new and\nuseful problem structures has been increasingly diﬃcult, often requiring deep\nknowledge of NLA alongside substantial domain expertise.\nIf we are to continue scaling our capabilities in matrix computations, then it is\nessential that we leverage all technologies that are on the table.\nAn underutilized technology.\nThis monograph concerns randomized numerical linear algebra, or RandNLA, for short.\nAlgorithms in this realm oﬀer compelling\nadvantages in a wide variety of settings. Some provide an unrivaled combination of\neﬃciency and reliability in computing approximate solutions for massive problems.\nOthers provide ﬁne-grained control when balancing accuracy and computational\ncost, as is essential for practitioners who are operating at the limits of what their\nmachines can handle. In many cases, the practicality of these algorithms can be seen\neven with elementary MATLAB or Python implementations, which increases their\nsuitability for adapting to new hardware by leveraging similarly powerful abstraction layers.\nFinally, although truly high-performance implementations are more\ncomplicated, they remain relatively easy to implement when given the right building blocks.\nPage 2\narXiv Version 2\n\n1.1. Our world\nIntroduction\nBut we are getting ahead of ourselves. What do we mean by “randomized algorithms,” as the term is used within RandNLA? First and foremost, these are\nalgorithms that are probabilistic in nature. They use randomness as part of their\ninternal logic to make decisions or compute estimates, which they can go on to use\nin any number of ways. These algorithms do not presume a distribution over possible inputs, nor do they assume the inputs somehow possess intrinsic uncertainty.\nRather, they use randomness as a tool, to ﬁnd and exploit structures in problem\ndata that would seem “hidden” from the perspective of classical NLA.\nWhat’s this about “ﬁnding hidden structures?”\nConsider the problem of highly overdetermined least squares, i.e., the problem of solving\nmin\nx∈Rn ∥Ax −b∥2\n2,\n(1.1)\nwhere A has m ≫n rows. It is well-known that if the columns of A are orthonormal\nthen (1.1) can be solved in O(mn) time by setting x = A∗b, where A∗is the\ntranspose of A. The trouble, of course, is that the columns of A are very unlikely\nto be orthogonal in any interesting application, and the standard algorithms for\nsolving this problem take O(mn2) time.\nHowever, what if, by some miracle, we could easily ﬁnd an n × n matrix C for\nwhich AC−1 was column-orthonormal? In this case, we could compute the exact\nsolution x = (C−1)(C−1)∗A∗b in time\nO\n\u0000mn + n3\u0001\nby suitably factoring C. Now, randomized algorithms do not work miracles, but at\ntimes they can approach the miraculous. In the speciﬁc case of (1.1), randomization\ncan be used to quickly identify a basis in which A is nearly column-orthonormal.\nAny such basis can be incorporated into a standard iterative method from classical\nNLA. With such an approach, one can reliably solve (1.1) to ϵ-error in time\nO\n\u0000mn log\n\u0000 1\nϵ\n\u0001\n+ n3\u0001\n(where we ask forgiveness for being vague about the meaning of “ϵ”).\nBack to the big picture.\nThe approach to least squares described above has been\nknown for well over ten years now. Since then, an entire suite of compelling results\non RandNLA has been established. What’s more, the literature also documents the\nexistence of high-performance proof-of-concept implementations that testify to the\npracticality of these methods. Indeed, as we explain below, randomized algorithms\nhave been developed to address the same basic challenges as classical methods. Randomized algorithms are also very well-suited to address many upcoming challenges\nwith which classical algorithms struggle. RandNLA as a ﬁeld is slowly achieving a\ncertain level of maturity.\nDespite this, substantial interdisciplinary gaps have impeded technology transfer\nfrom those doing research in RandNLA to those who might beneﬁt from it. This\nstems partly from the absence of work that organizes the RandNLA literature in a\nway that supports the development of high-quality software.\nThis monograph is our attempt at addressing that absence. With it, we aim\nto provide a principled and practical foundation for developing high-quality software to address future needs of large-scale linear algebra computations, for scientiﬁc computing, large-scale data analysis and machine learning, and other related\narXiv Version 2\nPage 3\n\nIntroduction\n1.1. Our world\napplications. Our particular approach is informed by plans to develop such highquality libraries – a “RandBLAS” and “RandLAPACK,” if you will. Towards this end,\nwe have implemented and tested many of the algorithms described herein in both\nMATLAB1 and Python.2 We provide more context on our approach and scope in\nSection 1.2. But ﬁrst, we elaborate on the value propositions of RandNLA and the\nrole of randomness in these algorithms.\n1.1.1\nFour value propositions of randomization\nOur goal here is to introduce (and only introduce!) some value propositions for\nRandNLA. We do this for as broad an audience as possible and we have attempted\nto keep our introductions short. While these descriptions are unlikely to convince\na skeptic, they should at least set the agenda for a debate.\nBackground: time complexity and FLOP counts.\nIn the sequential RAM model of\ncomputing, an algorithm’s time complexity is its worst-case total number of reads,\nwrites, and elementary arithmetic operations, as a function of input size. Precise\nexpressions for time complexity can be hard to come by and diﬃcult to parse.\nTherefore it is standard to describe complexity asymptotically, with big-O notation.\nIn NLA, we also care about how the size of an algorithm’s input aﬀects the number of arithmetic operations that it requires. Arithmetic operations are presumed\nto be ﬂoating point operations (“ﬂops”) by default, and it is common to refer to an\nalgorithm’s ﬂop count as a function of input size. Flop counts almost always agree\nasymptotically with time complexity. But, in contrast with time complexity, ﬂop\ncounts are often given with explicit constant factors. In determining the constant\nfactors accurately one must consider subtleties such as whether a fused multiplyadd instruction counts as one or two “ﬂops.” We do not consider such subtleties in\nthis monograph.\nFighting the scourge of superlinear complexity.\nThe dimensions of matrices arising in\napplications typically have semantic meanings. They might represent the number\nof points in a dataset, or they might be aﬀected by the “ﬁdelity” of a linear model\nfor some nonlinear phenomenon. A scientist who relies on matrix computations will\ninevitably want to increase the size of their dataset or the ﬁdelity of their model.\nThis is often diﬃcult because the complexity of classical algorithms for high-level\nlinear algebra problems rarely scale linearly with the semantic notion of problem\nsize. This brings us to the ﬁrst value proposition of RandNLA.\nFor many important linear algebra problems, randomization oﬀers entirely new avenues of computing approximate solutions with linear or\nnearly-linear complexity.\nTo get a sense of why this matters, suppose that one needs to compute a Cholesky\ndecomposition of a dense matrix of order n. The standard algorithm for this takes\nn3/3 ﬂops.\nAt time of writing, a higher-end laptop can do this calculation for\nn = 10,000 in about one second. However, if n is the semantic notion of problem\nsize, and if one wants to solve a problem ten times as large, then the calculation\nwith n = 100,000 takes over 15 minutes.\n1https://github.com/BallisticLA/MARLA\n2https://github.com/BallisticLA/PARLA\nPage 4\narXiv Version 2\n\n1.1. Our world\nIntroduction\nThere are two lessons in that simple example.\nThe ﬁrst is that superlinear\ncomplexity can be crippling when it comes to solving larger linear algebra problems.\nThe second is that an informed user does well to think of their problem size in a\nmore realistic way. In the case of this example, one should go into the problem\nthinking in terms of the number of free parameters in an n × n positive deﬁnite\nmatrix: around 50 million when n = 10,000 and around 5 billion when n = 100,000.\nRemark 1.1.1. One of RandNLA’s success stories is a fast algorithm for computing\nsparse approximate Cholesky decompositions of so-called graph Laplacians. In order\nto keep the length of this monograph under control, we have opted not to include\nalgorithms that only apply to sparse matrices. However, we do provide algorithms\nfor computing approximate eigendecompositions of regularized positive semideﬁnite\nmatrices, and these algorithms can be used to solve linear systems faster than\nCholesky in certain applications.\nResisting the siren call of galactic algorithms.\nThe problem of multiplying two n×n\nmatrices is one of the most fundamental in all of NLA. If we only consider asymptotics, then the fastest algorithms for this task run in less than O(n2.38) time.\nHowever, the fastest method that is practical (Strassen’s algorithm), runs in time\nO(nlog2 7).\nThe trouble with these “fast” algorithms is that they have massive constants\nhidden in their big-O complexity.\nSuch algorithms are called galactic, owing to\ncommon comparisons between the size of their hidden constants and the number\nof stars or atoms in the galaxy.\nAnd with this, we arrive at the second value\nproposition of RandNLA.\nFor a handful of important linear algebra problems, the asymptotically\nfastest (non-galactic) algorithms for computing accurate solutions are,\nin fact, randomized.\nHighly overdetermined least squares (see page 3) is one such problem.\nStriking the Achilles’ heel of the RAM model.\nThe RAM model of computing, although useful, is not high-ﬁdelity. Indeed, even in the setting of a shared-memory\nmulti-core machine, it fails to account for the fact that moving data from main\nmemory, through diﬀerent levels of cache, and onward to processor registers is much\nmore expensive than elementary arithmetic on the same data. This fact has been\nappreciated even in the earliest days of LAPACK’s development, over 30 years ago.\nIts principal consequence is that even if the time complexities of two algorithms\nmatch up to and including constant factors, their performance by wallclock time\ncan diﬀer by orders of magnitude. This is the third value proposition of RandNLA.\nRandomization creates a wealth of opportunities to reduce and redirect\ndata movement. Randomized algorithms based on this principle are signiﬁcantly faster than the best-available deterministic methods by wallclock time.\nRandomized algorithms for computing full QR decompositions with column pivoting\nﬁt this description.\narXiv Version 2\nPage 5\n\nIntroduction\n1.1. Our world\nFinite-precision arithmetic: once a curse, now a blessing.\nFinite-precision arithmetic\nand exact arithmetic are diﬀerent beasts, and this has real consequences for NLA.\nFor one thing, this limitation introduces many technicalities in understanding accuracy guarantees, even for seemingly straightforward problems like LU decomposition. It is tempting to view it as a curse. However, if we accept it as given, then it\ncan be used to our advantage. Certain computations can be performed with lower\nprecision without compromising the accuracy of a ﬁnal result.\nThis perspective brings us to our ﬁnal value proposition, stated in terms of the\nconcept of sketching, deﬁned momentarily.\nIn RandNLA, it is natural to perform computations on sketches of matrices in lower-precision arithmetic.\nDepending on how the sketch is\nconstructed, one can be (nearly) certain of avoiding degenerate situations that are known to cause common deterministic algorithms to fail.\n1.1.2\nWhat is, and isn’t, subject to randomness\nSampling sketching operators from sketching distributions.\nWe are concerned with\nalgorithms that use random linear dimension reduction maps called sketching operators. The sketching operators used in RandNLA come in a wide variety of forms.\nThey can be as simple as operators for selecting rows or columns from a matrix,\nand they can be even more complicated than algorithms for computing Fast Fourier\nTransforms. We refer to a distribution over sketching operators as a sketching distribution. Given this terminology, we can highlight the following essential fact.\nFor the vast majority of RandNLA algorithms, randomization is only\nused when sampling from the sketching distribution.\nFrom an implementation standpoint, one should know that while sketching distributions can be quite complicated, the sampling process always builds on some\nkind of basic random number generator. Upon specifying a seed for the random\nnumber generator involved in sampling, RandNLA algorithms become every bit as\ndeterministic as classical algorithms.\nForming and processing sketches.\nWhen a sketching operator is applied to a large\ndata matrix, it produces a smaller matrix called a sketch. A wealth of diﬀerent\noutcomes can be achieved through diﬀerent methods for processing a sketch and\nusing the processed representation downstream.\nSome processing schemes inevitably yield rough approximations to the\nsolution of a given problem. Other processing schemes can lead to highaccuracy approximations, if not exact solutions, under mild assumptions.\nAcross these regimes, one of the most popular trends in algorithm analysis is to\nemploy a two-part approach. In the ﬁrst part, the task is to characterize algorithm\noutput in terms of some simple property of the sketch. In the second part, one can\nemploy results from random matrix theory to bound the probability that the sketch\nwill possess the desired property.\nPage 6\narXiv Version 2\n\n1.2. This monograph, from an astronaut’s-eye view\nIntroduction\nConﬁdently managing uncertainty.\nThe performance of a numerical algorithm is\ncharacterized by the accuracy of its solutions and the cost it incurs to produce\nthose solutions. Naturally, one can expect some variation in algorithm performance\nwhen using randomized methods. Luckily, we have the following.\nMost randomized algorithms “gamble” with only one of the two performance metrics, accuracy or cost. Through optional algorithm parameters, users retain ﬁne-grained control over one of these two metrics.\nFurthermore, when cost is controllable, the algorithm parameters can be adjusted to\ninﬂuence accuracy; when accuracy is controllable, they can be adjusted to inﬂuence\ncost. The eﬀects of these inﬂuences can sometimes be masked by variability in runto-run performance. However, there is a general trend in RandNLA algorithms of\nbecoming more predictable as they are applied to larger problems. At large enough\nscales, many randomized algorithms are nearly as predictable as deterministic ones.\n1.2\nThis monograph, from an astronaut’s-eye view\nThis monograph started as a development plan for two C++ libraries for RandNLA,\nprimarily working within a shared-memory dense-matrix data model. We prepared\na preliminary plan for these libraries in short order by leveraging existing surveys.\nHowever, after pausing our writing for some number of months to receive feedback\nfrom community members, we found ourselves with many unanswered questions\nthat would aﬀect our implementations. Before long we found ourselves in a cycle\nof diving ever-deeper into the RandNLA literature with an eye to implementation,\neach time coming up with more answers and more questions.\nThis monograph does not answer every question we came across in the foregoing months. Rather, it represents what we know at a time when the best way to\nanswer our remaining questions is to focus on developing the libraries themselves.\nTherefore we provide the reader with this — a monograph that aggregates material from over 300 references on classical and randomized NLA — which functions\npartly as a survey and partly as original research. In it, we present new (unifying)\ntaxonomies, candidate application areas, and even a handful of novel theoretical\nresults and algorithms.\nAlthough its scope has greatly increased, the original purpose of this monograph\ninforms its structure. It also contains a number of clear statements about plans for\nour C++ libraries. Therefore, while we do not want to give the impression that\nthis monograph’s value depends on its connections to speciﬁc pieces of software, we\nprovide the following remarks on our planned libraries up-front.\nThe ﬁrst library, RandBLAS, concerns basic sketching and is the subject\nof Section 2. Our hope is that RandBLAS will grow to become a community standard for RandNLA, in the sense that its API would see wider\nadoption than any single implementation thereof. In order to achieve\nthis goal we think it is important to keep its scope narrowly focused.\nThe second library, RandLAPACK, concerns algorithms for traditional\nlinear algebra problems (Sections 3 to 5, on least-squares and optimization, low-rank approximation, and additional possibilities, respectively)\nand advanced sketching functionality (Sections 6 and 7). The design\narXiv Version 2\nPage 7\n\nIntroduction\n1.2. This monograph, from an astronaut’s-eye view\nspaces of algorithms for these tasks are large, and we believe that powerful abstractions are needed for a library to leverage this fact. Consistent\nwith this, we are developing RandLAPACK in an object-oriented programming style wherein algorithms are objects. Such a style is naturally\ninstantiated with functors when working in C++.\nWe have written this monograph to be modular and accessible, without sacriﬁcing depth.\nThe modularity manifests in how there are almost no technical\ndependencies across Sections 3 to 7. For the sake of accessibility, each section gives\nbackground on its core subject. We use two strategies to provide accessibility without sacriﬁcing depth. First, we make liberal use of appendices. In them, the reader\ncan ﬁnd proofs, background on special topics, low-level algorithm implementation\nnotes, and high-level algorithm pseudocode. Second, our citations regularly indicate\nprecisely where a given concept can be found in a manuscript. Therefore, if we give\ntoo brief a treatment on a topic of interest, the reader will know exactly where to\nlook to learn more.\nA word on “drivers” and “computational routines”\nWe designate most algorithms as either drivers or computational routines. These\nterms are borrowed from LAPACK’s API. In general, drivers solve higher-level problems than computational routines, and their implementations tend to use a small\nnumber of computational routines. In our context,\ndrivers are only for traditional linear algebra problems,\nwhile\ncomputational routines address a mix of traditional linear algebra problems and specialized problems that are only of interest in RandNLA.\nSections 3 and 4 cover drivers and the computational routines behind them; they\nare the most comprehensive sections in this monograph.\nSection 5 also covers\ndrivers, but at less depth than the two that precede it.\nIn particular, it does\nnot identify algorithmic building blocks that would be considered computational\nroutines.\nMeanwhile, the advanced sketching functionality in Sections 6 and 7\nwould only be considered for computational routines.\nOne reason why we use the “driver” and “computational routine” taxonomy is\nto push much of the RandNLA design space into computational routines. This is\nessential to keeping drivers simple and few in number. However, it has a side eﬀect:\nsince choices made in the computational routines decisively aﬀect the drivers, it is\nhard to state theoretical guarantees for the drivers without being prescriptive on\nthe choice of computational routine. This is compounded by two factors. First, we\nprefer to not be prescriptive on choices of computational routines within drivers,\nsince there is always a possibility that some problems beneﬁt more from some approaches than others. Second, even if we recommended speciﬁc implementations, it\nwould be very complicated to characterize their performance with consideration to\nthe full range of possibilities for their tuning parameters.\nAs a result of all this, we make relatively few statements about performance\nguarantees or computational complexity of driver-level algorithms. While this is a\nlimitation of our approach, we believe it is not severe. One can supplement this\nmonograph with a variety of resources discussed in Section 1.4.\nPage 8\narXiv Version 2\n\n1.3. This monograph, from a bird’s-eye view\nIntroduction\n1.3\nThis monograph, from a bird’s-eye view\nSection-by-section summaries are provided below to help direct the reader’s attention. While space limitations prevent them from being comprehensive, they are\neﬀective for what they are. They assume familiarity with standard linear algebra\nconcepts, including least squares models, singular value decomposition, Hermitian\nmatrices, eigendecomposition, and positive (semi)deﬁniteness. We deﬁne all of these\nconcepts in Section 1.5 for completeness. Finally, as one disclaimer, some problem\nformulations below have slight diﬀerences from those used in the sections themselves.\nEssential notation and conventions.\nThe adjoint of a linear operator A is denoted\nby A∗. When A is a real matrix, the adjoint is simply the transpose. Vectors have\ncolumn orientations by default, so the standard inner product of two vectors u, v\nis u∗v.\nWe sometimes call a vector of length n an n-vector. If we refer to an m × n\nmatrix as “tall” then the reader can be certain that m ≥n and reasonably expect\nthat m > n. If m is much larger than n and we want to emphasize this fact, then\nwe write m ≫n and would call an m × n matrix “very tall.” We use analogous\nconventions for “wide” and “very wide” matrices.\nBasic Sketching (Section 2)\nThis section documents our work toward developing a RandBLAS standard.\nIt\nbegins with remarks on the Basic Linear Algebra Subprograms (BLAS), which are\nto classical NLA as we hope the RandBLAS will be to RandNLA.\nSection 2.1 addresses high-level design questions for a RandBLAS standard. By\nstarting with a simple premise, we arrive at the conclusion that it should provide\nfunctionality for data-oblivious sketching (that is, sketching without consideration\nto the numerical properties of the data). We then oﬀer our thoughts on how such a\nlibrary should be organized and how it should handle random number generation.\nSection 2.2 summarizes a variety of concepts in sketching.\nIn it, we answer\nquestions such as the following.\n• What are the geometric interpretations of sketching?\n• How does one measure the quality of a sketch?\n• What are the “standard” properties for the ﬁrst and second moments of sketching operator distributions? When and how are these properties important in\nRandNLA algorithms?\nDetail-oriented readers should consider Section 2.2 alongside Appendix A.1, which\npresents a novel concept called eﬀective distortion that is useful in characterizing\nthe behavior of randomized algorithms for least squares and related problems.\nSections 2.3 to 2.5 review the three types of sketching operator distributions that\nthe RandBLAS might support. These types of distributions consist of dense sketching operators (e.g., Gaussian matrices), sparse sketching operators, and sketching operators based on subsampled fast trigonometric transforms (such as discrete\nFourier, discrete cosine, and Walsh-Hadamard transforms). As we explain in Section 2.4, we consider row-sampling and column-sampling as particular types of\narXiv Version 2\nPage 9\n\nIntroduction\n1.3. This monograph, from a bird’s-eye view\nsparse sketching. The interested reader is referred to Appendix A.2 for details on\na class of sparse sketching operators that is distinct from row or column sampling.\nThese details include notes on high-performance implementations that have not\nappeared in earlier literature.\nOur chapter on basic sketching concludes with Section 2.6, which presents a\nhandful of elementary sketching operations that are not naturally represented by\na linear transformation that acts only on the columns or only on the rows of a\nmatrix. These operations arise in the fastest randomized algorithms for low-rank\napproximation.\nLeast Squares and Optimization (Section 3)\nThis is one of three sections that cover driver-level functionality, and it is one of\ntwo that discuss drivers and computational routines. It is narrower in scope but\ngreater in depth than the other sections that address drivers.\nProblem classes.\nIn Section 3.1 we consider a variety of least squares problems\nwithin a common framework. The framework describes all problems in terms of an\nm×n data matrix A where m ≥n. Given A, any pair of vectors (b, c) of respective\nlengths (m, n) can be considered along with a parameter µ ≥0 to deﬁne “primal”\nand “dual” saddle point problems. The primal problem is always\nmin\nx∈Rn\n\b\n∥Ax −b∥2\n2 + µ∥x∥2\n2 + 2c∗x\n\n.\n(Pµ)\nThe dual problem takes one of two forms, depending on the value of µ:\nmin\ny∈Rm\n\b\n∥A∗y −c∥2\n2 + µ∥y −b∥2\n2\n\nif µ > 0\nmin\ny∈Rm\n\b\n∥y −b∥2\n2 : A∗y = c\n\nif µ = 0\n\n\n.\n(Dµ)\nSpecial cases of these problems include overdetermined and underdetermined least\nsquares, as well as ridge regression with tall or wide matrices. Appendix B.2 gives\nbackground on accuracy metrics, sensitivity analysis, and error estimation methods\nthat apply to the most prominent problems under this umbrella.\nSection 3.1 considers one type of problem that does not ﬁt nicely into the above\nframework. Speciﬁcally, for a positive semideﬁnite linear operator G and a positive\nparameter µ, it also considers the regularized quadratic problem\nmin\nw w∗(G + µI)w −2h∗w.\n(Rµ)\nWe note that (Pµ) and (Dµ) can be cast to this form when µ is positive. However,\nto make this reformulation would be to obfuscate the structure in a saddle point\nproblem, rather than reveal it.\nDrivers.\nWe start in Section 3.2.1 by covering a low-accuracy method for overdetermined least squares known as sketch-and-solve. This method is remarkable for\nthe simplicity of its description and its analysis. It is also the ﬁrst place where\nour newly-proposed concept of eﬀective distortion provides improved insight into\nalgorithm behavior.\nPage 10\narXiv Version 2\n\n1.3. This monograph, from a bird’s-eye view\nIntroduction\nSections 3.2.2 and 3.2.3 concern methods for solving problems (Pµ), (Dµ), and\n(Rµ) to high accuracy. These methods use randomization to ﬁnd a preconditioner.\nThe preconditioner is used to implicitly change the coordinate system that describes\nthe optimization problem, in such a way that the preconditioned problem can easily\nbe solved by iterative methods from classical NLA. These methods are intended for\nuse with certain problem structures (e.g. m ≫n) that we clearly identify.\nThe broader idea of sketch-and-solve algorithms has been successfully used for\nkernel ridge regression (KRR – see Appendix B.4.1 for a primer). In Section 3.2.4,\nwe reinterpret two algorithms for approximate KRR as sketch-and-solve algorithms\nfor (Rµ). We further identify how the sketched problems amount to saddle point\nproblems with m ≫n. Appendix B.4.2 details how the saddle point framework is\nuseful in the more complicated of these two settings.\nComputational routines.\nThe computational routines that we cover in Section 3.3\nonly pertain to drivers based on random preconditioning. We kick oﬀour discussion\nin Section 3.3.1 with background on saddle point problems. Then, Section 3.3.2\naddresses preconditioner generation for saddle point problems when m ≫n. It\nopens with a theoretical result (Proposition 3.3.1) characterizing the spectrum of\nthe preconditioned data matrix A (see also Appendix B.1) before providing a comprehensive overview of implementation considerations. Special attention is paid to\nhow one can generate the preconditioner when µ > 0 at no added cost compared\nto when µ = 0. In Section 3.3.3, we extend recently proposed methods from the\nliterature to deﬁne novel low-memory preconditioners for regularized saddle point\nproblems. Finally, Section 3.3.4 reviews a suite of deterministic iterative algorithms\nfrom classical NLA that are needed for randomized preconditioning algorithms.\nLow-rank Approximation (Section 4)\nLow-rank approximation problems take the following form.\nGiven as input an m × n target matrix A, compute suitably structured\nfactor matrices E, F, and G where\nˆA\n:=\nE\nF\nG\nm × n\nm × k\nk × k\nk × n\napproximates A. The accuracy of the approximation ˆA ≈A may vary\nfrom one application to another, but we require that k ≪min{m, n}.\nThis section summarizes the massive design spaces of randomized algorithms for\nsuch problems, as documented in the existing literature. One of its core contributions is to clarify what parts of this design space are relevant in what situations.\nProblem classes.\nSection 4.1 starts by explaining the signiﬁcance of the SVD and\neigendecomposition in relation to principal component analysis.\nFrom there, it\nintroduces the reader to a handful of submatrix-oriented decompositions – CUR,\none-sided interpolative decompositions (one-sided ID), and two-sided interpolative\ndecompositions (two-sided ID) – along with their applications. Section 4.1 concludes\nwith guidance on how one should and should-not quantify approximation error in\nlow-rank approximation problems. We note that this background is much more\ndetailed than that Section 3.1 provided on least squares and optimization problems.\nThis extra background will be important for many readers.\narXiv Version 2\nPage 11\n\nIntroduction\n1.3. This monograph, from a bird’s-eye view\nDrivers.\nSection 4.2 gives concise yet comprehensive overviews for RandNLA algorithms for SVD and Hermitian eigendecomposition (§4.2.1 and 4.2.2) as well as\nCUR and two-sided interpolative decomposition (§4.2.3). In the process, we take\ncare to prevent misunderstandings in what we mean by a Nystr¨om approximation\nof a positive semideﬁnite matrix. Pseudocode is provided for at least one algorithm\nfor each of these problems.\nComputational routines.\nAs is typical for surveys on this topic, we identify QB\ndecomposition (§4.3.2) and column subset selection (CSS) / one-sided ID (§4.3.4) as\nthe basic building blocks for most drivers. We also isolate power iteration (§4.3.1)\nand partial column-pivoted matrix decompositions (§4.3.3) as subproblems with\nnontrivial design spaces that are important to low-rank approximation.\nSome of the building blocks covered cumulatively from Sections 4.3.1 to 4.3.4 can\nbe used to compute low-rank approximations iteratively. If one seeks an approximation that is accurate to within some given tolerance, then these iterative algorithms\nrequire methods for estimating norms of linear operators; we cover such norm estimation methods brieﬂy in Section 4.3.5. Appendix C.2 contains pseudocode for\nseven computational routines and details their dependency structure.\nFurther Possibilities for Drivers (Section 5)\nThis section covers a handful of independent topics.\nSection 5.1 covers multi-purpose matrix decompositions. Section 5.1.1 explains\na simple algorithm for computing an unpivoted QR decomposition of a tall-andthin matrix of full column rank; the algorithm uses randomization to precondition\nCholesky QR for numerical stability. Section 5.1.2 ﬁrst describes an existing algorithm from the literature for Householder QRCP of matrices with any aspect ratio,\nand then presents an extension of preconditioned Cholesky QR that incorporates\npivoting and allows for rank-deﬁcient matrices. Section 5.1.3 summarizes methods\nfor computing decompositions known by various names (UTV, URV, QLP) that all\naim to serve as cheaper surrogates for the SVD.\nSection 5.2 addresses randomized algorithms for the solution of unstructured\nlinear systems. This includes direct methods based on accelerating (or safely bypassing) pivoting in matrix decompositions (§5.2.1) as well as iterative methods\n(§5.2.2). Some of these iterative methods were developed fairly recently and are a\nsubject of considerable practical interest.\nSection 5.3 considers the problem of estimating the trace of a linear operator. This problem is unique in the context of this monograph, since it makes no\nsense to consider in the shared-memory dense-matrix data model. We have opted\nto cover it anyway since randomized methods are extremely eﬀective for it. Section 5.3.1 introduces the elementary Girard–Hutchinson estimator developed in the\nlate 1980s. Section 5.3.2 covers methods that beneﬁt from contemporary developments on randomized algorithms for low-rank approximation. Finally, Section 5.3.3\ncovers methods for computing the trace of f(B) where B is a Hermitian matrix and\nf is a matrix function. We give a signiﬁcant amount of background material to help\nthe newcomer understand the methods in this last category.\nPage 12\narXiv Version 2\n\n1.4. Recommended reading\nIntroduction\nAdvanced Sketching: Leverage Score Sampling (Section 6)\nLeverage scores constitute measures of importance for the rows or columns of a\nmatrix. They can be used to deﬁne data-aware sketching operators that implement\nrow or column sampling.\nSection 6.1 introduces three types of leverage scores: standard leverage scores,\nsubspace leverage scores, and ridge leverage scores. We explain how each type is\nsuitable for sketching with diﬀerent downstream tasks in mind. For example, a\nproposition in Section 6.1.1 bounds the probability that a row-sampling operator\nsatisﬁes a subspace embedding property for the range of a matrix A. The bound\nshows that if rows are sampled according to a distribution q, then it becomes more\nlikely that the subspace embedding property holds as q approaches A’s standard\nleverage score distribution.\nSection 6.2 covers randomized algorithms for approximating leverage scores.\nSuch approximation methods are important since leverage scores are expensive to\ncompute except when working with highly structured problem data. The structure\nof these algorithms bears similarities to those seen in earlier sections. For example,\nSection 6.2.2 explains how a longstanding algorithm for approximating subspace\nleverage scores can be extended with QB approaches from Section 4.3.2.\nAdvanced Sketching: Tensor Product Structures (Section 7)\nTensor computations are the domain of multilinear algebra. As such, it is reasonable\nto exclude them from the scope of a standard library from RandNLA. However,\nat the same time, it is reasonable for a RandNLA library to support the core\nsubproblems in tensor computations that are linear algebraic in nature. Sketching\nimplicit matrices with tensor product structure ﬁts this description.\nThis section reviews eﬃcient methods for sketching matrices with Kronecker\nproduct or Khatri–Rao product structures (see §7.1 for deﬁnitions). The material\nin 7.2.1–7.2.4 concerns data-oblivious sketching distributions that are similar to\nthose from Section 2 but modiﬁed for the tensor product setting. Section 7.2.5, by\ncontrast, concerns data-aware sketching methods based on leverage score sampling.\nNotably, there are methods to eﬃciently sample from the exact leverage score distributions of tall matrices with Kronecker and Khatri–Rao product structures without\nexplicitly forming those matrices.\nFor completeness, Section 7.3 discusses motivating applications (speciﬁcally, tensor decomposition algorithms) that entail sketching matrices with these structures.\n1.4\nRecommended reading\nThis monograph is heavily inﬂuenced by a recent and sweeping survey by Martinsson\nand Tropp [MT20]. We draw detailed comparisons to that work in Section 1.4.5.\nBut ﬁrst, we give remarks on other resources of note for learning about RandNLA.\n1.4.1\nTutorials, light on prerequisites\nRandNLA: randomized numerical linear algebra, by Drineas and Mahoney [DM16].\nDepending on one’s background (and schedule!) this article can be read in one sitting. It requires no knowledge of NLA or probability. In fact, it does not even\narXiv Version 2\nPage 13\n\nIntroduction\n1.4. Recommended reading\npresume that the reader already cares about matrix computations. It starts with\nbasic ideas of matrix approximation by subsampling, explains the eﬀect of sampling\nin diﬀerent data-aware ways, and frames general data-oblivious sketching as “preprocessing followed by uniform subsampling.” It summarizes, at a very high level,\nsigniﬁcant results of RandNLA in least squares, low-rank approximation, and the\nsolution of structured linear systems known as Laplacian systems.\nLectures on randomized numerical linear algebra, by Drineas and Mahoney [DM18].\nThis book chapter is useful for those who want to see representative banner results in RandNLA with proofs. It covers algorithms for least squares and low-rank\napproximation. Its proofs emphasize decoupling deterministic and probabilistic aspects of analysis. Among resources that engage with the theory of RandNLA, it\nis notable for its brevity and its self-contained introductions to linear algebra and\nprobability.\n1.4.2\nBroad and proof-heavy resources\nSketching as a tool for numerical linear algebra, by Woodruﬀ[Woo14].\nThis monograph proceeds one problem at a time, starting with ℓ2 regression, then\non to ℓ1 regression, then low-rank approximation, and ﬁnally graph sparsiﬁcation.\nIt develops the technical machinery needed for each of these settings, at various\nlevels of detail. Among resources that address RandNLA theory, it is notable for\nits treatment of lower bounds (i.e., limitations of randomized algorithms).\nAn introduction to matrix concentration inequalities, by Tropp [Tro15].\nThis monograph gives an introduction to the theory of matrix concentration and\nits applications. It is not about RandNLA per se, but several of its applications do\nfocus on RandNLA. The course notes [Tro19] build on this monograph, exploring\ntheory and applications of matrix concentration developed after [Tro15] was written.\nLecture notes on randomized linear algebra, by Mahoney [Mah16].\nThese notes are fairly comprehensive in their coverage of results in RandNLA up\nto 2013. They address matrix concentration, approximate matrix multiplication,\nsubspace embedding properties of sketching distributions, as well as various algorithms for least squares and low-rank approximation. These notes are distinct from\n[Woo14] in that they address theory and practice. (Of course, being course notes,\nthey are not suitable as a formal reference.)\n1.4.3\nPerspectives on theory, light on proofs\nRandomized algorithms for matrices and data, by Mahoney [Mah11].\nThis monograph heavily emphasizes concepts, interpretations, and qualitative proof\nstrategies. It is a good resource for those who want to know what RandNLA can oﬀer\nin terms of theory for the least squares and low-rank approximation. It is notable\nfor the eﬀort it expends to connect RandNLA theory to theoretical developments\nin other disciplines.\nPage 14\narXiv Version 2\n\n1.4. Recommended reading\nIntroduction\nDeterminantal point processes in randomized numerical linear algebra, by Derezi´nski\nand Mahoney [DM21a].\nThis article gives an overview of RandNLA theory from the perspective of determinantal point processes and statistical data analysis. Among the many resources for\nlearning about RandNLA, it is notable for oﬀering a distinctly prospective (rather\nthan retrospective) viewpoint.\n1.4.4\nDeep investigations of speciﬁc topics\nFinding structure with randomness: probabilistic algorithms for constructing approximate matrix decompositions, by Halko, Martinsson, and Tropp [HMT11].\nAs of late 2022, this article is the single most inﬂuential resource on RandNLA. Its\nintroduction includes a history of how randomized algorithms have been used in\nnumerical computing, as well as a brief summary of (then) active areas of research\nin RandNLA. Following the introduction, it focuses exclusively on low-rank approximation. It is extremely thorough in its treatment of both theory and practice.\nThis article is now somewhat out of date and is partially subsumed by [MT20].\nHowever, it is still of distinct value for the fact that it proves all of its main results\n(and in certain cases, by novel methods). It also includes some algorithms that are\nnot found in [MT20].\nRandomized algorithms in numerical linear algebra, by Kannan and Vempala [KV17a].\nThis survey provides a detailed theory of row and column sampling methods. It\nalso includes methods for tensor computations.\nRandomized methods for matrix computations, by Martinsson [Mar18].\nThis book chapter focuses on practical aspects of randomized algorithms for lowrank approximation. In this regard, it is important to note that while [HMT11]\nprovided thorough coverage of this topic at the time, the more recent [Mar18] reviews\nimportant practical advances developed after 2011. Among resources that provide\nan in-depth investigation into low-rank approximation, is notable for how it also\nincludes algorithms for full-rank matrix decomposition.\n1.4.5\nRandomized numerical linear algebra:\nFoundations and Algorithms\nMartinsson and Tropp’s recent Acta Numerica survey, [MT20], covers a wide range\nof topics, each with substantial technical and historical depth. We have beneﬁted\nfrom it tremendously in developing our plans for RandBLAS and RandLAPACK.\nBecause we have found this resource so useful – and, at the same time, because we\nhave gone through the trouble of writing a distinct monograph that is just as long\n– we think there is value in highlighting how it diﬀers from our work.\nBasic sketching.\nBy comparison to [MT20], we focus more on implementation than\non theory. The outcome of this is the broadest-yet review of the literature relevant\nto the implementation of sketching methods. In the appendices, we provide novel\ntechnical contributions to sketching theory and practice.\nSee Section 2 and Appendix A, [MT20, §7 – §9].\narXiv Version 2\nPage 15\n\nIntroduction\n1.4. Recommended reading\nLeast squares and optimization.\nOur coverage of these concepts is comprehensive,\ninsofar as optimization can be reduced to linear algebra. It also includes a number\nof novel technical contributions and a review of relevant software. By comparison,\n[MT20] provides very limited coverage of this area, as acknowledged in [MT20, §1.6].\nSee Section 3 and Appendix B, [MT20, §10].\nLow-rank approximation.\nOur approach here is very diﬀerent than that of [MT20].\nIt provides eﬀective scaﬀolding for a reader to get a handle on the vast literature on\nlow-rank approximation. However, it comes at the price of creating fewer opportunities for mathematical explanations. Separately, our coverage here is distinguished\nby providing an overview of software that implements randomized algorithms for\nlow-rank approximation.\nSee Section 4 and Appendix C, [MT20, §11 – §15].\nFull-rank matrix decompositions.\nBy comparison to [MT20], we emphasize a broader\nrange of matrix decompositions and more algorithms for computing them. One of\nthe algorithms we cover is novel and is accompanied by proofs that characterize its\nbehavior. For the algorithms covered here and in [MT20], the latter provides more\nmathematical detail.\nSee Section 5.1 and Section 5.2.1, Appendix D, [MT20, §16].\nKernel methods.\nRandomized methods have proven very eﬀective in processing\nmachine learning models based on positive deﬁnite kernels. They are also eﬀective\nin approximating matrices from scientiﬁc computing induced by indeﬁnite kernels.\nBoth of these topics are addressed in [MT20]. We only address the former topic,\nand we do so in a way that emphasizes the resulting linear algebra problems.\nSee Sections 3.2.2, 3.2.4, and 6.1.3, Appendix B.4.1, [MT20, §19, §20].\nLinear system solvers.\nWe cover slightly more material for solving unstructured\nlinear systems than [MT20]. However, we do not cover methods that are speciﬁc to\nsparse problems. As a result, we do not cover a prominent method for approximate\nCholesky decompositions of sparse graph Laplacians.\nSee Sections 3.2.3 and 5.2, [MT20, §17, §18].\nTrace estimation.\nWe have the luxury of being able to cover recently-developed\nalgorithms that were not available when [MT20] was written. This includes two\nmethods that provide the ﬁrst major advances in trace estimation since the late\n1980s. We provide less depth than [MT20] on average, with the notable exception\nof stochastic Lanczos quadrature.\nSee Section 5.3, [MT20, §4, §6].\nPage 16\narXiv Version 2\n\n1.5. Notation and terminology\nIntroduction\nAdvanced sketching.\nBoth this monograph and [MT20] cover leverage score sampling and sketching operators with tensor product structures. We cover these topics\nin substantially more detail, spending a full ten pages on each of them. We do this\npartly because these topics complement one another: implicit matrices with tensor product structures are among the best candidates for practical leverage score\nsampling, nearly on par with kernel matrices from machine learning.\nSee Sections 6 and 7, [MT20, §7.4, §9.4, §9.6, §19.2.3].\n1.5\nNotation and terminology\nOur notation is summarized in Table 1.1; we also deﬁne some of this notation below\nas we explain basic concepts.\nMatrices and vectors\nLet A be an m×n matrix or linear operator. We use A∗to denote its adjoint (transpose, in the real case) and A† to denote its pseudo-inverse. It is called Hermitian\nif A∗= A and positive semideﬁnite if it is Hermitian and all of its eigenvalues are\nnonnegative. We often abbreviate “positive semideﬁnite” with “psd.”\nWe sometimes ﬁnd it convenient to write A ∈Rm×n. However, it should be\nunderstood that the methods in this monograph generally apply to both real and\ncomplex matrices. We therefore tend to deﬁne a matrix by phrases like “A is m-byn” or “an m-by-n matrix A.” We often call a vector of length n an n-vector; vectors\nare oriented as columns by default.\nFor m ≥n, a QR decomposition of A consists of an m × n column-orthonormal\nmatrix Q and an upper-triangular matrix R for which A = QR. Those familiar\nwith the NLA literature will note that this is typically called the economic QR\ndecomposition. If A has rank k < min(m, n), then we also consider it valid for\nQ to be m × k and for R to be k × n. We also consider QR decomposition with\ncolumn pivoting (QRCP). To describe QRCP, we say that if J = (j1, . . . , jn) is a\npermutation of JnK, then\nA[:, J] = [aj1, aj2, . . . , ajn]\nwhere ai is the ith column of A. In this notation, QRCP produces an index vector\nJ and factors (Q, R) that provide a QR decomposition of A[:, J].\nNow let A have rank k. Its singular value decomposition (SVD) takes the form\nA = UΣV∗, where the matrices (U, V) have k orthonormal columns and Σ =\ndiag(σ1, . . . , σk) is a square matrix with sorted entries σ1 ≥· · · ≥σk > 0. The SVD\ncan also be written as a sum of rank-one matrices: A = Pk\ni=1 σiuiv∗\ni , where (ui, vi)\nare the ith columns of (U, V) respectively. Those familiar with the NLA literature\nwill note that this is typically called the compact SVD.\nProbability and our usage of the term “random.”\nA Rademacher random variable uniformly takes values in {+1, −1}. The initialism\n“iid” expands to independent and identically distributed.\nWe often abuse terminology and say that a matrix “randomly” performs some\noperation. In reality, matrices only perform deterministic calculations, and randomness only comes into play when the matrix is ﬁrst constructed. This convention\narXiv Version 2\nPage 17\n\nIntroduction\n1.5. Notation and terminology\nextends to “matrices” that are abstract linear operators, in which case randomness\nis only involved in constructing the data that deﬁnes the operator.\nUnqualiﬁed use of the term “random” before performing an action with a ﬁnite\nset of outcomes (such as sampling components from a vector, applying a permutation, etc...) means the randomness is uniform over the space of possible actions.\nPage 18\narXiv Version 2\n\n1.5. Notation and terminology\nIntroduction\nTable 1.1: Notation\nArrays and indexing\nAij or A[i, j]\n(i, j)th entry of a matrix A\nai\nor A[:, i]\nith column of A\nvi\nor v[i]\nith component of a vector v\nJmK\nindex set of integers from 1 to m\nI or J\npartial permutation vector for indexing into an array\n|I|\nlength of an index vector\nA[I, :]\nsubmatrix consisting of (permuted) rows of A\nA[:, J]\nsubmatrix consisting of (permuted) columns of A\n:k\nindex into the leading k elements of an array,\nalong an axis of length at least k\nk:\nindex into the trailing n −k + 1 elements of an array,\nalong an axis of length n ≥k\nReserved symbols\nS\nsketching operator\nIk\nidentity matrix of size k × k\nδi\nith standard basis vector of implied dimension\n0n\nzero vector of length n\n0m×n\nzero matrix of size m × n\nLinear algebra\n∥x∥2 or ∥x∥\nEuclidean norm of a vector x\n∥A∥2\nspectral norm of A\n∥A∥F\nFrobenius norm of A\ncond(A)\nEuclidean condition number of A\nλi(A)\nith largest eigenvalue of A\nσi(A)\nith largest singular value of A\nA∗\nadjoint (transpose, in the real case) of A\nA†\nMoore–Penrose pseudoinverse of A\nA1/2\nHermitian matrix square root\nA ⪯B\nthe matrix B −A is positive semideﬁnite\nMatrix decomposition conventions\nA = QR\nQR decomposition (economic, by default)\n(Q, R, J) = qrcp(A)\nQR with column-pivoting; A[:, J] = QR.\nA = UΣV∗\nsingular value decomposition (compact, by default)\nR = chol(G)\nupper triangular Cholesky factor of G = R∗R\nProbability\nX ∼D\nX is a random variable following a distribution D\nE[X]\nexpected value of a random matrix X\nvar(X)\nvariance of a random variable X\nPr{E}\nprobability of the event E\narXiv Version 2\nPage 19\n\nIntroduction\n1.5. Notation and terminology\nPage 20\narXiv Version 2\n\nSection 2\nBasic Sketching\n2.1 A high-level plan .........................................................\n22\n2.1.1 Random number generation ........................................\n23\n2.1.2 Portability, reproducibility and exception handling .........\n24\n2.2 Helpful things to know about sketching ......................\n24\n2.2.1 Geometric interpretations of sketching ..........................\n25\n2.2.2 Sketch quality ..........................................................\n26\n2.2.3 (In)essential properties of sketching distributions ...........\n29\n2.3 Dense sketching operators ..........................................\n30\n2.4 Sparse sketching operators ..........................................\n32\n2.4.1 Short-axis-sparse sketching operators ...........................\n33\n2.4.2 Long-axis-sparse sketching operators ............................\n34\n2.5 Subsampled fast trigonometric transforms ..................\n35\n2.6 Multi-sketch and quadratic-sketch routines ................\n36\nThe BLAS (Basic Linear Algebra Subprograms) were originally a collection of\nFortran routines for computations including vector scaling, vector addition, and\napplying Givens rotations [LHK+79].\nThey were later extended to operations\nsuch as matrix-vector multiplication and triangular solves [DDH+88] as well as\nmatrix-matrix multiplication, block triangular solves, and symmetric rank-k updates [DDH+90]. These routines have subsequently been organized into three levels\ncalled BLAS 1, BLAS 2, and BLAS 3.\nOver the years the BLAS have evolved into a community standard, with implementations targeting diﬀerent machine architectures in many programming languages. This standardization has been instrumental in the development of linear\nalgebra libraries – from the early days of LINPACK, through to LAPACK, and on\nto modern libraries such as PLASMA and SLATE [DMB+79; DDD+87; ABB+99;\nADD+09; DGH+19; KWG+17; AAB+17]. It has also reduced the coupling between hardware and software design for NLA. Indeed, the spirit of the BLAS has\nbeen adapted to accommodate dramatic changes in prevailing architectures, such\nas those faced by ScaLAPACK and MAGMA [CDO+95; CDD+96; TDB10; NTD10].\nThis section summarizes our progress on the design of a “RandBLAS” library,\n21\n\nBasic Sketching\n2.1. A high-level plan\nwhich is to be to RandNLA as BLAS is to classical NLA. Section 2.1 begins by\nspeaking to high-level scope and design considerations. From there, Section 2.2\nsummarizes sketching concepts that remain important throughout this monograph;\nwe encourage the reader to not dwell too long on this section and instead return\nto it as-needed later on. Sections 2.3 through 2.6 present our plans for sketching\ndense data matrices. In brief: our near-term plans are for the RandBLAS to support sketching operators which could naturally be represented by dense arrays or\nby sparse matrices with certain structures; we consider row sampling and column\nsampling as particular types of sparse sketching.\n2.1\nA high-level plan\nWe begin with a simple premise.\nThe RandBLAS’ deﬁning purpose should be to facilitate implementation\nof high-level RandNLA algorithms.\nThis premise works to reduce the RandBLAS’ scope, as there are “basic” operations\nin RandNLA which do not support this purpose.1 Another way that we reduce the\nscope of the RandBLAS is to only consider sketching dense data matrices. It may be\nreasonable to lift this restriction in the future, and consider methods for producing\ndense sketches of sparse data matrices.\nOur premise for the RandBLAS suggests that it should be concerned with dataoblivious sketching – that is, sketching without consideration to the numerical properties of a dataset. We identify three categories of operations on this topic:\n• sampling a random sketching operator from a prescribed distribution,\n• applying a sampled sketching operator to a data matrix, and\n• sketching that is not naturally expressed as applying a single a linear operator\nto a data matrix.\nThese categories are somewhat analogous to BLAS 1, BLAS 2, and BLAS 3, insofar\nas their implementations admit more and more opportunities for machine-speciﬁc\nperformance optimizations.\nAt this time, however, we do not advocate for any\nformalization of “RandBLAS levels.”\nWe note that data-oblivious sketching is not the only kind of sketching of value in\nRandNLA. Indeed, data-aware sketching operators such as those derived from power\niteration are extremely important for low-rank approximation (see Section 4.3.1).\nMethods for row or column sampling based on leverage scores are also useful for kernel ridge regression and certain tensor computations; see Sections 6 and 7. Although\nimportant, most of the functionality for producing or applying these sketching operators should be addressed in higher-level libraries.\nIn the material under the next two headings, we address the questions of how\nto handle random number generation and reproducibility in the RandBLAS.\n1For example, the problem of accepting two matrices and using randomization to approximate\ntheir product is certainly basic, and it is of conceptual value [DKM06a]. However, it is rarely used\nas an explicit building block in higher-level RandNLA algorithms.\nPage 22\narXiv Version 2\n\n2.1. A high-level plan\nBasic Sketching\n2.1.1\nRandom number generation\nFor reproducibility’s sake it is important that the RandBLAS include a speciﬁcation\nfor random number generators (RNGs).\nWe believe the RandBLAS should use counter-based random number generators\n(CBRNGs), which were ﬁrst proposed in [SMD+11]. A CBRNG returns a random\nnumber upon being called with two integer parameters: the counter and the key.\nThe time required for the CBRNG to return does not depend on either of these\nparameters. A serial application can set the key at the outset of the program and\nnever change it.\nParallel applications (particularly parallel simulations) can use\ndiﬀerent keys across diﬀerent threads. Sequential calls to the CBRNG with a ﬁxed\nkey should use diﬀerent values for the counter. For a ﬁxed key, a CBRNG with a\np-bit integer counter deﬁnes a stream of random numbers with period length 2p.\nIn our context, CBRNGs are preferable to traditional state-based RNGs such\nas the Mersenne Twister. A key reason for this is that CBRNGs maximize ﬂexibility in the order in which a sketching operator is generated. For example, given a\nuser-provided counter oﬀset c which acts as a random seed, the (i, j)th entry of a\ndense d×m sketching operator can be generated with counter c+(i+dj). The fact\nthat these computations are embarrassingly parallel will be important for vendors\ndeveloping optimized RandBLAS implementations. We note that this ﬂexibility also\nprovides an advantage over widely-used linear congruential RNGs, which have separate shortcomings of performing very poorly on statistical tests [SMD+11, §2.2.1].\nParticular examples of CBRNGs include Philox, ARS, and Threeﬁsh, each of\nwhich was deﬁned in [SMD+11] and implemented in the Random123 library. These\nCBRNGs have periods of 2128, can support 264 diﬀerent keys, and pass standard\nstatistical tests for random number generators. Random123 provides the core of the\nsketching layer of the LibSkylark RandNLA library [KAI+15]. Implementations of\nPhilox and ARS can also be found in MKL Vector Statistics [Int19, §6.5].\nShift-register RNGs\nWe have observed that the CBRNGs in Random123 are signiﬁcantly more expensive than the state-based shift-register RNGs developed by Blackman and Vigna\n[BV21]. In fact, Blackman and Vigna’s generators are so fast that we have been\nable to implement a method for applying a Gaussian sketching operator to a sparse\nmatrix that beats Intel MKL’s sparse-times-dense matrix multiplication methods.\nHowever, in the application where we observed that performance, processing the\nsketch downstream was more expensive than computing the sketch in the ﬁrst place.\nTherefore, while CBRNGs were substantially more expensive in that application,\ntheir longer runtimes were inconsequential in that case. This longer runtime can be\nviewed as a price we pay for prioritizing reproducibility of sketching across compute\nenvironments with diﬀerent levels of parallelism.\nThe overall situation is this:\nState-based RNGs may be preferable to CBRNGs if sketching is the bottleneck in a RandNLA algorithm and where the cost of random number\ngeneration decisively aﬀects the cost of sketching. At this time we have\nno evidence that high-performance implementations of RandNLA algorithms run into such bottlenecks. Such evidence may arise in the future\nand warrant reconsideration to fast state-based RNGs for the RandarXiv Version 2\nPage 23\n\nBasic Sketching\n2.2. Helpful things to know about sketching\nBLAS, particularly if major advances are made in hardware-accelerated\nsketching algorithms.\n2.1.2\nPortability, reproducibility and exception handling\nWe believe it is important that the RandBLAS lends itself to portability across\nprogramming languages.\nTherefore we plan for the RandBLAS to have a procedural API and make use of no special data structures beyond elementary structs.\nHigher-level libraries should take responsibility for exposing RandBLAS functionality with sophisticated abstractions. In particular, we plan for RandLAPACK to\nexpose RandBLAS functionality through a suitable object-oriented linear operator\ninterface. A key goal of this interface will be to make it possible to implement highlevel RandNLA algorithms with minimal assumptions on the sketching operator’s\ndistribution. Such an interface will also reduce the coupling between determining\nRandBLAS’s procedural API and prototyping RandLAPACK.\nDebugging high-performance numerical code is notoriously diﬃcult. Care must\nbe taken in the design of the RandBLAS so as to not contribute to this diﬃculty.\nIndeed, it is essential that the RandBLAS be reproducible to the greatest extent\npossible. The actual extent of the reproducibility will depend on factors outside\nof our control. For example – the RandBLAS cannot oﬀer bitwise reproducibility\nguarantees unless the BLAS does the same (see Remark 2.1.1). Therefore the main\nchallenge for reproducibility for RandBLAS is in random number generation; this\nchallenge can be resolved comprehensively through the aforementioned CBRNGs.\nA key source of exceptions in NLA is the presence of NaNs or Infs in problem\ndata. Extremely sparse sketching matrices (such as those from Section 2.4.2) might\nnot even read every entry of a data matrix, and so they might miss a NaN or\nInf. Those routines will be clearly marked as carrying this risk. The majority\nof routines in the RandBLAS and RandLAPACK will not carry this risk: they will\npropagate NaNs and Infs. (See [DDG+22] for a more detailed discussion of how\nthe BLAS and LAPACK (should) deal with exceptions.) For any such routine the\nexact behavior will depend on how the random sketching operator interacts with the\nproblem data. For example, if a data matrix containing multiple Infs is sketched\ntwice using diﬀerent random seeds, then it is possible that an entry of the ﬁrst\nsketch is an Inf while the corresponding entry of the second sketch is a NaN.\nRemark 2.1.1. Making the BLAS bitwise reproducible is challenging because ﬂoatingpoint addition is not associative, and the order of summation can vary depending on\nthe use of parallelism, vectorization, and other matters [RDA18]. Summation algorithms that guarantee bitwise reproducibility do exist [ADN20]. These algorithms\nmay become practical on hardware that implements the latest IEEE 754 ﬂoating\npoint standard, which includes a recommended instruction for bitwise-reproducible\nsummation [IEE19]. However, we leave these matters to future work.\n2.2\nHelpful things to know about sketching\nThe purpose of sketching is to enact dimension-reduction so that computations\nof interest can be performed on a smaller matrix called a sketch. While precise\ncomputations performed on the sketch can vary dramatically, the simple statement\nof sketching’s purpose lets us deduce the following facts.\nPage 24\narXiv Version 2\n\n2.2. Helpful things to know about sketching\nBasic Sketching\n• Sketching operators applied to the left of a data matrix must must be wide\n(i.e., they must have more columns than rows).\n• Sketching operators applied to the right of a data matrix must be tall (i.e.,\nthey must have more rows than columns).\nThis is to say, in left-sketching we require that SA has fewer rows than A, and in\nright-sketching we require that AS has fewer columns than A. These facts are true\nregardless of the aspect ratio of the data matrix; see Figure 2.1 for an illustration.\nThe facts are important because sketching operators in the literature are often\ndeﬁned under the assumption of left-sketching.\nBefore we proceed further, we reiterate some important advice.\nWe encourage the reader to not dwell too long on this section (Section 2.2)\nand instead return to it as needed later on.\nWith that, Section 2.2.1 explains geometric interpretations of sketching from the left\nand right. It also introduces the concepts of “sketching in the embedding regime”\nand “sketching in the sampling regime.” Section 2.2.2 covers concepts of subspace\nembedding distortion and the oblivious subspace embedding property – these are\ncentral to RandNLA theory, but they play a modest role in this monograph. Section 2.2.3 states properties of sketching distributions that should hold as part of a\n‘sanity check’ for whether a proposed distribution is reasonable.\nSA\nS\nA\nAS\nA\nS\n(a)\n(b)\nFigure 2.1: The left plot (a) shows that a sketching operator S applied to the left\nof a matrix A is wide, whereas the right plot (b) shows that a sketching operator S\napplied to the right of a matrix A is tall. These stated properties hold universally;\nthere are no exceptions for any kind of sketching. Separately, we note that both cases\nin the ﬁgure illustrate sketching in the sampling regime in the sense of Section 2.2.1.\n2.2.1\nGeometric interpretations of sketching\nPrototypical left-sketching and right-sketching\nSketching A from the left preserves its number of columns. Therefore, it is suitable\nfor things such as estimating right singular vectors. We often interpret a left-sketch\nSA as a compression of the range of A to a space of lower ambient dimension. In\nthe special case when rank(SA) = rank(A), there is a sense in which the quality of\nthe compression is completely independent from the spectrum of A.\nSketching A from the right preserves its number of rows, and is suitable for things\nsuch as estimating left singular vectors. Conceptually, the right-sketch AS can be\ninterpreted as a sample from the range of A using a test matrix S. In the special\ncase when rank(AS) ≪rank(A) then it is appropriate to think of this as a “lossy\nsample” from a much larger “population,” and it is natural to want this sample to\ncapture as much information as possible for some sub-population of interest.\narXiv Version 2\nPage 25\n\nBasic Sketching\n2.2. Helpful things to know about sketching\nEquivalence of left-sketching and right-sketching\nLeft-sketching and right-sketching can be reduced to one another by replacing A\nand S by their adjoints. For example, a left-sketch SA can be viewed as a sample\nfrom the row space of A, equivalent to the right-sketch A∗S∗. Conversely, a rightsketch AS can be viewed as a compression of the row space of A, equivalent to the\nleft-sketch S∗A∗. Therefore it is artiﬁcial to strongly distinguish sketching operators\nby whether they are ﬁrst deﬁned for left-sketching or right-sketching.\nThis leads us to an important point.\nIf Dd,m is a distribution over wide d×m sketching operators, it is canonically extended to a distribution over tall n × d sketching operators by\nsampling T from Dd,n and then returning the adjoint S = T∗.\nThe notation in the statement above is carefully chosen: since our “data matrices”\nare typically m × n, a typical left-sketching operator requires m columns, and a\ntypical right-sketching operator requires n rows.\nThe embedding and sampling regimes\nWhile it is artiﬁcial to associate a sketching distribution only with left-sketching\nor only with right-sketching, there are indeed families of sketching operators that\nare suited to qualitatively diﬀerent situations. The following terms help with our\ndiscussion of such families.\nSketching in the embedding regime is the use of a sketching operator\nthat is larger than the data to be sketched. Sketching in the sampling\nregime is the use of a sketching operator that is far smaller than the\ndata to be sketched.\nIn the above deﬁnitions one quantiﬁes the size of an operator (or matrix) by the\nproduct of its number of rows and number of columns.\nIn Section 3, we will see that sketching in the embedding regime is nearly universal in randomized algorithms for least squares and related problems. In Section 4,\nwe will see that sketching in the sampling regime is the foundation of randomized\nalgorithms for low-rank approximation. Over these sections we tend to see sketching in the embedding regime happen from the left, and sketching in the sampling\nregime happen from the right. We stress that these tendencies are consequences of\nexposition; they do not always hold when developing or using RandNLA software.\n2.2.2\nSketch quality\nLet L be a subset of some high-dimensional Euclidean space Rm, S be a sketching\noperator deﬁned on Rm, and consider the sketch SL. Intuitively, SL should be\nuseful if its geometry is somehow “eﬀectively the same” as that of L.\nHere we\ndiscuss the preferred ways to quantify changes to geometry in RandNLA. We focus\non methods suitable for when L is a linear subspace, but we also consider when L\nis a ﬁnite point set.\nWe acknowledge up-front that it only does so much good to measure the quality\nof an individual sketch. Indeed, in order to make predictive statements about the\nbehavior of algorithms, it is necessary to understand how the distribution of a\nsketching operator S ∼D induces a distribution over measures of sketch quality in\nPage 26\narXiv Version 2\n\n2.2. Helpful things to know about sketching\nBasic Sketching\na given application. It is further necessary to analyze families of distributions Dd,m\nparameterized by an embedding dimension d, since the size of a sketch is often a\nkey parameter that a user can control.\nSubspace embeddings\nLet S be a d×m sketching operator and L be a linear subspace of Rm. We say that\nS embeds L into Rd and that it does so with distortion δ ∈[0, 1] if x ∈L implies\n(1 −δ)∥x∥2 ≤∥Sx∥2 ≤(1 + δ)∥x∥2.\n(2.1)\nWe often call such an operator an δ-embedding.\nThe concept of a subspace embedding was ﬁrst used implicitly in RandNLA by\n[DMM06]; the sketching operators used in [DMM06] were based on a type of dataaware sketching called leverage score sampling (discussed in Section 6). The ﬁrst\nexplicit deﬁnition of subspace embeddings was given by [Sar06], who focused on\ndata-oblivious sketching. We address data-oblivious subspace embeddings in detail\nmomentarily.\nThe most transparent use of subspace embedding distortion arises when L is the\nrange of a matrix A. In this context, S is a δ-embedding for L if and only if the\nfollowing two-sided linear matrix inequality holds:\n(1 −δ)2A∗A ⪯(SA)∗(SA) ⪯(1 + δ)2A∗A.\n(2.2)\nIn other words, the distortion of S as an embedding for range(A) is a measurement\nof how well the Gram matrix of SA approximates that of A.\nNote that in order for S to be a subspace embedding for L it is necessary that\nd ≥dim(L). Therefore if L is the range of an m × n matrix of full-column-rank,\nthe requirement that d ≥dim(L) means that subspace embeddings can only be\nachieved when “sketching in the embedding regime,” in the sense of Section 2.2.1.\nFurthermore, substantial dimension reduction can only be achieved in this framework when m ≫n.\nEﬀective distortion\nSubspace embedding distortion is the most common measure of sketch quality, but\nit is not without its limitations. Its greatest limitation is that it is not invariant\nunder scaling of S (i.e., it is not invariant under replacing S ←tS for t ̸= 0). This is\na signiﬁcant limitation since many RandNLA algorithms are invariant under scaling\nof S; existing theoretical analyses of RandNLA algorithms simply do not take this\ninto account.\nIn Appendix A.1 we explore a novel concept of eﬀective distortion that resolves\nthe scale-sensitivity problem. Formally, the eﬀective distortion of a sketching operator S for a subspace L is\nDe(S; L) = inf{ δ : 0 ≤δ ≤1, 0 < t\n(2.3)\ntS is a δ-embedding for L}.\nIn words, this is the minimum distortion that any sketching operator tS can achieve\nfor L, optimizing over t > 0. We brieﬂy reference this concept in our discussion of\nalgorithms for least squares and optimization (§3.2). Appendix B.1 makes deeper\nconnections between eﬀective distortion and randomized preconditioning methods\nfor least squares.\narXiv Version 2\nPage 27\n\nBasic Sketching\n2.2. Helpful things to know about sketching\nOblivious subspace embeddings\nData-oblivious subspace embedding (OSEs) were ﬁrst used in RandNLA in [Sar06]\nand were largely popularized by [Woo14].\nThere is a clean way to describe the\n“reliability” of a sketching distribution in this setting.\nConsider a distribution D over wide d × m matrices. We say that D has\nthe OSE property with parameters (δ, n, p) if, for every n-dimensional\nsubspace L ⊂Rm, we have\nPr{S ∼D is a δ-embedding for L} ≥1 −p.\nTheoretical analyses of sketching distributions often concern bounding d as a function of (δ, n, p) to ensure that D satisﬁes the OSE property. Naturally, all else equal,\nwe would like to achieve the OSE property for smaller values of d.\nTheoretical results can be used to select d in practice for very well-behaved distributions, particularly the Gaussian distribution. Results for the more sophisticated\ndistributions (such as those of sparse sketching operators) tend to be pessimistic\ncompared to what is observed in practice. Some of this pessimism stems from the\nexistence of esoteric constructions which indeed call for large embedding dimensions.\nSetting these constructions aside, we have reason to be optimistic since distortion is\nactually not the ideal measure of sketch quality in many settings. Indeed, eﬀective\ndistortion is far more relevant for least squares and optimization, and it will always\nbe no larger than the standard notion of distortion.\nAll in all, there is something of an art to choosing the best sketching distribution\nfor a particular RandNLA task. Luckily, for most RandNLA algorithms it is far\nfrom necessary to choose the “best” sketching distribution; good results can be\nobtained even when setting distribution parameters by simple rules of thumb.\nJohnson–Lindenstrauss embeddings\nLet S be a d × m sketching operator and L be a ﬁnite point set in Rm. We say that\nS is a Johnson–Lindenstrauss embedding (or “JL embedding”) for L with distortion\nδ if, for all distinct x, y in L, we have\n1 −δ ≤∥S(x −y)∥2\n2\n∥x −y∥2\n2\n≤1 + δ.\nThis property is named for a seminal result by William Johnson and Joram Lindenstrauss, who used randomization to prove the existence of operators satisfying\nthis property where d is logarithmic in |L| and linear in 1/δ2 [JL84].\nThe JL Lemma (as the result is now known) is remarkable for two reasons.\nFirst, the requisite value for d did not depend on the ambient dimension m and\nwas only logarithmic in |L|. Second, the construction of the transformation S was\ndata-oblivious – a scaled orthogonal projection. This latter fact led to questions\nabout how one might deﬁne alternative distributions over sketching operators, with\nthe aim of\n1. being simpler to implement than a scaled orthogonal projection, and\n2. attaining similar “data-oblivious JL properties.”\nPage 28\narXiv Version 2\n\n2.2. Helpful things to know about sketching\nBasic Sketching\nIt so happened that many constructions could achieve these goals. For example,\n[IM98] and [DG03] relaxed the condition of being a (scaled) orthogonal projector to\nS having iid Gaussian entries, which still results in a rotationally-invariant distribution. As another example, [Ach03] relaxed the rotational invariance by choosing\nthe entries of S to be scaled Rademacher random variables.\n2.2.3\n(In)essential properties of sketching distributions\nDistributions D over wide sketching operators are typically designed so that, for\nS ∼D, the mean and covariance matrices are\nE[S] = 0\nand\nE[S∗S] = I.\nThe property that E[S] = 0 is important – if not ubiquitous – in RandNLA. However, there is some ﬂexibility in the latter property, as in most situations it suﬃces\nfor the covariance matrix to be a scalar multiple of the identity.\nTo understand why we have ﬂexibility in the scale of the covariance matrix,\nconsider how E[S∗S] = I is equivalent to S preserving squared Euclidean norms\nin expectation. As it happens, the vast majority of algorithms mentioned in this\nmonograph do not need sketching operators to preserve norms. Rather, they rely\non sketching preserving relative norms, in the sense that ∥Su∥2/∥Sv∥2 should be\nclose to ∥u∥2/∥v∥2 for all vectors u, v in a set of interest. Such a property is clearly\nunaﬀected if every entry of S scaled by a ﬁxed nonzero constant (i.e., if S is replaced\nby tS for some t ̸= 0).\nThis section uses scale-agnosticism to help describe sketching distributions with\nreduced emphasis on whether the operator is wide or tall. For example, if the entries\nof S are iid mean-zero random variables of ﬁnite variance, then both E[S∗S] and\nE[SS∗] are scalar multiples of the identity matrix. Speaking loosely, the former\nproperty justiﬁes using S to sketch from the left and the latter property justiﬁes\nusing S∗to sketch from the right.\nWith this observation in mind, this section ignores most matters of scaling that\nis applied equally to all entries of a sketching operator. This manifests in how we\nregularly describe sketching operators as having entries in [−1, 1] even though it is\nmore common to have entries in [−v, v] for some positive v (which is set to achieve\nan identity covariance matrix). Note that this does not confer freedom to scale\nindividual rows, columns, or entries of a sketching operator separately from one\nanother.\nThe main places where scaling matters are in algorithms for norm estimation\n(see Section 4.3.5) and algorithms which only sketch a portion of the data in a larger\nproblem. The subtleties in this latter situation warrant a detailed explanation.\nScale sensitivity: partial sketching\nLet G be an n × n psd matrix and A be a very tall m × n matrix. Suppose that we\napproximate\nH = A∗A + G\nby a partial sketch\nHsk = (SoA)∗(SoA) + G\nwhere So is a d × m sketching operator. How should we understand the statistical\nproperties of Hsk as an estimator for H?\narXiv Version 2\nPage 29\n\nBasic Sketching\n2.3. Dense sketching operators\nAt the simplest level we can turn to the idea of subspace embedding distortion.\nUsing the characterization of distortion in (2.2), we could study the distribution of\nthe minimum δ ∈(0, 1) for which\n(1 −δ)2H ⪯Hsk ⪯(1 + δ)2H.\nOne can go beyond distortion by lifting to a higher-dimensional space. Letting\n√\nG\ndenote the Hermitian square root of G, we deﬁne the augmented sketching operator\nand augmented data matrix\nS =\n\u0014So\n0\n0\nI\n\u0015\nand\nAG =\n\u0014 A\n√\nG\n\u0015\nThis lets us express H = A∗\nGAG and Hsk = (SAG)∗(SAG). Therefore the statistical\nproperties of Hsk as an approximation to H can be understood in terms of how S\npreserves (or distorts) the range of AG.\nScale sensitivity: row sampling from block matrices\nThe concept of partial sketching can arise when sketching block matrices, which\nare indeed encountered in many applications. For example, it is widely appreciated\nthat a ridge regression problem with tall m × n data matrix A and regularization\nparameter µ can be lifted to an ordinary least squares problem with data matrix\nAµ := [A; √µI].\nSuppose we want to sketch Aµ by a row sampling operator S.\nIt is natural\nto treat the lower n rows of Aµ diﬀerently than its upper m rows. In particular,\nit is natural for S to be an operator that produces SA = [SoA; √µI] with some\nother d × m row sampling operator So. Here, even if So sampled rows from A uniformly at random, the map Aµ 7→SAµ would not sample uniformly at random from\nAµ. Therefore there is a sense in which partial sketching is a way of incorporating\nnon-uniform row sampling into other sketching distributions; see [DKM06a] for the\norigins of this interpretation. In the context of this speciﬁc example, the nonuniformity would necessitate that So be scaled to have entries in {0, ±1/\n√\nd}.\nWe\nrefer the reader to Section 2.4.2 and Section 6.1.1 for more discussion on sketching\noperators that implement row sampling.\n2.3\nDense sketching operators\nThe RandBLAS should provide methods for sampling sketching operators with iid\nentries drawn from distinguished distributions. Across this broad category, we believe the following types of operators stand out:\n• Rademacher sketching operators: entries are ±1 with equal probability;\n• uniform sketching operators: entries are uniform over [−1, 1];\n• Gaussian sketching operators: entries follow the standard normal distribution.\nWe believe the RandBLAS should also support sampling row-orthonormal or columnorthonormal matrices uniformly at random from the set of all such matrices.\nPage 30\narXiv Version 2\n\n2.3. Dense sketching operators\nBasic Sketching\nThe theoretical results for Gaussian operators are especially strong. However,\nthere is little practical diﬀerence in the performance of RandNLA algorithms between any of the three entrywise iid operators given above. This is reﬂected in\nimplementations such as [LLS+17] that only use uniform sketching operators. The\npractical equivalence between these types of sketching operators also has theoretical support through universality principles in high-dimensional probability [Ver18],\n[OT17], [MT20, §8.8], [DLL+20].\nIn what follows we speak to implementation\ndetails and the intended use cases for these operators.\nSampling iid-dense sketching operators\nSampling from the Rademacher or uniform distributions is the most basic operation\nof random number generators. Methods for sampling from the Gaussian distribution\ninvolve transforming random variables sampled uniformly from [0, 1]. There are two\ntransformations of interest for the RandBLAS: Box-Muller [BM58]; and the Ziggurat\ntransform [MT00]. The former should be included in the RandBLAS because it is\neasy to implement and parallelizes well. The latter method is far more eﬃcient on\na single thread, and it has been used within RandNLA (see [MSM14]), but it does\nnot parallelize well [Ngu07, §37.2.3]. We postpone any recommendation for whether\nit should be an option in the RandBLAS.\nRandNLA algorithms tend to be very robust to the quality of the random number generator. As a result, it is not necessary for us to sample from the Gaussian\ndistribution with high statistical accuracy. This is due in part to the aforementioned\nuniversality principles, and it can be seen through the success of sub-Gaussian distributions as an analysis framework in high-dimensional probability [Ver18, §2]. From\nan implementation standpoint, there is likely no need to sample from the Gaussian\ndistribution beyond single precision [Mar22a]. It is worth exploring if even lower\nprecisions (e.g., half-precision) would suﬃce for practical purposes.\nApplying iid-dense sketching operators\nIf a dense sketching operator is realized explicitly in memory then it can (and\nshould) be applied by an appropriate BLAS function, most likely gemm.\nMany\nRandNLA algorithms provide good practical performance even with such simple\nimplementations, although there is potential for reduced memory or communication\nrequirements if a sketching operator is applied without ever fully allocating it inmemory. There is a large design space for such algorithms with iid-dense sketching\noperators when using counter-based random number generators (see Section 2.1.1).\nSuch functionality could appear in an initial version of a RandBLAS standard. The\nreference implementations of such functions could start as mere wrappers around\nroutines to generate a sketching operator and then apply that operator via gemm.\nSampling and applying Haar operators\nIf we suppose left-sketching, then the Haar distribution is the uniform distribution\nover row-orthonormal matrices. If we instead suppose right-sketching, then it is the\nuniform distribution over column-orthonormal matrices. We call these operators\n“dense” because if one is sampled and then formed explicitly, it will be dense with\nprobability one.\narXiv Version 2\nPage 31\n\nBasic Sketching\n2.4. Sparse sketching operators\nThere are two qualitative approaches to sampling from this distribution. The\nnaive approach essentially requires sampling from a Gaussian distribution and performing a QR factorization, at a total cost of O(d2m); see [Li92, §1 - §4] and more\ngeneral methods in [Mez07]. A more eﬃcient approach – which costs only O(dm)\ntime – involves constructing the operator as a composition of suitable Householder\nreﬂectors [Ste80]. This approach has the secondary beneﬁt of not needing to form\nthe sketching operator explicitly.\nHaar operators are of interest not just for sketching in RandNLA algorithms but\nalso for generating test data for evaluating other sketching operators. As such, we\nbelieve they are natural to include in a ﬁrst version of a RandBLAS standard.\nIntended use-cases\nUsing terminology from Section 2.2.1, dense sketching operators are commonly used\nfor “sketching in the sampling regime.” In particular, they are the workhorses of\nrandomized algorithms for low-rank approximation. They also have applications\nin certain randomized algorithms for ridge regression and some full-rank matrix\ndecomposition problems.\nThese distributions are much less useful for sketching dense matrices “in the\nembedding regime” (again in the sense of Section 2.2.1).\nThis is because they\nare more expensive to apply to dense matrices than many other types of sketching\noperators. These types of sketching operators might be of interest in the embedding\nregime if applied to sparse or otherwise structured data matrices.\n2.4\nSparse sketching operators\nThe RandNLA literature describes many types of sparse sketching operators, almost\nalways under the convention of sketching from the left. We think it is important to\ndeﬁne sketching distributions in a way that is agnostic to sketching from the left or\nright. Indeed, while we often focus on left-sketching for ease of exposition, asserting\nthat this is “without loss of generality” ignores the plight of the user tasked with\nright-sketching.\nIn order to achieve our desired agnosticism, we use a taxonomy for sparse sketching operators which has not appeared in prior literature. To describe it, we use the\nterm short-axis vector in reference to the columns of a wide matrix or rows of a\ntall matrix. The term long-axis vector is deﬁned analogously, as the rows of a wide\nmatrix or columns of a tall matrix. In these terms, we have the following families\nof sparse sketching operators.\n• Short-axis-sparse sketching operators. The short-axis vectors of these operators are independent of one another. Each short-axis vector has a ﬁxed (and\nvery small) number of nonzeros. Typically, the indices of the nonzeros in each\nshort-axis vector are sampled uniformly without replacement.\n• Long-axis-sparse sketching operators. The long-axis vectors of these operators\nare independent of one another. For a given long-axis vector, the indices for its\nnonzeros are sampled with replacement according to a prescribed probability\ndistribution (which can be uniform). The value of a given nonzero is aﬀected\nby the number of times its index appears in the sample for that vector.\nPage 32\narXiv Version 2\n\n2.4. Sparse sketching operators\nBasic Sketching\n• Iid-sparse sketching operators.\nMathematically, these can be described as\nstarting with an iid-dense sketching operator and “zeroing-out” entries in an\niid-manner with some high probability. (From an implementation standpoint\nthis would work the other way around, randomly choosing a few entries to\nmake nonzero.)\nWhen abbreviations are necessary, we suggest that short-axis-sparse sketching operators be called SASOs and that long-axis-sparse sketching operators be called\nLASOs. Most of our use of such abbreviations appears here and in Appendix A.2.\nA visualization of these types of sketching operators is given in Figure 2.2.\nBefore proceeding further we should say that we are not in favor of including iidsparse sketching operators in the RandBLAS. Our ﬁrst reason for this is that their\ntheoretical guarantees are not as strong as either SASOs (see the discussion at the\nend of [Tro20, §7.4] and remarks in [Lib09, §2.4]) or LASOs [DLD+21; DLP+21].\nOur second reason is that their lack of predictable structure makes it harder to\nimplement eﬃcient parallel algorithms for applying these operators. Therefore in\nwhat follows we only give details on SASOs and LASOs.\nA\nS\nA\nS\nA\nS\n(a)\n(b)\n(c)\nFigure 2.2: Illustration of a SASO (a) with 3 non-zero entries per row, LASO (b)\nwith 3 non-zero entries per column, and an iid-sparse sketching operator (c) with\niid non-zero entries.\n2.4.1\nShort-axis-sparse sketching operators\nSASOs include sketching operators known as sparse Johnson–Lindenstrauss transforms, the Clarkson–Woodruﬀtransform, CountSketch, and OSNAPs [KN12; CW13;\nMM13; NN13]. These constructions are all described assuming we sketch from the\nleft, and as such, they are all stated for wide sketching operators. They are described\nas having a ﬁxed number of nonzeros within each column. The more general notion\n(for sketching from the left or right) is to say there is a ﬁxed number of nonzeros\nper short-axis vector.\nThe short-axis vectors of a SASO should be independent of one another. One\ncan select the locations of nonzero elements in diﬀerent ways; we are interested in\ntwo methods from [KN12]. For a wide d × m operator, we can\n1. sample k indices uniformly from JdK without replacement, once for each column, or\n2. divide JdK into k contiguous subsets of equal size, and then for each column\nwe select one index from each of the k index sets.\nThese deﬁnitions are extended from wide sketching operators to tall sketching operators in the natural way.\narXiv Version 2\nPage 33\n\nBasic Sketching\n2.4. Sparse sketching operators\nFor either method, the nonzero values in a SASO’s short-axis vector are canonically independent Rademachers. Alternatively, they can be drawn from other subGaussian distributions. For example, in the wide case, drawing the nonzeros independently and uniformly from a union of disjoint intervals, such as [−2, −1] ∪[1, 2],\ncan protect against the possibility of a given row of S being orthogonal to a column\nof a matrix to be sketched [Tyg22].\nDetails SASOs are provided in Appendix A.2. This includes implementation\nnotes, a short historical summary of relevant theory, and remarks on setting the\nsparsity parameter k. On the topic of theory, we note here that the state-of-the-art\nresults for SASOs are due to Cohen [Coh16]. More information can be found in\nthe lecture notes [Mah11; Mah16; DM18], [Tro20, §7.4] and the surveys [Woo14],\n[MT20, §9.2].\nRemark 2.4.1 (Naming conventions). The concept of what we would call a “wide\nSASO” is referred to in the literature as an OSNAP. We have a slight preference for\n“SASO” over “OSNAP” for two reasons. First, it pairs naturally with the abbreviation LASO for long-axis-sparse sketching operators, which is valuable for taxonomizing sparse sketching operators. Second, the literature consistently describes\nOSNAPs as having a ﬁxed number of nonzeros per column. While this description\nis appropriate for left sketching, it is not appropriate for right sketching.\n2.4.2\nLong-axis-sparse sketching operators\nThis category includes row and column sampling, LESS embeddings [DLD+21], and\nLESS-uniform operators [DLP+21].\nIn the wide case, a LASO has independent rows and a ﬁxed upper bound on the\nnumber of nonzeros per row. All rows are sampled with reference to a distribution\np over JmK (which can be uniform) and a positive integer k. Construction begins by\nsampling t1, . . . , tk from JmK with replacement according to p. Then we initialize\nS[i, :] =\n1\n√\ndk\n\u0012s\nb1\np1\n, . . . ,\ns\nbm\npm\n\u0013\n,\n(2.4)\nwhere bj is the number of times the index j appeared in the sample (t1, . . . , tk).\nWe ﬁnish constructing the row by multiplying each nonzero entry by an iid copy\nof a mean-zero random variable of unit variance (e.g., a standard Gaussian random\nvariable). Such a LASO will have at most k nonzeros per row and hence at most\ndk nonzeros in total. Note that this is much smaller than mk nonzeros required by\na SASO with the same parameters.\nThe quality of sketches produced by LASOs when p is uniform depends on the\nproperties of the matrix to be sketched. Speciﬁcally, it will depend on the leverage\nscores of the matrix.\nThe leverage score concept, introduced in Section 6.1, is\nimportant for constructing data-aware sketching operators that implement row or\ncolumn sampling. If p is the leverage score distribution of some matrix then the\nsketching operator is known as a Leverage Score Sparsiﬁed (LESS) embedding for\nthat matrix [DLD+21]. The term LESS-uniform has been used for long-axis-sparse\noperators that use the uniform distribution for p [DLP+21].\nRemark 2.4.2 (Scale). The scaling factor 1/\n√\ndk appearing in the initialization (2.4)\nis the same for all rows of S (in the wide case, i.e., for each long-axis vector). This\nfactor is necessary so that once the nonzeros in S are multiplied by mean-zero unitvariance random variables, we have E[S∗S] = Im. This scaling matters when one\nPage 34\narXiv Version 2\n\n2.5. Subsampled fast trigonometric transforms\nBasic Sketching\ncares about subspace embedding distortion or when one is only sketching a portion\nof the problem data (see Section 2.2.3).\nThis scaling has no eﬀect on eﬀective\ndistortion if p is uniform.\n2.5\nSubsampled fast trigonometric transforms\nFast trigonometric transforms (or fast trig transforms) are orthogonal or unitary\noperators that take m-vectors to m-vectors in O(m log m) time or better. The most\nimportant examples in this class are the Discrete Fourier Transform (for complexvalued inputs) and the Discrete Cosine Transform (for real-valued inputs). The\nWalsh-Hadamard Transform is also notable; although it only exists when m is a\npower of two, it is equivalent to a Kronecker product of log2 m Discrete Fourier\nTransforms of size 2 × 2, and the standard algorithm for applying it involves no\nmultiplications and entails no branching.\nTraditionally, trig transforms are valued for their ability to map dense input\nvectors with a periodic structure into sparse output vectors. Within RandNLA,\nwe are interested in them for the opposite reason: the fact that they map inputs\nthat lack periodic structure to dense outputs. This behavior is useful because if we\npreprocess an input to destroy any periodic structure with high probability, the resulting output should be easier to approximate by random coordinate subsampling.\nThis leads to the idea of a subsampled randomized fast trig transforms or SRFTs.\nTraditional SRFTs\nFormally, a d × m SRFT takes the form\nS =\np\nm/d RFD,\nwhere D is a diagonal matrix of independent Rademachers, F is a fast trig transform\nthat maps m-vectors to m-vectors, and R randomly samples d components from\nan m-vector [AC06; AC09]. For added robustness one can deﬁne SRFTs slightly\ndiﬀerently, replacing S by SΠ for a permutation matrix Π [MT20].\nSRFTs are appealing for their eﬃciency and theoretical guarantees. Speaking\nto the former aspect, a d × m SRFT can be applied to an m × n matrix in as\nlittle as O(mn log d) time by using methods for subsampled fast trig transforms\n[WLR+08, §3.3], [Lib09, §3.3]. Theoretical guarantees for SRFTs are usually established assuming F is the Walsh-Hadamard transform [DMM+11; Tro11; BG13].\nThese guarantees are especially appealing since they do not rely on tuning parameters such as sparsity parameters required by sketching operators from Section 2.4.\nThe trouble with SRFTs is that they are notoriously diﬃcult to implement\neﬃciently. Even their best-case O(mn log d) complexity is higher than the O(mnk)\ncomplexity of a SASO that is wide with k ≪log d nonzeros per column. However,\nSRFTs have an advantage when it comes to memory: if one overwrites A by the\nm × n matrix B :=\np\nm/d FDA in O(mn log m) time, then SA can be accessed as a\nsubmatrix of rows of B without losing access to A or A∗as linear operators. Further\ninvestigation is needed to determine the true value of this in-place nondestructive\nimplementation. For the time being, we do not believe that traditional SRFTs are\nessential for a preliminary RandBLAS standard.\narXiv Version 2\nPage 35\n\nBasic Sketching\n2.6. Multi-sketch and quadratic-sketch routines\nBlock SRFTs\nLet p be a positive integer, r = m/p be greater than d, and R be a matrix that\nrandomly samples d components from an r-vector.\nFor each index i ∈JpK, we\nintroduce a d × r sketching operator\nSi =\np\nr/d Dpost\ni\nRFDpre\ni\n,\nwhere Dpost\ni\nand Dpre\ni\nare diagonal matrices ﬁlled with independent Rademachers.\nThe block SRFT [BBG+22] is deﬁned columnwise as S = [S1\nS2\n. . .\nSp].\nBlock SRFTs can eﬀectively leverage parallel hardware using serial implementations of the fast trig transform. For concreteness, suppose A is m×n and distributed\nblock row-wise among p processors. We apply the block SRFT S by the formula\nSA =\nX\ni∈JpK\nSiAi,\nwhere Ai is the block of rows of A stored on processor i. The multiplication SiAi,\ncomputed locally on each processor, is followed by a reduction operation among\nprocessors to sum the local contributions.\nWe are undecided as to whether block SRFTs are appropriate for a preliminary\nRandBLAS standard. Their comparative ease of implementation is favorable. However, they are problematic in that the deﬁnition of the distribution changes as we\nvary p, which complicates reproducibility across platforms.\nHistorical remarks and further reading\nThe development of SRFTs began with fast Johnson–Lindenstrauss transforms\n(FJLTs) [AC06], which replace the matrix “R” in the SRFT construction by a particular type of sparse matrix. FJLTs were ﬁrst used in RandNLA for least squares\nand low-rank approximation by [Sar06]. The jump from FJLTs to SRFTs was made\nindependently in [DMM+11] and [WLR+08] for usage in least squares and low-rank\napproximation, respectively.\nFor more background on this topic we refer the reader to the book [Mah11],\nthe lecture notes [Mah16], [DM18], [Tro20, §7.5], and the survey [MT20, §9.3].\nWe also note that SRFTs are sometimes called randomized orthornomal systems\n[PW16; PW17; OPA19] or (with slight abuse of terminology) FJLTs. Finally, we\npoint out that a type of “SRFTs without subsampling” has been successfully used\nto approximate Gaussian matrices needed in random features approaches to kernel\nridge regression [LSS13].\nRemark 2.5.1 (Navigating the literature). The reader should be aware that [AC09] is\nthe journal version of [AC06]. Additionally, while [LWM+07] also describes SRFTs,\nit was actually written after [WLR+08].\n2.6\nMulti-sketch and quadratic-sketch routines\nFor many years, the performance bottleneck in NLA algorithms has been data\nmovement, rather than FLOPs performed on the data.\nFor example, a general\nmatrix-matrix multiply with n × n matrices would do O(n3) data movement if\nimplemented naively with three nested loops, but can be up to a factor of\n√\nM\nsmaller, where M is the cache size, if appropriately implemented using loop tiling.\nPage 36\narXiv Version 2\n\n2.6. Multi-sketch and quadratic-sketch routines\nBasic Sketching\nIn our context of RandNLA, the fastest randomized algorithms for low-rank\nmatrix approximation involve computing multiple sketches of a data matrix. Such\nmulti-sketching presents new challenges and opportunities in the development of\noptimized implementations with minimal data movement.\nWe believe the RandBLAS should include functionality for at least three types\nof multi-sketching, listed below. The end of Section 4.2.1 points to algorithms that\nuse these primitives. In all cases of which we are aware, these primitives are only\nused for sketching in the sampling regime.\n1. Generate S and compute Y1 = AS and Y2 = A∗AS. We illustrate the use of\nthis primitive explicitly in Algorithm 12.\n2. Generate independent S1, S2, and compute Y1 = AS1 and Y2 = S2A. Algorithms which use this primitive typically need to retain S1 or S2 for later use\n[HMT11, pg. 251], [YGL+17, Algorithm 2], [TYU+17b, § 1.4].\n3. Generate independent S1, S2, S3, S4, and compute Y1 = AS1, Y2 = S2A, and\nY3 = S3AS4.\nHaving identiﬁed these operations as basic building blocks, we arrive at the following\nquestion.\nWhat combination of sketching distributions should be supported in\nmulti-sketching of types 2 and 3?\nFor the former type (i.e., type 2), it is important to support at least the case that\nboth S1 and S2 are dense sketching operators, and it may be useful to support\nwhen both are fast operators. At this point, we do not see an advantage for one of\n(S1, S2) to be a fast operator and for the other to be dense. For the latter type (i.e.,\ntype 3), [TYU+17b, §7.3.2] suggests that (S1, S2) be Gaussian and that (S3, S4) be\nSRFTs. We believe that it would be reasonable to use SASOs in place of SRFTs in\nthis context.\nThe RandBLAS should provide methods to compute sketches that are quadratic\nin the data matrix.\nBy “quadratic sketch,” we mean a linear sketch of A∗A or\nAA∗. This operation is ubiquitous in algorithms for low-rank approximation. As\nwith multi-sketching, all uses of quadratic sketching (of which we are aware) entail\nsketching in the sampling regime. It is not possible to fundamentally accelerate this\nkind of sketching by using fast sketching operators.2 Therefore it would be reasonable for RandBLAS’s quadratic sketching methods to only support dense sketching\noperators.\n(The preceding comments also apply to type 1 multi-sketching.)\nIn\nessence, this asks for a high-performance implementation of the composition of the\nBLAS 3 functions syrk and gemm: (A, S) 7→AA∗S. There is a substantial amount\nof structure in quadratic sketching that could be leveraged for reduced data movement, which suggests that the RandBLAS would beneﬁt signiﬁcantly from having\noptimized routines for this functionality.\n2This point has also been made in a related setting [MT20, §11.6.1].\narXiv Version 2\nPage 37\n\nBasic Sketching\n2.6. Multi-sketch and quadratic-sketch routines\nPage 38\narXiv Version 2\n\nSection 3\nLeast Squares and Optimization\n3.1 Problem classes ...........................................................\n40\n3.1.1 Minimizing regularized quadratics ...............................\n41\n3.1.2 Solving least squares and basic saddle point problems .....\n41\n3.2 Drivers ........................................................................\n43\n3.2.1 Sketch-and-solve for overdetermined least squares\n.........\n43\n3.2.2 Sketch-and-precondition for least squares and saddle point\nproblems .................................................................\n44\n3.2.3 Nystr¨om PCG for minimizing regularized quadratics ......\n49\n3.2.4 Sketch-and-solve for minimizing regularized quadratics ...\n50\n3.3 Computational routines ..............................................\n51\n3.3.1 Technical background: optimality conditions for saddle point\nproblems .................................................................\n51\n3.3.2 Preconditioning least squares and saddle point problems:\ntall data matrices ......................................................\n53\n3.3.3 Preconditioning least squares and saddle point problems:\ndata matrices with fast spectral decay ..........................\n56\n3.3.4 Deterministic preconditioned iterative solvers ................\n57\n3.4 Other optimization functionality .................................\n58\n3.5 Existing libraries .........................................................\n60\nNumerical linear algebra is the backbone of the most widely-used algorithms\nfor continuous optimization. Continuous optimization, in turn, is a workhorse for\nmany scientiﬁc computing, machine learning, and data science applications.\nThe connections between optimization and linear algebra are often introduced\nwith least squares problems. Such problems have been used as a tool for curve ﬁtting\nsince the days of Gauss and Legendre over 200 years ago — several decades before\nCayley even deﬁned linear algebraic concepts such as the matrix-inverse! These\nproblems are also remarkable because algorithms for solving them easily generalize\nto more complicated settings. Indeed, one of this section’s key messages is that, by\nadopting a suitable perspective, one can use randomization in essentially the same\nway to solve a wealth of diﬀerent quadratic optimization problems.\n39\n\nLeast Squares and Optimization\n3.1. Problem classes\nOur perspective entails describing all least squares problems in terms of an m×n\ndata matrix A with at least as many rows as columns. Speciﬁcally, we express the\noverdetermined problem as\nmin\nx∈Rn ∥Ax −b∥2\n2\nfor a vector b in Rm, while we express the underdetermined problem as\nmin\ny∈Rm{∥y∥2\n2 | A∗y = c}\nfor a vector c in Rn. Of course, both of these models could be expressed in the\ncorresponding “argmin” formulation. We generally prefer the “min” formulation\nfor the optimization problem itself and use “argmin” only for the set of optimal\nsolutions.\nSection 3.1 introduces the problems we consider: minimization of regularized\nquadratics and various generalizations of least squares problems. For each problem,\nit provides high-level comments on structures and desired outcomes that can make\nrandomized algorithms preferable to classical ones.\nSection 3.2 covers the drivers for these problems based on RandNLA. It details the problem structures that stand to beneﬁt from a particular driver, and\nit highlights other linear algebra problems that largely reduce to solving problems\namenable to these drivers. Section 3.3 details some essential computational routines\nthat would power the drivers.\nThe rest of the Section 3 is largely supplemental. Section 3.4 reviews randomized\noptimization algorithms that we ﬁnd notable but out-of-scope, as well as one type of\ndeterministic computational routine that is potentially useful for (but not required\nby) the drivers. We conclude by describing existing RandNLA libraries for least\nsquares and optimization in Section 3.5.\n3.1\nProblem classes\nThis section covers drivers for two related classes of optimization problems: minimizing regularized positive deﬁnite quadratics (§3.1.1) and certain generalizations\nof overdetermined and underdetermined least squares which we refer to as saddle\npoint problems (§3.1.2). Problems in both classes can naturally be transformed to\nequivalent linear algebra problems.1 Functionality for solving these problems can\neasily provide the foundation for managing the core linear algebra kernels of larger\noptimization algorithms.\nHow can we measure the accuracy of an approximate solution?\nThe problem of quantifying the error of an approximate solution to a least squares\nor saddle point problem is very important. While we would like to address this topic\nup-front, a proper discussion requires technical background that we only cover in\nSection 3.3. Furthermore, even given this background, there are various subtleties\nand special cases that would be laborious to describe here.\nTherefore we defer\nthe important topic of error metrics for least squares and related problems to Appendix B.2.\n1As we explain later, the saddle point problems are equivalent to so-called saddle point systems,\nwhich are well-studied in the NLA literature; see [BGL05; OA17]\nPage 40\narXiv Version 2\n\n3.1. Problem classes\nLeast Squares and Optimization\n3.1.1\nMinimizing regularized quadratics\nLet G be a positive semideﬁnite (psd) linear operator, and let µ be a positive\nregularization parameter. One of the main topics of this section is algorithms for\ncomputing approximate solutions to problems of the form\nmin\nx x∗(G + µI) x −2h∗x.\n(3.1)\nNote that solving (3.1) is equivalent to solving (G + µI) x = h. We refer to such\nproblems in diﬀerent contexts throughout this section. In some contexts, we say\nthat G is n × n, and in others, we say it is m × m.\nThis section covers algorithms for solving these problems to varying degrees of\naccuracy.\n• Methods for solving to higher accuracy will access G repeatedly by matrixmatrix and matrix-vector multiplication.\n• Methods for solving to lower accuracy may vary in how they access G: they\nmay only entail selecting a subset of its columns; or they may perform a single\nmatrix-matrix multiplication GS with a tall and thin matrix S.\nNote that the low accuracy methods may be useful in machine learning contexts\nsuch as kernel ridge regression (KRR), where an inaccurate solution to (3.1) can\nstill be useful for downstream computational tasks.\nRemark 3.1.1. If the linear operator G implements the action of an implicit Gram\nmatrix A∗A (with A known) then it would be preferable to reformulate (3.1) as\n(3.2), below, with b = 0 and c = h.\nAmenable problem structures\nThe suitability of methods we describe for problem (3.1) will depend on how many\neigenvalues of G are larger than µ. Supposing G is n × n, it is desirable that the\nnumber of such eigenvalues is much less than n.\nThe data (G, µ) that arise in\npractical KRR problems usually have this property.\nIn an ideal setting, the user would have an estimate for the number of eigenvalues\nof G that are larger than µ. This is not a strong requirement when G is accessible by\nrepeated matrix-vector multiplication, in which case the accuracy of the estimate\nis unimportant. The standard RandNLA algorithm in this situation can easily be\nmodiﬁed to recycle the work in solving (3.1) for one value of µ towards solving (3.1)\nfor another value of µ.\n3.1.2\nSolving least squares and basic saddle point problems\nWe are interested in certain generalizations of overdetermined and underdetermined\nleast squares problems. The generalizations facilitate natural speciﬁcation of linear\nterms in composite quadratic objectives, which is a common primitive in many\nsecond-order optimization algorithms.\nWe frame these problems as complementary formulations of a common saddle\npoint problem. The deﬁning data for such a problem consists of a tall m × n matrix\nA, an m-vector b, an n-vector c, and a scalar µ ≥0. For simplicity, our descriptions\nin this paragraph assume A is full-rank. The primal saddle point problem is\nmin\nx∈Rn ∥Ax −b∥2\n2 + µ∥x∥2\n2 + 2c∗x.\n(3.2)\narXiv Version 2\nPage 41\n\nLeast Squares and Optimization\n3.1. Problem classes\nWhen µ is positive, the dual saddle point problem is\nmin\ny∈Rm ∥A∗y −c∥2\n2 + µ∥y −b∥2\n2.\n(3.3)\nIn the limit as µ tends to zero, Eq. (3.3) canonically becomes\nmin\ny∈Rm{∥y −b∥2\n2 : A∗y = c}.\n(3.4)\nNote that the primal problem reduces to ridge regression when c is zero, and it\nreduces to overdetermined least squares when both c and µ are zero. When b is\nzero, and depending on the value of µ, the dual problem amounts to ridge regression\nwith a wide data matrix or to basic underdetermined least squares.\nPros and cons of this viewpoint\nAdopting this more general optimization-based viewpoint on least squares problems\nhas two major beneﬁts.\n• It extends least squares problems to include linear terms in the objective. The\nlinear term in the primal problem is obvious. The linear terms in (3.3) and\n(3.4) are obtained by expanding ∥y −b∥2\n2 = ∥y∥2\n2 −2b∗y + ∥b∥2\n2 and ignoring\nthe constant term ∥b∥2\n2.\n• It renders the primal and dual problems equivalent for most algorithmic purposes. The equivalence is based on formulating the optimality conditions for\nthese problems in a so-called saddle point system over the variables (x, y).\nSection 3.3.1 details this equivalence.\nIt must be noted that the saddle point problems we consider can be ill-posed when\nµ is zero and A is rank-deﬁcient. Speciﬁcally, when µ = 0 and c is not orthogonal\nto the kernel of A, the primal problem (3.2) has no optimal solution and the dual\nproblem (3.4) has no feasible solution. In this setting, we assign canonical solutions\nby considering the limit as µ tends to zero. Appendix B.3 addresses the existence\nand form of these limiting solutions. The outcome of the limiting analysis is that\nwhen µ = 0, we obtain canonical solutions\nx = (A∗A)†(A∗b −c)\nand\ny = (A∗)†c + (I −AA†)b,\n(3.5)\nwhich are related through the identity y = b −Ax.\nAmenable problem structures\nThis section focuses on methods for solving these optimization problems to high\naccuracy. Indeed, later in this section we make the novel observation that methods\nfor solving problems (3.2)–(3.4) to high accuracy can be used as the core subroutine\nin solving (3.1) to low accuracy at extremely large scales. If m ≫n, then these\nmethods are eﬃcient regardless of numerical aspects of the problem data (A, b, c, µ);\nproblems such as poor numerical conditioning will be relevant insofar as they contribute to ﬂoating-point rounding errors in these eﬃcient algorithms. If m is only\nslightly larger than n, then the methods we describe will only be eﬀective when\nG := A∗A and µ have the properties alluded to in Section 3.1.1. These properties\nare detailed later in this section.\nPage 42\narXiv Version 2\n\n3.2. Drivers\nLeast Squares and Optimization\n3.2\nDrivers\nHere we present four families of drivers for the problems described in Section 3.1.\nTwo of the driver families belong to a paradigm in the RandNLA literature known\nas sketch-and-precondition. Algorithms in these families are capable of computing\naccurate approximations of a problem’s true solution. The other two driver families\nbelong to a paradigm known as sketch-and-solve.\nThey are less expensive than\nsketch-and-precondition methods (to varying degrees) but they are only suitable\nfor producing rough approximations of a problem’s true solution. The sketch-andsolve drivers described in Section 3.2.4 are novel in that they rely on separate\nsketch-and-precondition methods for their core subroutines.\n3.2.1\nSketch-and-solve for overdetermined least squares\nSketch-and-solve is a broad paradigm within RandNLA, and algorithms based on\nit have been central to early developments in the area [Mah11; Woo14; DM16;\nDM21a]. Its most notable manifestations have been for overdetermined least squares\n[DMM06; Sar06; DMM+11; CW13], overdetermined ℓ1 and general ℓp regression\n[DDH+09; YMM16], and ridge regression [ACW17a; WGM18].\nWe focus here on least squares for concreteness. In this case, one samples a\nsketching operator S, and returns\n(SA)†(Sb) ∈arg min\nx\n∥S(Ax −b)∥2\n2\n(3.6)\nas a proxy for the solution to minx ∥Ax −b∥2\n2. The quality of this solution can be\nbounded with the concept of subspace embeddings from Section 2.2.2. In particular,\nif S is a subspace embedding for V = range([A, b]) with distortion δ, then\n∥A(SA)†(Sb) −b∥2 ≤\n\u00121 + δ\n1 −δ\n\u0013\n∥AA†b −b∥2.\n(3.7)\nNote that (3.6) is invariant under scaling of S. This implies that (3.7) also holds\nwhen δ is the eﬀective distortion of S for V ; see (2.3) and Appendix A.1.\nImplementation considerations and a viable application are given below.\nMethods for the sketched subproblem\nDirect methods for (3.6) require computing an orthogonal decomposition of SA,\nsuch as a QR decomposition or an SVD, in O(dn2) time. In this context, sketchand-solve can be used as a preprocessing step for sketch-and-precondition methods\nat essentially no added cost. Indeed, this preprocessing step was used in [RT08].\nTherefore if a direct method is being considered for sketch-and-solve, then sketchand-precondition methods should also be viable when m ∈O(dn).\nOne can in principle apply an iterative solver to the problem deﬁned by (SA, Sb).\nThis strategy avoids the cost of factoring SA, and it reduces the per iteration cost\nrelative to running the iterative solver on the original problem. This is typically\nimplemented without preconditioning (but see [YCR+18]), in which case it leaves\nthe dependence on the condition number of the original problem, and so it can only\nbe recommended for problems where the condition number is known to be small.\narXiv Version 2\nPage 43\n\nLeast Squares and Optimization\n3.2. Drivers\nError estimation\nSince sketch-and-solve algorithms for overdetermined least squares are most suitable\nfor computing rough approximations to a problem’s true solution, it is important\nto have methods for estimating this error. Such estimates can either be used to\ninform downstream processing of the approximate solution or to determine if a more\naccurate solution (computed by a more expensive algorithm) might be needed. It\nis especially important that these methods work well in regimes where sketch-andsolve has a compelling computational proﬁle, such as when m ≫dn. Appendix E.2\nprovides one such estimator based on the principle of bootstrapping from statistics.\nApplication to tensor decomposition\nThe beneﬁts of sketch-and-solve for least squares manifest most prominently when\nthe following conditions are satisﬁed simultaneously: (1) m is extremely large, so\nA is not stored explicitly, and (2) A supports relatively cheap access to individual\nrows A[i, :]. Among other places, this situation arises in alternating least squares\napproaches to tensor decomposition. We touch upon that topic in Section 7.3.1,\nparticularly in the remarks after (7.17).\n3.2.2\nSketch-and-precondition for least squares and saddle point\nproblems\nThe sketch-and-precondition approach to overdetermined least squares was introduced by Rokhlin and Tygert [RT08]. When the m × n matrix A is very tall, the\nmethod is capable of producing accurate solutions with less expense than direct\nmethods. It starts by computing a d × n sketch Ask = SA in the embedding regime\n(i.e., d ≳n). The sketch is decomposed by QR with column pivoting AskΠ = QR,\nwhich deﬁnes a preconditioner M = ΠR−1. If the parameters for the sketching operator distribution were chosen appropriately, then AM will be nearly-orthogonal with\nhigh probability.2 The near-orthogonality of AM ensures rapid convergence of an\niterative method for the least squares problem’s preconditioned normal equations.\nIf Tsk denotes the time complexity of computing SA, then the typical asymptotic\nFLOP count to solve to ϵ-error is\nO(Tsk + dn2 + mn log(1/ϵ))\n(3.8)\nImportantly, this complexity has no dependence on the condition number of A.\nThis approach was extended with stronger theoretical guarantees, support for\nmore general least squares problems, and high-performance implementations through\nBlendenpik [AMT10] and LSRN [MSM14]. It has also recently been used to solve\npositive deﬁnite systems arising in linear programming algorithms [CLA+20]. All\nof these methods produce preconditioners M where AM is nearly-orthogonal and\nare intended for the regime where A is very tall.\nWhat constitutes “very tall” depends on the algorithm’s implementation and\nthe hardware that runs it. It is easy to implement these algorithms in Matlab or\nPython so that, on a personal laptop, they are competitive with LAPACK’s direct\nmethods when m ≥50n ≥105; see also Section 3.5.\n2The condition number of AM and the eﬀective distortion of S for range(A) completely characterize one another; see Appendices A.1 and B.1.\nPage 44\narXiv Version 2\n\n3.2. Drivers\nLeast Squares and Optimization\nYour attention, please!\nIf a saddle point problem features regularization (i.e., if µ > 0) and if A has rapid\nspectral decay, then randomized methods can be used to ﬁnd a good preconditioner\nin far less than O(n3) time, no matter the speciﬁc value of m ≥n. This is possible\nby borrowing ideas from Nystr¨om preconditioning [FTU21], which we introduce in\nSection 3.2.3 for the related problem of minimizing regularized quadratics. As a\nnovel contribution, Section 3.3.3 explains how Nystr¨om preconditioning can naturally be adapted to saddle point problems. Therefore while the material here (in\nSection 3.2.2) focuses on the case m ≫n, one should be aware that this requirement\ncan be relaxed.\nAlgorithms\nSketch-and-precondition algorithms can take diﬀerent approaches to sketching, preconditioner generation, and choice of the eventual iterative solver.\n• Blendenpik used SRFT sketching operators, obtained its preconditioner by unpivoted QR of Ask, and used LSQR [PS82] as its underlying iterative method.\n• LSRN used Gaussian sketching operators, obtained its preconditioner through\nan SVD of Ask, and defaulted to the Chebyshev semi-iterative method [GV61]\nfor its iterative solver.\nThese two examples hint at the huge range of possibilities for the implementation\nof sketch-and-precondition algorithms. Indeed, we discuss preconditioners in detail\nover Sections 3.3.2 and 3.3.3, and we review a suite of possible deterministic iterative\nmethods in Section 3.3.4. For now, we give Algorithms 1 and 2 (below) as footholds\nfor understanding the various design considerations.\nFor simplicity’s sake, both of these algorithms use a black-box function\nz = iterative ls solver(F, g, ϵ, L, zo)\nwhich computes an approximate solution to minz ∥Fz −g∥2\n2. The exact semantics\nof this function are unimportant for our present purpose. Its general semantics are\nthat the solver initializes an iterative procedure at zo and that it runs until either\nan implementation-dependent error tolerance ϵ is met or an iteration limit L is\nreached. Typical implementations would measure error with a suitably normalized\nversion of the normal equation residual ∥F∗(Fz −g) ∥2. If κ denotes the condition\nnumber of F then typical convergence rates are such that error ∥F(z−F†g)∥2 decays\nmultiplicatively by a factor of (κ −1)/(κ + 1) with each iteration.\narXiv Version 2\nPage 45\n\nLeast Squares and Optimization\n3.2. Drivers\nBesides the use of a common iterative solver, both algorithms below initialize\nthe iterative solver at the solution from a sketch-and-solve approach in the vein of\nSection 3.2.1. The time needed to perform this presolve step is negligible, but it\nshould save several iterations when solving to a prescribed accuracy. It also plays an\nimportant role in handling overdetermined least squares problems when b is in the\nrange of A. In such contexts, the sketch-and-solve result actually solves the least\nsquares problem exactly provided that rank(SA) = rank(A); this stands in contrast\nto using a preconditioned iterative method initialized at the origin, which would not\nbe able to achieve relative error guarantees for ∥Ax −b∥against ∥(I −AA†)b∥= 0.\nAlgorithm 1 SPO1: a Blendenpik-like approach to overdetermined least squares\n1: function SPO1(A, b, ϵ, L)\nInputs:\nA is m × n and b is an m-vector. We require m ≥n and expect\nm ≫n. The iterative solver’s termination criteria are governed by ϵ\nand L: it stops if the solution reaches error ϵ ≥0 according to the\nsolver’s error metric, or if the solver completes L ≥1 iterations.\nOutput:\nAn approximate solution to (3.2), with c = 0 and µ = 0.\nAbstract subroutines and tuning parameters:\nSketchOpGen generates an oblivious sketching operator.\nsampling factor ≥1 is the size of the embedding dimension relative to n.\n2:\nd = min{⌈n · sampling factor⌉, m}\n3:\nS = SketchOpGen(d, m)\n4:\n[Ask, bsk] = S[A, b]\n5:\nQ, R = qr econ(Ask)\n6:\nzo = Q∗bsk\n# R−1zo solves minx{∥S(Ax −b)∥2\n2}\n7:\nAprecond = AR−1 # as a linear operator\n8:\nz = iterative ls solver(Aprecond, b, ϵ, L, zo)\n9:\nreturn R−1z\nWhile Algorithm 1 is standard, Algorithm 2 is somewhat novel. Using the same\ndata that might be computed during a standard sketch-and-precondition algorithm\nfor simple overdetermined least squares, it transforms any saddle point problem —\nprimal or dual — into an equivalent primal saddle point problem with c = 0. To\nour knowledge, no such conversion routines have been described in the literature.\nThe conversion is advantageous because it opens the possibility of using iterative\nsolvers with excellent numerical properties that are speciﬁc to least squares problems. The validity of the algorithm’s transformation is explained towards the end\nof Section 3.3.1.\nPage 46\narXiv Version 2\n\n3.2. Drivers\nLeast Squares and Optimization\nAlgorithm 2 SPS2 : sketch, transform a saddle point problem to least squares,\nand precondition. A more eﬃcient version of this algorithm can be obtained using\nour observations on SVD-based preconditioning in Section 3.3.2.\n1: function SPS2(A, b, c, µ, ϵ, L)\nInputs:\nA is m × n, b is an m-vector, c is an n-vector, and µ is a nonnegative\nregularization parameter.\nWe require m ≥n and expect m ≫n.\nThe iterative solver’s termination criteria are governed by ϵ and L: it\nstops if the solution reaches error ϵ ≥0 according to its internal error\nmetric, or if it completes L ≥1 iterations.\nOutput:\nApproximate solutions to (3.2) and its dual problem.\nAbstract subroutines and tuning parameters:\nSketchOpGen generates an oblivious sketching operator.\nsampling factor ≥1 is the size of the embedding dimension relative to n.\n2:\nd = min{⌈n · sampling factor⌉, m}\n3:\nS = SketchOpGen(d, m)\n4:\nif µ > 0 then\n5:\nS =\n\u0014S\n0\n0\nIn\n\u0015\n,\nA =\n\u0014 A\n√µIn\n\u0015\n,\nb =\n\u0014b\n0\n\u0015\n6:\nAsk = SA\n7:\nU, Σ, V∗= svd(Ask)\n8:\nM = VΣ†\n9:\nbmod = b\n10:\nif c ̸= 0 then\n11:\nˆv = UΣ†V∗c # ˆv solves minv{∥v∥2\n2 : A∗S∗v = c}\n12:\nbshift = S∗ˆv\n# A∗bshift = c\n13:\nbmod = bmod −bshift\n14:\nzo = U∗Sbmod\n# Mzo solves min{∥S (Ax −bmod) ∥2\n2}\n15:\nAprecond = AM\n# deﬁne implicitly, as a linear operator\n16:\nz = iterative ls solver(Aprecond, bmod, ϵ, L, zo)\n17:\nx = Mz\n18:\ny = b[: m] −A[: m, :]x\n19:\nreturn x, y\narXiv Version 2\nPage 47\n\nLeast Squares and Optimization\n3.2. Drivers\nWe wrap up our introduction to sketch-and-precondition algorithms by speaking\nto their tradeoﬀs with sketch-and-solve. It is easy to see that if m ≪n2 and we\nperform sketch-and-solve using a direct method for Eq. (3.6), then performing an\nadditional constant number of steps of sketch-and-precondition’s iterative phase\ndoes not increase the FLOP count by even so much as a constant factor. However,\nif we are in the regime where m ≥n2, then even a single step of an iterative\nmethod in sketch-and-precondition can cost as much as an entire sketch-and-solve\nalgorithm. Therefore when an accurate solution is not required and m ≥n2, it may\nbe preferable to use sketch-and-solve rather than sketch-and-precondition.\nApplications\nOne application of these algorithms is to carry out the core subroutine in iterative\nmethods for solving linear systems by block projection. We explain the nature of\nthis connection later on, in Section 5.2.2.\nTo explain the next application, we need some context. Classical linear algebra\ntechniques to solve a KRR problem with m datapoints require O(m2) storage and\nO(m3) time. Rahimi and Recht’s random feature maps provide a framework for\nreplacing such a KRR problem with a more tractable ridge regression problem\n[RR07].\nA data matrix in a random features ridge regression problem is m × n\n(for a tuning parameter n < m) and is characterized by the KRR datapoints and\nfunctions f1, . . . , fn drawn from a suitable random distribution. The ith row in this\nmatrix is obtained by evaluating f1, . . . , fn on the ith KRR datapoint.\nThe randomness in random features ridge regression is not “sketching” in the\nsense meant by this monograph. Still, this approach is notable in our context because it provides a source of models that are amenable to the methodology described\nabove. The Nystr¨om preconditioning methodology (see Sections 3.2.3 and 3.3.3) has\nbeen reported to be especially eﬀective for such problems when n ≲m [FTU21].\nWhen is sketch-and-precondition asymptotically faster than QR?\nHere we detail the runtime of sketch-and-precondition algorithms under the assumption of sketching with SRFTs. These sketching operators were used in the original\nsketch-and-precondition paper [RT08] and subsequently by [AMT10]. We focus on\nthem here because they have no tuning parameters besides their embedding dimension. Minimizing the number of tuning parameters helps us make comparisons to\ndirect solvers based on QR decomposition that run in time O(mn2).\nRecall from Section 2.5 that it takes O(mn log d) time to apply a d × m SRFT\nto an m × n matrix. We can plug Tsk = mn log d into (3.8) to see that the “typical”\nruntime for sketch-and-precondition with an SRFT is\nO(mn log d + dn2 + mn log(1/ϵ)).\n(3.9)\nThis runtime is only “typical” because it does not address subtleties stemming from\nrandomness. In the algorithm’s true runtime, there is a random multiplicative factor\nF on the mn log(1/ϵ) term in (3.9). The distribution of F depends on (d, n) in a\ncomplicated way. In formal algorithm analysis, one describes how to choose d to\nupper-bound the probability that F exceeds some universal constant C. Then one\ncan say that (3.9) does describe the algorithm’s true runtime with some probability.\nThe convention in the ﬁeld is to describe how to choose d so the probability that\nF ≤C tends to one as problem size increases.\nPage 48\narXiv Version 2\n\n3.2. Drivers\nLeast Squares and Optimization\n[RT08] observed that taking d = sn for small constants s (e.g., s = 4) sufﬁced for (3.9) to accurately describe algorithm runtime in practice. However, the\ntheoretical analysis in [RT08] needed to take d ∈Ω(n2) to bound F with high probability. Therefore the best theoretical runtime guarantee for sketch-and-precondition\nwas originally obtained by plugging d = n2 into (3.9). The theoretical guarantees\nimproved following developments in the analysis of SRFTs. Speciﬁcally, [AMT10]\nobserved that a transparent application of a result by [NDT09] could be used to\nprove that d ∈Ω(n log n) suﬃced to bound F with high probability. Therefore\none can plug d = n log n into (3.9) to obtain a bound for algorithm runtime in\nterms of (m, n, ϵ) that holds with high probability. This is the appropriate bound\nto use when comparing the theoretical asymptotic runtime of SRFT-based sketchand-precondition to other algorithms. However, in practice, it is still preferred to\nuse d = sn for some small s > 1, since the resulting preconditioned matrices tend\nto be extremely well-conditioned.\n3.2.3\nNystr¨om PCG for minimizing regularized quadratics\nNystr¨om preconditioned conjugate gradient (Nystr¨om PCG) is a recently-proposed\nmethod for solving problems of the form (3.1) to fairly high accuracy [FTU21].\nWe describe it as a method to compute approximate solutions to linear systems\n(G + µI)x = h where G is n × n and psd.\nThe randomness in Nystr¨om PCG is encapsulated in an initial phase where it\ncomputes a low-rank approximation of G by a so-called “Nystr¨om approximation.”\nWe defer discussion on such approximations (including the potentially-confusing\nnaming convention) to Section 4.2.2.\nFor our purposes, what matters is that a\nrank-ℓNystr¨om approximation leads to a preconditioner P which can be stored in\nO(ℓn) space and applied in O(ℓn) time.\nNow let κ denote the condition number of Gp := P−1/2(G + µI)P−1/2. It is\nwell-known that each iteration of PCG requires one matrix-vector multiply with\nG, one matrix-vector multiply with P−1, and reduces the error of the candidate\nsolution to (3.1) by a multiplicative factor (√κ−1)/(√κ+1). As we discuss below,\none can expect that κ will be O(1) if the ℓth-largest eigenvalue of G is smaller than\nµ.\nIndeed, Nystr¨om PCG is most eﬀective for problems when this threshold is\ncrossed at some ℓ≪n. As a practical matter, users will not need to select the\napproximation rank parameter ℓmanually in order to use Nystr¨om PCG; [FTU21,\nAlgorithm E.2] is a specialized adaptive method for Nystr¨om approximation that\ncan determine an appropriate value for ℓgiven (G, µ).\nDetails on the preconditioner\nWe presume access to a low-rank approximation\nˆG = V diag(λ)V∗\n(3.10)\nwhere V is a column-orthonormal n × ℓmatrix that approximates the dominant ℓ\neigenvectors of G and λ1 ≥· · · ≥λℓ> 0 are the approximated eigenvalues. The\ndata (V, λ, µ) is then used to deﬁne a preconditioner\nP−1 = V diag(λ + µ)−1V∗+ (µ + λℓ)−1(In −VV∗).\n(3.11)\narXiv Version 2\nPage 49\n\nLeast Squares and Optimization\n3.2. Drivers\nAlternatively, following [FTU21] to the letter, P−1 can be the result of multiplying\nthe expression above by (µ + λℓ). Under this latter convention, P−1 acts as the\nidentity on range(V)⊥.\nWhile the form of this preconditioner may appear mysterious, its appropriateness\ncan be seen by considering a simple idealized setting. To make a precise statement\non this topic we adopt notation where λi(G) is the ith-largest eigenvalue of G.\nAssuming that (V, λ) are very good estimates for the top ℓeigenpairs of G and that\nλℓ(G) ≈λℓ+1(G), the condition number of Gp should be near\nκℓ(G, µ) := (λℓ(G) + µ)/(λn(G) + µ).\nTaking this for granted, the preconditioner (3.11) can only be eﬀective if ℓ≪n\nis large enough so that κℓ(G, µ) is bounded by a small constant. Using the fact\nthat κℓ(G, u) ≤1 + λℓ(G)/µ, we can simplify the criteria and say that a good\npreconditioner is possible when λℓ(G)/µ is O(1).\nRemark 3.2.1. The argument above can be made more rigorous by assuming that V\nis an n × (ℓ−1) matrix that contains the exact leading ℓ−1 eigenvectors of G, and\nthat λ1, . . . , λℓare the exact leading ℓeigenvalues of G. In this case, the condition\nnumber of Gp will be equal to κℓ(G, µ), which will be at most 1 + λℓ/µ.\n3.2.4\nSketch-and-solve for minimizing regularized quadratics\nRandomization oﬀers several avenues for solving problems of the form (3.1) to modest accuracy. We describe two possible methods here through novel interpretations\nof existing work on KRR. Our descriptions of the methods keep the focus on linear\nalgebra, and we refer the reader to Appendix B.4.1 for information on the KRR\nformalism. We note that our formulations of these methods are novel in how they\napply sketch-and-precondition as the core subroutine in what is otherwise a sketchand-solve style driver. Such “nested randomization” is a relatively under-explored\nand potentially powerful algorithm design paradigm.\nFor notation, we shall say that G is m × m, that µ = mλ for some λ > 0, and\nthat the optimization variable in (3.1) is denoted by “α” rather than “x.”\nA one-shot fallback on Nystr¨om approximations\nRather than solving (3.1) directly, it has been suggested that one solve\n(AA∗+ mλI) ˆα = h,\nwhere AA∗is a Nystr¨om approximation of G [AM15]. The computation of A only\nrequires access to G by a single sketch GS for a tall m×n sketching operator S. In the\nKRR context, it is especially popular for S to be a column sampling operator [WS00;\nKMT09b; GM16]. Section 6.1.3 discusses how such column-selection sketches GS\ncan be computed adaptively using the concept of ridge leverage scores. Regardless\nof how the approximation is obtained, there is an equivalence between computing\nˆα and solving a dual saddle point problem with matrix A and other data (b, c, µ) =\n(h, 0, mλ). That dual saddle point problem can naturally be approached by sketchand-precondition methods from Section 3.2.2. The preconditioner generation steps\nin this context are subtle and addressed in Appendix B.4.2.\nPage 50\narXiv Version 2\n\n3.3. Computational routines\nLeast Squares and Optimization\nApplying a random subspace constraint\nBy taking the gradient of the objective function in (3.1) and multiplying the gradient\nby the positive deﬁnite matrix G, we can recast (3.1) as minimizing\nQ(α) = α∗(G2 + mλG)α −2h∗Gα.\nIn [YPW17], a sketch-and-solve approach to the problem of minimizing this loss\nfunction is proposed. Speciﬁcally, one minimizes Q(α) subject to a constraint that\nα is in the range of a very tall m × n sketching operator S.\nThe constrained\nminimization problem is equivalent to minimizing z 7→Q(Sz) over n-vectors z.\nThis in turn is equivalent to solving a highly overdetermined least squares problem,\nwith an (m + n) × n data matrix A = [GS;\n√\nmλR] where R is any matrix for\nwhich R∗R = S∗GS. This problem can clearly be handled by our methods from\nSection 3.2.2.\nRemark 3.2.2. We note that [YPW17] presumes access to the sketches h∗GS, S∗GS,\nand S∗G2S, and advocates for solving the resulting n-dimensional minimization\nproblem by a direct method in O(n3) time. However, no guidance is given on how\nto compute the sketch S∗G2S. From what we can tell, the most eﬃcient way of\ndoing this would be to form the Gram matrix at cost O(mn2) assuming access to\nthe sketch GS. (Our usage of (m, n) is swapped relative to [YPW17].)\n3.3\nComputational routines\nTo contextualize the computational routines that follow, we begin in Section 3.3.1\nwith a brief discussion of optimality conditions for saddle point problems. From\nthere, we present in Sections 3.3.2 and 3.3.3 two families of methods for generating\npreconditioners needed by saddle point drivers; our presentation of both families\nincludes novel observations that lead to improved eﬃciency and numerical stability.\nThen in Section 3.3.4 we discuss deterministic preconditioned iterative methods\nfor positive deﬁnite systems and saddle point problems. Such iterative methods are\napplicable to all drivers from the previous section (although less so for Section 3.2.1).\nRoutines not detailed here\nThe driver from Section 3.2.3 requires methods to compute Nystr¨om approximations, which are described in Section 4. In addition, the drivers from Section 3.2.4\nwould beneﬁt from specialized data-aware methods for sketching kernel matrices,\nwhich are discussed in Section 6. We also note that this section does not describe\ncomputational routines for sketch-and-solve type drivers.\nThis is because those\ndrivers are extraordinarily simple to implement and there is no need to isolate their\nbuilding blocks into separate computational routines.\n3.3.1\nTechnical background: optimality conditions for saddle point\nproblems\nHere, we give a handful of characterizations of optimal solutions for saddle point\nproblems. Let us begin by calling an n-vector x primal-optimal if it solves (3.2).\nAnalogously, an m-vector y shall be called dual-optimal if it solves (3.3) when µ is\npositive or (3.4) when µ is zero.\narXiv Version 2\nPage 51\n\nLeast Squares and Optimization\n3.3. Computational routines\nPrimal-dual optimal solutions can be characterized with saddle point systems.\nThese are a class of 2 × 2 block linear systems that arise broadly in computational\nmathematics and especially in optimization. General introductions to these systems\ncan be found in the survey [BGL05] and the book [OA17]. We are interested in\nsaddle point systems of the form\n\u0014 I\nA\nA∗\n−µI\n\u0015 \u0014y\nx\n\u0015\n=\n\u0014b\nc\n\u0015\n.\n(3.12)\nA solution to such a system always exists when µ is positive or when the tall matrix\nA is full-rank. Given that assumption, it can be shown that a point ˜x is primaloptimal if and only if there is a ˜y for which (˜x, ˜y) solve (3.12). Similarly, a point ˜y\nis dual-optimal if and only if there is an ˜x for which (˜x, ˜y) solve (3.12).\nSaddle point systems are often reformulated into equivalent positive semideﬁnite\nsystems. The reformulation takes the system’s upper block to deﬁne y = b −Ax,\nand then substitutes that expression into the system’s lower block. This gives us\nthe normal equations\n(A∗A + µI)x = A∗b −c.\n(3.13)\nTherefore one can solve (3.12) by ﬁrst solving (3.13) and then setting y = b −Ax.\nSuch an approach to underdetermined least squares is suggested by Bj¨orck in his\nbooks [Bj¨o96; Bj¨o15].\nThinking in terms of the normal equations helps with the design of preconditioners. When accurate solutions are desired, however, it is preferable to employ reformulations that reduce the need for matrix-vector products with the linear\noperator A∗A. Such reformulations start by deﬁning an augmented data matrix\nAµ = [A; √µIn]. For dual saddle point problems, one solves\nmin{ ∥∆y∥2\n2 : ∆y ∈Rm+n, (Aµ)∗∆y = c −A∗b},\n(3.14)\nand subsequently recovers the dual-optimal solution y = [b1 + ∆y1; . . . ; bm + ∆ym].\nFor primal saddle point problems, one computes some bshift ∈Rm+n satisfying\n(Aµ)∗bshift = c and then deﬁnes bµ = [b; 0n] −bshift. Any solution to the resulting\nproblem\nmin\nx∈Rn\n\b\n∥Aµx −bµ∥2\n2\n\n(3.15)\nis primal-optimal. Of course, this reformulation is only useful if we have a cheap way\nto compute bshift. As it happens, however, randomized methods for preconditioner\ngeneration provide methods to compute a near-minimum-norm solution to A∗u = c\nin O(mn) extra time compared to when c = 0. We illustrated this process earlier\nwith an SVD-based preconditioner in Algorithm 2.\nInconsistent saddle point systems\nSuppose that µ is zero, so as to allow for the possibility that (3.12) is consistent.\nUnder this assumption, (3.12) is inconsistent if and only if c is not in the range\nof A∗. When framed in this way, we have that (3.12) is inconsistent if and only if\n(3.4) has no feasible solution. What’s more, since c ̸∈range(A∗) is equivalent to\nc ̸∈ker(A)⊥, we see that inconsistency of (3.12) is equivalent to (3.2) having no\noptimal solution. Therefore a saddle point system is consistent if and only if its\nassociated saddle point problems are well-posed; for ill-posed problems, recall that\nwe canonically assign solutions per (3.5).\nPage 52\narXiv Version 2\n\n3.3. Computational routines\nLeast Squares and Optimization\n3.3.2\nPreconditioning least squares and saddle point problems:\ntall data matrices\nThere is a simple unifying framework for preconditioner generation of the kind used\nin [RT08; AMT10; MSM14]. The framework is applicable to any least squares or\nsaddle point problem (3.2)–(3.4) in the regime m ≫n. We describe its general form\nbelow and then turn to its concrete instantiations.\nSketch and orthogonalize\nTo describe our framework, begin by deﬁning a sketch Ask = SA where the sketching\noperator S has d ≳n rows. We also deﬁne the augmented matrices\nAµ =\n\u0014 A\n√µI\n\u0015\nand\nAsk\nµ =\n\u0014\nAsk\n√µI\n\u0015\n.\nThese augmented matrices are only used as a formalism. They reﬂect the inﬂuence of the normal equations (3.13) on preconditioner design. We emphasize that\nwe speciﬁcally allow for µ = 0 and one need not form these augmented matrices\nexplicitly in memory.\nNext, we introduce two key terms.\nWe say that a matrix M orthogonalizes Ask\nµ if the columns of Ask\nµ M are\nan orthonormal basis for the range of Ask\nµ . Such a matrix is called a\nvalid preconditioner for Aµ if, in addition, rank(Ask\nµ ) = rank(Aµ).\nWe note that the rank requirement of a valid preconditioner is nearly universal\nin practice. For example, it holds with probability one for uniform and Gaussian\noperators (§2.3). We conjecture that it holds with exponentially-high probability\nfor suitable SASOs (§2.4.1) and for SRFTs (§2.5).\nHow good are these preconditioners?\nIn our context, M is a good preconditioner if\nthe spectrum of AµM can be divided into a small number of tightly clustered groups.\nGiven the tools at our disposal in RandNLA, we mostly aim for the spectrum of\nthis matrix to be tightly clustered into a single group, i.e., for its condition number\nto be small. In this regard, we can provide the following principle.\nIf M is a valid preconditioner for Aµ, then the condition number of AµM\ndoes not depend that of Aµ.\nThis principle can be formalized with the following proposition, which we state\nwithout regularization for the sake of clarity.\nProposition 3.3.1. Let U be a matrix whose columns form an orthonormal basis\nfor the range of A. If M is a valid preconditioner for A, then the spectrum of AM\nis equal to that of (SU)†.\n[RT08, Theorem 1] gives a very similar statement under the assumption that A is\nfull-rank. [MSM14, Lemma 4.2] improved upon [RT08] by supporting the rankdeﬁcient case, at the price of strong assumptions on the sketching operator and the\nform of the preconditioner. In Appendix B.1 we provide what to our knowledge is\nthe ﬁrst proof of Proposition 3.3.1 in its general form; we also explain its application\nto regularized problems.\narXiv Version 2\nPage 53\n\nLeast Squares and Optimization\n3.3. Computational routines\nUp next.\nWe now turn to how one can compute orthogonalizers. To keep things at\na reasonable length we only speak to QR-based and SVD-based methods, although\nothers could also be used.\nOur goal is to give a general overview that includes\ntime and space complexity considerations. As to the latter consideration, we must\nnote that these preconditioners have insubstantial space requirements when A is\ndense and m ≫d ≳n.\nSeparately, we note that details of the preconditioner\ngeneration process can aﬀect the sketch-and-solve preprocessing step in sketch-andprecondition algorithms. For more information on theoretical properties of these\npreconditioners in a RandNLA context, we refer the reader to [CFS21].\nQR-based preconditioning in the full-rank case\nQR-based preconditioning when µ = 0 is very simple; one need only run Householder\nQR on Ask and return M = R−1 as a linear operator. We note that specialized\nmethods for QR decomposition of very tall matrices would not be appropriate here,\nsince the d × n matrix Ask will have d ≳n. Householder-type representations of\nAsk’s QR decomposition are especially useful since they require a modest amount\nof added workspace on top of storing Ask.\nThe case with µ > 0 is more complicated if we want to avoid forming Ask\nµ\nexplicitly. To describe it, suppose we have an initial QR decomposition Ask = QoRo.\nIt is easy to show that the factor R from a QR decomposition of Ask\nµ is the same as\nthe triangular factor from a QR decomposition of ˆR := [Ro; √µI]. This observation\nis useful because there are specialized algorithms for QR decomposition of matrices\ngiven by an implicit vertical concatenation of a triangular matrix and a diagonal\nmatrix; these specialized algorithms only require O(n) additional workspace. The\nfactor Q from a QR decomposition of Ask\nµ can also be recovered with this approach,\nalthough the representation would be somewhat complicated.\nIf A is not too ill-conditioned then the same preconditioner can be obtained by\nCholesky-decomposing the regularized Gram matrix\n(Ask\nµ )∗(Ask\nµ ) = (Ask)∗(Ask) + µI,\nsince the upper-triangular Cholesky factor of that matrix is the same as the factor\nR from the QR decomposition of Ask\nµ . This approach is simple to implement, and\nits time and space requirements are unaﬀected by whether or not µ is zero.\nA\nsophisticated implementation could even try to form the regularized Gram matrix\nwithout allocating dn space for Ask as an intermediate quantity. Although, it is\nclear that unless such a sophisticated implementation is used, there is no material\nmemory savings compared to the Q-less QR approach described above. This approach also aﬀects sketch-and-solve preprocessing by requiring that we solve the\nnormal equations, which is not a numerically stable approach [Bj¨o96].\nQR-based preconditioning in the rank-deﬁcient case\nSuppose for ease of exposition that µ = 0 and let k = rank(Ask) ≲n. One can use\na variety of methods to compute preconditioners that are morally triangular in the\nsense that they are of the form M = PR−1 for an n × k partial-permutation matrix\nP and a triangular matrix R. As long as the preconditioner orthogonalizes Ask,\nwe can postprocess zsol = argmin ∥AMz −b∥2\n2 to obtain xsol = Mz which solves\nmin ∥Ax −b∥2\n2.\nPage 54\narXiv Version 2\n\n3.3. Computational routines\nLeast Squares and Optimization\nThe subtlety here is that when k < n there is a nontrivial aﬃne subspace of\noptimal solutions to min ∥Ax −b∥2\n2. Our stated goal in the rank-deﬁcient case is\nto ﬁnd the minimum-norm solution to the least squares problem (see Eq. (3.5)).\nUnfortunately, if we assume that b has no role in deﬁning M, then it is clearly\nimpossible to guarantee that the norm of the recovered solution is anywhere near\nthe minimum possible among all minimizers of ∥Ax −b∥2\n2.\nSVD-based preconditioners\nLet us denote the SVD of Ask by U diag(σ)V∗.\nFirst we consider preconditioner generation when µ = 0. In this case we must\naccount for the fact that Ask might be rank-deﬁcient. Letting k denote the rank of\nAsk, the SVD-based preconditioner is the n × k matrix\nM =\n\u0014 v1\nσ1\n, . . . , vk\nσk\n\u0015\n.\nThis construction is important, because it can be shown that if z⋆solves\nmin\nz ∥AMz −b∥2\n2 + c∗Mz\n(3.16)\nthen x = Mz⋆satisﬁes (3.5). We note in particular that (3.16) has a unique optimal\nsolution and so computing z⋆is a well-posed problem.\nSVD-based preconditioning is conceptually simpler when µ is positive, since\nin that case it does not matter if Ask is rank-deﬁcient. However, it is harder to\neﬃciently implement compared to when µ = 0. Here we present an eﬃcient construction based on the relationship between the SVD of a matrix and the eigendecomposition of its Gram matrix. Speciﬁcally, recall that the right singular vectors\nof a matrix F are the eigenvectors of F∗F, and that the singular values of F are the\nsquare roots of the eigenvalues of F∗F.\nWhen used in our context this fact implies that the right singular vectors of Ask\nµ\nare equal to those of Ask, and that its singular values are\nˆσi =\nq\nσ2\ni + µ.\nThese observations alone are suﬃcient to recover the preconditioner\nM = V diag\n\u0012 1\nˆσ1\n, . . . , 1\nˆσn\n\u0013\nwhich orthogonalizes Ask\nµ .\nAs a ﬁnal point we consider the problem of recovering the left singular vectors\nof Ask\nµ given the SVD of Ask. This is useful in settings such as Algorithm 2 for\npresolve and problem transformation purposes. Moreover, it can actually be done\neﬃciently. If we deﬁne\nD1 = diag\n\u0012σ1\nˆσ1\n, . . . , σn\nˆσn\n\u0013\nand\nD2 = diag\n\u0012√µ\nˆσ1\n, . . . ,\n√µ\nˆσn\n\u0013\nthen by assumption on M the left singular vectors of Ask\nµ are given by\n\u0014\nAsk\n√µI\n\u0015\nM =\n\u0014\nAskM\n√µM\n\u0015\n=\n\u0014UD1\nVD2\n\u0015\n.\narXiv Version 2\nPage 55\n\nLeast Squares and Optimization\n3.3. Computational routines\nWe note that the column-orthonormality of this matrix can easily be veriﬁed by its\nrightmost representation.\nTo recap, we have introduced three key beneﬁts of SVD-based preconditioning\nfor tall least squares and saddle point problems. First, it can be used to ﬁnd the\nminimum norm solutions in (3.5) in the rank-deﬁcient case. Second, an SVD-based\npreconditioner can be computed in the presence of regularization given only the\nsingular values and right singular vectors of Ask. Third, the SVD of Ask is suﬃcient\nto recover the SVD of Ask\nµ , which facilitates sketch-and-solve as a preprocessing step\nin sketch-and-precondition.\nRemark 3.3.2 (Computational complexity). The default algorithm for SVD is currently divide-and-conquer [GE95]. Two somewhat-outdated algorithms for computing the SVD are described in [GV13, §8.6]; [GV13, Figure 8.6.1] gives complexity\nestimates for these algorithms depending on whether the left singular vectors need\nto be computed.\n3.3.3\nPreconditioning least squares and saddle point problems:\ndata matrices with fast spectral decay\nInterpreting the Nystr¨om preconditioner.\nRecall from Section 3.2.3 that the Nystr¨om\npreconditioning approach to solving (G + µI)x = h starts by constructing a lowrank approximation of G. That approximation deﬁnes a preconditioner P satisfying\nthree properties:\n1. P is positive deﬁnite.\n2. (G + µI)P−1 is well-conditioned on a subspace L that contains G’s dominant\neigenspaces.\n3. P acts as the identity on L⊥(the orthogonal complement of L).\nSuch a preconditioner will be eﬀective when the action of G on L⊥is “not too\npronounced” compared to that of µI. More formally, if we deﬁne the restricted\nspectral norm of G on L⊥\n∥G∥L⊥= max{∥Gz∥2 : z ∈L⊥, ∥z∥2 = 1}\nthen the preconditioner will be eﬀective if ∥G∥L⊥/µ is O(1).\nAdaptation to saddle point problems.\nNystr¨om preconditioners can naively be used\nfor regularized saddle point problems by taking G = A∗A and considering the\nnormal equations (3.13). However, the numerical properties of iterative least squares\nsolvers that only access A∗A tend to be less robust than those of iterative solvers\nthat access A and A∗separately (i.e., solvers such as LSQR). This motivates having\nan extension of the Nystr¨om preconditioner to be compatible with the latter type\nof solver.\nTowards this end, let us express P−1 with a (possibly non-symmetric) matrix\nsquare-root M, satisfying the relation P−1 = MM∗. We appeal to the well-known\nfact that running PCG on (G + µI)z = h with preconditioner P is equivalent to\nrunning the “unpreconditioned” CG algorithm on\nM∗(G + µI)Mz = M∗h.\nPage 56\narXiv Version 2\n\n3.3. Computational routines\nLeast Squares and Optimization\nWhen framed in this way, we can ask how M should relate to A and µ so that it\nwould be a good preconditioner if it were used on the normal equations.\nTo answer this question we work with the augmented matrix Aµ = [A; √µIn].\nThe basic criteria for a P as a Nystr¨om preconditioner for the normal equations\n(3.13) can be stated with M as follows:\n1. AµM should be well-conditioned on a subspace L that includes the dominant\nright singular vectors of Aµ.\n2. we should have AµMx = Aµx for all x ∈L⊥.\nWhether such a preconditioner will be eﬀective can be stated with the “restricted\nspectral norm” as deﬁned above. Speciﬁcally, M should be eﬀective if the above\nconditions hold and ∥A∥L⊥/√µ is O(1). We note that the requisite matrix M can\nbe constructed eﬃciently by similar principles as methods for low-rank SVD in\nSection 4, and we leave the details to future work.\n3.3.4\nDeterministic preconditioned iterative solvers\nMost of the drivers described in Section 3.2 amount to using randomization to obtain\na preconditioner and then calling a traditional iterative solver that can make use\nof that randomized preconditioner. Here we list some iterative solvers that could\nbe of use for these drivers. We note up front that many factors can aﬀect the ideal\nchoice of iterative method in a given setting.\n• CG [HS52] is the most broadly applicable solver in our context. It applies to\nthe regularized positive deﬁnite system (3.1) and the normal equations of the\nprimal saddle point problem (3.13).\n• CGLS [HS52] applies when c is zero. It is equivalent to CG on the normal\nequations in exact arithmetic, but is more stable than CG in ﬁnite-precision\narithmetic.\n• LSQR [PS82] applies when at least one of c or b is zero. When considered for\noverdetermined problems it is algebraically (but not numerically) equivalent\nto CGLS. It is more stable than CG [Bj¨o96, § 7.6.3], [Bj¨o15, § 4.5.4] and\nCGLS [PS82, § 9] for ill-conditioned problems.\n• CS (the Chebyshev semi-iterative method) [GV61] applies to the same class\nof systems as CG. It has fewer synchronization points in each iteration and so\ncan take better advantage of parallelism. It requires knowledge of an upper\nbound and a lower bound on the eigenvalues of the system matrix. We refer\nthe reader to [Bj¨o96, § 7.2.5], [Bj¨o15, § 4.1.7] for information on this method.\n• LSMR [FS11] applies to the same problems as LSQR. For overdetermined\nleast squares it is algebraically equivalent to MINRES on the normal equations. In that context, the residual of the normal equations will decrease with\neach iteration, which makes it safer to stop early compared to LSQR.\nThese algorithms vary in how they accommodate preconditioners. Some require\nimplicitly preconditioning the problem data, calling the “unpreconditioned” solver,\nthen applying some (cheap) postprocessing to the returned solution. We note that\nit is necessary to “precondition” any regularization term in the problem’s objective\narXiv Version 2\nPage 57\n\nLeast Squares and Optimization\n3.4. Other optimization functionality\nwhen using such an algorithm.3 For other algorithms, a preconditioner is supplied\nalongside the problem data, and the algorithm returns a solution that requires no\npostprocessing.\nThe diﬀerence between these situations is that diﬀerent quantities end up being available for use in termination criteria (at least for oﬀ-the-shelf\nimplementations). We emphasize that appropriate choices of termination criteria\ncan be crucial for iterative solvers to work eﬀectively, and we refer the reader to\nAppendix B.2 for discussion on this and other topics.\nAny standard library implementing the drivers from Section 3 should include\ncomputational methods for (preconditioned) CG and LSQR. LSQR is most naturally applied to dual saddle point problems by reformulation to (3.14) and to primal\nsaddle point problems by reformulation to (3.15). More full-featured RandNLA libraries would do well to include implementations of CS or LSMR, and “blocked”\nversions of iterative solvers. Such blocked methods apply to linear systems and least\nsquares problems with multiple right-hand sides; they take better advantage of parallel hardware and have slightly faster convergence rates than their non-blocked\ncounterparts.\n3.4\nOther optimization functionality\nHere, we brieﬂy discuss other RandNLA algorithms of note for least squares or\noptimization, often commenting on how they ﬁt into our plans for RandLAPACK.\nSome of these algorithms are out-of-scope for a linear algebra library but can be\ndirectly facilitated by the drivers we described in Section 3.\nFacilitating second-order optimization algorithms\nMany second-order optimization algorithms need to solve sequences of saddle point\nsystems, where A, b, c vary continuously from one iteration to the next. RandLAPACK will support such use-cases indirectly through methods that help amortize\nthe dominant computational cost of a single randomized algorithm across multiple saddle point solves. See [PW17; RM19] for uses of RandNLA for second-order\noptimization.\nThe most common way for A to vary is by a reweighting: when A = WAo for a\nﬁxed matrix Ao and an iteration-dependent matrix W. The matrix W is typically\n(but not universally) a matrix square root of the Hessian of some separable convex\nfunction. The randomized algorithms described in this section will only be useful for\nsuch problems when W and its adjoint can be applied to m-vectors in O(m) time.\nThis condition is satisﬁed in limited but important situations such as in algorithms\nfor logistic regression, linear programming, and iteratively-reweighted least squares.\nStochastic Newton and subsampled Newton methods\nNewton Sketch is a prototype algorithm developed over two papers [PW16; PW17]\nwhich is closely related to subsampled Newton methods [XRM17; YXR+18; RM19].\nEach is suited to optimization problems that feature non-quadratic objective functions or problems with constraints other than linear equations.\nThese methods\nentail sampling a new sketching operator (and applying it to a new data matrix) in\n3That is, if we precondition a ridge regression problem, then it is necessary to precondition the\naugmented matrix [A; √µI] in an unregularized version of the problem.\nPage 58\narXiv Version 2\n\n3.4. Other optimization functionality\nLeast Squares and Optimization\neach iteration, with the aim of approximating the Hessian of the objective at the\ngiven iterate. The algorithms described in this section can easily serve as the main\nsubroutine in Newton Sketch and subsampled Newton methods.\nNewton Sketch has a natural specialization for least squares which entails sampling and applying only one sketching operator. This specialization can be viewed\nas sketch-and-precondition, where the iterative method for solving the saddle point\nsystem is based on preconditioned steepest-descent. The asymptotic convergence\nof this approach can be established in various ways [OPA19; LP19; Tro20]. It has\nbeen shown that “traditional” sketch-and-precondition methods (based on CG or\nthe Chebyshev semi-iterative method) exhibit faster convergence [LP19]. Therefore\nwe do not expect to incorporate this method into RandLAPACK.\nThere are two recently proposed extensions of Newton Sketch that may be suitable for solving the saddle point problems described in Section 3.1.2: Hessian averaging [NDM22] and stochastic variance reduced Newton (SVRN) [Der22b]. The\nperformance proﬁles of these methods are better when A is very tall. When specialized to least squares, the former method amounts to preconditioned steepest-descent\nwhere the preconditioner is updated at each iteration. By comparison (again in the\nleast squares setting), SVRN amounts to steepest-descent with a ﬁxed preconditioner that incorporates variance-reduced sketching methods (adapted from [JZ13])\nto approximate the gradient at each iteration.\nRandom features preconditioning for KRR\nA random-features approach for computing accurate solutions to problems of the\nform (3.1) in the context of KRR is proposed in [ACW17b]. Speciﬁcally, [ACW17b]\nadvocates for using random features to obtain a preconditioner for use in an iterative method such as PCG. Since any such iterative solver requires access to G by\nmatrix-vector multiplication, Nystr¨om PCG can be applied to the same problems\nas this random-features preconditioning. Empirical results strongly suggest that the\nNystr¨om approach has better performance than random-features preconditioning\non shared-memory machines [FTU21]. Thus, we do not plan for RandLAPACK to\nsupport random-features preconditioning at this time.\nUtilities for iterative reﬁnement\nIterative reﬁnement can be used as a tool to compensate for rounding errors in\notherwise reliable linear system solvers. These methods typically work by computing\nresiduals to higher precision than that used by the solver, running the solver with the\nupdated residual, and then adding the new solution to the original solution [Bj¨o96,\n§2.9.2].4 LAPACK has some procedures of this kind. See [Hig97] and [DHK+06] for\ntheoretical and practical analyses.\nThe best way to use iterative reﬁnement routines to support randomized algorithms in this section is yet to be determined. On the one hand, it may suﬃce to include methods similar to those in LAPACK. However, sketch-and-precondition algorithms might pose unique numerical problems that require diﬀerent techniques. See\n[AMT10, §5.7] for some discussion of numerical issues in the sketch-and-precondition\ncontext. It might also be natural for a RandNLA library to have more iterative re4In some situations, it can suﬃce to recompute the residual with the same precision used by\nthe underlying solver [Bj¨o96, §2.9.3].\narXiv Version 2\nPage 59\n\nLeast Squares and Optimization\n3.5. Existing libraries\nﬁnement methods than LAPACK in order to better exploit low-precision arithmetic\nand accelerators.\n3.5\nExisting libraries\nWe know of four high-performance libraries with sketch-and-precondition methods\nfor least squares: Blendenpik [AMT10], LSRN [MSM14], LibSkylark [KAI+15], and\nSki-LLS [CFS21].5 To our knowledge, LibSkylark is the only RandNLA library which\nsupports least squares and low-rank approximation (see Section 4.5). None of these\nlibraries support saddle point problems of the kind we consider, and none of them\nmake use of Nystr¨om preconditioning.\nBlendenpik.\nThis library is written in C and callable from Matlab; it is currently\navailable on the Matlab File Exchange. It uses LSQR as the deterministic iterative\nsolver, and obtains the preconditioner by running QR on a sketch Ask = SA, where\nS is an SRFT. Blendenpik also adaptively calls LAPACK if a problem is deemed too\npoorly scaled or if the iterative method performs poorly. It was shown to outperform\nan unspeciﬁed LAPACK least squares solver on a machine with 8GB RAM and an\nAMD Opteron 242 processor [AMT10].\nLSRN.\nThis comprises a C++ implementation callable from Matlab and a Python\nimplementation. The C++ implementation was shown to outperform LAPACK’s\nDGELSD on large dense problems, and Matlab’s backslash (SuiteSparseQR) on sparse\nproblems. The Python implementation has demonstrated that LSRN scales well on\nAmazon Elastic Compute Cloud clusters. We note that the Python implementation\nrelies on an auxiliary Python package with a custom C-extension for sampling from\nthe Gaussian distribution via the ziggurat method.\nLibSkylark.\nThis library is written in C++ and is available on GitHub. Its support for least squares problems is very general and includes a few deterministic\npreconditioned iterative solvers. Its sketch-and-precondition functionality includes\nimplementations in the styles of Blendenpik and LSRN. LibSkylark has a Python\ninterface, but only for Python 2.7. Its linear algebra kernels are implemented partly\nin the Elemental distributed linear algebra library [PMG+13]. Unfortunately, Elemental is no longer maintained.\nSki-LLS.\nThis is a recently developed C++ library for solving dense and sparse\nhighly overdetermined least squares problems. It is distinguished by its ﬂexibility in\npreconditioner generation. In particular, it supports sketching by SRFTs, Gaussian\noperators, and SASOs. It also supports factoring the sketch SA by several methods,\nincluding a standard SVD algorithm, a randomized algorithm for full-rank columnpivoted QR (see Section 5.1.2), and a standard algorithm for sparse QR. We record\nthe following (adapted) quote from the GitHub repository that hosts this software:\nSki-LLS is faster and more robust than Blendenpik and LAPACK on\nlarge over-determined data matrices, e.g., matrices having 40,000 rows\nand 4,000 columns.\nSki-LLS is 10 times faster than Sparse QR and\n5Note that “Blendenpik” and “LSRN” are names for algorithms and libraries.\nPage 60\narXiv Version 2\n\n3.5. Existing libraries\nLeast Squares and Optimization\nincomplete-Cholesky preconditioned LSQR on sparse data matrices that\nare ill-conditioned and suﬃciently large, e.g., with 120,000 rows, 5,000\ncolumns, and 1% non-zeros.\nFalkon.\nFinally, we note the recently developed Falkon library for sketch-and-solve\napproaches to KRR powered by multi-GPU machines [MCR+20; MCD+22]. While\nthis library works outside of our primary data model, it is of interest to anyone\ndeveloping software for KRR based on RandNLA.\narXiv Version 2\nPage 61\n\nLeast Squares and Optimization\n3.5. Existing libraries\nPage 62\narXiv Version 2\n\nSection 4\nLow-rank Approximation\n4.1 Problem classes ...........................................................\n64\n4.1.1 Spectral decompositions .............................................\n65\n4.1.2 Submatrix-oriented decompositions ..............................\n68\n4.1.3 On accuracy metrics ..................................................\n72\n4.2 Drivers ........................................................................\n73\n4.2.1 Methods for SVD ......................................................\n74\n4.2.2 Methods for Hermitian eigendecomposition ...................\n75\n4.2.3 Methods for CUR and two-sided ID .............................\n78\n4.3 Computational routines ..............................................\n80\n4.3.1 Power iteration .........................................................\n81\n4.3.2 Orthogonal projections: QB and rangeﬁnders ................\n81\n4.3.3 Column-pivoted matrix decompositions ........................\n83\n4.3.4 One-sided ID and CSS ...............................................\n85\n4.3.5 Estimating matrix norms ...........................................\n88\n4.3.6 Oblique projections ...................................................\n89\n4.4 Other low-rank approximations ..................................\n89\n4.5 Existing libraries .........................................................\n91\nModern scientiﬁc computing, machine learning, and data science applications\ngenerate massive matrices that need to be processed for reduced run time, reduced\nstorage requirements, or improved interpretability. Low-rank approximation is a\nworkhorse approach for achieving these goals. Here, given a target matrix A, the\ntask is to produce a suitably factored representation of a low-rank matrix ˆA of the\nsame dimensions which approximates the matrix A.\nWe can express the main aspects of a low-rank approximation as computing\nfactor matrices E and F where\nA\n≈\nˆA\n:=\nE\nF\nm × n\nm × n\nm × k\nk × n\n(4.1)\nfor some k ≪min{m, n}. We note that it is very common to have a k × k “inner\nfactor” that appears in between E and F above.\n63\n\nLow-rank Approximation\n4.1. Problem classes\nSuch representations facilitate data interpretation by choosing the factors to\nhave useful structure, such as having orthonormal columns or rows, or being submatrices of the target. The extent of storage reduction from low-rank approximation depends on whether A is dense or sparse. In the dense case, ˆA is stored in\nO(mk + nk) space. In the sparse case, one representation consists of a dense k × k\ninner factor, a slice of k rows of A, and a slice of k columns of A.\nThe rank k used in a low-rank approximation is a tuning-parameter that the\nuser can control to trade-oﬀbetween approximation accuracy and data compression.\nThe best choice of this parameter depends on context. For instance, one may want\nto choose k small enough to graphically visualize coherent structure in the target.\nIn such a setting one would not expect that ˆA is close to A in an absolute sense, but\none can still ask that the distance is near the minimum among all approximations\nwith the desired structure and rank. Alternatively, one might know that A can be\nwell-approximated by a low-rank matrix, and yet not know the rank necessary to\nachieve a good approximation. Such matrices are called numerically low-rank and\narise in applications across the social, physical, biological, and ecological sciences.\nFor example, they can arise as discretizations of diﬀerential operators, where the\nextent to which the matrix is numerically low-rank depends on the details of the\noperator and the discretization; and they can arise as noisy corruptions of general\n(hypothesized) data matrices with low exact rank. When dealing with such matrices\none can iteratively build ˆA until a desired distance ∥A −ˆA∥is small. This section\ncovers a variety of eﬃcient and reliable low-rank approximation algorithms for both\nof these scenarios.\n4.1\nProblem classes\nLow-rank approximation is naturally formalized as an optimization problem; one\nchooses ˆA and its factors to minimize a loss function subject to some constraints.\nThe most common loss functions are distances ˆA 7→∥A −ˆA∥induced by the Frobenius or spectral norms. Alternatively, one can use the discontinuous loss function\nˆA 7→rank(ˆA) as a measure of the storage requirements for ˆA. Constraints depend\non the loss function in a complementary way. When minimizing a norm-induced\ndistance, one imposes rank constraints by limiting the dimensions of the factors.\nWhen minimizing the rank of ˆA (i.e., when seeking an approximation that admits\nthe smallest-possible representation) one constrains the approximation error ∥A−ˆA∥\nto be at most some speciﬁed value. One can also impose structural constraints on\nthe factors of ˆA, such as being orthogonal, diagonal, or a submatrix of the target.\nOur overview of randomized algorithms for low-rank matrix approximation is\norganized around such structural constraints. Accordingly, we use the term problem class for loose groups of low-rank approximation problems wherein the factors\nfacilitate similar downstream tasks.\nCurrently, our two problem classes are the\nfollowing.\n• Spectral decompositions (§4.1.1): this consists of low-rank SVD and Hermitian eigendecomposition.\n• Submatrix-oriented decompositions, i.e., decompositions with factors based\non submatrices of the target matrix (§4.1.2): this consists of so-called CUR\nand interpolative decompositions.\nPage 64\narXiv Version 2\n\n4.1. Problem classes\nLow-rank Approximation\nOptimal decompositions in the ﬁrst class often serve as baselines in theoretical\nanalyses of randomized algorithms for low-rank decomposition in both classes. That\nis, such comparisons are made regardless of whether the approximation is spectral\nor submatrix-oriented. This fact can blur the distinction between the two problem\nclasses, and the distinctions can blur even further when one considers methods for\neﬃciently converting from one decomposition to another. Still, keeping the problem\nclasses separate is useful as an organizing principle for the most fundamental lowrank approximation problems in RandNLA.\nRemark 4.1.1. Low-rank approximations that impose no requirements on ˆA’s representation are brieﬂy addressed in Section 4.3.6 in the context of computational\nroutines. Methods for low-rank approximation with other representations (e.g., QR,\nUTV, LU, nonnegative factorization) are discussed in Section 4.4.\n4.1.1\nSpectral decompositions\nIn what follows we give an overview of the SVD and Hermitian eigendecomposition,\nwith emphasis on the roles of these decompositions in low-rank approximation.\nAfter covering these concepts we explain how they provide two perspectives on\nprincipal component analysis (PCA). We advise the reader to at least skim this\noverview material even if they are already familiar with the relevant concepts; lowrank approximation is much more prominent in RandNLA than it is in classical\nNLA.\nSingular value decomposition\nThe SVD is widely used to compute low-rank approximations and as a workhorse\nalgorithm for PCA. Given a m × n matrix A, where m ≥n (without loss of generality), its SVD is\nA\n=\nU\nΣ\nV∗\nm × n\nm × n\nn × n\nn × n ,\n(4.2)\nwhere U = [u1, . . . , un] and V = [v1, . . . , vn] are column-orthonormal matrices that\ncontain the left and right singular vectors of A. The matrix Σ = diag(σ1, . . . , σn)\ncontains the corresponding singular values; we use the convention that they appear\nin decreasing order σ1 ≥. . . ≥σn ≥0.\nWe can also think about the SVD as\nexpressing A as the sum of n rank-one matrices\nA =\nn\nX\ni=1\nσiuiv∗\ni .\n(4.3)\nIn applications it is common to encounter data matrices with low-rank structure,\ni.e., matrices for which r = rank(A) is smaller than the ambient dimensions m and\nn of A. In this case, the singular values {σi : i ≥r + 1} are zero, the corresponding\nsingular vectors span the left and right null spaces, and it is natural to consider the\ncompact SVD where the sum in (4.3) is truncated at i = r. For a matrix A with\napproximate low-rank structure, we can obtain approximations with low exact rank\nby truncating this sum even earlier, at some k < r:\nA ≈ˆAk :=\nk\nX\ni=1\nσiuiv∗\ni\n=[u1, . . . , uk] diag(σ1, . . . , σk)[v1, . . . , vk]∗= UkΣkV∗\nk,\n(4.4)\narXiv Version 2\nPage 65\n\nLow-rank Approximation\n4.1. Problem classes\nTruncating trailing singular values yields optimal rank-constrained approximations,\nin the sense of solving\nˆAk ∈arg min\nrank(ˆA\n′)=k\n∥A −ˆA\n′∥.\n(4.5)\nThis holds for every k ∈JrK. In other words, if A is approximated by a rank-k\nmatrix ˆAk given through its SVD, no further computation is needed to canonically\nobtain approximations of A with any rank k ≤r.\nThe optimality result of (4.5) holds for any unitarily invariant matrix norm, and\nit is known as the Eckart-Young-Mirsky Theorem when considered for the spectral\nnorm or Frobenius norm. The reconstruction errors according to these norms are\n∥A −ˆAk∥2 = σk+1(A)\nand\n∥A −ˆAk∥F =\nsX\nj>k\nσ2\nj (A).\n(4.6)\nThese facts are important in applications, where it is common to see rank(A) =\nmin{m, n} in exact arithmetic and yet many of the trailing singular values are so\nsmall that they can be presumed to be noise. That is, the truncation introduced in\n(4.4) is often used as a denoising technique.\nHermitian eigendecomposition\nA matrix is called Hermitian if it is equal to its adjoint, i.e., if A = A∗. For real\nmatrices, being Hermitian is the same as being symmetric. The eigendecomposition\nof a Hermitian matrix A is\nA\n=\nV\nΛ\nV∗\nn × n\nn × n\nn × n\nn × n ,\n(4.7)\nwhere V is an orthogonal matrix of eigenvectors and Λ = diag(λ1, . . . , λn) is a real\nmatrix containing the eigenvalues of A. A Hermitian matrix is further called positive\nsemideﬁnite (or “psd”) if λi ≥0 for all i.\nWe use the convention of sorting eigenvalues in decreasing order of absolute\nvalue: |λ1| ≥· · · ≥|λn|. This allows for a more direct comparison to the SVD,\nsince we obtain low-rank approximations\nA ≈ˆAk :=\nk\nX\ni=1\nλiviv∗\ni\n=[v1, . . . , vk] diag(λ1, . . . , λk)[v1, . . . , vk]∗= VkΛkV∗\nk\n(4.8)\nfor which the spectral and Frobenius-norm distances to A match those from (4.6).\nIndeed, a (truncated) eigendecomposition can be converted to a (truncated) SVD\nby taking the columns of V as the right singular vectors, setting the left singular\nvectors according to\nui =\n(\nvi\nif λi > 0\n−vi\notherwise\nand setting the singular values to σ = |λ| (elementwise).\nIf a matrix is Hermitian then it is better to compute (and work with) its eigendecomposition, rather than its SVD. The ﬁrst reason for this is that a rank-k eigendecomposition requires almost half the storage of a rank-k SVD. The second reason\nPage 66\narXiv Version 2\n\n4.1. Problem classes\nLow-rank Approximation\nis that algorithms for computing low-rank eigendecompositions are able to leverage\nstructure in the matrix for improved eﬃciency. These eﬃciency improvements can\nbe dramatic for psd matrices, where an eigendecomposition is technically also an\nSVD.\nConnections to principal component analysis\nPCA is a linear dimension reduction technique that is widely used in data science\napplications for extracting features, or for visualizing and summarizing complicated\ndatasets. The idea of PCA is to form k new variables (components) Z = [z1, . . . , zk]\nas linear combinations of the variables X = [x1, . . . , xn] ∈Rm×n (that are assumed\nto have been preprocessed to have column-wise zero empirical mean). Speciﬁcally,\ngiven the data matrix X, one forms the variables as Z = XW where the weights\nW = [w1, . . . , wk] ∈Rn×k are chosen so that the ﬁrst component z1 accounts\nfor most of the variability in the data, the second component z2 for most of the\nremaining variability, and so on.\nFormally, we can formulate this problem as a variance maximization problem\nw1 := arg max\n∥w∥2\n2=1\nVar(Xw)\n(4.9)\nwhere we deﬁne the variance operator as Var(Xw) :=\n1\nm−1∥Xw∥2\n2. Deﬁning the\nsample covariance matrix C :=\n1\nm−1X∗X, this problem can be stated as\nw1 := arg max\n∥w∥2\n2=1\nw∗Cw.\n(4.10)\nWe recognize (4.10) as the variational formulation of the dominant eigenvector of a\nHermitian matrix. That is, w1 satisﬁes\nCw1 = λ1(C)w1.\n(4.11)\nMore generally, PCA ﬁnds the weights w1, . . . , wk by diagonalizing the empirical\nsample covariance matrix C as C = WΛW∗, and retaining only the top k eigenvectors.\nHow one computes the PCA depends on how the data is presented and accessed.\nThere are two situations of interest. In situations where the covariance matrix C\nis given by the problem at hand—and thus where we access C directly, but do not\ndirectly access the data matrix X—one can directly employ a low-rank Hermitian\neigendecomposition to compute the dominant k eigenvectors.\nIf instead we are\npresented with the variables in form of a mean-centered data matrix X, then the\nlow-rank SVD becomes the preferable approach to computing the weights W. This\nis because we can relate the eigenvalue decomposition of the inner product X∗X to\nthe SVD of X = UΣV∗by\nX∗X = (VΣU∗)(UΣV∗) = VΣ2V∗.\n(4.12)\nHence, we obtain the k weights W = [w1, . . . , wk] by computing the top k right\nsingular vectors Vk = [v1, . . . , vk], and the eigenvalues of the sample covariance\nmatrix C are given by the diagonal elements\n1\nm−1Σ2.\nFast randomized algorithms for computing the SVD and eigenvalue decomposition enable scaling PCA to especially large problems. However, one does not need a\nlarge problem to beneﬁt from these randomized algorithms. From 2016 until at least\ntime of writing, the pca function SciKit-Learn defaults to a randomized algorithm\nwhen d = min{m, n} ≥500 and k is less than 0.8d [Gri16].\narXiv Version 2\nPage 67\n\nLow-rank Approximation\n4.1. Problem classes\n4.1.2\nSubmatrix-oriented decompositions\nHere we describe four types of submatrix-oriented decompositions: a CUR decomposition and three types of interpolative decompositions. Historically, these have been\nused far less often than spectral decompositions. However, their value propositions\nhave become much more compelling in recent years:\n• They can oﬀer reduced storage requirements compared to spectral decompositions, especially for sparse data matrices. This can be very valuable in\nprocessing massive data sets.\n• They provide for more transparent data interpretation. This is especially true\nwhen data is modeled as a matrix as a matter of convenience, rather than as\na statement about the data deﬁning a meaningful linear operator A 7→Av.\nRemark 4.1.2. The following material is dense. We encourage the reader to return\nto it as needed while reading later parts of this section.\nCUR decomposition\nDeﬁnition.\nA CUR decomposition is a low-rank approximation of the form\nA\n≈\nC\nU\nR,\nm × n\nm × k\nk × k\nk × n\n(4.13)\nwhere the factors C and R are formed by small subsets of actual columns and rows\nof A, and the linking matrix U is chosen so that some norm of A −CUR is small.\nMotivation.\nThe literature on CUR decomposition traces back to work by Gore˘ınov,\nZamarashkin, and Tyrtyshnikov, who proved existential results for CUR decompositions with certain approximation error bounds [GZT95; GTZ97]. Gore˘ınov et al.\nmotivated their investigations by pointing out that CUR decompositions have far\nlower storage requirements than partial SVDs when A is sparse.More speciﬁcally,\nthey advocated for applying CUR decompositions for low-rank approximation of\noﬀ-diagonal blocks in block matrices.\nThe usage of CUR as a data-analysis tool was popularized by Mahoney and\nDrineas [MD09], following the development of eﬃcient randomized algorithms for\ncomputing CUR decompositions with good approximation guarantees [DMM08].\nThe argument of Mahoney and Drineas was that experts often have a clear understanding of the actual meaning of certain columns and rows in a matrix, and this\nmeaning is preserved by the CUR. In contrast, SVD (or PCA) forms linear combinations of the columns or rows of the input matrix; these linear combinations can\nprove diﬃcult to interpret and destroy structures such as sparsity or nonnegativity.\nWords of warning.\nDespite its simple deﬁnition, there are important subtleties in\nworking with and understanding CUR decompositions.\nFirst, CUR is unique among submatrix-oriented decompositions in that it involves taking products of submatrices of A. Therefore if A has low numerical rank\nand its CUR decomposition is computed to high accuracy, then all three factors (C,\nU, and R) will be ill-conditioned. This can have detrimental eﬀects on numerical\nbehavior, particularly in the computation of U, which will behave qualitatively like\ninverting a matrix of low numerical rank.\nPage 68\narXiv Version 2\n\n4.1. Problem classes\nLow-rank Approximation\nSecond, out of all the submatrix-oriented decompositions, CUR is of the least\ninterest for providing exact decompositions of full-rank matrices. For example, if\nA = CUR is a tall matrix of rank n, then we would necessarily have C = AP\nand U = P∗R−1 for some permutation matrix P. Therefore the only real freedom\nin full-rank CUR of a tall matrix is in choosing the spanning set of rows that\ndeﬁne R.\nA similar conclusion applies when A is a wide matrix of rank m; an\nexact decomposition would necessarily have R = P∗A and U = C−1P for some\npermutation matrix P, and the only real freedom would be in choosing the spanning\nset of columns that deﬁne C.\nThe consequences of this second fact will be seen when we discuss randomized\nalgorithms for computing CUR decompositions.\nIn particular, we will see that\nrandomized algorithms for (low-rank) CUR generally do not involve computing\n“full-rank CUR decompositions” on smaller matrices. This will stand in contrast to\nrandomized algorithms for (low-rank) SVD and interpolative decompositions, which\nusually do involve computing the corresponding full-rank decomposition on smaller\nmatrices.\nInterpolative decompositions\nLow-rank interpolative decompositions (IDs) come in three diﬀerent ﬂavors that\nshare two common notes. The ﬁrst shared note of these ﬂavors is that exactly one\nof the factors that deﬁne ˆA is a submatrix of A. The second shared note concerns\nthe factors of ˆA that are not submatrices of A. These factors, called interpolation\nmatrices, are subject to certain regularity conditions.\nOur crash course on ID comes in three parts. First, we deﬁne versions of ID\nthat only involve one interpolation matrix, so-called one-sided IDs. After that, we\nexplain how accuracy guarantees of low-rank ID are aﬀected by regularity conditions\non interpolation matrices. This explanation is important; we reference it repeatedly\nwhen we cover randomized algorithms for one-sided ID (§4.3.4). We wrap up by\nintroducing the two-sided ID.\nThe one-sided IDs: column ID and row ID.\nIn the low-rank case, a column ID is an\napproximation of the form\nA\n≈\nC\nX\nm × n\nm × k\nk × n\n(4.14)\nwhere C is given by a small number of columns of A and X is a wide interpolation\nmatrix. Full-rank column IDs can be of interest to us when A is very wide, in which\ncase we have k = m ≪n and X = C−1A. Next, we consider row IDs. In the\nlow-rank case, these are approximations of the form\nA\n≈\nZ\nR.\nm × n\nm × k\nk × n\n(4.15)\nwhere R is given by a small number of rows of A and Z is a tall interpolation matrix.\nThe submatrices C and R can be represented by ordered column and row index sets,\nwhich we denote by J and I, that satisfy\nC = A[:, J]\nand\nR = A[I, :].\nThese ordered index sets are called skeleton indices.\narXiv Version 2\nPage 69\n\nLow-rank Approximation\n4.1. Problem classes\nOur deﬁnitions of row and column IDs are not complete until we specify the\nregularity conditions on X and Z. The skeleton indices (J, I) play a central role\nhere. Most importantly, we require that the interpolation matrices satisfy\nX[:, J] = Ik = Z[I, :].\nThese regularity conditions have two direct consequences, namely\nA[:, J] = A[:, J]X[:, J] , and\nA[I, :] = Z[I, :]A[I, :],\nIn addition to these conditions, the literature on ID typically requires that the\nentries of X and/or Z are bounded in modulus by a small constant M. While we do\nnot subscribe to this requirement in our deﬁnition of IDs, there is good motivation\nbehind it. We address this motivation next.\nQuality-of-approximation in low-rank IDs.\nSuppose that ˆA is a low-rank column ID\nof A. It is immediate from our deﬁnition of low-rank ID that there are at most\n\u0000n\nk\n\u0001\npossible values for the subspace range(ˆA). In general, it can happen that none of\nthese subspaces coincides with a dominant k-dimensional left singular subspace of\nA. When this happens, it will necessarily be the case that ∥A −ˆA∥2 > σk+1(A),\nwhereas a general rank-k approximation could achieve an error equal to σk+1(A).\nThis raises a question.\nHow much of a price do we pay by imposing that requirement that ˆA be\na low-rank ID?\nThe following proposition (which we prove in Appendix C.1.1 by standard techniques) gives a partial answer to this question. The answer is notable in that it\nreveals the value of bounding the interpolation matrices.\nProposition 4.1.3. Let ˜A be any rank-k approximation of A that satisﬁes the\nspectral norm error bound ∥A −˜A∥2 ≤ϵ. If ˜A = ˜A[:, J]X for some k × n matrix X\nand an index vector J, then ˆA = A[:, J]X is a low-rank column ID that satisﬁes\n∥A −ˆA∥2 ≤(1 + ∥X∥2)ϵ.\n(4.16)\nFurthermore, if |Xij| ≤M for all (i, j), then\n∥X∥2 ≤\np\n1 + M 2k(n −k).\n(4.17)\nWe note that the assumptions on the matrix ˜A from the proposition statement\nimply that ˜A[:, J] is full column-rank. This implies that (˜A[:, J])†(˜A[:, J]) = Ik and\nhence that X is completely determined from the index vector J. Since a rank-k\nmatrix will typically have multiple subsets of k columns that span its range, we\nﬁnally see that the matrix ˜A does not uniquely determine the interpolation matrix\nX used in the proposition. Indeed, the range of possibilities for the interpolation\nmatrix is rather remarkable. While it is known that there always exists an index set\nfor which maxij |Xij| = 1 [Pan00], it is NP-hard to compute this index set [C¸M09].\nAt the same time, algorithms such as strong rank-revealing QR can be applied to\n˜A with typical runtime O(mnk), while ensuring maxij |Xij| ≤2 [GE96].\nPage 70\narXiv Version 2\n\n4.1. Problem classes\nLow-rank Approximation\nAll-in-all, the main point of an interpolative decomposition is to provide a lowrank approximation that prominently features a submatrix of the target. Therefore\nwhile an upper bound M on the entries of an interpolation matrix gives the bound\n(4.16) indirectly by way of (4.17), such a bound should not the end of the story.\nThe spectral norm ∥X∥2 is ultimately more informative for this purpose, even if it\nis harder to compute.\nOf course, all of the points we have raised here for column IDs apply to row IDs\nwith minor modiﬁcations.\nTwo-sided ID.\nThe concept of a one-sided ID can be extended to a two-sided ID\nby considering simultaneous row and column IDs. In the low-rank case, a two-sided\nID is an approximation of the form\nA\n≈\nZ\nA[I, J]\nX.\nm × n\nm × k\nk × k\nk × n\n(4.18)\nMethods for full-rank one-sided ID can be useful in computing low-rank two-sided\nIDs. That is, if we ﬁrst compute a low-rank column ID ˆA = CX by some black-box\nmethod, then after obtaining a full-rank row ID of the tall matrix C = A[:, J] =\nZA[I, J] we have ˆA = ZA[I, J]X.\nOn relationships between submatrix-oriented decompositions\nThe landscape of methods for submatrix-oriented decompositions is very intertwined. As we have already indicated, algorithms for one-sided ID are the main\nbuilding blocks of algorithms for low-rank two-sided ID. Later in this section we also\nmention several algorithms for CUR decomposition which depend on algorithms for\none-sided ID. In view of this, we emphasize the following point.\nFor our purposes, it is helpful to introduce hierarchical relationships\namong submatrix-oriented decompositions. As such, we designate algorithms for one-sided ID as computational routines, while methods for\nCUR and two-sided ID are designated as drivers.\nNext, let us point out a sense in which CUR and two-sided ID are “dual” to one\nanother. Both are submatrix-oriented decompositions that have three factors. For\nCUR, the outer factors are submatrices of A and no requirements are placed on the\ninner factor (the linking matrix). In particular, if we specify the outer factors by\nordered index sets J and I, then a CUR decomposition is expressed as\nA\n≈\nA[:, J]\nU\nA[I, :].\nm × n\nm × k\nk × k\nk × n\n(4.19)\nThis can be contrasted with two-sided ID as deﬁned in (4.18). There, the inner\nfactor is a submatrix of A, and moderate requirements are placed on the outer\nfactors (the interpolation matrices).\nThe properties of “linking matrices” and “interpolation matrices” are diﬀerent\nenough to warrant their diﬀerent names. The problem of computing a low-rank\napproximation via two-sided ID can be better numerically behaved, compared to\nlow-rank approximation by CUR [MT20, §13]. However, CUR oﬀers far greater\npotential for storage reduction when dealing with sparse matrices. The diﬀerences\narXiv Version 2\nPage 71\n\nLow-rank Approximation\n4.1. Problem classes\nbetween two-sided ID and CUR can become less pronounced when one considers\nmethods for losslessly converting one such representation to the other, as we mention\nin Section 4.2.3.\n4.1.3\nOn accuracy metrics\nThe problems from Section 3 were mostly unconstrained minimization of convex\nquadratics. Such problems are very nice, since the gradient of the quadratic loss\nfunction constitutes a canonical error metric that can be driven to zero. Low-rank\napproximation problems can likewise be framed as optimization problems. However,\nthese formulations either involve constraints or a nonconvex objective function. This\ndistinction is important, since these structures rule out checking for a zero gradient\nas a cheap optimality condition.\nThe main error metrics in low-rank approximation are norm-induced distances.\nFor reasons that we give under the next two headings it is not appropriate to\nconsider distances from a computed approximation ˆA to some nominally “optimal”\napproximation. Instead, one measures the distance from the approximation to the\ntarget, most often in the spectral or Frobenius norms.\nDistance to optimal approximations\nNon-unique solutions and sensitivity to perturbations.\nRecall from Section 4.1.1 how\ntruncating A’s SVD at rank k gives an optimal rank-k approximation in any unitarily invariant norm. Unfortunately, this truncation will be non-unique when A has\nmore than one singular value equal to σk. This is easiest to see when A = I is the\nidentity matrix, in which case every diagonal {0, 1}-matrix of rank k is an optimal\nrank-k approximation to A.\nMore generally, if A has multiple singular values that are close to σk, then\nextremely small perturbations to A can result in large changes to the singular vectors\ncorresponding to these singular values; see [Bha97, §6 – §8] for details. This has\na secondary complication: it is harder to estimate the dominant k singular vectors\nof a matrix than it is to ﬁnd a rank-k approximation that is “near optimal” in the\nsense of (4.5).\nIntractability of computing optimal approximations.\nWhen working with submatrixoriented decompositions, we do not even have the luxury of deﬁning “optimal”\napproximations in the manner of truncated SVDs. Indeed, the problem of ﬁnding\nan “optimal” ID necessitates specifying any regularity conditions such as the bound\nM in a constraint |Xij| ≤M. As we mentioned before, even when ˆA has exact rank\nk, a rank-k column ID with M = 1 always exists but is NP-hard to ﬁnd [C¸M09].\nGoing to another extreme, we could set aside the matter of M and simply set\nX = C†A for a matrix C containing k columns of A. In this case it is not known if\nthe columns can be chosen to minimize Frobenius- or spectral-norm error ∥ˆA −A∥\nin time less than O(nk). Still, there are theoretical guarantees for approximation\nquality by CUR relative to approximation quality achievable by SVD. We refer\nthe reader to [DMM08; BMD09], [VM16, §1 - §2], and Appendix C.1.1 for more\ninformation about CUR and ID in our context.\nPage 72\narXiv Version 2\n\n4.2. Drivers\nLow-rank Approximation\nDistance relative to that of a reference approximation\nIt is problematic to use a distance from ˆA to A as an error metric for ˆA. This is\nbecause there are situations when any such distance will be large even when ˆA is\nclose to an “optimal” approximation. The simplest example of this is PCA, in which\ncases the approximation rank is O(1), independent of the dimensions of the matrix\nor properties of its spectrum. More generally, it can be hard to obtain a low-rank\napproximation that is very close to A when A has slow spectral decay, in the sense\nthat the distribution of its singular values has a heavy tail. Accurate approximations\ncan also be hard to come by if the factors of ˆA are highly constrained.\nOne handles this situation by considering the distance between A and ˆA relative\nto that between A and some reference matrix Ar. Formally, we concern ourselves\nwith the smallest value of ϵ needed to achieve\n∥A −ˆA∥≤(1 + ϵ)∥A −Ar∥.\nThe reference matrices Ar used in RandNLA theory are not available to us when\nperforming computations. In fact, they are usually not optimal for the formal lowrank approximation problem at hand. The most common source of non-optimality\nis that the reference is subject to a more stringent rank constraint: rank(Ar) <\nrank(ˆA) ≤rank(A). Another source of non-optimality is that it may not be possible to decompose the reference into factors with the required structure (e.g., the\nstructure required by low-rank CUR). For example, an approximation of A obtained\nby a rank-k truncated SVD cannot (in general) be converted into a rank-k CUR\ndecomposition using submatrices of A.\n4.2\nDrivers\nThere exist many randomized algorithms for computing low-rank approximations\nof matrices. This section focuses on low-rank approximation algorithms that take\nthe two-stage approach popularized by [HMT11], because this approach has been\ndemonstrated to be eﬃcient and highly reliable over the years.\nThe high-level\nidea of the two-stage approach is the following: ﬁrst one constructs a “simple”\nrepresentation of ˆA with the aid of randomization, and then one deterministically\nconverts that representation of ˆA into a more useful form.\nIn order to discuss these drivers for low-rank approximation, it is necessary to\nmention brieﬂy the following two concepts (these are handled by computational\nroutines, to be discussed in detail in Section 4.3):\n• A QB decomposition is a simple representation that is useful for SVD and\neigendecomposition. The representation takes the form ˆA = QB for a tall\nmatrix Q with orthonormal columns and B = Q∗A. The important point\nhere is that the QB decomposition involves explicit construction of and access\nto both Q and B. We discuss QB algorithms in Section 4.3.2.\n• Column subset selection (CSS) is the problem of selecting from a matrix a set\nof columns that is “good” in some sense. CSS algorithms largely characterize\nalgorithms for one-sided ID. We discuss methods for these two problems in\nSection 4.3.4. They are important here because a one-sided ID can be used\nfor the simple representation of ˆA when working toward an SVD, eigendecomposition, two-sided ID, or CUR decomposition.\narXiv Version 2\nPage 73\n\nLow-rank Approximation\n4.2. Drivers\n4.2.1\nMethods for SVD\nThe are several families of randomized algorithms for computing low-rank SVDs.\nHere we describe a few families that give ˆA = UΣV∗through its compact SVD.\nA ﬂexible method\nWe begin with Algorithm 3.\nThis algorithm uses randomization to compute a\nQB decomposition of A, then deterministically computes QB’s compact SVD, and\nﬁnally truncates that SVD to a speciﬁed rank.\nThis algorithm assumes that the QBDecomposer is iterative in nature. It assumes\nthat each iteration adds some number of columns to Q and rows to B, and that\nthe algorithm can terminate once an implementation-dependent error metric for\nQB ≈A falls below ϵ or once QB reaches a rank limit. Here we have set the rank\nlimit to k + s where s is a nonnegative “oversampling parameter.”\nAlgorithm 3 SVD1 : QB-backed low-rank SVD (see [HMT11] and [RST10])\n1: function SVD1(A, k, ϵ, s)\nInputs:\nA is an m × n matrix. The returned approximation will have rank at\nmost k. The approximation produced by the randomized phase of the\nalgorithm will attempt to A to within ϵ error, but will not produce\nan approximation of rank greater than k + s.\nOutput:\nThe compact SVD of a low-rank approximation of A.\nAbstract subroutines:\nQBDecomposer generates a QB decomposition of a given matrix; it\ntries to reach a prescribed error tolerance but may stop early if it\nreaches a prescribed rank limit.\n2:\nQ, B = QBDecomposer(A, k + s, ϵ) # QB ≈A\n3:\nr = min{k, number of columns in Q}\n4:\nU, Σ, V∗= svd(B)\n5:\nU = U[: , : r]\n6:\nV = V[: , : r]\n7:\nΣ = Σ[: r , : r]\n8:\nU = QU\n9:\nreturn U, Σ, V∗\nThe literature recommends setting s to a small positive number (e.g., s = 5\nor s = 10) to account for the fact that the trailing singular vectors of QB may\nnot be good estimates for the corresponding singular vectors of A. However, using\nany positive oversampling parameter complicates the interpretation of the error\ntolerance ϵ. If a user deems this problematic then they can simply set k ←k + s\nand s ←0.\nSuch an approach can be reasonable if tuning parameters for the\nQB algorithm are chosen appropriately. Speciﬁcally, if techniques such as power\niteration are used (see Section 4.3.1) then the trailing singular vectors of QB can\nbe reasonably good approximations to the corresponding singular vectors of A.\nPage 74\narXiv Version 2\n\n4.2. Drivers\nLow-rank Approximation\nSacriﬁcing accuracy for speed\nConverting from an ID.\nSetting our sights beyond Algorithm 3, it is noteworthy\nthat if ˆA is given in any compact representation then it can be losslessly converted\ninto an SVD without ever accessing A. For example, given a column ID ˆA = CX,\nwe would compute a QR decomposition C = QR, set B = RX, and then proceed\nwith (Q, B) as in Algorithm 3. As another example, conversion from a row ID to\nan SVD is illustrated implicitly in [HMT11, Algorithm 5.2].\nSuch approaches are potentially useful because one-sided ID can easily be implemented in a way that accesses A with a single matrix-matrix multiplication and\nthen selects a row or column submatrix. However, this comes at a cost of a much\nless accurate solution compared to typical QB methods.\nSingle-pass algorithms.\nFor very large problems the main measure of an algorithm’s\ncomplexity is the number of times it moves A through fast memory. Besides the\nabove ID-based method, there are three algorithms for low-rank SVD which move\nA through fast memory only once. Each of them uses multi-sketching in the sense\nof Section 2.6. The ﬁrst and second options simply use Algorithm 3, but speciﬁcally\nwith single-pass QB methods based on type 1 or type 2 multi-sketching. Discussion\nof such QB algorithms is deferred to Section 4.3.2. The third option is the algorithm\ndescribed in [TYU+17b, §7.3.2], which relies on type 3 multi-sketching.\nRemark 4.2.1. Algorithms designed to minimize the number of views of a matrix\nare usually analyzed in the pass eﬃcient model for algorithm complexity [DKM06a].\nEarly work on randomized pass-eﬃcient and single-pass algorithms can be found in\n[FKV04; DKM06a; DKM06b].\nError estimation in sketched one-sided SVD.\nSingle-pass algorithms are unlikely to\nproduce highly accurate approximations of singular vectors or singular values. However, their results may be accurate enough to be useful in certain applications. This\nmotivates methods for estimating the errors of approximations returned by these\nalgorithms. Appendix E.3 provides a bootstrap-based error estimator for a simple\nrandomized algorithm that recovers approximate singular vectors from one side of\nthe matrix.\n4.2.2\nMethods for Hermitian eigendecomposition\nEach randomized algorithm for low-rank SVD has a corresponding version that is\nspecialized to Hermitian matrices. We recount those specialized algorithms here,\nand we mention an additional algorithm that is unique to the approximation of psd\nmatrices. In general, we shall say that A is n × n and that the algorithms represent\nˆA = V diag(λ)V∗, where V is a tall column-orthonormal matrix and λ is a vector\nwith entries sorted in decreasing order of absolute value.\nA ﬂexible method for Hermitian indeﬁnite matrices\nAlgorithm 4 is a variation of [HMT11, Algorithm 5.3].\nIts parameters (k, ϵ, s)\nhave essentially the same interpretations as Algorithm 3. When s = 0, its output\nis simply is a compact eigendecomposition of QCQ∗, where C = Q∗AQ and Q is\nobtained from a black-box QBDecomposer. The main diﬀerence between this method\nand Algorithm 3 is that ϵ is scaled down by a factor 1/2 before being passed to\narXiv Version 2\nPage 75\n\nLow-rank Approximation\n4.2. Drivers\nQBDecomposer. This change is needed to so that if s = 0 and ∥QB −A∥≤ϵ then\nthe ﬁnal approximation satisﬁes ∥ˆA −A∥≤ϵ; see [HMT11, §5.3].\nAlgorithm 4 EVD1 : QB-backed low-rank eigendecomposition; see [HMT11]\n1: function EVD1(A, k, ϵ, s)\nInputs:\nA is an n × n Hermitian matrix. The returned approximation will\nhave rank at most k. The approximation produced by the randomized\nphase of the algorithm will attempt to A to within ϵ error, but will\nnot produce an approximation of rank greater than k + s.\nOutput:\nApproximations of the dominant eigenvectors and eigenvalues of A.\nAbstract subroutines:\nQBDecomposer generates a QB decomposition of a given matrix; it\ntries to reach a prescribed error tolerance but may stop early if it\nreaches a prescribed rank limit.\n2:\nQ, B = QBDecomposer(A, k + s, ϵ/2)\n3:\nC = BQ # since B = Q∗A, we have C = Q∗AQ\n4:\nU, λ = eigh(C) # full Hermitian eigendecomposition\n5:\nr = min{k, number of entries in λ}\n6:\nP = argsort(|λ|)[: r]\n7:\nU = U[: , P]\n8:\nλ = λ[P]\n9:\nV = QU\n10:\nreturn V, λ\nSacriﬁcing accuracy for speed with Hermitian indeﬁnite matrices\nConverting from an ID.\n[HMT11, Algorithm 5.4] is a second approach to Hermitian eigendecomposition, based on postprocessing a low-rank row ID of A. We do\nnot include pseudocode for this algorithm in this monograph. However, the basic observation underlying the approach is that one can use the symmetry of A to\ncanonically approximate an initial row ID ˜A = ZA[I, :] ≈A by the Hermitian matrix ˆˆA = ZA[I, I]Z∗. The compact representation of this Hermitian matrix makes\nit easy to compute its eigendecomposition by a lossless process.\nWhen should one use [HMT11, Algorithm 5.4] over Algorithm 4? Our answer\nis the same as for using [HMT11, Algorithm 5.2] over Algorithm 3. That is, the\nID approach is only of interest when it moves A through fast memory once, and it\nshould be considered alongside other low-rank eigendecomposition algorithms with\nsimilar data movement patterns.\nSingle-pass algorithms.\nJust as with fast algorithms for low-rank SVD, one can\nobtain fast algorithms for low-rank Hermitian eigendecomposition by using Algorithm 4 with the QB methods based on type 1 or type 2 multi-sketching.\nBesides those approaches, we make note of [HMT11, Algorithm 5.6], which accesses A exclusively through a single sketch Y = AS and makes no assumptions\nPage 76\narXiv Version 2\n\n4.2. Drivers\nLow-rank Approximation\non the representation of A in-memory. This access pattern is possible because the\nalgorithm solves a least squares problem involving Y, orth(Y), and S to project a\nsmall k × k “core matrix” onto the set of Hermitian matrices.\nNystr¨om approximations for positive semideﬁnite matrices\nNow we suppose that the n × n matrix A is psd, in which case we can deﬁne the\nNystr¨om approximation of A with respect to a matrix X as\nˆA = (AX) (X∗AX)† (AX)∗.\n(4.20)\nWhen framed this way, the Nystr¨om approximation is deﬁned for any matrix X\nwith k ≤n columns. Indeed, it does not even presume that X is random. However,\nin RandNLA, we ultimately set X to a sketching operator and produce a compact\nspectral decomposition ˆA = V diag(λ)V∗. For any given type of sketching operator,\nlow-rank approximation of psd matrices by Nystr¨om approximations tend to be\nmore accurate than approximations produced by comparable algorithms for general\nHermitian eigendecomposition.\nWhat’s in a name?\nDisambiguating “Nystr¨om approximation.”\nThe literature on\nrandomized algorithms for Nystr¨om approximation heavily emphasizes using column selection operators [WS00; DM05; Pla05; KMT09a; KMT09b; LKL10; Bac13;\nGM16; RCR15; DKM20]. This stems from an analogy between sampling columns\nof kernel matrices in machine learning and the Nystr¨om method from integral equation theory. See [DM05, §5] for a detailed discussion. In view of the prominence\nof column sampling in Nystr¨om approximations, part of Section 6 is dedicated to\nspecialized methods for sampling columns from psd matrices.\nWhen X is a general sketching operator, the approximation (4.20) has been called\na “projection-based SPSD approximation” [GM16]. The diﬀerent terminology for\ngeneral sketching operators X is helpful for distinguishing the resulting approximations from those referred to as “Nystr¨om approximations” in the machine learning\nliterature. However, it is not in line with our philosophy that RandNLA concepts\nshould be described with minimal assumptions on the nature of the sketching distribution. This philosophy, ﬁrst advocated for by Drineas and Mahoney [DMM+11;\nMD16], leads us to adopt the following convention.\nThe term “Nystr¨om approximation” shall be used for any approximation\nof the form (4.20), even when X is a general sketching operator.\nWe note that this also follows the convention used by El Alaoui and Mahoney in\ntheir work on kernel ridge regression (see [AM15, Theorem 1]).\nAlgorithms.\nAlgorithm 5 is a practical approach to low-rank eigendecomposition\nby Nystr¨om approximation. It includes a function call TallSketchOpGen(A, k + s)\nwhich returns a sketching operator S with n rows and k + s columns. Our reason\nfor specifying a sketching operator in this way is to provide ﬂexibility in whether\nthe sketching operator is data-oblivious or data-aware. In this context, the main\ntype of data-aware sketching operator would be based on so-called power iteration;\nsee Section 4.3.1 and [GM16].\narXiv Version 2\nPage 77\n\nLow-rank Approximation\n4.2. Drivers\nAlgorithm 5 EVD2 : for psd matrices only; adapts [TYU+17a, Algorithm 3]\n1: function EVD2(A, k, s)\nInputs:\nA is an n×n psd matrix. The returned approximation will have rank\nat most k, but the sketching operator used in the algorithm can have\nrank as high as k + s.\nOutput:\nApproximations of the dominant eigenvectors and eigenvalues of A.\nAbstract subroutines:\nTallSketchOpGen generates a sketching operator with a prescribed\nnumber of columns, for use in sketching a given matrix from the\nright.\n2:\nS = TallSketchOpGen(A, k + s)\n3:\nY = AS\n4:\nν = √n · ϵmach · ∥Y∥# ϵmach is machine epsilon for current numeric type\n5:\nY = Y + νS\n# regularize for numerical stability\n6:\nR = chol(S∗Y) # R is upper-triangular and R∗R = S∗Y = S∗(A + νI)S\n7:\nB = Y(R∗)−1\n# B has n rows and k + s columns\n8:\nV, Σ, W∗= svd(B) # can discard W\n9:\nλ = diag(Σ2)\n# extract the diagonal\n10:\nr = min{k, number of entries in λ that are greater than ν}\n11:\nλ = λ[:r] −ν\n# undo regularization\n12:\nV = V[:, :r].\n13:\nreturn V, λ\nThe role of the parameter s in Algorithm 5 is analogous to that in Algorithms 3\nand 4 – the algorithm eﬀectively computes data needed for a rank k + s eigendecomposition before truncating that approximation to rank k. However, unlike\nAlgorithms 3 and 4, Algorithm 5 has no control of approximation error. We refer the reader to [FTU21, Algorithm E.2] for a more sophisticated version of this\nalgorithm which can accept an error tolerance.\n4.2.3\nMethods for CUR and two-sided ID\nHere we describe two approaches to CUR and one approach to two-sided ID. The\ndescriptions are largely qualitative in that they are stated in terms of algorithms\nfor low-rank column ID and CSS (which are detailed in Section 4.3.4).\nCUR by falling back on CSS\nPerhaps the simplest approach to CUR computes the row and column indices (I, J)\nin one stage and then computes the linking matrix U in a second stage. The column\nindices J are obtained by a randomized algorithm for CSS on A, then the row indices\nI are obtained by some CSS algorithm on C∗= A[:, J]∗.1 Because the matrix C is\n1Of course, this process could be reversed to compute I and then J.\nPage 78\narXiv Version 2\n\n4.2. Drivers\nLow-rank Approximation\nso much smaller than A, it is often practical to use a deterministic algorithm when\nperforming CSS on C∗.\nThere are two canonical choices for the linking matrix in this context: one\nobtained by projection\nUproj = (A[:, J])† A (A[I, :])†\nand one obtained by submatrix inversion\nUsub = A[I, J]†.\nIt should be clear that the approximation error incurred by using the former matrix\nwill never be larger than when using the latter. Furthermore, the process of computing the former matrix is better conditioned than the process of computing the\nlatter. Therefore it is generally preferable to use Uproj as the linking matrix when\nimplementing CUR via randomized CSS.\nRemark 4.2.2. Randomized algorithms for CUR based on the pattern above were\nﬁrst proposed in [DMM08; BMD09], particularly with linking matrices closer to\nthe form Usub. Deterministic analyses of CUR approximation quality with various\nlinking matrices can be found in [GZT95; GTZ97].\nCUR by a combination of column ID and CSS\nSuppose we have access to data (X, J) from a column ID of an initial low-rank\napproximation of A. Given this data, we can recover the row index set I and U for\na CUR decomposition by running CSS on C∗= A[:, J]∗and setting U = X (A[I, :])†.\nThis approach only requires the application of one pseudo-inverse, which compares favorably to the two applications of pseudo-inverses needed to compute Uproj\nin the ﬁrst approach to CUR. At the same time, if the randomized algorithm for\ncomputing (X, J) happens to return an interpolation matrix satisfying X = C†A,\nthen the resulting decomposition could have been obtained by the elementary CUR\nalgorithm with linking matrix Uproj. Therefore there is a sense in which this template algorithm generalizes the elementary approach to CUR.\nWe instantiate this template algorithm in Algorithm 6. The CSS step of the\nalgorithm calls a deterministic function for computing a QR decomposition with\ncolumn pivoting, with the semantics indicated in Table 1.1. Whether the algorithm\nstarts with a row ID or column ID depends on the aspect ratio of the data matrix;\n[MT20, §13.3] recommend starting with a row ID when A is wide in the related\ncontext of computing two-sided IDs.\nTwo-sided ID via one-sided ID\nTwo-sided IDs are canonically computed by a simple reduction to one-sided ID: ﬁrst\nobtain (X, J) by a column ID of A and then obtain (I, Z) by a row ID of A[:, J].\nThe initial column ID of A will be computed by a randomized algorithm and hence\nwill always be low-rank. However, it is not expensive to compute a full-rank row ID\nA[:, J] = ZA[I, J] by a deterministic method under the standard assumption that\n|J| ≪min{m, n}. Such an approach is described in [VM16, §2.4, §4].\nFinally, we note that a two-sided ID can be naturally repurposed for CUR\ndecomposition by either of the two qualitative approaches to CUR described above.\nIn the ﬁrst case one only needs the index sets (I, J) and computes the linking matrix\nby any desired method. In the second case one needs the index sets and one of the\narXiv Version 2\nPage 79\n\nLow-rank Approximation\n4.3. Computational routines\nAlgorithm 6 CURD1 : CUR by randomizing an initial ID [VM16; DM21b]\n1: function CURD1(A, k, s)\nInputs:\nA is an m × n matrix. The returned approximation will have rank at\nmost k. The ColumnID abstract subroutine can use sketching operators of rank up to k + s in its internal calculations.\nOutput:\nA low-rank CUR decomposition of A.\nAbstract subroutines:\nColumnID produces a low-rank column ID of a given matrix, up to\nsome speciﬁed rank.\n2:\nif m ≥n then\n3:\nX, J = ColumnID(A, k, s) # |J| = k and A[:, J]X ≈A\n4:\nQ, T, I = qrcp(A[:, J]∗)\n# only care about the indices I\n5:\nI = I[: k]\n6:\nU = X (A[I, :])†\n7:\nelse\n8:\nZ∗, I = ColumnID(A∗, k, s) # |I| = k and ZA[I, :] ≈A.\n9:\nQ, T, J = qrcp(A[I, :])\n# only care about the indices J\n10:\nJ = J[: k]\n11:\nU = (A[:, J])† Z\n12:\nreturn J, U, I\ninterpolation matrices (i.e., one of Z or X).\nThe latter approach for converting\nfrom two-sided ID to CUR is used in [VM16, §3 and §4]. General discussion on\nconverting from two-sided ID to CUR can be found in [Mar18, §11.2], [MT20,\n§13.2], and [DM21b, §4.1].\n4.3\nComputational routines\nThe last section explained how randomized algorithms for low-rank approximation\nexhibit a great deal of modularity. Here we summarize the design spaces for the\nconstituent modules.\nSections 4.3.1 to 4.3.4 cumulatively cover QB, column ID, CSS, and building\nblocks for the same. We acknowledge up-front that we treat column ID and CSS\nin far detail than we do QB. This imbalance is not a statement about the importance of column ID or CSS over QB. Rather, it stems from our desire to clarify\nbroader concepts surrounding column ID that can be diﬃcult to tease out from\nother literature.\nFrom there, Section 4.3.5 lists methods for norm estimation which are important\nfor solving low-rank approximation problems to ﬁxed accuracy. Our last topic in the\nrealm of computational routines for low-rank approximation is the notion of lowrank approximations from oblique projections (§4.3.6). This framework motivates\na type of low-rank approximation that is cheap to compute but that does not have\nmeaningfully-structured factors.\nPage 80\narXiv Version 2\n\n4.3. Computational routines\nLow-rank Approximation\nWe emphasize that this section mentions many algorithms for a wide variety\nof problems. Due to practical constraints we only address a handful of these algorithms in detail. Pseudocode for select algorithms can be found in Appendix C;\nthese algorithms have been selected based on some combination of their conceptual\nsigniﬁcance and their practicality.\n4.3.1\nPower iteration\nGiven a matrix A, suppose we sketch Y = AS using a very tall sketching operator S.\nIn a low-rank approximation context – regardless of whether we work with spectral\nor submatrix-oriented decompositions – it is generally preferable for range(Y) to be\nwell-aligned with the span of A’s dominant left singular vectors. This, in turn, is\nfacilitated by having range(S) be well-aligned with the span of A’s dominant right\nsingular vectors. To accomplish this, RandNLA libraries should include methods\nfor generating such sketching operators based on power iteration.\nA basic approach to power iteration makes alternating applications of A and A∗\nto an initial data-oblivious sketching operator So, to obtain a data-aware sketching\noperators such as\nS = (A∗A)q So\nor\nS = (A∗A)q A∗So,\n(4.21)\nfor some parameter q ≥0. Practical implementations need to incorporate some\nform of stabilization in between the successive applications of A and A∗).\nWe\ngive a general formulation of such a method in Appendix C.2.1 with Algorithm 8.\nNotably, this general method allows an arbitrary number of passes over the data\nmatrix (including zero passes, as an API convenience).\nRemark 4.3.1. The closest relative to Algorithm 8 in the literature is probably\n[ZM20, Algorithm 3.3]. However, the core idea behind this algorithm was explored\nearlier by Bjarkason [Bja19].\n4.3.2\nOrthogonal projections: QB and rangeﬁnders\nWe begin with two deﬁnitions.\nGiven a matrix A, a QB decomposition is given by a pair of matrices\n(Q, B) where Q is column-orthonormal and B = Q∗A. It is intended\nthat QB serve as an approximation of A. An algorithm that computes\nonly the factor Q from a QB decomposition is called a rangeﬁnder.\nThe value of QB decompositions stems from how they deﬁne approximations\nby orthogonal projection: ˆA = QB = QQ∗A. It is important to note that QB\nalgorithms do not necessarily ﬁrst compute Q in one phase and then set B = Q∗A\nin a second phase.\nIndeed, the beneﬁt of the rangeﬁnder abstraction is that it\nconsiders an equivalent problem while setting aside the potentially-nuanced matter\nof computing B.\nBefore proceeding to algorithms, we note that these concepts are not useful in\nthe “full-rank” setting. Consider, for example, when A has full row-rank. Here, any\northogonal matrix Q and accompanying B = Q∗A are valid outputs of rangeﬁnders\nand QB decomposition algorithms. Therefore full-rank QB decompositions can be\nentirely unstructured. Despite this caveat, QB decompositions are of fundamental\nimportance in randomized algorithms for low-rank approximation.\narXiv Version 2\nPage 81\n\nLow-rank Approximation\n4.3. Computational routines\nRangeﬁnder algorithms and basic QB\nThe simplest rangeﬁnders are based on power iteration.\nFor example, one can\nprepare a data-aware sketching operator S of the form (4.21), compute Y = AS,\nand then return Q = orth(Y); this is formalized as Algorithm 9 in Appendix C.2.2.\nMore advanced rangeﬁnders use block Krylov subspace methods. Speciﬁc examples\nof such rangeﬁnders can be found in [MM15], [Bja19, §7], and [MT20, §11.7].\nAlgorithm 10 in Appendix C.2.2 is the simplest approach to QB. It obtains Q\nby calling an abstract rangeﬁnder and obtains B by explicitly computing B = Q∗A.\nIterative QB algorithms\nThe most eﬀective QB algorithms work by building (Q, B) iteratively [MV16].\nGenerically, each iteration of such a QB method entails some number of matrixmatrix multiplications with A (as a rangeﬁnder step), adds a speciﬁed number of\ncolumns to Q and rows to B, and makes a suitable in-place update to A. Iterations\nterminate once some metric of approximation error A ≈QB (e.g., ∥A−QB∥2 or an\napproximation thereof) falls below a certain level.\nIterative QB methods were improved by [YGL18]. In particular, Algorithm 2 of\n[YGL18] does not modify A, it uses a power-iteration-based rangeﬁnder to compute\nnew blocks for (Q, B), and it eﬃciently updates the Frobenius error ∥A −QB∥F\nas the iterations proceed. This method is useful because it has complete control\nover the Frobenius norm error of the returned approximation.\nAlgorithm 11 in\nAppendix C.2.2 generalizes this method by allowing an abstract rangeﬁnder in the\niterative loop that updates (Q, B).\nMeanwhile, Algorithm 4 of [YGL18] performs power iteration before entering\nits main iterative loop, it does not access A while in the iterative loop, and it can\nterminate early if a target accuracy is met before a pre-speciﬁed rank-limit. This\nalgorithm has the advantage of reducing the number of times A is moved through\nfast memory. In fact, when power iteration is omitted, it can be implemented as\na single-pass method based on Type 1 multi-sketching. The downside is that this\nalgorithm may waste a substantial amount of work if the rank limit is much higher\nthan necessary. This downside is compounded when power iteration is omitted. We\nreproduce this method with slight modiﬁcations in Appendix C.2.2 as Algorithm 12.\nStopping criteria for iterative QB algorithms\nThe Frobenius norm is easily computed for sparse matrices and dense matrices that\nare stored explicitly in memory. However, it can be diﬃcult to compute for abstract\nlinear operators when the matrix is accessed only via matrix-vector multiplies, and\nthis can pose problems in computing QB decompositions to speciﬁed accuracy. One\napproach to address this situation is by careful application of a well-known randomized Frobenius norm estimator as part of the QB decomposition [GCG+19, §3.4,\n§3.5, Eq. (3.26)]. We also note that those looking for high-quality approximations\noften prefer that error be bounded in spectral norm (and only use the Frobenius\nnorm because it is usually very cheap to compute). The problem of estimating spectral norms is well-studied in the NLA literature. Section 4.3.5 reviews randomized\nalgorithms for estimating matrix norms.\nPage 82\narXiv Version 2\n\n4.3. Computational routines\nLow-rank Approximation\nApproximate single-pass QB via Type 2 multi-sketching\nRecall that a Type 2 multi-sketch of A is a sketch of the form (Y1, Y2) = (AS1, S2A)\nfor independent sketching operators (S1, S2). This sketch can be used to compute a\nQB decomposition that is approximate, in the sense that QB ≈A holds for columnorthonormal Q, but we drop the hard requirement that B = Q∗A.\nPut simply, the method is to compute Q = orth(Y1) and then B = (S2Q)†Y2.\nThe intuition behind this approach is that if QQ∗A is a good approximation for A,\nthen we would have Y2 ≈S2QQ∗A, which would imply B ≈Q∗A. We refer the\nreader to [TYU+17b, §4.2] for a proper explanation of this method.\n4.3.3\nColumn-pivoted matrix decompositions\nThroughout this section we work with an r × c matrix G. As before, we begin with\nsome deﬁnitions.\nA column-pivoted decomposition of G is any decomposition of the form\nGP = FT\n(4.22)\nwhere P is a permutation matrix and T is upper-triangular. The permutation matrix is encoded by a vector of pivots, J, so that G[:, J] = GP.\nThere are many ways of producing such decompositions. We are only interested in\nthe ways where rank-k matrices obtained by truncation\nˆG := (F[:, :k])(T[:k, :])P∗\n(4.23)\nprovide reasonably good rank-k approximations of G. The meaning of “reasonably\ngood” is subjective. It depends on the computational cost of the algorithm, and\nit depends on how well ˆG approximates G compared to the best approximation\nobtained by a truncated column-pivoted matrix decomposition. Interestingly, some\nrandomized algorithms for CSS (see §4.3.4) do not use the factors that appear in\n(4.23); instead, these algorithms only care that the pivots J could make approximations form (4.23) reasonably accurate.\nHow much do we truncate?\nWhen a randomized algorithm uses this primitive for\nlow-rank approximation, the matrix G is usually a sketch of the target data matrix\nA, and k is close to min{r, c}. It helps to consider diﬀerent situations when trying\nto build intuition for why these randomized algorithms work. Speciﬁcally, it helps\nto consider when G is equal to A, or a low-rank approximation thereof. In both\nsuch cases one should have k ≪min{r, c}.\nBasics of column-pivoted decomposition algorithms\nThere are two main families of algorithms for producing these decompositions: those\nbased on QR with column pivoting (QRCP) and those based on LU with partial\npivoting (LUPP).2 Algorithms in the former family deﬁne the decomposition (4.22)\nin the natural way, where F is column-orthonormal. For algorithms in the latter\nfamily, one must consider how pivoted LU traditionally uses row pivoting. Therefore,\n2The standard process of computing an LU decomposition with partial pivoting is called Gaussian elimination with partial pivoting and is abbreviated as GEPP.\narXiv Version 2\nPage 83\n\nLow-rank Approximation\n4.3. Computational routines\nto compute (4.22) via LUPP, one must compute the transposed factors (P∗, F∗, T∗)\nin a row-pivoted decomposition,\nP∗G∗= T∗F∗,\nwhere T∗is lower-triangular (with unit diagonal) and F∗is upper-triangular.\nRoughly speaking, QRCP-based algorithms prioritize accuracy, while LUPPbased algorithms prioritize speed. The extent to which a speciﬁc algorithm does\nwell on these performance metrics depends on the algorithm’s pivoting rule. The\nmost widely used QRCP-based methods use the same pivoting rule as LAPACK’s\nGEQP3. Meanwhile, the most widely used LUPP-based methods use the pivoting\nrule from LAPACK’s GETRF. It is valid to rely on either of these functions for the\ncolumn-pivoted decomposition steps that arise in randomized algorithms. However,\none should be aware of two potential sources of error when using GETRF for a columnpivoted decomposition rather than GEQP3.\nWhat can we expect of GETRF’s pivots?\nWhen GETRF is applied to G∗, the process of\ncomputing the pivots up to and including the ℓth pivot makes decisions based only\non the ﬁrst ℓrows of G. Therefore it is unwise to use GETRF unless one has reason\nto believe that the information in G’s trailing r −k rows would not drastically alter\nthe columns chosen as pivots based on the ﬁrst k rows. Similarly, it is unwise to use\nˆG as an approximation of G when k ≪min{r, c}, since this would suppose that G’s\nleading k rows are signiﬁcantly more important than all others. One is most likely\nto ﬁnd meaningful information in the column-pivoted LU decomposition when G is\nvery wide (r ≪c) and k is close to r.\nWhat isn’t in the pivots?\nSuppose F has w = min{r, c} columns. It is easy to verify\nthat for any nonsingular upper-triangular matrix U of order w, the decomposition\nproduced after a change-of-basis\n(F, T) ←(FU−1, UT)\nwill preserve (4.22) for the same permutation matrix P.\nIt is informative to consider how such changes of basis aﬀect ˆG. For example,\nin simplest case, it is easy to see that ˆG would not change if U were diagonal.\nThis simple case shows that conditioning of the factors (F, T) is unimportant in our\nformalism of column-pivoted decomposition.\nTo speak to a more interesting case, let us partition F and T into blocks [F1, F2]\nand [T1; T2] so that F1 has k columns and T1 has k rows. Straightforward calculations show that\n∥G −F1T1P∗∥= ∥F2T2∥\n(4.24)\nholds in any unitarily-invariant norm. Meanwhile, less straightforward calculations3\nshow that there is always an upper-triangular nonsingular matrix U for which\n∥G −(FU−1)[:, :k](UT[:k, :])∥= ∥(I −F1F†\n1)F2T2∥≤∥F2T2∥.\n(4.25)\nNote that if there is substantial overlap between range(F1) and range(F2), then the\ninequality in (4.25) will be strict by a signiﬁcant margin. Therefore if accuracy of\nthe approximation (4.23) is important, then our decomposition should make sure\nthat the columns of F are orthogonal to one another. This is ensured by QR-based\nmethods, but it is not ensured by LU-based methods.\n3See Proposition C.1.3 in Appendix C.1.2.\nPage 84\narXiv Version 2\n\n4.3. Computational routines\nLow-rank Approximation\nPartial decompositions\nStandard algorithms for computing a column-pivoted decomposition of an r × c\nmatrix require Θ(min{rc2, cr2}) operations. One can get away with spending less\neﬀort when only a partial decomposition is needed. Formally, in a (k-step) partial\ncolumn-pivoted decomposition, we relax the requirement that T be triangular. Instead, we require that T can be partitioned into a 2-by-2 block triangular matrix\nwhere the upper-left block is k × k and triangular in the proper sense.\nThe aforementioned standard algorithms for column-pivoted decomposition can\nbe modiﬁed to compute k-step partial decompositions of r × c matrices in Θ(rck)\noperations. There are plans for a version of LAPACK subsequent to 3.10 to support\nthis functionality as it pertains to QRCP.4 At a practical level, it is certainly worth\nusing this functionality when it is available.\nHowever, this functionality is not\ncritical, since randomized algorithms for low-rank approximation rarely need to\ncompute k-step partial decompositions with k ≪min{r, c}.\nMore details on column-pivoted decomposition algorithms\nThe pivots chosen by the strong rank-revealing QR (strong RRQR) algorithm from\n[GE96] lead to the best theoretical guarantees for low-rank approximation by a partial column-pivoted QR decomposition. In practice, it is more common to truncate\nthe output of LAPACK’s GEQP3, which is faster than strong RRQR.\nAlgorithms based on LUPP are typically faster than those based on pivoted\nQR. While the LUPP approach comes with weaker guarantees (as explained above),\nthese limitations are less signiﬁcant in a randomized context where we seek nearly\nfull-rank decompositions of wide sketches. Indeed, there is little practical diﬀerence in solution quality between LUPP-based and QRCP-based versions of some\nrandomized algorithms for CSS and column ID [Mar22b; DM21b].\nOther possibilities for column-pivoted matrix decomposition include LU or QR\nwith tournament pivoting [GDX11; DGG+15]. Algorithms based on tournament\npivoting exhibit reduced communication and hence can be more eﬃcient without\nsigniﬁcant loss of accuracy.\nFinally, we note that Section 5.1.2 includes a randomized algorithm for full-rank\nQRCP. It is easy enough to modify that algorithm to support early termination.\nSome variants of this algorithm speciﬁcally focus on low-rank approximation (e.g.,\nthe SRQR algorithm from [XGL17]).\n4.3.4\nOne-sided ID and CSS\nColumn ID and CSS are nearly equivalent problems. That is, a method for CSS\ncan canonically be extended to a method for column ID by taking X = (A[:, J])†A.\nConversely, a method for column ID can be adapted to CSS by discarding any\ncalculations that are only needed to form X. This section covers deterministic and\nrandomized algorithms for both of these problems. Readers who are particularly\ninterested in theoretical aspects of these algorithms should consult [BMD09].5\n4See https://github.com/Reference-LAPACK/lapack/issues/661.\n5We frame all of our discussion of one-sided ID around column ID, rather than row ID.\narXiv Version 2\nPage 85\n\nLow-rank Approximation\n4.3. Computational routines\nTemplate deterministic algorithms\nSuppose we want to compute a rank-k column ID of an r × c matrix G. There is a\ntemplate deterministic algorithm for handling this problem based on the notion of\ncolumn-pivoted decompositions, as discussed in Section 4.3.3.\nThe template algorithm works in two phases.\nThe ﬁrst phase produces the\ndecomposition GP = FT where P is a permutation matrix and T is upper-triangular.\nThe second phase is a matter of simple postprocessing. The postprocessing begins\nby partitioning T into a 2-by-2 block triangular matrix\nT =\n\u0014\nT11\nT12\n0\nT22\n\u0015\n.\nwhere T11 is k × k and triangular in the usual sense.\nFrom here, one sets the\ninterpolation matrix to X = [Ik×k, T−1\n11 T12]P∗, and one sets the skeleton indices to\nthe vector J that provides X[:, J] = Ik.\nThe importance of this algorithm stems from how its output can equivalently be\nanalyzed as a truncated column-pivoted matrix decomposition. That is, the column\nID induced by (J, X) satisﬁes\nG[:, J]X = F[:, :k]T[:k, :]P∗.\nWe can therefore gain insights into the behavior of this column ID algorithm by\nappealing to results such as Proposition C.1.3, which we alluded to earlier in our\ndiscussion of LU and QR based pivoting methods.\nThis approach to column ID is illustrated more formally as Algorithm 13 in\nAppendix C.2.3. It is easy to see that the CSS version of this algorithm does not\nneed to compute X, since the deﬁnition of X implies that J can be determined from\nthe ﬁrst k rows of P.\nRandomized algorithms\nWe list ﬁve randomized algorithms for CSS and column ID below.\nWith some\nexception for the ﬁfth algorithm, we do not comment on the theoretical guarantees\nof these methods.\n1. For CSS, one can sample columns with probability proportional to their\nnorms, where column norms are updated by projecting out selected columns\nas a QRCP-like factorization proceeds [DV06]. Applying the standard postprocessing scheme to the (partial) factorization yields the interpolation matrix\nX needed for a column ID.\n2. Also for CSS, one can sample columns according to a probability distribution\nrelated to so-called leverage scores of the matrix under consideration.\nWe\ndiscuss leverage score sampling in detail in Section 6. For now, we note that\nthis approach especially useful for computing Nystr¨om approximations.\n3. The algorithm in [BMD09] approaches CSS with a combination of leverage\nscore sampling and postprocessing by deterministic QRCP. The factorization\nproduced by this postprocessing can be processed further to produce the interpolation matrix for a column ID.\nPage 86\narXiv Version 2\n\n4.3. Computational routines\nLow-rank Approximation\n4. [XGL17, §V.D] suggests solving CSS by taking the pivots from a randomized\nalgorithm for QRCP. The output of the randomized algorithm for QRCP can,\nof course, be processed to recover the interpolation matrix for a column ID.\n5. [VM16, §5.1] approaches low-rank column ID by computing a (nearly) fullrank column ID of a sketch Y = SA. The unmodiﬁed data (X, J) is used to\ndeﬁne the low-rank column ID A[:, J]X ≈A.\nThe last of these methods is simple and practical. It appears with slight modiﬁcations in Appendix C.2.3 as Algorithm 14, while the corresponding CSS version\nappears as Algorithm 15. For both the column ID and CSS versions, it is recommended that S be a data-aware sketching operator based on power iteration. To\ngain intuition for this method, one should ﬁrst verify that if (X, J) deﬁnes a fullrank column ID of Y, then it also deﬁnes a full-rank column ID of ˜A = (AY†)Y.\nWith that given, we can apply Proposition 4.1.3 to see that the induced low-rank\ncolumn ID satisﬁes an error bound\n∥A −A[:, J]X∥2 ≤(1 + ∥X∥2)∥A −˜A∥2.\nThis bound is noteworthy for the following reason: using power iteration to prepare\na data-aware sketching operator S will drive ˜A closer to a rank-k approximation of\nA obtained by a truncated SVD. That, in turn, would give ∥A −˜A∥2 ≈σk+1(A).\nRemark 4.3.2. The value of power iteration in the context of CSS / column ID and\nin the context of rangeﬁnders / QB is a key reason for considering power iteration\nas a basic primitive of RandNLA.\nOn ﬁxed-accuracy one-sided ID\nStandard implementations of deterministic QRCP-based algorithms for one-sided\nID can compute approximations to speciﬁed accuracy. Randomized algorithms for\nlow-rank one-sided ID do not possess this capability to the same extent. In some\nrespects, this is a principal disadvantage of low-rank approximation by ID compared\nto QB. However, there are partial workarounds.\nFor example, suppose we approximate A via a QB decomposition, ˜A = QB. If\nwe computed (X, J) by a full-rank column ID of B, then we would also have a fullrank column ID of ˜A. If X was obtained by the standard postprocessing of output\nfrom strong rank-revealing QR, then we would have |Xij| ≤2. A straightforward\napplication of Proposition 4.1.3 shows that if we had\n∥A −QB∥2 ≤\nϵ\n(1 +\np\n1 + 4k(n −k))\n(4.26)\nfor a rank-k QB decomposition, then we could be certain that ∥A −A[:, J]X∥2\nwas at most ϵ. Therefore, in principle, one could compute the QB decomposition\niteratively, and only compute the ID of B once (4.26) is satisﬁed.\nThe above approach is not without its shortcomings. For one thing, reducing\n∥A−QB∥2 entails increasing k, and so the termination criterion is a moving target.\nAs another issue, it needs to bound spectral norms of implicitly represented linear\noperators. We address the problem of estimating matrix norms next.\narXiv Version 2\nPage 87\n\nLow-rank Approximation\n4.3. Computational routines\n4.3.5\nEstimating matrix norms\nNorm estimation plays an important role in stopping criteria for iterative low-rank\napproximation algorithms, particularly for QB and Nystr¨om approximations. Here\nwe summarize methods that would be appropriate for expensive norms or norms of\nabstract linear operators that are only accessible by matrix-vector multiplications.\nRemark 4.3.3. The material presented here is covered in greater detail in [HMT11,\n§4.3 - §4.4] and [MT20, §5 - §6, §12.0 - §12.4].\nA cheap spectral norm bound.\nLet the vectors z1, . . . , zr ∈Rn be vectors with\ncomponents drawn iid from the standard normal distribution and let β > 1 be a\ntuning parameter. Then, for any A, it is known that the inequality\n∥A∥2 ≤β\nr\n2\nπ max\nj∈JrK ∥Azj∥2\n(4.27)\nholds with probability at least 1 −β−r [HMT11; WLR+08].\nFurthermore, this\nbound is easy to compute because the necessary vectors Azj can be formed with a\nsingle matrix-matrix product with A.\nA basic Frobenius norm estimator.\nLet Z ∈Rn×r be the matrix whose columns\nare the random vectors z1, . . . , zr mentioned above. Then, it turns out that the\nquantity 1\nr∥AZ∥2\nF is an unbiased estimate for the squared Frobenius norm, in the\nsense that\nE\nh\n1\nr∥AZ∥2\nF\ni\n= ∥A∥2\nF.\n(4.28)\nIn addition to being unbiased, the variance of the error estimate can also be controlled according to\nvar\n\u0010\n1\nr∥AZ∥2\nF\n\u0011\n≤\n2\nr∥A∥2\n2∥A∥2\nF,\n(4.29)\nas shown in [Gir89]. Hence, as long as r is suﬃciently large, then the error estimate\n1\nr∥AZ∥2\nF is likely to be close to ∥A∥2\nF. From a computational standpoint, this error\nestimate is similar to the one described above for the spectral norm, insofar as it\nonly requires r matrix-vector products with A.\nA cheap Schatten p-norm estimator.\nLetting σ denote the vector of singular values\nof A, the Schatten 2p-norm of A is ∥A∥(S,2p) :=\n\u0010Pmin{m,n}\ni=1\nσ2p\ni\n\u00111/2p\n.\nTaking\np = 1 reduces to the Frobenius norm. The spectral norm is obtained in the limit\nas p →∞. In fact, deterministic bounds show that the spectral norm and Schatten\np-norm more or less coincide when p ≳log min{m, n}.\nThe Kong-Valiant estimator [KV17b] can be used to cheaply estimate these\nnorms. It only accesses A by multiplication with an n × k data-oblivious sketching\noperator, where k can be materially smaller than min{m, n}. See [MT20, §5.4] for\na statement of the algorithm and remarks on its theoretical guarantees.\nAccurate spectral norm estimators.\nThere is a large literature on deterministic and\nrandomized algorithms for estimating spectral norms. Much of this literature is\nbased on methods designed for estimating the largest eigenvalue of a positive deﬁnite\nmatrix (which can naively be applied since\np\n∥A∗A∥2 = ∥A∥2). Most notably, Dixon\nPage 88\narXiv Version 2\n\n4.4. Other low-rank approximations\nLow-rank Approximation\nwas the ﬁrst to study the randomized power method [Dix83], and Kuczy´nski and\nWo´zniakowski were the ﬁrst to study randomized Lanczos methods [KW92]. See\n[MT20, Algorithm 5] for a basic randomized Lanczos method and the subsequent\nremarks on block randomized Lanczos [MT20, §6.5].\n4.3.6\nOblique projections\nLow-rank approximations can be expressed in a manner resembling the triple-sketch\nfrom Section 2.6. For sketching operators S1 ∈Rn×k and S2 ∈Rd×m, we can deﬁne\nˆA = AS1(S2AS1)†S2A = Y1Y†\n3Y2,\nwhere\nY1 = AS1,\nY2 = S2A,\nand\nY3 = S2AS1.\nThis construction obtains each column of ˆA by projecting the corresponding column\nof A onto the range of Y1, where the projection is orthogonal with respect to\nthe possibly degenerate inner product (u, v) 7→⟨S2u, S2v⟩. We call ˆA an oblique\nprojection of A.\nThe simplest oblique projections use column and row selection operators for\n(S1, S2). This provides a CUR decomposition where Y†\n3 is the linking matrix U.\nThe connection to CUR foreshadows a more general fact: the sketching operators\nused in oblique projection are not necessarily independent of one another [DMM08].\nAn example in this regard is that Nystr¨om approximations amount to oblique projections that use S2 = S∗\n1.\nIt is natural to consider oblique projections where S1 and S2 are independent\n(e.g., independent Gaussian operators). Such approximations can entail extremely\nill-conditioned computations if one is not careful.\nThis ill-conditioning can be\navoided through the numerically stable approach described by Nakatsukasa [Nak20].\nThese approximations employ oversampling for S2 (relative to S1) and split Y3 (or\na regularized variant thereof) into two factors. The representation returned by this\napproach consists of four matrices.\nHistorical notes\nOblique projections for low-rank approximation are closely related to the rank reduction formula described in [CFG95]. Drineas et al. ﬁrst used oblique projections\nfor low-rank approximation via CUR decomposition [DMM08], wherein S1, S2 are\ncolumn and row selection matrices respectively. Clarkson and Woodruﬀpioneered\nthe use of general oblique projections in randomized algorithms for low-rank approximation [CW09, Theorem 4.7]. Oblique projections have since been discussed\nin the context of a generalized LU factorization [DGR19].\n4.4\nOther low-rank approximations\nHere we review a handful of other low-rank approximation problems and algorithms,\nparticularly speaking to our development plans for RandLAPACK.\narXiv Version 2\nPage 89\n\nLow-rank Approximation\n4.4. Other low-rank approximations\nDomain-speciﬁc representations.\nSeveral low-rank approximation problems of interest involve specialized factorizations. We plan for RandLAPACK to eventually support nonnegative matrix factorization [EMW+18], dynamic mode decomposition\n(DMD) [EMK+19; EBK19], and possibly sparse PCA [EZM+20]. Among these\nmethods, we expect that DMD will have highest priority, since full-rank DMD is\nslated for inclusion into LAPACK in the near future [Drm22]. For a general introduction to DMD we refer the reader to [TRL+14].\nLow-rank Cholesky.\nAs a separate topic, there is also a longstanding algorithm\nfor “low-rank Cholesky” decompositions [XG16].\nWe are unsure of its eventual\nrole in RandLAPACK, since a representation of the form ˆA = LL∗for a very tall\nlower-triangular matrix L oﬀers almost no beneﬁt over L being dense.\nStill, it\nwill be considered in the near future alongside the recently proposed algorithm by\n[CET+22] for randomly pivoted partial Cholesky decomposition.\nLow-rank QR.\nSuppose A is a large full column-rank matrix with QR decomposition A = QR.\nThis decomposition has two especially prominent uses: (1) it\nfacilitates application of a pseudoinverse A†v = R−1Q∗v in O(mn) time, and (2) it\ncan be used as preprocessing for more complicated orthogonal decompositions such\nas SVD. Unfortunately, low-rank QR decomposition, which is simply the economic\nQR decomposition of a rank-k approximation of A, does not fully realize either of\nthese use-cases.\nThe trouble with low-rank QR is that a k × n upper-triangular matrix with\nk ≪n is eﬀectively a full matrix. That is, the mere representation of a low-rank\nmatrix by a QR decomposition is not much more useful than representation by QB\ndecomposition. Note also that unpivoted QR makes no eﬀort to produce a rankrevealing representation, compared to pivoted QR. Therefore RandLAPACK will not\noﬀer methods for low-rank approximation by unpivoted QR.\nLow-rank UTV.\nA UTV decomposition ˆA = UTV∗uses column-orthogonal matrices U, V and a triangular matrix T. UTV (also called QLP) can be thought of as a\ncheaper alternative to SVD. As we discuss in the next section, RandLAPACK might\ninclude algorithms for UTV when ˆA is full-rank [GM18; MQH19; KCL21]. Some of\nthose algorithms (e.g., that in [MQH19]) proceed iteratively and can be terminated\nearly. If RandLAPACK supports full-rank UTV by such an algorithm then it will\nexpose the low-rank variant.\nSeveral algorithms for producing low-rank approximations represented by UTV\nare given in [DG17; FXG19; WX20; RB20; KC21]. We would need a better understanding of those methods, particularly how they compare to our planned methods\nfor low-rank SVD, before making decisions on which of them to support.\nLow-rank LU.\nLU is central to solving systems of linear equations in the full-rank\ncase.\nThere is a small literature on low-rank LU within the ﬁeld of RandNLA:\n[SSA+18; DGR19; ZM20]. In RandLAPACK we anticipate restricting our attention\nto algorithms that are related to a Gaussian elimination process (that is, where the\nerror matrix can be expressed as a Schur complement of a block matrix), along the\nlines of [DGR19]. These algorithms are likely more useful for low-rank approximation with a ﬁxed accuracy requirement rather than with a ﬁxed rank requirement.\nThey are based on an oblique projection with k = d, that is S2AS1 is square.\nPage 90\narXiv Version 2\n\n4.5. Existing libraries\nLow-rank Approximation\nRandLAPACK might include the LU algorithms from [SSA+18] if they can\nbe proven to be signiﬁcantly faster than high-quality implementations of QB algorithms.\nIf proven useful, we will consider in the future generalized LU-based\nlow-rank approximation, as introduced in [DGR19]. The algorithms for low-rank\nLU in [ZM20] are based on QB and so are unlikely to be included in RandLAPACK.\n4.5\nExisting libraries\nHere we review established numerical libraries that support randomized low-rank\napproximation. All of the libraries that focus on RandNLA (save for one) implement\nadvanced sketching operators such as SRFTs.\nNLA and data science packages that use RandNLA.\nThere are a few packages for\nNLA that include a method for low-rank SVD based on QB decomposition:\n• MLSVD RSI in the Tensorlab MATLAB toolbox\n• rsvd in SciKit-CUDA,\n• cusolverDnXgesvdr in NVIDIA’s cuSOLVE,\n• and randomized svd in SciKit-Learn.\nThe last of these functions warrants special emphasis. SciKit-Learn’s pca function\nactually defaults to randomized svd for suﬃciently large matrices [Gri16]. In this\nway, one of the most important functions in the most widely-used Python package\nfor data science already relies on RandNLA.\nID.\nID is a Fortran library for ID/CUR [MRS+14]. It is callable as part of the\nSciPy Python library. ID provides indirect support for SVD as part of its methods\nfor converting one low-rank factorization into another. It also includes routines for\nrank estimation and norm estimation. RandLAPACK will include many of these\nsame utilities as ID while expanding its scope of driver-level functions.\nRSVDPACK.\nRSVDPACK is a C and MATLAB library for low-rank SVD and\nID/CUR [VM15]. It is callable after building from source code which is provided on\nGitHub. Its SVD algorithms are based on a particular QB implementation [VM15,\n§3.4] and its ID/CUR algorithms follow [VM16]. RSVDPACK comes in diﬀerent\nimplementations which target diﬀerent architectures.\nBy comparison, RandLAPACK will take more general approaches to QB and\nID/CUR, and it will include methods for other factorizations such as eigendecomposition via Nystr¨om approximations. RandLAPACK will target diﬀerent architectures\nby building on LAPACK++ as a portability layer [GLA+17].6\nRistretto.\nRistretto is available on the Python Package Index. This library is based\non the rsvd package implemented in R [EVB+19].\nIt supports low rank SVD,\nID/CUR, LU, Nystr¨om, PCA, Hermitian eigendecomposition, nonnegative matrix\nfactorization [EMW+18], dynamic mode decomposition [EMK+19; EBK19; ED16],\n6This library is developed as part of SLATE [KWG+17; AAB+17].\narXiv Version 2\nPage 91\n\nLow-rank Approximation\n4.5. Existing libraries\nand sparse PCA [EZM+20]. One algorithm is provided for each distinct type of\nfactorization. Many of these algorithms are based on QB [EVB+19, §3.3], while its\nID/CUR algorithms also follow [VM16]. This library has also been demonstrated\nto be useful for ﬁnding patterns in large-scale climate data [VEK+19], and for\nproviding routines for randomized tensor decompositions [EMB+20].\nWe plan for RandLAPACK to eventually support the same range of factorizations\nas Ristretto (with the exception of low-rank LU). However, our priority is to focus\non the factorizations in Section 4.2, and to oﬀer a range of algorithms for computing\neach of these decompositions. Our longer-term plans include making RandLAPACK’s\nC++ implementation callable from Python.\nLibSkylark.\nLibSkylark [KAI+15] is written in C++ and callable after installing\nfrom source, which is available on GitHub.\nTo our knowledge, it is the only\nRandNLA library that supports both least squares and low-rank approximation.\nIts low-rank approximation functionality is restricted to SVD through a QB approach. See Section 3.5 for its least squares functionality.\nLowRankApprox.jl.\nLowRankApprox.jl is a Julia library for low-rank SVD, QR, ID,\nCUR, and Hermitian eigendecomposition. It is callable after installation with the\nJulia package manager. Most of its algorithms are based on ﬁrst computing an ID,\nrather than a QB decomposition. Note that this is quite diﬀerent from the plans\nwe have outlined for RandLAPACK over Sections 4.2 and 4.3.\nOther implementations.\nThe many algorithms considered in [Bja19] are accompanied by Python implementations hosted on GitHub.\nThe RandNLA tutorial\n[Wan15] covers a wide range of algorithms for low-rank approximation and hosts\nsome MATLAB implementations on GitHub.\nPage 92\narXiv Version 2\n\nSection 5\nFurther Possibilities for Drivers\n5.1 Multi-purpose matrix decompositions .........................\n94\n5.1.1 QR decomposition of tall matrices ...............................\n94\n5.1.2 QR decomposition with column pivoting ......................\n95\n5.1.3 UTV, URV, and QLP decompositions ..........................\n98\n5.2 Solving unstructured linear systems ........................... 100\n5.2.1 Direct methods ......................................................... 100\n5.2.2 Iterative methods ...................................................... 102\n5.3 Trace estimation ......................................................... 104\n5.3.1 Trace estimation by sampling ..................................... 104\n5.3.2 Trace estimation with help from low-rank approximation\n105\n5.3.3 Estimating the trace of f(B) via integral quadrature ...... 107\nThis section covers multi-purpose matrix decompositions, the solution of unstructured linear systems, and trace estimation. These are the last problems we\ncover that might be handled by “drivers” in a high-level RandNLA library. We emphasize that this monograph does not exhaust the set of prominent linear algebra\nproblems that are amenable to randomization. We make no eﬀort to cover randomized algorithms for general eigenvalue problems, nor do we cover randomized\nalgorithms for computing the action of matrices produced from matrix functions\n(i.e., computing f(A)b for an analytic matrix function f), even though there are\neﬀective algorithms for both of these problems [NT21; GS22; CKN22].\nWe have chosen the topics of this section because they require comparatively\nlittle background material to state, and we believe our summary of the relevant\nalgorithms has some contribution to the literature. For example, the key contribution from Section 5.1 is a novel algorithm for QR with column pivoting based on\nCholesky QR. The algorithm is notable for its ability to handle ill-conditioned or\neven outright rank-deﬁcient matrices. The contributions from Section 5.2 include\ndetailed introductions to recently-developed iterative methods for solving general\nlinear systems. Finally, our coverage of trace estimation in Section 5.3, includes\nstate-of-the-art algorithms and implementations that were not available when earlier RandNLA surveys were published.\n93\n\nFurther Possibilities for Drivers\n5.1. Multi-purpose matrix decompositions\n5.1\nMulti-purpose matrix decompositions\nEarly in the year 2000, the IEEE publication Computing in Science & Engineering\npublished a list of the top ten algorithms of the twentieth century. Among this list\nwas the decompositional approach to matrix computation, on which G. W. Stewart\ngave the following remark.\nThe underlying principle of the decompositional approach of matrix\ncomputation is that it is not the business of matrix algorithmists to\nsolve particular problems but to construct computational platforms from\nwhich a variety of problems can be solved. This approach, which was in\nfull swing by the mid-1960s, has revolutionized matrix computation.\nThis section covers three decompositions that provide broad platforms for problem solving. They are addressed in an order where randomization oﬀers increasing\nbeneﬁts over purely deterministic algorithms. We note in advance that these randomized algorithms do not aim for an asymptotic speedup over deterministic methods. Rather, the aim is to signiﬁcantly reduce time-to-solution by taking better\nadvantage of modern computing hardware.\n5.1.1\nQR decomposition of tall matrices\nAlgorithms for computing unpivoted QR decompositions are true workhorses of\nnumerical linear algebra.\nThey are the foundation for the preferred algorithms\nfor solving least squares problems with full-rank data matrices. They are also an\nimportant ingredient in preprocessing for more expensive algorithms.\nFor example, suppose we want to decompose a very tall m × n matrix A via QR\nwith column pivoting. The instinctive thing to do here is to reach for the LAPACK\nfunction GEQP3. However, on modern machines, it is much faster to compute an\nunpivoted decomposition A = QR, and then run GEQP3 on R. The ﬁnal decomposition would be mathematically equivalent to calling GEQP3 directly on A, just\nrepresented in a diﬀerent format.\nWith this signiﬁcance of unpivoted QR in mind, we brieﬂy cover two types of\nrandomized algorithms for computing such decompositions.\nOrthogonality in the standard inner product\nCholesky QR is a method for computing unpivoted QR decompositions of matrices\nwith linearly independent columns. It is based on the following elementary observation: given a QR decomposition A = QR of a full-column-rank matrix A, the\nfactor R is simply the upper-triangular Cholesky factor of the Gram matrix A∗A.\nTherefore in principle one can compute a QR decomposition as follows.\n1. Compute a Cholesky decomposition of the Gram matrix A∗A = R∗R.\n2. Perform a matrix-matrix triangular solve to obtain Q = AR−1.\nImplementing Cholesky QR only requires three functions: syrk from BLAS, potrf\nfrom LAPACK, and trsm from BLAS. Standard implementations of these functions\nparallelize extremely well. As a result, Cholesky QR can oﬀer substantial speedups\nover Householder QR (and even Tall-and-Skinny QR [DGG+15]) for very tall matrices on modern machines.\nPage 94\narXiv Version 2\n\n5.1. Multi-purpose matrix decompositions\nFurther Possibilities for Drivers\nDespite the speed advantage of Cholesky QR, it is rarely used in practice, since\nit is unsuitable for even moderately ill-conditioned matrices. Recently it has been\nshown that randomization can overcome this limitation by preconditioning Cholesky\nQR to ensure stability [FGL21]. For detailed analysis of this method we refer the\nreader to the results in [Bal22b] on the algorithm called “RCholeskyQR2.”\nIn Section 5.1.2 we extend this methodology to rank-deﬁcient matrices, and we\nconnect it to an existing randomized algorithm for QRCP of general matrices.\nOrthogonality in a sketched inner product\nIn [BG22], Balabanov and Grigori propose a randomized Gram–Schmidt (RGS)\nprocess that orthogonalizes n vectors in Rm with respect to a sketched inner product\n⟨u, v⟩S = (Su)∗(Sv).\n(5.1)\nWe call such vectors S-orthogonal or sketch-orthogonal. When [BG22, Algorithm 2]\nis run on the columns of a matrix A, values computed during sketched projections\nare assembled in an upper-triangular matrix R so that A = QR and (SQ)∗(SQ) = In.\nOne can choose the distribution from which S is drawn so that Q will be nearlyorthonormal with respect to the standard inner product, with high probability.\nEmpirical and theoretical results show RGS is faster than classic Gram–Schmidt\nbut as stable as modiﬁed Gram–Schmidt.\nThe idea of computing QR decompositions where Q is sketch-orthogonal can be\ntaken in several directions. For example, a block version of RGS is proposed and\nanalyzed in [BG21]. Taking this approach to the extreme where the block size is the\nnumber of columns in the matrix, one can compute the factor R by Householder QR\non SA and then represent Q = AR−1 as a linear operator. Note that this procedure\nis the essence of sketch-and-precondition for least squares, as proposed in [RT08].\nA detailed numerical analysis of this last method can be found in [Bal22b], where\nthe algorithm is called RCholeskyQR.\n5.1.2\nQR decomposition with column pivoting\nWe recall the following reformulation of QR with column pivoting (QRCP) for the\nreader’s convenience.\nGiven a matrix A, produce a column-orthogonal matrix Q, an uppertriangular matrix R, and a permutation vector J so that\nA[:, J] = QR.\nThe diagonal entries of R should approximate A’s singular values, and the columns\nof Q should approximate A’s left singular vectors. These stipulations reﬂect QRCP’s\nmain use cases:\nin low-rank approximation and in solving ill-conditioned least\nsquares problems. As usual, we say that our matrix A is m × n.\nIt’s all in the pivots.\nWe note that if m ≥n, then for any permutation vector\nJ, the economic QR decomposition of A[:, J] is unique.1 Therefore J completely\n1Technically, it is only unique up to sign ﬂips on the columns of Q and rows of R. But it is\nclear how signs must be chosen if the diagonal of R is to approximate the singular values of A.\narXiv Version 2\nPage 95\n\nFurther Possibilities for Drivers\n5.1. Multi-purpose matrix decompositions\ndetermines how well the columns of Q (resp., diagonal entries of R) approximate\nthe left singular vectors of A (resp., singular values of A).\nThe method of choosing pivots that sees the widest use today (a simple method\nbased on column norms) was ﬁrst described in [BG65]. The straightforward implementation of this method can have subtle failure cases in ﬁnite-precision arithmetic,\nhowever, this can be resolved by carefully restructuring norm calculations [DB08].\nAn established randomized algorithm for general matrices\nHere, we outline a remarkable algorithm ﬁrst developed by Martinsson [Mar15] and\nDuersch and Gu [DG17], and then reﬁned by Martinsson, Quintana-Ort´ı, Heavner,\nand van de Geijn [MHG17]. This reﬁned algorithm was introduced with the name\nHouseholder QR with Randomization for Pivoting or HQRRP. As this name implies,\nthe factor Q from HQRRP is an m×m operator deﬁned by n Householder reﬂectors.\nThe algorithm can run much faster than standard QRCP methods by processing the\nmatrix in column blocks, which makes it possible to cast the overwhelming majority\nof its operations in terms of BLAS 3, instead of about half BLAS 2.\nWhile a full description of HQRRP is beyond our scope, we can outline its\nstructure. As input, it requires that the user provide a block size parameter b and\nan oversampling parameter s. Typical values for these parameters are b = 64 and\ns = 10. HQRRP starts by forming a thin (b + s) × n sketch Y = SA, and then it\nenters the following iterative loop.\n1. Use any QRCP method to ﬁnd Pblock: the ﬁrst b pivots for Y.\n2. Process the panel A[:, Pblock] by QRCP.\n3. Suitably update (A, Y) and return to Step 1.\nThe update to A at Step 3 can be handled by standard methods, such as those used\nin blocked unpivoted Householder QR. The update to Y is more subtle. If done\nappropriately (particularly, by Duersch and Gu’s method [DG17]) then the leading\nterm in the FLOP count for HQRRP is identical to that of unpivoted Householder\nQR. The one downside of this algorithm is that the diagonal entries of R are not\nguaranteed to decrease across block boundaries.\nImplementation notes.\nWe adapted the C implementation from [MHG17] into C++\ncode at\nhttps://github.com/rileyjmurray/hqrrp.\nOur main change was to access BLAS and LAPACK through BLAS++ and LAPACK++. The modiﬁed code also allows for matrix dimensions to be speciﬁed with\neither 32-bit or 64-bit integers and includes a small test suite.\nWe brieﬂy point out two opportunities to improve the performance of this algorithm. The ﬁrst is to use mixed-precision arithmetic. Speciﬁcally, both the sketch\nof A and the call to deterministic QRCP on that sketch could use reduced precision.\nGiven that the real purpose of QRCP on the sketch is to select the block pivot indices for A, it might be that loss of accuracy in that phase does not compromise the\naccuracy of the larger algorithm. The second opportunity is to call unpivoted QR\non the very matrix Apanel in the second phase of processing a block; if pivoting is\nused in the second phase then the pivots can be determined by deterministic QRCP\non the R factor from the unpivoted QR of Apanel.\nPage 96\narXiv Version 2\n\n5.1. Multi-purpose matrix decompositions\nFurther Possibilities for Drivers\nA novel randomized algorithm for very tall matrices\nThe following algorithm overcomes the limitation of the preconditioned Cholesky\nQR methodology from [FGL21] of requiring full-rank data matrices. It does so by\nusing a randomized preconditioner based on QRCP.\nAlgorithm 7 QRCP via sketch-and-precondition and Cholesky QR.\n1: function [Q, R, J] = sap chol qrcp(A, d)\nInputs:\nA matrix A ∈Rm×n, an integer d satisfying n ≤d ≪m\nOutput:\nColumn-orthonormal Q ∈Rm×k, upper-triangular R ∈Rk×n, and a\npermutation vector J of length n.\nAbstract subroutines:\nSketchOpGen generates an oblivious sketching operator\n2:\nS = SketchOpGen(d, m)\n# S is d × m\n3:\n[Qsk, Rsk, J] = qrcp(SA) # SA[:, J] = QskRsk\n4:\nk = rank(Rsk)\n5:\nApre = A[:, J[:k]](Rsk[:k, :k])−1\n6:\n[Q, Rpre] = chol qr(Apre)\n7:\nR = RpreRsk[:k, :]\n8:\nreturn Q, R, J\nRemark 5.1.1. This monograph was released as a technical report in November\n2022. It has come to our attention that Algorithm 7 was discovered slightly earlier\nby Balabanov; it is termed RRRCholesyQR2 in arXiv:2210.09953:v2 [Bal22b].\nThe following proposition states that Algorithm 7 produces correct output in\nexact arithmetic, under mild assumptions on (S, A). We prove the proposition in\nAppendix D.\nProposition 5.1.2. Consider the context of Algorithm 7. If rank(SA) = rank(A)\nthen A[:, J] = QR.\nA practical implementation of Algorithm 7 would need to consider aspects of\nﬁnite-precision arithmetic. One such aspect is that we cannot use the exact rank\nfor Rsk on Line 4. Instead, some tolerance-based scheme would be needed.\nIn analyzing the behavior of this algorithm our main concern is the condition\nnumber of Apre. Indeed, if that matrix is not well-conditioned, then the factor Q\nfrom Cholesky QR may not be orthonormal to machine precision. More generally,\nif cond(Apre) ≥ϵ−1/2 (where ϵ is the working precision), then it is possible for\nCholesky QR to fail outright.\nOur next proposition says that if Apre is formed in exact arithmetic then its condition number depends on neither the conditioning of A nor that of Ask. Therefore\nif the distribution of the sketching operator is chosen judiciously, then the algorithm\nshould return an accurate decomposition with extremely high probability.\narXiv Version 2\nPage 97\n\nFurther Possibilities for Drivers\n5.1. Multi-purpose matrix decompositions\nProposition 5.1.3. Consider the context of Algorithm 7 and let U be an orthonormal basis for the range of A. If rank(SA) = rank(A), then the singular values of\nApre are the reciprocals of the singular values of SU.\nProposition 5.1.3 follows easily from Proposition 3.3.1; we omit a formal proof.\nApplication to matrices with any aspect ratio.\nAlthough Cholesky QR only applies\nto very tall matrices, one could apply it to any m × n matrix A (with m ≥n) by\nprocessing the matrix in blocks.\nIn fact, it would be natural to use Cholesky QR as the subroutine for processing a\nblock of columns of A in HQRRP. Since each iteration of HQRRP performs QRCP\non a sketch of A, the triangular factor from that run of QRCP can be used as\nthe preconditioner in processing the subsequent panel of A. However, there is a\ncomplication in this approach.\nHQRRP’s update rule for A requires that each panel’s orthogonal factor\nis represented as a composition of b Householder reﬂectors, where each\nreﬂector is m × m. By contrast, Cholesky QR only returns an explicit\nm × b column-orthonormal matrix Q.\nThis issue can be resolved by using a method to restore the full Householder representation of the explicit column-orthonormal matrix Q. In LAPACK, this is done\nwith sorhr col, which amounts to unpivoted LU factorization.\nWhile pairing\nCholesky QR with sorhr col will reduce its speed beneﬁt, it may still be faster\nthan Householder QR (GEQRF) and Tall-and-Skinny QR (GEQR) in certain settings.\nDetailed analysis of and benchmarks for this method are forthcoming.\n5.1.3\nUTV, URV, and QLP decompositions\nIf QRCP cannot be relied upon to provide an adequate surrogate for the SVD, then\none can consider decompositions of the form\nA = UTV∗,\nwhere U, V are column-orthogonal and T is square and triangular. This recovers\nthe SVD when T is the diagonal matrix of singular values of A. It also recovers\nQRCP when V is a permutation matrix. These decompositions were ﬁrst meaningfully studied by Stewart [Ste92; Ste93; Ste99]. They are known by various names,\nincluding UTV, URV, and QLP. We have a slight preference for the name “UTV”\nfor aesthetic reasons.\nDeterministic algorithms\nStewart’s best-known algorithm for UTV (see [Ste99]) is as follows.\n1. Run QRCP on the original matrix: A = Q1R1(P1)∗.\n2. Run QRCP on (R1)∗, to obtain R1 = P2(R2)∗(Q2)∗.\n3. Grouping terms, we ﬁnd the factors\nA =\n\u0000Q1P2\n| {z }\nU\n\u0001\n(R2)∗\n| {z }\nT\n\u0000P1Q2\n| {z }\nV\n\u0001∗.\nNote in particular that T is lower triangular.\nPage 98\narXiv Version 2\n\n5.1. Multi-purpose matrix decompositions\nFurther Possibilities for Drivers\nAssuming the standard pivoting scheme is used in the second call to QRCP, one\ncan be certain that the diagonal entries of T are in decreasing order: Tii ≥Tjj for\nj ≤i. Numerical experiments show that the diagonal of T can track the singular\nvalues of A much better than the diagonal of R1 (see, e.g., [Ste99, §3]). One can\nﬁnd intuition for this by considering the similarities between the successive calls to\nQRCP with the successive calls to QR in the well-known QR iteration. In [FHH99],\nStewart’s UTV algorithm is even described as “half a QR iteration.” Remarkably,\nthis algorithm can be modiﬁed to interleave the computation of R1 with factoring\nR1 [Ste99, §5]. The resulting method, like QRCP, can be stopped early at a speciﬁed\nrank or once some accuracy metric is satisﬁed.\nComplete Orthogonal Decomposition\nThere is a notion of a UTV decomposition\nthat is not the SVD, not QRCP, and yet predates Stewart’s UTV by several decades.\nIt is called the complete orthogonal decomposition (COD), and it is computed by\none call to QRCP followed by one call to unpivoted QR [HL69]. The main use of a\nCOD is to facilitate the application of a pseudoinverse A† when A is rank-deﬁcient.\nWe note that this is only modestly in line with the “spirit” of UTV, which asks for\na decomposition that can be used as a surrogate for the SVD more generally. Still,\nthe COD does have some historical importance in the development of randomized\nUTV algorithms.\nRandomized algorithms\nThe ﬁrst randomized algorithm for UTV was described in [DDH07, §5]. It used a\nrandom orthogonal transformation as a preconditioner for computing a COD, which\nmade it safe to replace the usual call to QRCP with a call to unpivoted QR. This\napproach does not produce good surrogates for the SVD on its own, however, it has\nsince been extended with power iteration ideas through the PowerURV algorithm\n[GM18, §3].2 PowerURV is able to obtain better approximations of the SVD than\nStewart’s UTV without using any pivoted QR decompositions.\nMuch of the value in Stewart’s algorithm for UTV is its ability to compute\nthe decomposition incrementally. The earliest randomized algorithm for UTV that\nenjoys this capability is given in [MQH19, Figure 4]. Qualitatively, this algorithm\ncan be thought of as extending the ideas of HQRRP without relying on HQRRP\nas a black box. In a historical context, it is notable because it is the ﬁrst full-rank\nUTV algorithm to use sketching (i.e., random dimension reduction) rather than\nrandom rotations.\nAs we wrap up the discussion on this topic, we note that one can trivially\nincorporate randomization into Stewart’s UTV by using HQRRP for the requisite\nQRCP calls. There would be a downside to this approach in that the diagonal\nentries of T would not be guaranteed to decrease across block boundaries. However,\nthat downside could be circumvented by using HQRRP for the initial QRCP of\nA and then using a standard QRCP algorithm (e.g., LAPACK’s GEQP3) for the\nQRCP of (R1)∗. The speedup of such an approach over Stewart’s UTV would be\nfundamentally limited, but it should still be observable for n×n matrices even when\nn is as small as a few thousand.\n2We note that the authors of [DDH07] were not trying to develop a randomized algorithm for\nits own sake. Rather, they used randomization as a tool to reduce many linear algebra problems to\na format amenable to recursive unpivoted QR, which can be accelerated by black-box fast matrix\nmultiplication methods.\narXiv Version 2\nPage 99\n\nFurther Possibilities for Drivers\n5.2. Solving unstructured linear systems\n5.2\nSolving unstructured linear systems\nTwo broad methodologies have emerged for incorporating randomization into general linear solvers. The ﬁrst aims to ameliorate the cost of common safeguards that\nare applied to fast but potentially unreliable direct methods. The second aims to\nrestructure computations in existing general-purpose iterative methods. There is\ngenerally more excitement in the community for methods of this second kind, but\nmethods of the ﬁrst kind remain a subject of practical interest.\n5.2.1\nDirect methods\nDirect methods for solving systems of linear equations center on ﬁnding a factored\nrepresentation of the system matrix. Most famously, we have the LU decomposition\nof a general n × n matrix, which takes the form\nA = LU\nfor a lower-triangular matrix L with unit diagonal (Lii = 1 for all i) and an uppertriangular matrix U. For Hermitian matrices, there is the LDL decomposition\nA = LDL∗,\nwhere L is unit lower-triangular and D is block diagonal with blocks of size one\nand two.\nThese are some of the most fundamental matrix decompositions. Once in hand,\nthey can be used to solve linear systems involving A in O(n2) operations.\nThe\nstandard methods for their computation exhibit good data locality and are naturally\nadapted to parallel processing environments. However, these decompositions should\nbe used cautiously; there are some nonsingular matrices for which they do not\nexist, or for which they cannot be computed stably in ﬁnite precision arithmetic.\nTherefore these decompositions need to be carefully modiﬁed to ensure reliability\nwithout sacriﬁcing too much speed.\nStability through randomized pivoting\nPivoting is the standard paradigm to modify LU and LDL for improved numerical\nstability. For LU, we have partial pivoting and complete pivoting, which look like\nPA = LU\nand\nP1AP2 = LU\n(5.2)\nrespectively, where P, P1, P2 are permutation matrices.\nThe standard algorithms for computing these decompositions are Gaussian elimination with partial pivoting (GEPP) and Gaussian elimination with complete pivoting (GECP). While GEPP is substantially faster than GECP, it has weaker theoretical guarantees than GECP when it comes to numerical behavior. In [MG15],\nMelgaard and Gu propose a randomized algorithm for partially pivoted LU that\nmakes pivoting decisions in a manner similar to HQRRP (see page 96). The randomized algorithm achieves eﬃciency comparable to that of GEPP, while also satisfying\nGECP-like element-growth bounds with high probability.\nFor LDL, pivoted decompositions take the form\nA = (PL)D(PL)∗,\n(5.3)\nPage 100\narXiv Version 2\n\n5.2. Solving unstructured linear systems\nFurther Possibilities for Drivers\nwhere (again) D is block-diagonal with blocks of size one and two. There are a variety of ways to introduce pivoting into LDL decompositions. The most notable are\nBunch–Kaufman [BK77] and bounded Bunch–Kaufman (which uses rook pivoting)\n[AGL98], both of which are available in LAPACK. In [FXG18], Feng, Xiao, and Gu\npropose a randomized algorithm for pivoted LDL that is as stable as GECP and\nyet only slightly slower than Bunch–Kaufman and bounded Bunch–Kaufman.\nStability through randomized rotations\nIn Section 5.1.3, we mentioned how the ﬁrst randomized algorithm for UTV used\nrandomized preconditioning to compute a COD-like factorization using only unpivoted QR decompositions. This was not the ﬁrst use of randomization to remove\nthe need for pivoting in matrix decompositions. In fact, this idea was explored by\nParker in 1995 to remove the need for pivoting in Gaussian elimination [Par95].\nHere we summarize Parker’s approach.\nWe begin by introducing some terms. For an integer d ≥1, a butterﬂy matrix\nof size 2d × 2d is a two-by-two block matrix, with d × d diagonal matrices in each\nof the four blocks. Speaking loosely, a recursive butterﬂy transformation (RBT) is\na product of a chain of matrices, each with butterﬂy matrices as diagonal blocks.\nRBTs of order n (i.e., RBTs of size n × n) are usually analyzed when n is a power\nof two for the sake of simplicity. The recursive structure in RBTs makes it possible\nto apply them with FFT-like methods. In particular, an RBT of order n = 2ℓcan\nbe applied to an n-vector in O(nℓ) time. Detailed discussion on RBTs of general\norder can be found in [Pec21].\nWe are interested in RBTs that are orthogonal and random. The orthogonality\nis useful since it means the same FFT-like algorithms used to apply an RBT can\nbe used to apply its inverse. The randomness in orthogonal RBT stems from how\none chooses the entries in the diagonal matrices. While there are a variety of ways\nthat this can be done [Par95], we simply speak in terms of a distribution Dn over\northogonal RBTs of order n.\nOne of the major contributions of [Par95] was to prove that for any nonsingular\nmatrix A of order n, one can sample B1, B2 iid from a certain distribution Dn, so\nthat matrix B1AB2 has an unpivoted LU decomposition with high probability. Put\nanother way, the decomposition\nA = (B1)∗LU(B2)∗\nexists with high probability.\nThe high speed at which RBTs can be applied and the excellent data locality\nproperties of unpivoted matrix decompositions have led to substantial interest in\nRBTs from the HPC community. For example, implementation considerations for\nhybrid CPU/GPU machines were studied in [BDH+13] (in the single-node setting)\nand [LLD20] (in the distributed setting).\nThe idea of using RBTs to precondition an “unsafe” unpivoted method naturally\napplies to LDL. In this case, one obtains factorizations of the form\nA = (BL)D(BL)∗\nwhere B is the random RBT. Again, this methodology has received recent attention\nfrom the HPC community; see [BBB+14] for work in the multi-core distributedmemory setting [BBB+14] and [BDR+17] for work in the setting of a single machine\nwith a hybrid CPU/GPU architecture.\narXiv Version 2\nPage 101\n\nFurther Possibilities for Drivers\n5.2. Solving unstructured linear systems\nRemarkably, although the idea of RBTs seems predicated on destroying sparsity\nstructure present in the matrix A, the random RBT methodology can be applied\nto sparse matrices without catastrophic ﬁll-in. See [BLR14] for work on this topic\nfor both general matrices and symmetric/Hermitian indeﬁnite matrices.\n5.2.2\nIterative methods\nBackground on GMRES\nGMRES is a well-known iterative method for solving linear systems of the form\nAx = b where A is n × n and nonsingular. The trajectory (xp)p≥1 it generates has\na simple variational characterization. Speciﬁcally, xp minimizes L(x) = ∥Ax −b∥2\n2\nover all vectors x in the p-dimensional Krylov subspace\nKp = span\n\b\nb, Ab, . . . , Ap−1b\n\n.\n(5.4)\nThe standard implementation of GMRES uses the Arnoldi process. This can be\nseen as a specialization of (modiﬁed) Gram–Schmidt to orthogonalize implicitlydeﬁned matrices of the form Kp = [b, Ab, . . . , Ap−1b]. In particular, as iterations\nproceed, the Arnoldi process maintains a column-orthonormal matrix Vp where\nrange(Vp) = Kp. Optionally, it can also maintain an Arnoldi decomposition, which\nrepresents AVp = Vp+1Hp in terms of an n × (p + 1) column-orthonormal matrix\nVp+1 and a (p + 1) × p upper-Hessenberg matrix Hp.3\nLetting Tmv(A) denote the cost of a matrix-vector multiply with A, the Arnoldi\ndecomposition up to step p can be computed in time\nO(pTmv(A) + np2).\nIf we are given this decomposition, then the least squares problem deﬁning xp can\nbe solved in O(np) time by applying a suitable direct method. Strictly speaking,\none does not need to compute xp−1 to compute xp.\nWe summarize some ways to introduce randomness into GMRES below. They\nall work by relaxing the requirement that Vp be column-orthonormal while retaining\nthe requirement that range(Vp) = Kp. Some of them work by changing the loss\nfunction L(x) to be minimized by xp.\nThese methods are of interest when the\ncost of the matrix-vector multiplies is dwarfed by the complexity of maintaining\nthe Arnoldi decomposition. We note that this situation can only arise when A is a\nsparse or otherwise structured operator.\nRandomized GMRES: Arnoldi decompositions in a sketch-orthogonal basis\nThe method from [BG22, § 4.2] can be interpreted as using a “sketched Arnoldi\nprocess” based on sketched Gram–Schmidt. It works by building up Vp so that\nits columns are S-orthogonal in the sense of (5.1), where S is a d × n sketching\noperator (p ≲d ≪n). Along the way, it maintains an Arnoldi-like decomposition\nAVp = Vp+1Hp, where Vp+1 is likewise S-orthogonal. Access to this decomposition\nat step p makes it possible to minimize the loss function ∥S(Ax −b)∥2\n2 over all x in\nKp in only O(np) added time.\n3A matrix is called upper-Hessenberg if all entries below the ﬁrst subdiagonal are zero.\nPage 102\narXiv Version 2\n\n5.2. Solving unstructured linear systems\nFurther Possibilities for Drivers\nTo understand the quality of the solution obtained by this method it is helpful\nto consider the unconstrained formulation\nmin\nz ∥AVpz −b∥2\n2.\n(5.5)\nGMRES would return x⋆= Vpz⋆where z⋆solves (5.5) exactly.\nThe sketched\nArnoldi approach eﬀectively approximates this solution by applying sketch-andsolve to (5.5).\nThis puts us in a position to draw from our coverage of sketchand-solve in Section 3.2.1.\nIf δ is the eﬀective distortion of S for the subspace\nKp+1, then the solution xsk obtained by the sketched Arnoldi approach will satisfy\n∥Axsk −b∥2 ≤(1 + δ)∥Ax⋆−b∥2.\nThe big-O time complexity of the sketched Arnoldi process is unchanged relative\nto the classic Arnoldi process. However, the ﬂop count for the sketched process can\nbe up to a factor of two smaller. The sketched process also makes better use of\nBLAS 2 over BLAS 1, and it has fewer synchronization points compared to the\nclassic Arnoldi process based on modiﬁed Gram–Schmidt. Taken together, using\nthe sketched process can signiﬁcantly reduce the wallclock time needed to obtain\nthe decomposition of AVp while retaining the reliability of classic Arnoldi.\nWe note that a block version of this algorithm (for linear systems with multiple\nright-hand sides) is presented in the preprint [BG21]. For MATLAB implementations of these methods, see [Bal22a].\nRandomized GMRES: handling general non-orthogonal bases\nBoth classic GMRES and the randomized variant given above maintain Arnoldi-like\ndecompositions of matrices AVp at a cost of O(np2) time complexity. Interestingly,\nthis cost cannot be asymptotically reduced by forgoing the decomposition of AVp.\nThe trouble is that building Vp with full orthogonalization – in the standard sense\nor the S-orthogonal sense – already takes O(np2) time.\nIn [NT21], Nakatsukasa and Tropp identiﬁed that (5.5) has precisely the form\nneeded to beneﬁt from randomized algorithms, independent from how Vp and AVp\nare generated.\nBased on this observation they called attention to longstanding\nclassical methods for computing non-orthogonal bases of Krylov subspaces.\nFor\nexample, one can compute Vp by a truncated k-step Arnoldi process for some k ≪p.\nThis can be done in O(npk) time and can easily be implemented to provide a dense\nrepresentation of AVp at no added cost. Alternatively, it may be practical to use\nthe Chebyshev method if one has knowledge of the spectrum of A.\n[NT21] primarily advocates for approximately solving (5.5) via sketch-and-solve,\nwhere the sketched subproblem is handled by factoring SAVp. Note that in exact\narithmetic the solutions obtained from this method would coincide with those of\nGMRES based on sketched Arnoldi. On the one hand this is very appealing, since\nthe cost of running this method for p iterations can easily undercut the O(np2) cost\nof sketched Arnoldi. On the other hand, the behavior of these methods can diﬀer\nin ﬁnite-precision arithmetic. If one is too lax in building the basis matrix Vp then\nthe condition number of AVp can explode as p increases.\nAll in all, the design space for this methodology is large and worth navigating\nwith care.\nValuable advice in this regard is given throughout [NT21, §3 – §5].\nOne particularly compelling comment is that one could simply solve (5.5) to high\naccuracy via a sketch-and-precondition method, such as Algorithm 1. The resulting\nsolution in this case would be very close to that produced by GMRES.\narXiv Version 2\nPage 103\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nNested randomization in block-projection and block-descent methods\nHaving discussed GMRES at length, we now speak to a family of iterative solvers\nthat do not use the Krylov subspace approach.\nThis family came into focus with the development of sketch-and-project – a\ntemplate iterative algorithm for solving linear systems of the form Fz = g, where\nF ∈RM×m has at least as many rows as columns (M ≥m) [GR15]. Its special cases\ninclude randomized Kaczmarz [SV08] and randomized block Kaczmarz [NT14]. It\nalso has variants that are speciﬁcally designed for overdetermined least squares\nproblems [GIG21].\nWithout getting into the mechanics of sketch-and-project in detail, we note that\nthese methods share a signiﬁcant weakness: their convergence rates worsen as one\nconsiders larger and larger problems. We think they are most likely to be useful\nwhen one cannot ﬁt an m×m matrix in memory. While such situations fall outside\nour primary data model, the subproblems encountered in sketch-and-project are\namenable to methods we have covered. Indeed, the subproblems are equivalent to\nproblems of the form\nmin\ny∈Rm{∥y −b∥2\n2 : A∗y = c},\n((3.4), revisited)\nwhere the number of columns n in A is a user-selected tuning parameter n ≪m ≤\nM. Such problems are clearly amenable to Algorithm 2.\nRecently, a general analysis framework for randomized linear system solvers\nbased on block projection or block descent has been proposed [PJM22]. We refer\nthe reader to Table 3 of [PJM22] (and appendices A.15 – A.26) for an extensive\nlist of new and old randomized linear system solvers that are amenable to their\nproposed analysis framework.\nSome of these methods are distinguished in their\napplicability to underdetermined problems. As with sketch-and-project, the subproblems encountered in essentially all of these methods can be chosen to have a\nstructure amenable to Algorithm 2.\n5.3\nTrace estimation\nMany scientiﬁc computing and machine learning applications require estimating\nthe trace of a square linear operator A that is represented implicitly. Randomized\nmethods are especially eﬀective for such problems.\n5.3.1\nTrace estimation by sampling\nLet A be n × n and {e1, . . . , en} be the standard basis vectors in Rn. Clearly, one\ncan compute the trace of A with n matrix-vector products by using the identity\ntr(A) =\nn\nX\ni=1\ne∗\ni Aei.\nRandomization creates opportunities to estimate this quantity using m ≪n matrixvector multiplications. The most basic method uses the fact that if ω ∼D is a\nrandom vector satisfying E[ωω∗] = In, then\ntr(A) = E [ω∗Aω] .\nPage 104\narXiv Version 2\n\n5.3. Trace estimation\nFurther Possibilities for Drivers\nIt is natural to approximate the expected value by the empirical mean. That is,\nupon drawing m independent vectors ωi ∼D, we estimate\ntr(A) ≈1\nm\nm\nX\ni=1\nω∗\ni Aωi.\n(5.6)\nThe idea for this method goes back to 1987 with work by Girard [Gir87], who\nproposed that D be the uniform distribution over the ℓ2 hypersphere with radius\n√n. Shortly thereafter, Hutchinson proposed that one take D as a distribution\nover Rademacher random vectors [Hut90]. Hutchinson’s choice of D minimizes the\nvariance of the estimator when A is ﬁxed, while Girard’s choice minimizes the worstcase variance over sets of matrices that are closed under conjugation by unitary\nmatrices; see [Epp23] for an explanation of this point.\nWe call the right-hand side of (5.6) a Girard–Hutchinson estimator. Such estimators require m ∈Ω(1/ϵ2) samples to approximate tr(A) to within ϵ error for\nsome constant failure probability.\n5.3.2\nTrace estimation with help from low-rank approximation\nCompress and trace\nIn [SAI17], Saibaba, Alexanderian, and Ipsen propose two randomized algorithms\nfor estimating the trace of a psd linear operator A.\nWhen A is accessible by matrix-vector products, the proposed method begins\nwith a rangeﬁnder step to ﬁnd a column-orthonormal n × m matrix Q where\nQQ∗AQQ∗≈A. The method then approximates\ntr(A) ≈tr(QQ∗AQQ∗) = tr(Q∗AQ).\nWhether or not this bound is accurate depends on the rate of A’s spectral decay\nand on how well-aligned Q is with the dominant eigenvectors of A. This method\ncan provide for better relative error bounds than a Girard–Hutchinson estimator if\nA’s spectral decay is suﬃciently fast and Q is obtained by power iteration.\nTrace estimation is especially challenging when matrix-vector products with A\nare expensive. This often happens when A is the image of another matrix B under\na matrix function, in the sense of [Hig08]. Saibaba et al. consider the case where\nA = log(I + B)\nfor a psd matrix B.4 The idea here is again to ﬁnd a tall n×m column-orthonormal\nQ so that QQ∗BQQ∗is a good low-rank approximation of B. Then we approximate\ntr(A) ≈\nm\nX\ni=1\nlog (1 + λi(Q∗BQ))\nwhere λi(·) returns the ith-largest eigenvalue of the given matrix. Error bounds can\nbe obtained for this estimate under suitable assumptions on the spectral decay of\nB. We note that some of the techniques used to prove these bounds extend to any\nmatrix function that is operator-monotone, such as the matrix square-root.\n4For any positive deﬁnite matrix M, we use log(M) to denote the Hermitian matrix with the\nsame eigenvectors as M and whose eigenvalues are the logs of the eigenvalues of M. One can verify\nthat tr(log(M)) = log det M holds for any positive deﬁnite M.\narXiv Version 2\nPage 105\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nSplit, trace, and approximate\nIn [MMM+21], Meyer et al. combined ideas from low-rank approximation with the\nGirard–Hutchinson estimator to obtain Hutch++. This estimator starts by sampling\na matrix Q uniformly at random from the set of n×m column-orthonormal matrices.\nIt then deﬁnes the low-rank approximation ˆA = QQ∗AQQ∗and computes the trace\nof this approximation by the formula\ntr(ˆA) = tr(Q∗AQ).\nThe last phase of Hutch++ applies Girard–Hutchinson to the deﬂated matrix\n∆= (I −QQ∗)A(I −QQ∗),\nand adds this estimate to tr(ˆA).\nThe basic validity of Hutch++ follows by splitting the trace of A into two parts:\ntr(A) = tr(ˆA) + tr(A −ˆA)\nand verifying that tr(A −ˆA) = tr(∆). As a splitting and deﬂation approach, this\nmethod is very eﬀective in reducing the variance of the Girard–Hutchinson estimator. Early results along these lines can be found in [GSO17], which investigated the\nuse of deﬂation in estimating the trace of an inverted matrix.\nThe initial results proven for Hutch++ applied only to psd matrices. In that\ncontext, Hutch++ can (with some small ﬁxed failure probability) compute tr(A) to\nwithin ϵ relative error using only O(1/ϵ) matrix-vector products. This is a substantial improvement upon the O(1/ϵ2) matrix-vector products that are required\nby plain Girard–Hutchinson estimators. In fact, the sample complexity of Hutch++\ncannot be improved when considering a large class of algorithms [MMM+21, Theorems 4.1 and 4.2].\nPersson, Cortinovis, and Kressner have since extended Hutch++ so that it can\nproceed adaptively, only terminating once some error tolerance has been achieved\n(up to a controllable failure probability) [PCK21]. The analysis of their modiﬁed\nHutch++ method notably accommodates symmetric indeﬁnite matrices A. We note\nthat the accuracy guarantees of trace estimators for indeﬁnite matrices cannot be as\nstrong as those for positive deﬁnite matrices. Indeed, relative error guarantees are\nessentially impossible when tr(A) = 0. Persson et al., therefore, provide additive\nerror guarantees in this setting.\nLeveraging the exchangability principle\nIn [ETW23], Epperly, Tropp, and Webber develop a trace estimator based on the\nexchangability principle. In the context of trace estimation, this principle stipulates\nthat if an algorithm computes its estimate based on m pairs {(ωi, Aωi)}m\ni=1 where\nωi are iid random vectors, then the minimum-variance unbiased estimator for tr(A)\nmust be invariant under relabelings {ωi}m\ni=1 ←{ωσ(i)}m\ni=1 for permutations σ.\nHutch++ does not respect the exchangability principle, since it uses randomness\nin two distinct stages: ﬁrst to compute the matrix Q and then to estimate the trace\nof the ∆by a a Girard–Hutchinson estimator.\nThe XTrace algorithm proposed in [ETW23] can be thought of as a symmetrized\nversion of Hutch++. Given m samples {(ωi, Aωi)}m\ni=1, its estimate is an average of m\nruns of Hutch++, where the jth run uses Qj = orth([Aωi]i̸=j) and estimates tr(∆j)\nPage 106\narXiv Version 2\n\n5.3. Trace estimation\nFurther Possibilities for Drivers\nby ω∗\nj ∆jωj.\nImplementing XTrace naively would be very expensive.\nHowever,\nas explained in [ETW23, §2.1], a careful implementation can achieve the same\nasymptotic complexity as Hutch++. XTrace also comes with adaptive-stopping and\nvariance estimation methods analogous to those developed in [PCK21].\n5.3.3\nEstimating the trace of f(B) via integral quadrature\nSection 5.3.2 touched on a method for estimating the trace of A = log(B + I),\nwhere B is psd and log(·) is the matrix logarithm. This section covers powerful\nmethods for a broader class of trace estimation problems. The original method,\nnow known as stochastic Lanczos quadrature (SLQ), was introduced to the linear\nalgebra community in [BFG96], was popularized by [UCS17], and has since extended\nin a few diﬀerent ways [CH22; PK22; CTU22].\nTo begin, consider how any function f : R →R can canonically be extended to\nact on a Hermitian matrix by acting separately on the eigenvalues of the matrix.\nThat is, if we expand B in its eigenbasis\nB =\nn\nX\ni=1\nλiuiu∗\ni ,\nthen we can deﬁne\nf(B) =\nn\nX\ni=1\nf(λi)uiu∗\ni .\nHere we cover quadrature-based methods for approximating the trace of such matrices. The concepts behind these methods apply whenever f is suﬃciently smooth\nand B is Hermitian. Theoretical guarantees for these methods are usually obtained\nunder stronger assumptions, such as f being analytic on [λn, λ1], or B being psd.\nTechnical background\nThe concepts we summarize below are detailed in the book [GM10].\nRiemann-Stieltjes integrals.\nLet µ be a real-valued function on R. The expression\nZ\nR\nf(t)dµ(t)\n(5.7)\nis called the Riemann-Stieltjes integral of f against µ. We do not provide a formal\ndeﬁnition of this integral.\nRather, we oﬀer two footholds for understanding it.\nFirst, if µ is continuously diﬀerentiable, then (5.7) is simply the Riemann integral\nof t 7→f(t)( d\ndtµ(t)). Second, recall the interpretation of Riemann integration in\nwhich one identiﬁes dt ≈tℓ+1 −tℓ, where tℓ< tℓ+1 are consecutive points in a\npartition of the region of integration. If the analogous interpretation is applied to\n(5.7), then we would say that dµ(t) ≈µ(tℓ+1) −µ(tℓ).\nFor our purposes we can assume that µ is nondecreasing. We also assume that\nthere are constants L and U where µ(t) = µ(L) for all t ≤L and µ(t) = µ(U) for\nall t ≥U. Under these assumptions, (5.7) is well-deﬁned whenever f is continuous.\narXiv Version 2\nPage 107\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nQuadrature and orthogonal polynomials.\nAn s-point quadrature rule for (5.7) speciﬁes s-vectors w and θ (of weights and nodes respectively) to deﬁne an approximation\nZ\nR\nf(t)dµ(t) ≈\ns\nX\nℓ=1\nwℓf(θℓ).\n(5.8)\nThe nodes and weights selected by any reliable quadrature method will depend\non µ. One prominent approach to deﬁning quadrature rules is to require that (5.8)\nholds with equality whenever f is a polynomial of degree d, where d is suitably\nbounded in terms of s. The idea behind this is that s increases, we should be able\nto accommodate polynomials of higher degree.\nGaussian quadrature achieves optimal sample complexity. This method is exact\nfor polynomials up to degree 2s−1, and there is no rule that can guarantee exactness\nfor polynomials of degree higher than 2s −1 with only s samples. There is a deep\nconnection between Gaussian quadrature and orthogonal polynomials that make\nthis quadrature rule viable in practice. The connection uses the fact that, under\nour assumptions on µ, it can be taken to deﬁne an inner product\n⟨p, q⟩µ =\nZ\nR\np(t)q(t)dµ(t).\nThis inner product can be used to deﬁne an orthonormal basis for the set of polynomials that have at most some prescribed degree. When this orthonormal basis is\nsorted by degree, the resulting sequence of polynomials must satisfy a three-term\nrecurrence relationship [GM10, §2]. The coeﬃcients of the recurrence relationship\nup to step s can be assembled in a tridiagonal matrix J, called the Jacobi matrix, of\nsize s×s. The nodes and weights of Gaussian quadrature against µ can be recovered\nfrom an eigendecomposition of the Jacobi matrix [GM10, Theorem 6.2].\nStochastic Lanczos quadrature : approximating Girard–Hutchinson\nA Girard–Hutchinson estimator for the trace of f(B) takes the form\nT = 1\nm\nm\nX\ni=1\nTi\nwhere\nTi = ω∗\ni f(B)ωi\nfor independent random vectors ωi drawn from a suitable distribution. The naive\nway to compute this estimator would be to call a black-box function that implements\nthe action of f(B); for each sample i one would compute vi = f(B)ωi and then\ntake a dot product Ti = ω∗\ni vi. Here we describe an alternative approach which\nbegins with an integral representation for Ti and then approximates that integral\nvia Gaussian quadrature [BFG96]. The resulting method for trace estimation is\nnow known as stochastic Lanczos quadrature (SLQ) [UCS17].\nIntegral representation of a single sample.\nLet u denote the piecewise constant function that is zero for t ≤0 and one for t > 0. This function can be used to deﬁne a\nRiemann-Stieltjes integral that samples f at any prescribed point. Speciﬁcally, for\nany scalar z, we have f(z) =\nR\nR f(t)du(t −z). Therefore upon setting\nµi(t) =\nn\nX\nj=1\n|ω∗\ni uj|2 u(t −λj),\n(5.9)\nPage 108\narXiv Version 2\n\n5.3. Trace estimation\nFurther Possibilities for Drivers\nthe following identity is immediate from the deﬁnition of f(B):\nTi =\nZ\nR\nf(t)dµi(t).\n(5.10)\nWe note that this integral is written as being over all of R, but it would suﬃce to\nintegrate over the interval [λn, λ1].\nQuadrature of a sample’s integral representation.\nIntegration is often described as a\ncontinuous analog of summation. As such, one usually thinks of quadrature as an\nact of approximating a continuous operation by a discrete operation. Quadrature of\nRiemann-Stieltjes integrals does not always follow this pattern. Indeed, the integral\n(5.10) can already be expressed as a weighted sum of s = n point evaluations\nof f, with weights wiℓ= |ω∗\ni uℓ|2 and nodes θiℓ= λℓ.\nThe problem with this\nrepresentation is that we do not know the weights or nodes a-priori. Therefore in\nthe setting of Riemann-Stieltjes integration it is possible that quadrature acts as\na means of approximating an unknown integrator µi by a known integrator ˆµi for\nwhich we can eﬃciently compute\nR\nR f(t)dˆµi(t).\nEnter, Lanczos quadrature.\nThis is a method for computing the Gaussian\nquadrature rule (or variations thereof) of Riemann-Stieltjes integrals with integrators of the form (5.9) [GM10, §7]. It uses the fact that the polynomials that are\northogonal with respect to the integrator µi are none other than the Lanczos polynomials associated with (B, ωi) (see [GM10, Theorem 4.2]). Hence, the Lanczos\nalgorithm for computing an orthonormal basis for the s-dimensional Krylov subspace\nspan{ωi, Bωi, . . . , B(s−1)ωi}\ncan be used to compute the Jacobi matrix. Given that, standard tridiagonal eigensolvers can provide us with the nodes and weights needed for Gaussian quadrature.\nImplementation notes.\nSLQ entails approximating m samples of the form ω∗\ni f(B)ωi,\nwhere each ωi is an independent random vector drawn from some distribution D.\nThe quality of each approximate sample depends on the number of nodes allowed\nin the Gaussian quadrature rule, and hence on the number of steps in the Lanczos\nalgorithm.\nTaking s steps of the Lanczos algorithm will always require s −1 matrix-vector\nproducts with B. The arithmetic and storage complexity needed to compute each\nsample in SLQ depends on whether we run Lanczos proper or a version of Lanczos\nthat only computes the data needed for Gaussian quadrature. Indeed, in the latter\ncase we have a substantial amount of freedom to make tradeoﬀs between computational complexity and numerical stability. At one end this tradeoﬀ, s iterations of\nLanczos with full orthogonalization costs O(ns) storage and O(ns2) arithmetic. At\nthe other end of the tradeoﬀ, performing no reorthogonalization reduces the costs\nto only O(n) storage and O(ns) arithmetic. (We emphasize that these costs do not\naccount for the s −1 matrix-vector products needed with B.)\nSLQ is a powerful tool, with important applications in Gaussian process regression. For an implementation of this method that scales to petascale problems by\nrunning on GPU farms, we refer the reader to the IMATE Python package [Ame22].\narXiv Version 2\nPage 109\n\nFurther Possibilities for Drivers\n5.3. Trace estimation\nBeyond stochastic Lanczos quadrature\nAccelerated quadrature-based methods.\nLet A = f(B).\nThe convergence rate of\nSLQ for estimating tr(A) can only be as good as a Girard–Hutchinson estimator.\nAs such, one needs m ∈Ω(1/ϵ2) samples in order to estimate tr(A) to within ϵ\nerror for some constant failure probability. This leaves substantial improvement\nfor SLQ in the case when A is positive deﬁnite, where Hutch++ could make do\nwith m ∈Ω(1/ϵ) queries to A. Luckily, it is possible to extend SLQ to use similar\nsplitting techniques that Hutch++ employs for its variance reduction; see [CH22]\nand [PK22] for details.\nSpectral density estimation.\nOne of SLQ’s remarkable properties is that its quadrature rule for approximating (5.10) does not depend on f. As such, if the quadrature\nnodes and weights are computed to estimate tr(f(B)) for one function f, then one\ncan use those same nodes and weights to compute an estimate for tr(g(B)) for\nanother function g. This gives some motivation for directly estimating the function\nφ(t) = Pn\nj=1 u(t −λi)\n(5.11)\nwhich satisﬁes\nR\nR f(t)dφ(t) = tr(f(B)) for all continuous functions f : R →R.\nNote that φ is a nonnegative nondecreasing function with limt→∞φ(t) = n. As\nsuch, φ/n is a cumulative probability distribution function that can be uniquely\nidentiﬁed with the spectrum of B.\nThe problem of estimating a function of the form (5.11) is a particular case\nof spectral density estimation.\nThis problem, which has broader applications in\nthe physical sciences than trace estimation, has been approached explicitly with\nrandomized algorithms [Lin16]. Approaching the linear algebraic problem of trace\nestimation with this in mind can lead to new insights on how to leverage prior knowledge on the structure of f or B for algorithmic purposes. In particular, [CTU22]\nprovides a systematic treatment of quadrature-based trace estimation algorithms\nbased on this perspective.\nPage 110\narXiv Version 2\n\nSection 6\nAdvanced Sketching:\nLeverage Score Sampling\n6.1 Deﬁnitions and background ........................................ 112\n6.1.1 Standard leverage scores ............................................ 112\n6.1.2 Subspace leverage scores ............................................ 115\n6.1.3 Ridge leverage scores ................................................. 116\n6.2 Approximation schemes .............................................. 117\n6.2.1 Standard leverage scores ............................................ 117\n6.2.2 Subspace leverage scores ............................................ 118\n6.2.3 Ridge leverage scores ................................................. 119\n6.3 Special topics and further reading .............................. 120\n6.3.1 Leverage score sparsiﬁed embeddings ........................... 120\n6.3.2 Determinantal point processes .................................... 121\n6.3.3 Further variations on leverage scores ............................ 122\nLeverage scores quantify the extent to which a low-dimensional subspace aligns\nwith coordinate subspaces. They are fundamental to RandNLA theory since they\ndetermine how well a matrix can be approximated through sketching by row or\ncolumn selection, and thus indirectly how well a matrix can be approximated by\nsparse data-oblivious sketching methods [DM16]. They have algorithmic uses in\nleast squares [DMM06; DMM+12] and low-rank approximation [DMM08; BMD09;\nMD16] among other topics. More broadly, they play a key role in statistical regression diagnostics [CH88; MMY15].\nThe computational value of leverage scores stems from how they induce dataaware probability distributions over the rows or columns of a matrix. Leverage score\nsampling refers to sketching by row or column sampling according to a leverage score\ndistribution (or an approximation thereof). The quality of sketches produced by\nleverage score sampling is relatively insensitive to numerical properties of the matrix\nto be sketched. This can be contrasted with sketching by uniform row or column\nsampling, which can perform very poorly on certain families of matrices.\n111\n\nLeverage Score Sampling\n6.1. Deﬁnitions and background\nLeverage score distributions can be computed exactly with standard deterministic algorithms. However, exact computation is expensive except in very speciﬁc\ncases (see Section 7). Therefore in practice it is necessary to use randomized algorithms to approximate leverage score distributions. On the one hand, this point is\nsigniﬁcant since the costs of the approximation algorithms undermine the eﬃciency\ngains obtained from sketching by simple row or column selection, making the cost\ncomparable to implementing data-oblivious random projection methods. On the\nother hand, uniform sampling is clearly suboptimal in many cases, e.g., in that it\ncan miss important nonuniformity structures needed to obtain data-aware subspace\nembeddings. In general, the practical utility of leverage scores derives from when\nrow or column selection of a matrix is required by a particular application. Leverage scores, therefore, compete with both uniform sampling and other methods for\ncolumn (or row) selection as discussed in Section 4.3.4.\nWe emphasize that we have made no concrete plans regarding RandLAPACK’s\nsupport for leverage score sampling methods. We review them here since they are\nprominent and sophisticated sketching methods, and they might be appropriate to\nsupport in RandLAPACK via a suite of computational routines.\nIn what follows we introduce three ﬂavors of leverage scores (§6.1) and methods for approximately computing them (§6.2). We also cover three special topics:\nSection 6.3.1 explains how leverage scores can be used to deﬁne long-axis-sparse\nsketching operators (in the sense of Section 2.4.2), and Sections 6.3.2 and 6.3.3\ndiscuss generalizations of leverage scores.\n6.1\nDeﬁnitions and background\nHere we cover three types of leverage scores and corresponding approaches to leverage score sampling. The ﬁrst type of leverage score (which we mean by default) is\napplicable to sketching in the embedding regime. As such, it is applicable primarily\nto highly overdetermined least squares problems or other saddle point problems with\ntall data matrices. We spend more time on this ﬁrst type of leverage score since it\nhas theoretical value in understanding the behavior of RandNLA algorithms. The\nsecond type is used for sketching in the sampling regime and has applications in\na variety of low-rank approximation problems. The third type is speciﬁcally for\napproximating psd matrices (typically kernel matrices) in the presence of explicit\nregularization.\n6.1.1\nStandard leverage scores\nLet U be an n-dimensional linear subspace of Rm and PU be the orthogonal projector from Rm to U. The ith leverage score of U is\nℓi(U) = ∥PUδi∥2\n2 = PU[i, i].\n(6.1)\nwhere δi is the ith standard basis vector. Collectively, leverage scores describe how\nwell the subspace U aligns with the standard basis in Rm. They have algorithmic\nimplications when we consider induced leverage score distributions, deﬁned by\npi(U) =\nℓi(U)\nPm\nj=1 ℓj(U) = ℓi(U)\nn\n.\n(6.2)\nPage 112\narXiv Version 2\n\n6.1. Deﬁnitions and background\nLeverage Score Sampling\nGiven a matrix A, one can associate as many sets of leverage scores to that\nmatrix A, as one can associate subspaces to A. Two of the most important such\nsubspaces are U = range(A) and V = range(A∗). In these contexts we say that\nthe leverage score for the ith row of A is ℓi(U), while the leverage score for the\njth column is ℓj(V ). Such leverage scores provide leverage score distributions over\nthe rows and columns of A, respectively. Note that only one of these distributions\ncan be nonuniform if A is full-rank. Therefore when speaking of leverage scores we\ntypically assume the m × n matrix A is tall, which allows for the possibility that\np(U) is nonuniform.\nMoving forward, we routinely replace U by A in (6.1) and (6.2), with the understanding that U = range(A).\nProbabilistic guarantees of sketching via row sampling\nSuppose S is a wide d×m sketching operator that implements row sampling according to a probability distribution q. We are interested in evaluating the statistical\nquality of S as a row sampling operator for an m × n matrix A. Here, our measure\nof sketch quality the smallest ϵ ∈(0, 1) where y ∈range(A) implies\n(1 −ϵ)∥y∥2\n2 ≤∥Sy∥2\n2 ≤(1 + ϵ)∥y∥2\n2.\n(6.3)\nNote that this metric is very similar to subspace embedding distortion.\nIn this\nmonograph we have generally advocated for measuring sketch quality by a scaleinvariant metric called eﬀective distortion. Despite this, we care about (6.3) since\nit provides for the following standard result (which we prove in Appendix A.3).\nProposition 6.1.1. Suppose A is an m × n matrix of rank n. If\nr := min\nj∈JmK\nqj\npj(A)\nthen for all 0 < ϵ < 1, we have\nPr {(6.3) fails for (S, A, ϵ)} ≤2n\n\u0012\nexp(ϵ)\n(1 + ϵ)(1+ϵ)\n\u0013rd/n\n(6.4)\nand exp(ϵ) < (1 + ϵ)(1+ϵ).\nThe proposition’s basic message is that the probability of SA being a good\nsketch improves as q gets closer to the leverage score distribution p(A), where\n“closer” means that the value r becomes larger. This makes it desirable for q to\napproximate the leverage score distribution. In practice, such approximations would\nbe obtained by ﬁrst estimating leverage scores (e.g., via the method described in\nSection 6.2.1) and then normalizing according to the estimates. That is, we compute\nˆℓas an estimate of ℓ(A), then set\nqi =\nˆℓi\nPm\nj=1 ˆℓj\n.\nWith this in mind, we turn to our next question: how large should d be so that the\nfailure probability (6.4) tends to zero as n tends to inﬁnity?\narXiv Version 2\nPage 113\n\nLeverage Score Sampling\n6.1. Deﬁnitions and background\nAs a short answer, it can be shown that taking d ∈O\n\u0000n log n/rϵ2\u0001\nis suﬃcient\nfor (6.4) to tend to zero as n tends to inﬁnity.1 With (exact) leverage score sampling\nwe are fortunate to have r = 1, and so it suﬃces for the embedding dimension to\nsatisfy\ndlev ∈O\n\u0012n log n\nϵ2\n\u0013\n.\n(6.5)\nTo describe the bound with uniform sampling, we introduce the coherence of A as\nC (A) := m max\ni∈JmK ℓi(A).\nIt is easily be shown that coherence is bounded by n ≤C (A) ≤m and that uniform\nsampling leads to r = n/C (A). In view of these facts, the embedding dimension for\nuniform sampling should be on the order of\ndunif ∈O\n\u0012C (A) log n\nϵ2\n\u0013\n.\nThis is no better than leverage score sampling, and it can be much worse.\nAs a ﬁnal point on the eﬀectiveness of sketching by row selection methods,\nconsider the situation of using approximate leverage scores where we have a bound\nqj ≥βpj(A) for all j. In such a situation we would have β ≤r and setting d = dlev/β\nwould suﬃce to achieve the same guarantees as leverage score sampling.\nPreconditioned leverage score sampling, hidden in plain sight\nMany data-oblivious sketching operators can be described as applying a “rotation”\nand then performing coordinate subsampling. Here are two such examples.\n• A wide d × m Haar sketching operator S can be viewed as a composition of\nan m × m orthogonal matrix followed by a coordinate sampling operator.\n• The diagonal sign ﬂip and the fast trig transform in an SRFT amounts to a\nrotation, and the full action of the SRFT is just applying coordinate sampling\nto the rotated input.\nIn both cases, the rotation acts as a type of preconditioner for sampling, i.e., as\na transformation that converts a given problem into a related form that is more\nsuitable for sampling methods [DM16]. The example of SRFTs is especially informative, since using an embedding dimension d ∈O(n log n) suﬃces for a d × m\nSRFT to be a subspace embedding with constant distortion (say, distortion 1/2)\nwith high probability [AMT10].\nFormulas for leverage scores\nThere are many concrete ways to express the leverage scores of a tall m × n matrix\nA. Here is an expression that emphasizes the matrix itself, without making explicit\nreference to its range:\nℓj(A) = A[j, :] (A∗A)† A[j, :]∗.\n(6.6)\n1Technically, this choice of d also gives an explicit rate at which the probability tends to zero,\nbut we do not dwell on that here.\nPage 114\narXiv Version 2\n\n6.1. Deﬁnitions and background\nLeverage Score Sampling\nWe can obtain other concrete expressions for the leverage scores by considering\nany matrix U whose columns form an orthonormal basis for U = range(A). For\nexample, this matrix U could be the Q from a QR decomposition or the U from\nthe SVD or any other such matrix. Any such matrix suﬃces since P = UU∗, as the\northogonal projector onto U, satisﬁes\nℓj(A) = ∥U[j, :]∥2\n2 = (UU∗)[j, j].\nThe subspace perspective is useful since it shows that leverage scores are unchanged\nif A is replaced by AA∗. More generally, if A = EF and F has full row-rank then\nthe leverage scores of E match those of A.\n6.1.2\nSubspace leverage scores\nThe standard leverage scores described in Section 6.1.1 are not suitable for lowrank approximation. The ﬁrst problem is that it is perfectly reasonable to ask for a\nlow-rank approximation of a matrix that is invertible but has many small singular\nvalues. In such situations both the row and column leverage scores will be uniform,\nand hence contain no information. The second problem is that the map from a\nmatrix to its leverage scores is not locally continuous at A whenever A is rankdeﬁcient. (As a general rule, it is diﬃcult to solve linear algebra problems where\nthe map from problem data to the solution is discontinuous.)\nThese shortcomings can partially be addressed with the concept of subspace\nleverage scores, which are also called rank-k leverage scores and leverage scores\nrelative to the best rank-k approximation; see [DMM+12, §5] along with [DMM08]\nas an earlier conference version of the same.\nExpressing the m × n matrix A by its compact SVD, A = UΣV∗, the rank-k\nleverage scores for its range are\nℓk\nj (A) = ∥U[j, :k]∥2\n2.\nNote that the rank-k leverage scores can be nonuniform regardless of the aspect\nratio of the matrix. Indeed, so long as k < rank(A), the rank-k leverage scores of\nboth range(A) and range(A∗) can be nonuniform. The problem of discontinuity of\nthe map from a matrix to its rank-k subspace leverage scores can still persist. More\ngenerally, there is a problem that a matrix may admit multiple distinct “best rank-k\napproximations” for a given value of k. These problems are less troublesome if one\nassumes that the kth spectral gap σk(A) −σk+1(A) is bounded away from zero.\n(This assumption is perhaps more often made than well-justiﬁed.) Alternatively,\none can consider how well the computed scores approximate the leverage scores for\nsome “nearby” rank-k space [DMM+12].\nLet us turn to how subspace leverage scores are used. Continuing to focus on\nthe case of row sampling, we are interested in the rank-k leverage score distribution\npk\nj (A) =\nℓk\nj (A)\nPm\ni=1 ℓk\ni (A).\nIf S denotes a d × m row-sampling operator induced by pk(A), then the sketch\nY = SA leads naturally to the approximation ˆA = AY†Y. Letting Ak denote some\nbest-rank-k approximation of A in a unitarily invariant matrix norm “∥· ∥,” it is\npossible to choose d suﬃciently large so that\n∥A −ˆA∥≲∥A −Ak∥\n(6.7)\narXiv Version 2\nPage 115\n\nLeverage Score Sampling\n6.1. Deﬁnitions and background\nholds with high probability. Note that if Y were an arbitrary matrix then it would\nbe possible to choose Y so that the projection ˆA = AY†Y was equal to some bestrank-k approximation of A. However, the restriction that the rows of Y are scaled\nrows of A signiﬁcantly limits the projectors that could be used to deﬁne ˆA. Because\nof this limitation, one may need d ≫k to have any chance that (6.7) holds.\nRemark 6.1.2. One rarely samples according to an exact rank-k leverage score distribution in practice. Rather, one uses randomized algorithms to approximate them.\nThe key fact that enables this approximation is that leverage scores (“standard” or\n“subspace”) are preserved if we replace A by AA∗. Moreover, as leverage scores\nquantify a notion of eigenvector localization, we should note that in many applications one has domain knowledge that eigenvalues should be localized [SCS10], and\nthis could be used to construct approximations.\n6.1.3\nRidge leverage scores\nRidge leverage scores are used to approximate matrices in the presence of explicit\nregularization. That is, we are given an m × m psd matrix K and a positive regularization parameter λ, and we approximate K + λI by ˆK + λI where K is a psd\nmatrix of rank n ≪m. The low-rank structure in these approximations makes it\nmuch cheaper to apply (ˆK + λI)−1 compared to (K + λI)−1. This motivates the\nfollowing question.\nWhat rank n is needed for (ˆK + λI)−1 to approximate (K + λI)−1 up to\nsome ﬁxed accuracy?\nIt turns out that this is determined by quantity tr(K(K + λI)−1), which is called\nthe eﬀective rank of K. Using µi to denote the ith-largest eigenvalue of K, we can\nexpress the eﬀective rank as\ntr(K(K + λI)−1) =\nm\nX\ni=1\nµi\nµi + λ.\nSince we are working with psd matrices it is natural to deﬁne ˆK as a Nystr¨om\napproximation of K with respect to some sketching operator S (see Section 4.2.2).\nTaking that as given, this leaves the question of how to choose the distribution for S.\nHere it is worth considering how many numerically-low-rank psd matrices arising in\napplications are deﬁned implicitly through pairwise evaluations of a kernel function\non a given dataset. Taking S as a column-selection operator is especially appealing\nin these settings.\n[AM15] introduced ridge leverage scores as a framework for data-aware column\nsampling in this context. Formally, the ridge leverage scores of (K, λ) are\nℓi(K; λ) =\n\u0010\nK (K + λI)−1\u0011\n[i, i].\n(6.8)\nIn certain cases – particularly for estimating ridge leverage scores – it can be convenient to express these quantities in terms of a matrix B that satisﬁes K = BB∗and\nthat has at least as many rows as columns. Speciﬁcally, by expressing B in terms\nof its compact SVD, one can show that\nℓi(K; λ) = b∗\ni (B∗B + λI)−1 bi\n(6.9)\nwhere b∗\ni is the ith row of B. We note that the identity matrix appearing in (6.9)\nwill be smaller than that from (6.8) if B is not square.\nPage 116\narXiv Version 2\n\n6.2. Approximation schemes\nLeverage Score Sampling\n6.2\nApproximation schemes\nComputing leverage scores exactly is an expensive proposition. If A is a tall m × n\nmatrix, then it takes O(mn2) time to compute the standard leverage scores exactly.2\nIf one is interested in subspace leverage scores and k is small, then one can in\nprinciple use Krylov methods to approximate the dominant k singular vectors in\nfar less than O(mn2) time. Such methods are not very reliable for producing good\napproximations of the truncated SVD, but they might suﬃce for estimating leverage\nscores. If we want to compute the ridge leverage scores of an m×m matrix K exactly,\nthen the straightforward implementation takes O(m3) time.\nThese facts necessitate the development of eﬃcient and reliable methods for\nleverage score estimation, which we discuss below. While these methods are generally too sophisticated for the RandBLAS, they may be appropriate for higher-level\nlibraries such as RandLAPACK.\n6.2.1\nStandard leverage scores\nSuppose the m×n matrix A is very tall, i.e., m ≫n. Here we summarize a method\nby Drineas et al. that can compute approximate leverage scores, to within a constant\nmultiplicative error factor, in O(mn log m) time, i.e., in roughly the time it takes\nto implement a random projection, with some constant failure probability bounded\naway from one [DMM+12]. This can oﬀer improved eﬃciency over straightforward\nO(mn2) approaches when m ≫n and yet m ∈o(2n).\nWe set the stage for this method by expressing leverage scores as follows\nℓj(A) = ∥δ∗\nj U∥2\n2 = ∥δ∗\nj UU∗∥2\n2 = ∥δ∗\nj AA†∥2\n2\n(6.10)\nwhere we note that the second equality in the above display follows from unitary\ninvariance of the spectral norm. The method proceeds by approximating two operations in the right-most expression in (6.10). First we approximate the pseudoinverse\nof A and then we approximate the matrix-matrix product AA†. It is important to\nnote that using approximations in both steps is essential for asymptotic complexity\nimprovements, since traditional methods would take O(mn2) for the ﬁrst step and\nO(m2n) time for the second step. (In extreme situations, depending on the hardware that would be used, it may be worth performing the matrix-matrix product\nof the second step explicitly.)\nThe pseudoinverse computation is approximated by applying a wide d1 × m\nSRFT S1 to the left of A. Letting U1Σ1V∗\n1 be an SVD of this d1 × n sketched\nmatrix S1A, we approximate\nℓj(A) ≈ˆℓj(A) = ∥δ∗\nj A(S1A)†∥2\n2\n= ∥δ∗\nj AV1Σ−1\n1 U∗\n1∥2\n2\n= ∥δ∗\nj AV1Σ−1\n1 ∥2\n2\nat a cost of O(d1n2). However, we are not out of the woods yet, since multiplying A\nwith V1Σ−1\n1\nwould still cost O(mn2). This is addressed by applying a tall sketching\n2The preferred way to do this would be to take the row norms of the factor Q from a thin QR\ndecomposition of A.\narXiv Version 2\nPage 117\n\nLeverage Score Sampling\n6.2. Approximation schemes\noperator S2 of size n × d2 to the right of V1Σ−1\n1\nbefore multiplying it by A. That\nis, we further approximate\nˆℓj(A) ≈ˆˆℓj(A) = ∥δ∗\nj A(V1Σ−1\n1 S2)∥2\n2.\n(6.11)\nThis reduces the cost of the matrix multiplication to O(mnd2) and hence the cost of\nthe overall procedure to O(d1n2 + d2mn). [DMM+12] gives details on how large d1\nand d2 must be to ensure useful accuracy guarantees for the approximate leverage\nscores; see also [MMY15, §5.2] for a related evaluation.\nThis estimation method can be adapted to eﬃciently compute “cross-leverage\nscores,” as well as subspace leverage scores; see [DMM+12] for details. It also has\nnatural adjustments to make it faster. For example, [CW17] suggest replacing the\nSRFT S1 by ˜S1 = FC where C is a CountSketch and F is an SRFT that further\ncompresses the output of C; [NN13] propose replacing S1 by a SASO (recall from\nSection 2.4.1 that a SASO is generalized CountSketch), which yields a similar speedup as that achieved in [CW17].\n6.2.2\nSubspace leverage scores\nThere is a wide range of possibilities for estimating subspace leverage scores. We\ndescribe two such methods here (slightly adapted) from [DMM+12]. Let us say that\nwe want to estimate the rank-k leverage scores of A for some k ≪min{m, n}. Both\nof the algorithms below work by ﬁnding the exact leverage scores of an implicit\nrank-k matrix ˆA, for which a distance ∥ˆA −A∥is near-optimal among all rank-k\napproximations.\nAn adaptation of [DMM+12, Algorithm 5]\nThe original goal of this algorithm was to return the leverage scores of a rank-k\napproximation of A that was near-optimal in Frobenius norm. Framing things more\nabstractly, the approach requires that the user specify an oversampling parameter\ns ∈O(k). Its ﬁrst step is to compute a rank-(k + s) QB decomposition of A (e.g.,\nby some method from Section 4.3.2) A ≈QB. Next, it computes the top k left\nsingular vectors of B by some traditional method. Letting Uk denote the (k +s)×k\nmatrix of such leading left singular vectors, the algorithm takes the columns of QUk\nto deﬁne approximations of the leading k left singular vectors of A. The row-norms\nof this matrix deﬁne the approximate rank-k leverage scores.\nIn context, [DMM+12, Algorithm 5] used an elementary QB decomposition with\nQ = orth(AS) for an n×(k+s) Gaussian operator S. The analysis of this algorithm\npresumed that s ≥⌈k/ϵ + 1⌉for some tolerance parameter ϵ. The meaning of ϵ was\nas follows: when viewed as random variables, the returned leverage scores coincide\nwith those of a rank-k approximation ˆA where\nE∥ˆA −A∥2\nF ≤(1 + ϵ)\nX\nj>k\nσj(A)2.\nLooking back at this error bound from our present perspective, it is clear that\na huge variety of similar bounds can be obtained by using diﬀerent methods for\nthe QB decomposition. One possibility on this front would be to use adaptive QB\nalgorithms that approximate A to some prescribed accuracy.\nSubspace leverage\nscores obtained in this way may be well-suited for approximating A by a low-rank\nsubmatrix-oriented decomposition up to prescribed accuracy.\nPage 118\narXiv Version 2\n\n6.2. Approximation schemes\nLeverage Score Sampling\nA description of [DMM+12, Algorithm 4]\nThis is a two-stage method to ﬁnd the leverage scores of a rank-k approximation to\nA that is near-optimal in spectral norm.\nTo understand the ﬁrst stage, recall that some of the simplest QB algorithms\nmake use of power iteration as described in Section 4.3.1. That is, rather than\nsetting Q = orth(AS) for Gaussian S, they set S = (A∗A)qS0 for Gaussian S0.\nPractical implementations of QB based on power iteration introduce stabilization\nbetween successive applications of A and A∗. Such stabilization preserves the range\nof AS, but it may change its singular vectors. If such stabilization is not used,\nthen the left singular vectors of A(A∗A)qS0 for Gaussian S0 would be reasonable\napproximations to the leading left singular vectors of A (modulo numerical problems\nthat are sure to arise for moderate q).\nThe observation above is the basis for [DMM+12, Algorithm 4]. In context, its\nﬁrst stage is to compute Sq+1 = (AA∗)qAS0 from an n × 2k Gaussian operator S0.\nIn a second stage, approximate leverage scores of Sq+1 – call them ˆℓi – are obtained\nfrom any method that ensures\n|ˆℓi −ℓi(Sq+1)| ≤ϵ ℓi(Sq+1).\nThese approximations are the estimates for the rank-k leverage scores of A.\n[DMM+12, Lemma 15 and Theorem 16] prescribe a value for q (as a function\nof m, n, k, and ϵ) that ensures an approximation guarantee for the leverage score\nestimates given above.\nSpeciﬁcally, for the prescribed q, the estimated leverage\nscores are within a factor\n1−ϵ\n2(1+ϵ) of the leverage scores of a rank-k matrix ˆA that\nsatisﬁes\nE∥ˆA −A∥2 ≤(1 + ϵ/10)σk+1(A).\nAs before, the randomness in this expectation is over the randomness used to estimate the leverage scores.\n6.2.3\nRidge leverage scores\nA wide variety of algorithms have been devised to estimate ridge leverage scores or\ncarry out approximate ridge leverage score sampling. The simplest such algorithm,\nproposed in [AM15] alongside the deﬁnition of ridge leverage scores, proceeds as\nfollows:\n• Start with a distribution p = (pi)i∈JmK over the column index set of K.\n• Construct a column selection operator S with n columns, where each column\nis independently set to δi ∈Rm with probability pi.\n• Compute the Nystr¨om approximation of K with respect to S. Suppose the\napproximation is represented as ˆK = BB∗for an m × n matrix B.\n• Using bi ∈Rn for the ith row of B, take ˜ℓi := b∗\ni (B∗B + λI)−1bi as an\napproximation for the ith ridge leverage score of K with regularization λ.\nOne can of course start with p = (1/m)i∈JmK as the uniform distribution over\ncolumns of K. An alternative starting point is the distribution p = diag(K)/ tr(K).\nWhile the latter distribution can lead to useful theoretical guarantees (see [AM15,\nTheorem 4]) it is not suitable for computing very accurate approximations.\narXiv Version 2\nPage 119\n\nLeverage Score Sampling\n6.3. Special topics and further reading\nIterative methods should be used if accurate approximations to ridge leverage\nscores are desired. Notably, most of the iterative methods in the literature simultaneously estimate the ridge leverage scores and sample columns from K according\nto the estimates [MM17; CLV17; RCC+18]. This algorithmic structure blurs the\ndistinction between approximating ridge leverage scores and producing a Nystr¨om\napproximation of K via column selection. This precise nature of the blurring can\nalso vary substantially from one algorithm to another. For example, [MM17, Algorithms 2 and 3] are very diﬀerent from [CLV17, Algorithm 1], which in turn is\nmaterially diﬀerent from [RCC+18, Algorithms 1 and 2].\nThe abundance and sophistication of these methods make it impractical for us\nto summarize them here.\nWe instead settle for stating their general qualitative\nconclusions. Letting d = tr(K(K + λI)−1) denote the eﬀective rank of K, one can\nconstruct an approximation ˆK of rank n ∈O(d log d) for which ∥K −ˆK∥2 ≤λ holds\nwith high probability. Furthermore, these approximations can be constructed in\ntime O(mn2) using only n column samples from K. We refer the reader to the cited\nworks above for details on speciﬁc algorithms.\n6.3\nSpecial topics and further reading\nHere, we mention a handful of generalizations and variants of leverage score sampling that, while not part of our immediate plans, may be of longer-term interest. The interested reader should consult the source material for details of what\nwe describe below.\nIn addition to those source materials, the interested reader\nis referred to Sobczyk and Gallopoulos’ paper [SG21], which is accompanied by\na carefully developed C++ and Python library called pylspack [SG22]. We also\nrecommend Larsen and Kolda’s recent work [LK20, §4 and Appendix A] – which\nincludes practical advice on leverage score sampling and theoretical results with\nexplicit constant factors.\n6.3.1\nLeverage score sparsiﬁed embeddings\nOur concept of long-axis-sparse operators from Section 2.4.2 is based on the Leverage\nScore Sparsiﬁed or LESS embeddings of Derezi´nski et al.\n[DLD+21].\nHere we\nexplain the role of leverage scores when using these sketching operators.\nLet S be a a random d × m long-axis-sparse operator (d ≪m) with sparsity\nparameter k and sampling distribution p = (p1, . . . , pm). The idea of LESS embeddings is that varying k should provide a way to interpolate between the low cost\nof sketching by row sampling and the high cost of sketching by Gaussian operators, while still obtaining a sketch that is meaningfully Gaussian-like. Indeed, if\nk ≈n = rank(A), then [Der22a] showed that, with high probability, the resulting\nsketching operator is nearly indistinguishable from a dense sub-gaussian operator\n(such as Gaussian or Rademacher), despite the reduction from O(dmn) time to\nO(dn2).\nThis performance comparison was demonstrated for several estimation\ntasks involving the inverse covariance matrix A∗A [DLD+21], as well for the Newton Sketch optimization method [DLP+21].\nAs with other uses of leverage scores, approximate leverage scores suﬃce for\nLESS embeddings; and the computational cost of a LESS embedding is typically\ndominated by the cost of estimating the leverage scores of A. The use of leverage\nscores in the sparsiﬁcation pattern is essential for theoretically showing that a LESS\nPage 120\narXiv Version 2\n\n6.3. Special topics and further reading\nLeverage Score Sampling\nembedding exhibits nearly identical performance to a Gaussian operator for all\nmatrices A. Good empirical performance observed in practice, to a varying degree,\nalso when p is the uniform distribution and k ≪rank(A) [DLP+21, §5].\n6.3.2\nDeterminantal point processes\nIn many data analysis applications, submatrix-oriented decompositions such as\nNystr¨om approximation via column selection are desirable for their interpretability.\nIn this context, we may wish to produce a very small but high-quality sketch of the\nmatrix A, using a method more reﬁned (albeit slower) than leverage score sampling.\nHere we discuss Determinantal Point Processes (DPPs; [DM21a]) as one of many\nsuch methods from the literature.\nLet A be an m×m psd matrix. A Determinantal Point Process is a distribution\nover index subsets J ⊆JmK such that:\nP(J = S) = det(A[S, S])\ndet(A + I) .\nThe above DPP formulation is known as an L-ensemble, and it is also sometimes\ncalled volume sampling [DRV+06; DM10]. Unlike leverage score sampling, individual indices sampled in a DPP are not drawn independently, but rather jointly, to\nminimize redundancies in the sampling process. In fact, a DPP can be viewed as\nan extension of leverage score sampling that incorporates dependencies between the\nsamples, inducing diversity in the selected subset [KT12].\nDPP sampling can be used to construct improved Nystr¨om approximations\nˆA = (AS)(S∗AS)†(AS)∗where the selection matrix S corresponds to the random\nsubset J. In particular, [DRV+06; GS12; DKM20] established strong guarantees\nfor this approach in terms of the nuclear norm error relative to the best rank k\napproximation: ∥ˆA −A∥∗≤(1 + ϵ)∥Ak −A∥∗, where k is the target rank and\nthe subset size |J| is chosen to be equal or slightly larger than k. DPPs have also\nfound applications in machine learning [KT12; DKM20; DM21a] as a method for\nconstructing diverse and interpretable data representations.\nIt is challenging to implement eﬃcient methods for sampling from a DPP, and\nthis is an area of ongoing work. One promising method has recently been proposed\nby Poulson [Pou20]. Two other classes of methods can be obtained by exploiting\nthe connection between DPPs and ridge leverage scores.\n1. One can use intermediate sampling with ridge leverage scores to produce a\nlarger index set T, which is then trimmed down to produce a smaller DPP\nsample J ⊆T [Der19; DCV19; CDV20].\n2. One can use iterative reﬁnement on a Markov chain, where we start with\nan initial subset J1, and then we gradually update it by swapping out one\nindex at a time, producing a sequence of subsets J1, J2, J3, ..., which rapidly\nconverges to a DPP distribution [AGR16; AD20; ADV+22].\nThe computational cost of these procedures is usually dominated by the cost of\nestimating the ridge leverage scores (recall that there are methods for doing this that\ndo not need to access all of A). However, these procedures carry additional overhead\nsince some of the ridge leverage score samples must be discarded to produce the\nﬁnal DPP sample.\narXiv Version 2\nPage 121\n\nLeverage Score Sampling\n6.3. Special topics and further reading\n6.3.3\nFurther variations on leverage scores\nIn the case of tall data matrices A, leverage scores are useful for ﬁnding data-aware\nsketching operators S so that the Euclidean norms of vectors in the range of SA\nare comparable to the Euclidean norms of vectors in the range of A. A related\nconcept called Lewis weights can be used for matrix approximation where we want\nS to approximately preserve the p-norm of vectors in the range of A for some p ̸= 2\n[CP15]. These are improved versions of leverage-like scores used by Dasgupta et al.\nfor ℓp regression [DDH+09]. A more recently proposed concept samples according\nto the probabilities\npi =\n\n(A∗A)−1 A[i, :]\n\n2\nPm\nj=1\n\n(A∗A)−1 A[j, :]\n\n2\nin order to estimate the variability of sketch-and-solve solutions to overdetermined\nleast squares problems [MZX+22]; see also [MMY15] for related importance sampling probabilities that come with useful statistical properties. Similar probabilities\n(where all norms in the above expression were squared) were studied in [DCM+19].\nPage 122\narXiv Version 2\n\nSection 7\nAdvanced Sketching:\nTensor Product Structures\n7.1 The Kronecker and Khatri–Rao products ................... 124\n7.2 Sketching operators .................................................... 125\n7.2.1 Row-structured tensor sketching operators .................... 125\n7.2.2 The Kronecker SRFT ................................................ 126\n7.2.3 TensorSketch ............................................................ 127\n7.2.4 Recursive sketching ................................................... 127\n7.2.5 Leverage score sampling for implicit matrices with tensor\nproduct structures .................................................... 128\n7.3 Partial updates to Kronecker product sketches .......... 130\n7.3.1 Background on the CP decomposition .......................... 131\n7.3.2 Sketching for the CP decomposition ............................. 132\n7.3.3 Background on the Tucker decomposition ..................... 133\n7.3.4 Sketching for the Tucker decomposition ........................ 134\n7.3.5 Implementation considerations .................................... 134\nThis section considers eﬃcient sketching of data with tensor product structure.\nWe speciﬁcally focus on implicit matrices with Kronecker and Khatri–Rao product\nstructure. These structures are of interest in RandNLA due to their prominent role\nin certain randomized algorithms for tensor decomposition. A secondary point of\ninterest is that the operators discussed in this section can also be used for sketching\nunstructured matrices. They may, for example, be used as alternatives to unstructured test vectors in norm and trace estimation [BK21]. In this case, the main\nbeneﬁt would not be improved speed but reduced storage requirements for storing\nthe sketching operator.\nIn Section 7.1 we deﬁne the Kronecker and Khatri–Rao matrix products. Section 7.2 presents four families of sketching operators that can be applied eﬃciently\nto matrices that are stored implicitly with these product structures. Section 7.3 discusses implementation considerations for the structured sketching operators in this\nsection, with a focus on how they can be used in tensor decomposition algorithms.\n123\n\nTensor Product Structures\n7.1. The Kronecker and Khatri–Rao products\nA note on scope\nWe should emphasize that algorithms for general tensor computations are out-ofscope for RandLAPACK. The functionality described here would only be made available as utility functions (i.e., computational routines) for facilitating certain tensor\ncomputations. This is part of a broader idea that RandLAPACK should facilitate\nadvanced sketching operations of interest in RandNLA that are outside the scope\nof the RandBLAS.\n7.1\nThe Kronecker and Khatri–Rao products\nSuppose that B is an m×n matrix and C is a p×q matrix. The Kronecker product\nof B and C is the mp × nq matrix\nB ⊗C =\n\n\nB[1, 1] · C\nB[1, 2] · C\n· · ·\nB[1, n] · C\nB[2, 1] · C\nB[2, 2] · C\n· · ·\nB[2, n] · C\n...\n...\n...\nB[m, 1] · C\nB[m, 2] · C\n· · ·\nB[m, n] · C\n\n.\nIf B and C have the same number of columns (i.e., if n = q), then their Khatri–Rao\nproduct is the mp × n matrix\nB ⊙C =\n\u0002B[:, 1] ⊗C[:, 1]\nB[:, 2] ⊗C[:, 2]\n· · ·\nB[:, n] ⊗C[:, n]\u0003\n.\nThe Khatri–Rao product is sometimes also referred to as the matching columnwise\nKronecker product for transparent reasons. The Kronecker and Khatri–Rao products for more than two matrices are deﬁned in the obvious way. Note that for two\nvectors x and y we have that\nx ⊗y = x ⊙y = vec(x ◦y)\nwhere ◦denotes the outer product and vec(·) is an operator that turns a matrix\ninto a vector by vertically concatenating its columns. We also use ⊛to denote the\nelementwise (Hadamard) product.\nMatrices with Kronecker and Khatri–Rao product structure tend to be very\nlarge. For example, consider matrices B1, . . . , BL, all of size m × n. Their Kronecker product B1 ⊗· · · ⊗BL is an mL × nL matrix and their Khatri–Rao product\nB1 ⊙· · · ⊙BL is an mL × n matrix.\nThe exponential dependence on L means\nthat these products can become very large even if the matrices B1, . . . , BL are not\nespecially large. Even just forming and storing these products may therefore be\nprohibitively expensive.\nKronecker and Khatri–Rao product matrices feature prominently in algorithms\nfor tensor decomposition (i.e., decomposition of multidimensional arrays into sums\nand products of more elementary objects, see Section 7.3). They also appear in a\nvariety of other contexts when sketching techniques are helpful, such as for representation of polynomial kernels [PP13; ANW14; WZ20; WZ22], when ﬁtting polynomial chaos expansion models in surrogate modeling [TNX15; SNM17; CMX+22],\nmulti-dimensional spline ﬁtting [DSS+18], and in PDE inverse problems [CLN+20].\nPage 124\narXiv Version 2\n\n7.2. Sketching operators\nTensor Product Structures\n7.2\nSketching operators\nSection 7.2.1 introduces sketching operators that are distinguished by having rows\nwith particular structures. Section 7.2.2 discusses a variant of the SRFT with an additional tensor-produce structure. Section 7.2.3 discusses TensorSketch operators,\nwhich are analogous to CountSketch operators from Section 2.4.1. In Section 7.2.4\nwe describe sketching operators that are recursive and have multi-stage structure.\nThese incorporate some of the sketching operators discussed in the previous subsections as stepping stones. Section 7.2.5 covers row sampling methods for tall matrices\nwith tensor product structure.\nWe note that the sketching operators in Sections 7.2.1–7.2.4 are all oblivious,\nwhereas the sampling-based methods in Section 7.2.5 are not. We also note that\nall of the oblivious sketching operators we discuss could be applied to unstructured\nmatrices. This would yield no speed beneﬁt compared to using their unstructured\ncounterparts, but it would reduce the storage requirement compared to traditional\ndense sketching operators of the kind supported by the RandBLAS.\n7.2.1\nRow-structured tensor sketching operators\nHere we describe three types of sketching operators whose rows can be applied to\nKronecker and Khatri–Rao product matrices very eﬃciently. The second of these\nmethods requires notions of tensor representations such as the CP format, which\nwe will revisit in Section 7.3.\nKhatri–Rao products of elementary sketching operators\nThe most basic row-structured sketching operator takes the form\nS = (S1 ⊙S2 ⊙· · · ⊙SL)∗,\n(7.1)\nwhere each Sk is an appropriate random matrix of size mk × d for k ∈JLK. Such\nan operator maps (m1 · · · mL)-vectors to d-vectors. It can be eﬃciently applied to\nKronecker product vectors, which in turn means that it can be applied eﬃciently\n(column-wise) to both Kronecker and Khatri–Rao product matrices. Consider vectors x1, . . . , xL where xk is a length-mk vector. The operator in (7.1) is then applied\nto a vector v = x1 ⊗· · · ⊗xL via the formula\nSv = (S∗\n1x1) ⊛(S∗\n2x2) ⊛· · · ⊛(S∗\nLxL).\nTo the best of our knowledge, [BBB15] were the ﬁrst to use random matrices\nof the form (7.1) to accelerate tensor computations in the spirit of RandNLA.1\nThey suggest drawing the entries of each Sk independently from a distribution with\nmean zero and unit variance, but they do not provide any theoretical guarantees for\nthe performance of such sketching operators. Sun et al. [SGT+18] independently\npropose using operators of the form (7.1) where the submatrices Sk are chosen\nto be either Gaussian or sparse operators. They also propose a variance-reduced\nmodiﬁcation which is an appropriate rescaling of the sum of several maps of the\nform (7.1). They provide theoretical guarantees for sketching operators in (7.1)\nwith L = 2 (and its variance-reduced modiﬁcation) when S1 and S2 have entries\nthat are drawn independently from an appropriately scaled mean-zero sub-Gaussian\ndistribution, leaving analysis for the case when L > 2 open for future work.\n1Similar ideas were used earlier for applications in diﬀerential privacy; see [KRS+10; Rud12].\narXiv Version 2\nPage 125\n\nTensor Product Structures\n7.2. Sketching operators\nRow-wise vectorized tensors\nRakhshan and Rabusseau [RR20] propose a distribution of sketching operators for\nwhich the ith row is given by S[i, :] = vec(Xi)∗, where Xi is a tensor in some\nfactorized format and vec is a function that returns a vectorized version of its input\nas a column vector (vec(X) = X(:) in Matlab notation). More speciﬁcally, they\nconsider two cases: In the ﬁrst case, Xi is in CP format and deﬁned elementwise\nvia\nXi[j1, j2, . . . , jL] =\nR\nX\nr=1\na(i,1)\nr\n[j1] · a(i,2)\nr\n[j2] · · · a(i,L)\nr\n[jL]\n(7.2)\nwhere the vector entries a(i,n)\nr\n[jn] are drawn independently from an appropriately\nscaled Gaussian distribution.\nIn the second case, Xi is in so-called tensor train\nformat and deﬁned elementwise via\nXi[j1, j2, . . . , jL] = A(i,1)\nj1\nA(i,2)\nj2\n· · · A(i,L)\njL\n,\n(7.3)\nwhere L is the number of tensor modes, and each matrix A(i,n)\njn\nis of size Rn ×Rn+1\nwhere R1 = RL+1 = 1 to ensure that the product is a scalar. The entries of A(i,n)\njn\nare drawn independently from an appropriately scaled Gaussian distribution.\nFor both of the constructs described above, the inner product of vec(Xi) and\nKronecker product vectors can be computed eﬃciently due to the special structure\nof the CP and tensor train formats. This makes eﬃcient application of the operator\nto Kronecker and Khatri–Rao product matrices possible. Theoretical guarantees are\nprovided for these vectorized tensor sketching operators in [RR20]. The follow-up\nwork [RR21] shows that the results for the tensor train-based sketching operators\nalso extend to the case when the cores are drawn from a Rademacher distribution.\nTwo-stage operators\nIwen et al. [INR+20] propose a two-stage sketching procedure for mapping (m1 · · · mL)vectors to d-vectors. The ﬁrst step consists of applying a row-structured matrix\n(S1 ⊗· · · ⊗SL), where each Sk is a sketching operator of size pk × mk. This maps\nthe (m1 · · · mL)-vector to an intermediate embedding space of dimension (p1 · · · pL).\nThis is then followed by another sketching operator T of size d × (p1 · · · pL) which\nmaps the intermediate representation to the ﬁnal d-dimensional space.\n7.2.2\nThe Kronecker SRFT\nKronecker SRFTs are a variant of the SRFTs discussed in Section 2.5. They can be\napplied very eﬃciently to a Kronecker product vector without forming the vector\nexplicitly. They were ﬁrst proposed by [BBK18] for eﬃcient sketching of the Khatri–\nRao product matrices that arise in tensor CP decomposition. Theoretical analysis\nof these sketching operators can be found in [JKW20; MB20; BKW21].\nThe Kronecker SRFT that maps (m1 · · · mL)-vectors to d-vectors takes the form\nS =\nrm1 · · · mL\nd\nR\n\u0010\nL\nO\nk=1\nFk\n\u0011\u0010\nL\nO\nk=1\nDk\n\u0011\n,\n(7.4)\nwhere each Dk is a diagonal mk × mk matrix of independent Rademachers, each Fk\nis an mk ×mk fast trigonometric transform, and R randomly samples d components\nPage 126\narXiv Version 2\n\n7.2. Sketching operators\nTensor Product Structures\nfrom an (m1 · · · mL)-vector. The Kronecker SRFT replaces the F and D operators\nin the standard SRFT by Kronecker products of smaller operators of the same form.\nWith x1, . . . , xL deﬁned as in Section 7.2.1, the operator in (7.4) can be applied\neﬃciently to x1 ⊗· · · ⊗xL via the formula\nrm1 · · · mL\nd\nR\n\u0010\nL\nO\nk=1\nFk\n\u0011\u0010\nL\nO\nk=1\nDk\n\u0011\u0010\nL\nO\nk=1\nxk\n\u0011\n=\nrm1 · · · mL\nd\nR\n\u0010\nL\nO\nk=1\nFkDkxk\n\u0011\n.\nThe formula shows that only those entries in N\nk FkDkxk that are sampled by R\nneed to be computed. From this, we can back out the indices of each vector FkDkxk\nthat need to be computed. Given these indices one could compute the relevant\nentries of these vectors using subsampled FFT methods of the kind alluded to in\nSection 2.5. We note that this formula is straightforwardly extended to Kronecker\nand Khatri–Rao product matrices.\n7.2.3\nTensorSketch\nA TensorSketch operator is a kind of structured CountSketch that can be applied\nvery eﬃciently to Kronecker product matrices.2 The improved computational eﬃciency of TensorSketch comes at the cost of needing a larger embedding dimension\nthan CountSketch. TensorSketch was ﬁrst proposed in [Pag13] for fast approximate\nmatrix multiplication. It was further developed in [PP13; ANW14; DSS+18] where\nit is used for low-rank approximation, regression, and other tasks.\nLet x1, . . . , xL be deﬁned as in Section 7.2.1. A TensorSketch, which we denote\nby S below, maps an (m1 · · · mL)-vector v = x1 ⊗· · · ⊗xL to a d-vector via the\nformula\nSv = DFT−1 \u0010\nL⊛\nk=1\nDFT(Skxk)\n\u0011\n,\n(7.5)\nwhere each Sk is an independent CountSketch that maps mk-vectors to d-vectors.\nHere, DFT denotes the discrete Fourier transform which can be eﬃciently applied\nusing fast Fourier transform (FFT) methods.\nTensorSketches use the fact that\npolynomials can be multiplied using the DFT, which is why DFT and its inverse\nappear in the formula above; see [Pag13] for details.\nRemark 7.2.1. We have not investigated whether fast trig transforms other than\nthe discrete Fourier transform (e.g., the discrete cosine transform) can be used for\nthis type of sketching operator.\n7.2.4\nRecursive sketching\nIn order to achieve theoretical guarantees, the sketching operators discussed so far\nrequire an embedding dimension d which scales exponentially with L when embedding a vector of the form x1 ⊗· · · ⊗xL. Ahle et al. [AKK+20] propose sketching\noperators that are computed recursively and have the remarkable property that\ntheir requisite embedding dimensions scale polynomially with L. Since [AKK+20]\nare concerned with oblivious subspace embedding of polynomial kernels, they consider the case when all x1, . . . , xL are of the same length. However, their results\n2Recall that a CountSketch is a SASO in the sense of Section 2.4.1. Each short-axis vector in\na CountSketch has a single nonzero entry, sampled from the Rademacher distribution.\narXiv Version 2\nPage 127\n\nTensor Product Structures\n7.2. Sketching operators\nshould extend to the general case when the vectors are of diﬀerent lengths (for\nexample, see [Mal22, Corollary 18]).\nSuppose x1, . . . , xL are m-vectors and that L = 2q for a positive integer q. The\nrecursive sketching operator ﬁrst computes\ny(0)\nk\n= Tkxk\nfor\nk ∈JLK\nwhere T1, . . . , TL are independent SASOs (e.g., CountSketches, see Section 2.4.1)\nthat map m-vectors to d-vectors. The d-vectors y(0)\n1 , . . . , y(0)\nL\nare now combined\npairwise into L/2 = 2q−1 vectors. This is done by computing\ny(1)\nk\n= Sk(y(0)\n2k−1 ⊗y(0)\n2k )\nfor\nk ∈JL/2K\nwhere S1, . . . , SL/2 are independent sketching operators that map d2-vectors to dvectors. If the initial T1, . . . , TL are CountSketches then the Si are canonically TensorSketches. If instead T1, . . . , TL are more general SASOs then the Si are canonically Kronecker SRFTs. Regardless of which conﬁguration we use, the pairwise combination of vectors is repeated for a total of q = log2(L) steps until a single d-vector\nremains, which is the embedding of x1 ⊗· · · ⊗xL. The case when L is not a power\nof two is handled by adding additional vectors xk = e1 for k = L + 1, . . . , 2⌈log2(L)⌉\nwhere e1 is the ﬁrst standard basis vector in Rm. Recursive sketching operators are\nlinear despite their somewhat complicated description.\nSong et al. [SWY+21] develop a similar recursive sketching operator which takes\ninspiration from the one discussed above and applies it to the sketching of polynomial kernels. For the degree-L polynomial kernel, this involves sketching of matrices\nof the form A⊙L = A ⊙· · · ⊙A, where the matrix A appears L times in the righthand side.\nThe recursive sketching operator by [AKK+20] can be described by a binary\ntree, with each node corresponding to an appropriate sketching operator. Ma and\nSolomonik [MS22] generalize this idea by allowing for other graph structures, but\nlimit nodes in these graphs to be associated with Gaussian sketching operators.\nUnder this framework, they develop a structured sketching operator whose embedding dimension only scales linearly with L. These operators can be adapted for\neﬃcient application to vectors with general tensor network structure which includes\nKronecker products of vectors as a special case.\n7.2.5\nLeverage score sampling for implicit matrices with tensor\nproduct structures\nConsider the problem of sketching and solving a least squares problem\nmin\nx\n∥AX −Y∥F\n(7.6)\nwhen the columns of A have tensor product structure and Y is a thin unstructured\nmatrix. The sketching operators discussed so far in this section can be eﬃciently\napplied to A. However, since Y lacks structure, these sketching operators require\naccessing all nonzero elements of Y. This can be prohibitively expensive in applications such as the following.\n• In iterative methods for tensor decomposition, one typically solves a sequence\nof least squares problems for which A is structured and Y contains all the\nPage 128\narXiv Version 2\n\n7.2. Sketching operators\nTensor Product Structures\nentries of the tensor being decomposed [KB09]. When Y has a ﬁxed proportion\nof nonzero entries, the cost will therefore scale exponentially with the number\nof tensor indices—a manifestation of the curse-of-dimensionality.\n• When ﬁtting polynomial chaos expansion functions in surrogate modeling\n[TNX15; SNM17; CMX+22], A contains evaluations of a multivariate polynomial on a structured quadrature grid and Y (which will now be a column\nvector) contains the outputs of an expensive data generation process (e.g., an\nexperiment or high-ﬁdelity PDE simulation).\nIn both example applications, it is clearly desirable to avoid using all entries of\nY when solving (7.6). As discussed in Section 6, leverage score sampling can be\nused to sketch-and-solve least squares problems without accessing all entries of the\nright-hand side Y while still providing performance guarantees. Here we discuss\nhow to take advantage of the structure of A to speed up leverage score sampling.\nKronecker product structure\nConsider a Kronecker product A = A1 ⊗· · · ⊗AL of mk × nk matrices Ak. It is\npossible to perform exact leverage score sampling on A without even forming it.\nCheng et al. [CPL+16] used this fact to approximately solve least squares problems\nwith Kronecker product design matrices, which has applications in algorithms for\nTucker tensor decomposition. Formal statements and proofs of these results later\nappeared in [DJS+19].\nTo see how the sampling works, let (pi) be the leverage score sampling distribution of A and let (pik) be the leverage score sampling distribution of Ak for k ∈JLK.\nFor any i ∈JQL\nk=1 mkK and corresponding multi-index (i1, . . . , iL) satisfying\nA[i, :] = A1[i1, :] ⊗· · · ⊗AL[iL, :],\n(7.7)\nit holds that\npi = p(1)\ni1 p(2)\ni2 · · · p(L)\niL .\n(7.8)\nTherefore, instead of drawing an index i according to (pi), one can draw the index\nik according to (p(k)\nik ) for each k ∈JLK. Due to (7.7), the row corresponding to the\ndrawn index can be computed and rescaled without constructing A. This process\ncan be easily adapted to drawing multiple samples.\nFahrbach et al. [FFG22] discuss how the sampling approach above can be adapted\nfor use in ridge regression when the design matrix is a Kronecker product. Malik\net al. [MXC+22] show an approach for eﬃcient sampling according to the exact\nleverage scores of matrices of the form A[:, J] when A is a Kronecker product and\nJ is an index vector that satisﬁes certain monotonicity properties.\nKhatri–Rao product structure\nSampling according to the leverage scores of a Khatri–Rao product matrix A =\nA1 ⊙· · · ⊙AL is more challenging than it is for a Kronecker product matrix. Still,\nseveral approaches for doing so have been proposed.\nWe divide them into two\ncategories. The methods in the ﬁrst category sample according to the leverage scores\nof the Kronecker product of A1, . . . , AL instead of the Khatri–Rao product since this\nallows for simple and eﬃcient sampling. This can be viewed as sampling from a\ncoarse approximation of the Khatri–Rao product leverage scores. The methods in\narXiv Version 2\nPage 129\n\nTensor Product Structures\n7.3. Partial updates to Kronecker product sketches\nthe second category sample according to exact or high-accuracy approximations of\nthe Khatri–Rao product leverage score distribution.\nSampling according to Kronecker product leverage scores\nAs noted by [CPL+16;\nBBK18], the leverage scores of A can be upper bounded by\nℓi(A) ≤\nL\nY\nk=1\nℓik(Ak),\n(7.9)\nwhere (i1, . . . , iL) is the multi-index corresponding to i. The two papers [CPL+16;\nLK20] use the expression on the right-hand side of (7.9) as an approximation to the\nexact leverage scores on the left-hand side. By using the bound (7.9), they are able\nto prove theoretical performance guarantees when this approach is used for sketchand-solve in least squares problems. More precisely, Cheng et al. [CPL+16] sample\naccording to a mixture of the distribution in (7.8) and a distribution which depends\non the magnitude of the dependent variables (i.e., the entries in the “right-hand\nsides” in the least squares problem). Larsen and Kolda [LK20] sample with respect\nto only the distribution in (7.8).\nBharadwaj et al. [BMM+22] extend the work\nby [CPL+16; LK20] to a distributed-memory setting and provide high-performance\nparallel implementations. Ideas similar to those in [LK20] are developed for the more\ncomplicated design matrices that arise in algorithms for tensor ring decomposition\nin [MB21]. Those matrices have columns that are sums of vectors with Kronecker\nproduct structure.\nSampling according to exact or high-quality approximations of leverage scores\nMalik\n[Mal22] proposes a diﬀerent approach for the Khatri–Rao product least squares\nproblem. By combining some of the ideas for fast leverage score estimation (see\n§6.2) and recursive sketching (see §7.2.4) with a sequential sampling approach,\nhe improves the sampling and computational complexities of [LK20].\nMalik et\nal. [MBM22] simplify and generalize the method by [Mal22] to a wider family of\nstructured matrices. An upshot of this work is a method for eﬃciently sampling\na Khatri–Rao product matrix according to its exact leverage score distribution\nwithout forming the matrix.\nMotivated by applications in kernel methods, [WZ20] develop a recursive leverage score sampling method for sketching of matrices of the form A⊙L = A⊙· · ·⊙A.\nTheir method starts by sampling from a coarse approximation to the leverage score\nsampling distribution and then iteratively reﬁning it. These ideas are further reﬁned in [WZ22] where the method is also extended to general Khatri–Rao products\nof matrices that can all be distinct.\n7.3\nPartial updates to Kronecker product sketches\nThe structured sketching operators discussed in Section 7.2 are notable in that\nthey are deﬁned in terms of multiple smaller sketching operators. Here we discuss\nsituations when it is advantageous to reuse some of these smaller sketches across\nmultiple calls to the structured sketching operator. The examples we discuss come\nfrom works that use sketching in tensor decomposition algorithms. Our goal with\nthis discussion is to bring to light some of the functionality we think is important\nPage 130\narXiv Version 2\n\n7.3. Partial updates to Kronecker product sketches\nTensor Product Structures\nfor structured sketches to have in order to best support potential usage in the\ntensor community.\nBy tensor, we mean multi-index arrays containing real numbers. A tensor with\nL indices is called an L-way tensor. Vectors and matrices are one- and two-way\ntensors, respectively. Calligraphic capital letters (e.g., X) are used to denote tensors\nwith three or more indices. Much like matrix decomposition, the purpose of tensor\ndecomposition is to decompose a tensor into some number of simpler components.\nWe only give minimal background material on tensor decomposition here; see the\nreview papers [KB09; CMD+15; SDF+17] for further details.\n7.3.1\nBackground on the CP decomposition\nWe ﬁrst consider the CANDECOMP/PARAFAC (CP) decomposition which is also\nknown as the canonical polyadic decomposition [KB09, §3]. It decomposes an L-way\ntensor X of size m1 × m2 × · · · × mL into a sum of R rank-1 tensors:\nX =\nR\nX\nr=1\na(1)\nr\n◦a(2)\nr\n◦· · · ◦a(L)\nr\n,\n(7.10)\nwhere ◦denotes the outer product. The mn ×R matrices A(n) =\nh\na(n)\n1\n· · ·\na(n)\nR\ni\nfor n ∈JLK are called factor matrices. When R is suﬃciently large, we can express\nthe factor matrices as the solution to\narg min\nA(1),...,A(L)\n\nX −\nR\nX\nr=1\na(1)\nr\n◦a(2)\nr\n◦· · · ◦a(L)\nr\n\nF,\n(7.11)\nwhere ∥· ∥F denotes the Frobenius norm as generalized to tensors in the obvious\nway. The broader problem of tensor decomposition is concerned with approximately\nsolving (7.11). In particular, it is common to seek locally optimal solutions to this\nproblem even when R is too small for an identity of the form (7.10) to hold for X.\nIt is computationally intractable to solve (7.11) in the general case. However,\nthe problem admits several heuristics that are eﬀective in practice. One of the most\npopular heuristics is alternating minimization, wherein one solves for only one factor\nmatrix at a time while keeping the others ﬁxed. That is, one solves a sequence of\nproblems of the form\nA(n) = arg min\nA\n\nX −\nR\nX\nr=1\na(1)\nr\n◦· · · ◦a(n−1)\nr\n◦ar ◦a(n+1)\nr\n◦· · · ◦a(L)\nr\n\nF\n(7.12)\nfor n ∈JLK. If we adopt appropriate notation then (7.12) can be stated as a familiar\nlinear least squares problem. Accordingly, this alternating minimization approach\nis called alternating least squares (ALS). The ALS approach cycles through the\nindices n ∈JLK multiple times until some termination criteria is met.\nTypical\ntermination criteria include reaching a maximum number of iterations or seeing\nthat the improvement in the objective falls below some threshold.\nFormulating and solving the least squares problem\nWe begin by introducing ﬂattened representations of X. Speciﬁcally, for n ∈JLK,\nthe mn×\n\u0010Q\nj̸=n mj\n\u0011\nmatrix X(n) is given by horizontally concatenating the mode-n\narXiv Version 2\nPage 131\n\nTensor Product Structures\n7.3. Partial updates to Kronecker product sketches\nﬁbers X[i1, . . . , in−1, :, in+1, . . . , iN] as columns. Such a matrix can be expressed in\nMatlab notation as follows\nX(n) = reshape\n\npermute(X, [n, 1, . . . , n −1, n + 1, . . . , L]), mn,\nY\nj̸=n\nmj\n\n.\n(7.13)\nNext, we introduce the following ﬂattened tensorizations of the factor matrices:\nA̸=n := A(L) ⊙· · · ⊙A(n+1) ⊙A(n−1) ⊙· · · ⊙A(1) =:\n1\nK\nj=L\nj̸=n\nA(j).\n(7.14)\nWhere we emphasize that the order of the matrices in the above product is important; our notation for the Khatri–Rao product at right reﬂects how the order\nproceeds from j = L to j = 1, skipping j = n.\nIn terms of these matrices, the ALS update rule for the nth factor matrix is\nA(n) = arg min\nA\n\nA̸=nA∗−X∗\n(n)\n\nF.\n(7.15)\nWe note that the Gram matrix for this least squares problem can be computed\neﬃciently by the formula\nA̸=n∗A̸=n = (A(L)∗A(L)) ⊛· · · ⊛(A(n+1)∗A(n+1))\n⊛(A(n−1)∗A(n−1)) ⊛· · · ⊛(A(1)∗A(1)).\n(7.16)\nTherefore solving the least squares problem in (7.15) via the normal equations can\nbe very eﬃcient [KB09, §3.4]. Indeed, the ALS update rule for the nth factor matrix\nbecomes\nA(n) = X(n)A̸=n(A̸=n∗A̸=n)†.\n(7.17)\nThe most expensive part of this update is actually computing X(n)A̸=n [BBK18,\n§3.1.1], which is analogous to the vector F∗h in the normal equations for an overdetermined least squares problem minz ∥Fz −h∥2\n2. Therefore, the fact that computing this matrix is the computational bottleneck in solving (7.15) is the opposite of\nwhat one would expect when not working with tensors. This phenomenon is why\nrow-sampling sketching operators have been successful in ALS algorithms that use\nsketch-and-solve for the least squares subproblems [LK20].\nRemark 7.3.1. Although it is cheap to form the Gram matrix (7.16), there is potential for very bad conditioning even when L is small. We do not know how seriously\nthe poor conditioning aﬀects ALS approaches to CP decomposition in practice.\n7.3.2\nSketching for the CP decomposition\nBattaglino et al. [BBK18] apply the Kronecker SRFT from Section 7.2.2 to the least\nsquares problem in (7.15). Letting Tj and Fj be of size mj × mj, the sketching\noperator used before solving for the nth factor matrix is\nSn =\nv\nu\nu\nt\nQL\nj=1\nj̸=n\nmj\nd\nR\n\u0010\n1\nO\nj=L\nj̸=n\nTj\n\u0011\u0010\n1\nO\nj=L\nj̸=n\nFj\n\u0011\n.\n(7.18)\nPage 132\narXiv Version 2\n\n7.3. Partial updates to Kronecker product sketches\nTensor Product Structures\nOur notation for the Kronecker product operator indexes from j = L to j = 1 so as\nto mimic our earlier notation for the Khatri–Rao product (see (7.14)).\nA by-the-book application of this operator would require drawing new R and\n(Fj)j̸=n every time it is applied in (7.15), i.e., L times for every execution of the for\nloop. Battaglino et al. [BBK18, Alg. 4] instead propose drawing F1, . . . , FL once and\nthen reusing them throughout the algorithm, only drawing R anew for each least\nsquares problem. This reduces the computational cost considerably since it allows\nfor greater reuse of various computed quantities. More speciﬁcally, the expensive\napplication of the full Kronecker SRFT to the unstructured matrix X∗\n(n) does not\nhave to be computed for every least squares problem.\nLarsen and Kolda [LK20] also sketch the least squares problems in (7.15).\nThey use the eﬃcient leverage score sampling scheme for Khatri–Rao products\ndiscussed in Section 7.2.5.\nThis approach also allows for some reuse between\nsubsequent sketches.\nWhen solving for A(n) in (7.15), a row with multi-index\n(i1, . . . , in−1, in+1, . . . , iL) is sampled with probability p(1)\ni1 · · · p(n−1)\nin−1 p(n+1)\nin+1 · · · p(L)\niL ,\nwhere (p(k)\nik ) is the leverage score sampling distribution for A(k). Since each A(k)\nonly change for every Lth least squares problem, the probability distribution (p(k)\nik )\ncan be used in L −1 least squares problems before it needs to be recomputed.\n7.3.3\nBackground on the Tucker decomposition\nThe Tucker decomposition [KB09, §4] is another popular method that decomposes\nan L-way tensor X of size m1 × m2 × · · · × mL into\nR1\nX\nr1=1\nR2\nX\nr2=1\n· · ·\nRL\nX\nrL=1\nG[r1, r2, . . . , rL] a(1)\nr1 ◦a(2)\nr2 ◦· · · ◦a(L)\nrL ,\n(7.19)\nwhere the so-called core tensor G is of size R1×R2×· · ·×RL. The mn×Rn matrices\nA(n) =\nh\na(n)\n1\n· · ·\na(n)\nRn\ni\nfor n ∈JLK are called factor matrices. Similarly to the\nCP decomposition, the core tensor and factor matrices can be initialized randomly\nand then updated iteratively via ALS:3\nFor n in JLK :\nA(n) = arg min\nA\n∥B̸=nG∗\n(n)A∗−X∗\n(n)∥F,\n(7.20)\nG = arg min\nZ\n∥B vec(Z) −vec(X)∥F,\n(7.21)\nwhere\nB̸=n = A(L) ⊗· · · ⊗A(n+1) ⊗A(n−1) ⊗· · · ⊗A(1),\nB = A(L) ⊗· · · ⊗A(1),\nand the unfolding G(n) is deﬁned analogously to X(n) in (7.13). The steps in (7.20)\nand (7.21) are then repeated until some convergence criterion is met.\nWe note\nthat the least squares problems (7.20) and (7.21) are highly overdetermined when\n(Rn)n∈JLK are small compared to (mn)n∈JLK.\n3The update rules in (7.20) and (7.21) have been formulated as least squares problems in order\nto show where sketching can be applied in the ALS algorithm. A more standard formulation of\nthe update rules can be found in [KB09, §4.2].\narXiv Version 2\nPage 133\n\nTensor Product Structures\n7.3. Partial updates to Kronecker product sketches\n7.3.4\nSketching for the Tucker decomposition\nMalik and Becker [MB18] apply the TensorSketch discussed in Section 7.2.3 to these\nproblems. From a straightforward adaption of (7.5) to matrix Kronecker products,\nwe have that the TensorSketch of the design matrix B̸=n is computed via the formula\nDFT−1\n\u0012\u0010\n1\nK\nj=L\nj̸=n\n\u0000DFT(SjA(j))\n\u0001⊤\u0011⊤\u0013\n,\nwhere Sj is a d×mj CountSketch, and where ⊤denotes transpose without complex\nconjugation. The formula for sketching B is the same except for that it also includes\nthe nth term in the Khatri–Rao product.\nInstead of drawing new CountSketches for every application of TensorSketch,\n[MB18, Alg. 2] draw two sets of independent CountSketches at the start of the\nalgorithm: (S(1)\nj )L\nj=1 where S(1)\nj\nis of size d1 × mj, and (S(2)\nj )L\nj=1 where S(2)\nj\nis of\nsize d2 × mj. These two sets of sketches are then reused throughout the algorithm:\n(S(1)\nj ) are used for sketching (7.20) and (S(2)\nj ) are used for sketching (7.21). The\nlatter least squares problems are much larger than the former.\nUsing two sets\nof sketching operators makes it possible to choose a larger embedding dimension\nfor the latter problem, i.e., choosing d2 > d1. By reusing CountSketches in this\nfashion, considerable improvement in run time is achieved. Moreover, it is possible\nto compute all relevant sketches of unfoldings of X at the start of the algorithm,\nleading to an algorithm that requires only a single pass of X in order to decompose it.\n7.3.5\nImplementation considerations\nWe deem it most appropriate to implement the structured sketches discussed in\nSection 7.2 in RandLAPACK rather than RandBLAS. In order to facilitate the applications discussed in Section 7.3, it should be possible to update or redraw speciﬁc\ncomponents of the sketching operator after it has been created. For example, when\napplying the operator in (7.18) in an ALS algorithm for CP decomposition as in\n[BBK18, Alg. 4], we want to keep the random diagonal matrices F1, . . . , FL ﬁxed\nbut draw a new sampling operator R before each application of Sn.\nIn the applications above, components are shared across the L diﬀerent sketching\noperators that are used when updating the L diﬀerent factor matrices. Rather than\ndeﬁning L diﬀerent sketching operators with shared components, it is better to\ndeﬁne a single operator that contains all components and which allows “leaving\none component out” when applied to a matrix or vector. For example, consider\na Kronecker SRFT from (7.4) but with reversed order in the Kronecker products.\nIt contains the components R and F1, . . . , FL. A user should be able to supply a\nparameter n which indicates that the nth term in the Kronecker products should\nbe left out when computing the sketch, resulting in a sketch of the form (7.18).\nPage 134\narXiv Version 2\n\nAppendix A\nDetails on Basic Sketching\nA.1 Subspace embeddings and eﬀective distortion ............ 135\nA.1.1 Eﬀective distortion of Gaussian operators ..................... 137\nA.2 Short-axis-sparse sketching operators ......................... 137\nA.2.1 Implementation notes\n............................................... 137\nA.2.2 Theory and practical usage ......................................... 139\nA.3 Theory for sketching by row selection ......................... 140\nThis appendix covers sketching theory and implementation of sketching operators. Its contents are relevant to Sections 2, 3 and 6.\nA.1\nSubspace embeddings and eﬀective distortion\nLet S be a wide d × m sketching operator and L be a linear subspace of Rm. Recall\nfrom Section 2.2.2 that S embeds L into Rd with distortion δ ∈[0, 1] if\n(1 −δ)∥x∥2 ≤∥Sx∥2 ≤(1 + δ)∥x∥2\nholds for all x in L. We often use the term δ-embedding for such an operator. Note\nthat if S is a δ-embedding and δ′ is greater than δ, then S is also a δ′-embedding.\nIt can be useful to speak of the smallest distortion δ for which S is a δ-embedding\nfor L; we call this the distortion of S for L, and denote it by\nD(S; L) = inf{ δ : 0 ≤δ ≤1\nS is a δ-embedding for L}.\nIn this notation, we have D(S; L) ≥1 when ker S∩L is nontrivial. If there is a unit\nvector x in L where ∥Sx∥> 2, then D(S; L) = +∞.\nSubspace embedding distortion has a signiﬁcant limitation in that it depends on\nthe scale of S, while many RandNLA algorithms have no such dependence. This\nshortcoming leads us to deﬁne the eﬀective distortion of S for L as\nDe(S; L) = inf\nt>0 D(tS; L).\n(A.1)\nHere, the inﬁmum is over t > 0 rather than t ̸= 0 since D(S; L) = D(−S; L).\n135\n\nDetails on Basic Sketching\nA.1. Subspace embeddings and eﬀective distortion\nThere is a convenient formula for eﬀective distortion using concepts of restricted\nsingular values and restricted condition numbers. Restricted singular values are a\nfairly general concept of use in random matrix theory; see, e.g., [OT17]. They are\nmeasures an operator’s “size” when considered from diﬀerent vantage points within\na set of interest. Formally, we deﬁne the largest and smallest restricted singular\nvalues of a sketching operator S for a subspace L as\nσmax(S; L) = max\nx∈L {∥Sx∥2 : ∥x∥2 = 1}\nand\nσmin(S; L) = min\nx∈L {∥Sx∥2 : ∥x∥2 = 1}\nGiven these concepts, we deﬁne the restricted condition number of S on L as\ncond(S; L) = σmax(S; L)\nσmin(S; L) ,\nwhere we take c/0 = +∞for any c ≥0.\nWe have formulated the concepts of restricted singular values and condition\nnumbers in a way that reﬂects their geometric meaning. More concrete descriptions\ncan be obtained by considering any matrix U whose columns provide an orthonormal\nbasis for L. With this one can see that σmin(S; L) and σmax(S; L) coincide with the\nextreme singular values of SU, and that cond(S; L) = cond(SU).\nNext, we provide the connection between restricted condition numbers and effective distortion. Appendix B.1.1 explores this connection more in the context of\nsketch-and-precondition algorithms for saddle point problems.\nProposition A.1.1. Let L be a linear subspace and S be a sketching operator on\nL. The eﬀective distortion of S for L is\nDe(S; L) = κ −1\nκ + 1\nwhere we take (∞−1)/(∞+ 1) = 1.\nProof. The scaled sketching operator tS is a δ-embedding for L if and only if\n(1 −δ)∥x∥2 ≤t∥Sx∥2 ≤(1 + δ)∥x∥2\nfor all x in L.\nThis is equivalent to\n1 −δ\nt\n≤∥Sx∥2\n∥x∥2\nand\n∥Sx∥2\n∥x∥2\n≤1 + δ\nt\nfor all x in L \\ {0}.\nTo simplify these bounds, we optimize over x. Abbreviating σ1 := σmax(S; L) and\nσn := σmax(S; L) for n = dim(L), we ﬁnd that tS is a δ-embedding if and only if\n1 −δ\nt\n≤σn\nand\n1 + δ\nt\n≤σ1.\nThese identities can be rearranged to ﬁnd the following constraints on δ:\n1 −σ1t ≤δ\nand\ntσn −1 ≤δ.\nPage 136\narXiv Version 2\n\nA.2. Short-axis-sparse sketching operators\nDetails on Basic Sketching\nThe value of t which permits minimum δ is that which makes the lower bounds\ncoincide. That is, the optimal t is t⋆= 2/(σ1 + σn). Plugging this into the bounds\nabove, our constraints on δ reduce to\nδ ≥1 −σ1t⋆= t⋆σn −1 = σ1 −σn\nσ1 + σn\n= κ −1\nκ + 1,\nas desired.\nA.1.1\nEﬀective distortion of Gaussian operators\nIt is informative to consider the concepts of restricted condition numbers and effective distortion for Gaussian sketching operators. Therefore, let us suppose that\nour d×m sketching operator S has iid mean-zero Gaussian entries, and consider an\nn-dimensional subspace L in Rm. By rotational invariance of Gaussian distribution,\nwe can infer that the distribution of cond(S; L) coincides with that of cond(˜S) for a\nd × n Gaussian matrix ˜S. Strong concentration results are available to understand\nthe distribution of cond(˜S).\nSpeciﬁcally, letting d = sn for a constant s > 1, results by Silverstein [Sil85] and\nGeman [Gem80] imply\ncond(S; L) →\n√s + 1\n√s −1\nalmost surely as\nn →∞.\n(A.2)\nThis can be turned around using Proposition A.1.1 to obtain\nDe(S; L) →1\n√s\nalmost surely as\nn →∞.\n(A.3)\nWe emphasize that (A.2) and (A.3) hold for any ﬁxed n-dimensional subspace L.\nThese facts justify aggressively small choices of embedding dimension when using\nGaussian sketching operators in randomized algorithms for least squares. Meng,\nSaunders, and Mahoney come to the same conclusion in their work on LSRN\n[MSM14, Theorem 4.4].\nA.2\nShort-axis-sparse sketching operators\nIn this appendix we make liberal use of the abbreviation SASO (for “short-axissparse sketching operator”) introduced in Section 2.4.1. Without loss of generality,\nwe describe SASOs in the wide case, i.e., when S is d × m with d ≪m.\nA.2.1\nImplementation notes\nConstructing SASOs column-wise\nIt is extremely cheap to construct and store a wide SASO. The precise storage\nformat depends on how one wants to apply the SASO later on, which can vary\ndepending on context. However, the construction is embarrassingly parallel across\ncolumns provided one uses CBRNGs (counter-based random number generators;\nsee §2.1.1), and this structure leads to canonical methods for generating SASOs.\nWe ﬁrst consider the SASO construction where row indices are partitioned into\nindex sets I1, . . . , Ik of roughly equal size. Given such a partition, the indices of\narXiv Version 2\nPage 137\n\nDetails on Basic Sketching\nA.2. Short-axis-sparse sketching operators\nnonzeros for a given column are chosen by taking one element (independently and\nuniformly) from each of the index sets Ij. The naive implementation can sample\nthese row indices with k parallel calls to the CBRNG.\nNow consider the construction where the row indices for a column are chosen by\nselecting k elements from JdK uniformly without replacement. This can be done in\nO(km) time by using Fisher-Yates sampling and carefully re-using workspace. For\na concrete implementation, we refer the reader to\nhttps://github.com/BallisticLA/RandBLAS/blob/19sept22/src/sjlts.\ncc#L14-L78.1\nWhile the implementation above is sequential, it is easy to parallelize. Given T\nthreads, the natural modiﬁcation to the algorithm takes O(mk/T) time and requires\nO(Td) workspace. The constants in the big-O notation are small.\nRemarks on storage formats\nIt is reasonable for a standard library to restrict SASOs to only the most common\nsparse matrix formats. We believe both compressed sparse row (CSR) and compressed sparse column (CSC) are worth considering. CSC is the more natural of the\ntwo since (wide) SASOs are constructed columnwise. If CSR format is preferred for\nsome reason, then we recommend constructing S columnwise while retaining extra\ndata to facilitate conversion to CSR.\nIn principle, if the nonzero entries of S are ±1 and CSC is used as the storage\nformat, then one could do away with storing the nonzero values explicitly; one could\ninstead store the sign information using signed integers for the row indices. We do\nnot favor this approach since it precludes working with SASOs with more than two\ndistinct nonzero values.\nFor the matrices A and Ask, we must consider whether they are in column-major\nor row-major format. Indeed, both formats need to be supported since Section 3\nframed all least squares problems with tall data matrices. While this was without\nloss of generality from a mathematical perspective, a user with an underdetermined\nleast squares problem involving a wide column-major data matrix B is eﬀectively\nneeding to sketch the tall row-major matrix A = B∗.\nApplying a wide SASO\nThere are four combinations of storage formats we need to consider for (S, A).\nS is CSC, A is row-major.\nSuppose we have P processors. Our suggested approach\nis to partition the row index set JdK into I1, . . . , IP and to have each processor be\nresponsible for its own block of rows. The pth processor computes its row block by\nstreaming over the columns of S and rows of A, accumulating outer products as\nindicated below\nAsk[Ip, :] =\nX\nℓ∈JmK\nS[Ip, ℓ]A[ℓ, :].\nAn individual term S[Ip, ℓ]A[ℓ, :] can cheaply be accumulated into Ask[Ip, :] by using\nthe fact that S[Ip, ℓ] is extremely sparse.\nIf R denotes the number of nonzeros\nin S[Ip, ℓ], then the outer-product accumulation can be computed with R axpy\n1This code was written when we used the term “SJLT” for what we now call a “SASO.”\nPage 138\narXiv Version 2\n\nA.2. Short-axis-sparse sketching operators\nDetails on Basic Sketching\noperations involving the row A[ℓ, :]. Note that since S has k nonzeros per column\n(with row indices distributed uniformly at random), this value R is a sum of |Ip|\niid Bernoulli random variables with mean k/d. Therefore the expected number of\naxpy’s performed by processor p for term ℓis |Ip|k/d.\nS is CSR, A is row-major.\nHere, we suggest that the d rows of Ask be computed\nseparately from one another.\nAn individual row is given by Ask[i, :] = S[i, :]A.\nEvaluating the product of this sparse vector and dense matrix can be done by\ntaking a linear combination of a small number of rows of A. Speciﬁcally, if R is\nthe number of nonzeros in S[i, :] then computing Ask[i, :] only requires R rows from\nA. Since the columns of S are independent, R is a sum of m iid Bernoulli random\nvariables with mean k/d. Therefore we expect to access mk/d rows of A in order\nto compute Ask[i, :].\nS is CSC, A is column-major.\nHere, we suggest that the n columns of Ask be computed separately from one another. An individual column is given by Ask[:, j] =\nSA[:, j]. We evaluate this product by taking a linear combination of the columns of\nS, according to\nAsk[:, j] =\nX\nℓ∈JmK\nS[:, ℓ]A[ℓ, j].\nNote that each of the ℓterms in this sum is a sparse vector with k nonzero entries.\nBased on our preliminary experiments, this method has mediocre single-thread performance, but it has excellent scaling properties for many-core machines.\nS is CSR, A is column-major.\nWe were unable to determine a method that parallelizes well for this pair of data formats. The most eﬃcient algorithm may be to\nconvert S to CSC and then to apply the preferred method when S is CSC and A is\ncolumn-major.\nA.2.2\nTheory and practical usage\nSASO theory\nA precursor to the SASOs we consider is described in [DKS10], which sampled row\nindices for nonzero entries from JdK with replacement. The ﬁrst theoretical analysis\nof the SASOs we consider was conducted in [KN12] and concerned the distributional\nJohnson-Lindenstrauss property. Shortly thereafter, [CW13] and [MM13] studied\nOSE properties for SASOs with a single nonzero per column; the latter referred to\nthe construction as “CountSketch.”\nTheoretical analyses for OSE properties of general SASOs (i.e., those with more\nthan one nonzero per column) were ﬁrst carried out by [NN13; KN14] and subsequently improved by [BDN15; Coh16].\nMuch of the SASO analysis has been\nthrough the lens of “hashing functions,” and does not require that the columns of\nthe sketching operator are fully independent.\nWe do not know of any practical\nadvantage to SASOs with partial dependence across the columns.\nRemark A.2.1 (Navigating the literature). [CW17] is a longer journal version of\n[CW13]. [KN14] and [KN12] have the same title, and the former is considered a\nmore developed journal version of the latter.\narXiv Version 2\nPage 139\n\nDetails on Basic Sketching\nA.3. Theory for sketching by row selection\nSelecting parameters for SASOs\nWe are in the process of developing recommendations for how to set the parameters\nd and k for a distribution over SASOs. So far we have observed that when d is\nﬁxed the sketch quality increases rapidly with k before reaching a plateau.\nAs\none point of reference, we have observed that there is no real beneﬁt in k being\nlarger than eight when embedding the range of a 100, 000 × 2, 000 matrix into a\nspace with ambient dimension d = 6, 000. Furthermore, for the data matrices we\ntested, the restricted condition numbers of those sketching operators were tightly\nconcentrated at O(1). Extensive experiments with parameter selection for SASOs\nin a least squares context are given in [Ura13].\nA.3\nTheory for sketching by row selection\nHere we prove Proposition 6.1.1. Our proof is inspired by [Tro20, Problem 5.13],\nwhich begins with the following adaptation of [Tro15, Theorem 5.1.1].\nTheorem A.3.1. Consider an independent family {X1, . . . , Xs} ⊂Hn of random\npsd matrices that satisfy λmax(Xi) ≤L almost surely. Let Y = Ps\ni=1 Xi, and deﬁne\nthe mean parameters\nµmax = λmax(EY)\nand\nµmin = λmin(EY).\nOne has that\nPr {λmax(Y −(1 + t)EY) ≥0} ≤n\n\u0014\nexp(t)\n(1 + t)(1+t)\n\u0015µmax/L\nfor t > 0,\nand\nPr {λmax((1 −t)EY −Y) ≥0} ≤n\n\u0014\nexp(−t)\n(1 −t)(1−t)\n\u0015µmin/L\nfor t ∈(0, 1).\nHere, we restate the result we aim to prove.\nProposition A.3.2 (Adaptation of Proposition 6.1.1). Suppose A is an m × n\nmatrix of rank n, q is a distribution over JmK, and t is in (0, 1). Let S be a d × m\nsketching operator with rows that are distributed iid as\nS[i, :] =\nδj\np\ndqj\nwith probability qj,\nand let E(t, S) denote the event that\n(1 −t)∥y∥2\n2 ≤∥Sy∥2\n2 ≤(1 + t)∥y∥2\n2 ∀y ∈range A.\nUsing r := minj∈JmK\nqj\npj(A), we have\nPr {E(t, S) fails} ≤2n\n\u0012\nexp(t)\n(1 + t)(1+t)\n\u0013rd/n\n.\nProof. The way that we use Theorem A.3.1 is along the lines of the hint in [Tro20,\nProblem 5.13, Part 3]. We begin by considering the Gram matrices G = A∗A and\nGsk = A∗S∗SA. The event E(t, S) is equivalent to\n(1 −t)In ⪯G−1/2GskG−1/2 ⪯(1 + t)In.\nPage 140\narXiv Version 2\n\nA.3. Theory for sketching by row selection\nDetails on Basic Sketching\nThe sketched Gram matrix can be expressed as a sum of d outer products of\nrows of SA. Each of the d outer products is conjugated by G−1/2 to obtain our\nmatrices {X1, . . . , Xd}. That is, we set\nXi = G−1/2 ((SA) [i, :])∗((SA) [i, :]) G−1/2\n(A.4)\nso that Y = Pd\ni=1 Xi satisﬁes EY = In. A union bound provides\nPr{E(t, S) fails} ≤Pr{λmax(Y) ≥1 + t} + Pr{1 −t ≥λmin(Y)}.\nNote that the claim of this proposition only invokes Theorem A.3.1 in the special\ncase when t is between zero and one. Moreover, our particular choice of Y leads to\nµmin = µmax = 1. Given these restrictions, it can be shown that the larger of the\ntwo probability bounds in the theorem is that involving the term exp(t)/(1+t)(1+t).\nTherefore we have\nPr{E(t, S) fails} ≤2n\n\u0010\nexp(t)/(1 + t)(1+t)\u00111/L\n.\nNext, we turn to ﬁnding the smallest possible L given this construction, so as to\nmaximize 1/L.\nLet i be an arbitrary index in JdK. By the deﬁnition of S, the following must\nhold for some k ∈JmK:\nXi = 1\nd\n\u0012 1\nqk\nG−1/2A[k, :]∗A[k, :]G−1/2\n\u0013\n.\nOur next step is to use the fact that the trace of a rank-1 psd matrix is equal to its\nlargest eigenvalue. Cycling the trace shows that\nλmax\n\u0010\nG−1/2A[k, :]∗A[k, :]G−1/2\u0011\n= A[k, :]G−1A[k, :]∗= ℓk(A),\nand hence\nL = 1\nd max\nj∈JmK\n\u001aℓj(A)\nqj\n\u001b\nis the smallest value that guarantees λmax(Xi) ≤L.\nTo complete the proof we use the assumption that A is of full rank n to express\nthe leverage score ℓj(A) as ℓj(A) = npj(A). This shows that\nL = n\nd max\nj∈JmK\npj(A)\nqj\n,\nand the proposition’s claim follows from just a little algebra.\narXiv Version 2\nPage 141\n\nDetails on Basic Sketching\nA.3. Theory for sketching by row selection\nPage 142\narXiv Version 2\n\nAppendix B\nDetails on Least Squares\nand Optimization\nB.1 Quality of preconditioners ........................................... 143\nB.1.1 Eﬀective distortion in sketch-and-precondition ............... 145\nB.2 Basic error analysis ..................................................... 146\nB.2.1 Concepts: forward and backward error ......................... 146\nB.2.2 Basic sensitivity analysis for least squares problems ........ 147\nB.2.3 Sharper sensitivity analysis for overdetermined problems . 149\nB.2.4 Simple constructions to bound backward error ............... 149\nB.2.5 More advanced concepts ............................................ 151\nB.3 Ill-posed saddle point problems\n................................. 152\nB.4 Minimizing regularized quadratics .............................. 153\nB.4.1 A primer on kernel ridge regression .............................. 153\nB.4.2 Eﬃcient sketch-and-solve for regularized quadratics ........ 155\nThis appendix covers a few distinct topics. Appendix B.1 proves a novel result\nrelevant to sketch-and-precondition algorithms for saddle point problems, and connects this result to the idea of eﬀective distortion. In Appendix B.2, we provide\nbackground from classical NLA on what it means to compute an “accurate” solution\nto a least squares problem (overdetermined or underdetermined). Appendix B.3\nderives limiting solutions of saddle point problems as the regularization parameter tends to zero from above. These limiting solutions are important for treating\nsaddle point problems as linear algebra problems even when their optimization formulations are ill-posed. Finally, Appendix B.4 gives background on kernel ridge\nregression and details a new approach to sketch-and-solve of regularized quadratics.\nB.1\nQuality of preconditioners\nHere we consider preconditioners of the kind described in Section 3.3.2. These are\nobtained by sketching a tall m×n data matrix A in the embedding regime, factoring\nthe sketch, and using the factorization to construct an orthogonalizer.\n143\n\nLeast Squares and Optimization\nB.1. Quality of preconditioners\nProposition B.1.1 (Adaptation of Proposition 3.3.1). Consider a sketch Ask =\nSA and a matrix U whose columns are an orthonormal basis for range(A).\nIf\nrank(Ask) = rank(A) and the columns of AskM are an orthonormal basis for the\nrange of Ask, then singular values of AM are the reciprocals of the singular values\nof SU.\nObserve that this proposition is a linear algebraic result, i.e., there is no randomness. When applied to randomized algorithms, the randomness enters only via\nthe construction of the sketch.\nThis result can be applied to problems with ridge regularization by working\nwith augmented matrices in the vein of Section 3.3.2. In that context it is necessary\nto not only replace (A, Ask) by (ˆA, ˆAsk), but also to replace S by the augmented\nsketching operator ˆS that takes ˆA to ˆAsk. The augmented sketching operator in\nquestion was already visualized in Algorithm 2.\nOur proof of Proposition B.1.1 requires ﬁnding an explicit expression for M.\nTowards this end, we prove the following lemma.\nLemma B.1.2. Suppose Ask is a tall d×n matrix and that M is a full-column-rank\nmatrix for which the columns of AskM form an orthonormal basis for range(Ask).\nIf B is a full-row-rank matrix for which Ask = AskMB, then we have M = B†.\nProof of Lemma B.1.2. Let k = rank(Ask) = rank(AskM). Since the columns of\nAskM are orthonormal we can infer that it has dimensions d × k. Similarly, since\nM is full column-rank we can infer that it is n × k. We prove that B = M†, which\namounts to showing four properties:\n1. MBM = M,\n2. BMB = B,\n3. BM is an orthogonal projector, and\n4. MB is an orthogonal projector.\nBy the lemma’s assumption we have the identity Ask = AskMB. Left multiply this\nexpression through by (AskM)∗to see that\nM∗A∗\nskAsk = B.\n(B.1)\nNext, we right multiply both sides of (B.1) by M and use column orthonormality\nof AskM to obtain BM = Ik — this is suﬃcient to show the ﬁrst three conditions\nfor the pseudoinverse. Showing the fourth and ﬁnal condition takes more work. For\nthat we left multiply (B.1) by M so as to express\nMM∗A∗\nskAsk = MB.\nTherefore our task is to show that MM∗A∗\nskAsk is an orthogonal projector.\nConsider the compact SVD Ask = UskΣskVsk. Since Ask is rank-k we have that\nUsk has k columns and Σ is a k × k invertible matrix. Since the columns of AskM\nform an orthonormal basis for the range of Ask, it must be that AskM = UskW\nfor some k × k orthogonal matrix W. Furthermore, this orthogonal matrix can be\nexpressed as ΣskV∗\nskM = W, which implies\nVskV∗\nskM = VskΣ−1\nsk W.\n(B.2)\nPage 144\narXiv Version 2\n\nB.1. Quality of preconditioners\nLeast Squares and Optimization\nWe have reached a checkpoint in the proof. Our next task is to obtain an expression\nfor M by simplifying (B.2).\nConsider the subspaces X = range Vsk, Y = ker Ask, and Z = range M, all\ncontained in Rn. We know that Y ∩Z is trivial since rank(AskM) = rank(M). At\nthe same time, since Y and Z are of dimensions n −k and k respectively, it must\nbe that Z = Y ⊥. This fact can be combined with Y = X⊥(from the fundamental\ntheorem of linear algebra) to obtain Z = X, which in turn implies VskV∗\nskM = M.\nTherefore (B.2) simpliﬁes to\nM = VskΣ−1\nsk W.\nThis expression is precisely what we need; when the dust settles, it tells us that\nMM∗A∗\nskAsk = VskV∗\nsk.\nProof of Proposition B.1.1. Let k = rank(A).\nIt suﬃces to prove the statement\nwhere U is the m × k matrix containing the left singular vectors of A. Our proof\ninvolves working with the compact SVD A = UΣV∗, where V is n × k and Σ is\ninvertible. Noting that Ask = SUΣV∗holds by deﬁnition of Ask, we can replace SU\nby its economic QR factorization SU = QR to see\nAsk = QRΣV∗.\n(B.3)\nFurthermore, since rank(Ask) = k it must be that rank(SU) = k. This tells us that\nR is invertible and that Q provides an orthonormal basis for the range of Ask.\nBy assumption on M, the matrix AskM is also an orthonormal basis for the range\nof Ask. Therefore there exists a k × k orthogonal matrix P where QP = AskM. We\ncan rewrite (B.3) as\nAsk = (QP) (P∗RΣV∗) .\nSince the left factor in the above display is simply AskM, we have\nAsk = AskM (P∗RΣV∗) .\n(B.4)\nThe next step is to abbreviate B = P∗RΣV∗and apply Lemma B.1.2 to infer that\nB = M†. Invoking the column-orthonormality of V and invertibility of (Σ, R, P) we\nfurther have B† = M = VΣ−1R−1P. Plug in this expression for M to see that\nAM = (UΣV∗)\n\u0000VΣ−1R−1P\n\u0001\n= UR−1P.\n(B.5)\nThe proof is completed by noting that the singular values of R−1 are the reciprocals\nof the singular values of QR = SU.\nB.1.1\nEﬀective distortion in sketch-and-precondition\nRecall from Appendix A.1 that if the columns of U are an orthonormal basis for a\nlinear subspace L, then the restricted condition number of S on L is\ncond(S; L) = cond(SU).\nThis identity combines with Proposition B.1.1 to make for a remarkable fact.\nNamely, if L = range(A) and M is an orthogonalizer of a sketch SA, then\ncond(S; L) = cond(AM).\n(B.6)\nLet us contextualize this fact algorithmically.\narXiv Version 2\nPage 145\n\nLeast Squares and Optimization\nB.2. Basic error analysis\nIf A is an m × n matrix (m ≫n) in a saddle point problem, and if\nthat problem is approached by the sketch-and-precondition methodology\nfrom Section 3.2.2, then the condition number of the preconditioned\nmatrix handed to the iterative solver is equal to the restricted condition\nnumber of S on range(A).\nBut we can take this one step further. By invoking Proposition A.1.1 and applying\n(B.6), we obtain the following expression for the eﬀective distortion of S for L:\nDe(S; L) = cond(AM) −1\ncond(AM) + 1.\n(B.7)\nAlarm bells should be going oﬀin some readers’ heads.\nThe right-hand-side of\n(B.7) is none other than the convergence rate of LSQR (or CGLS) for a least\nsquares problem with data matrix AM! This shows a deep connection between our\nproposed concept of eﬀective distortion and the venerated sketch-and-precondition\nparadigm in RandNLA.\nB.2\nBasic error analysis for least squares problems\nWhen solving a computational problem numerically it is inevitable that the computed solutions deviate from the problem’s exact solution. This is a simple consequence of working in ﬁnite-precision arithmetic, and it remains true even when\nusing very reliable algorithms. Furthermore, for large-scale computations it is often of interest to trade oﬀcomputational complexity with solution accuracy; this\nhas led to algorithms that produce approximate solutions even when run in exact\narithmetic.\nThese facts were encountered in the earliest days of NLA. Their consequence\nin applications has motivated the development of a vast literature on quantifying\nand bounding the error of approximate solutions to computational problems. Since\nseveral of the randomized algorithms from Section 3.2 purport to solve problems to\nany desired accuracy, it is prudent for us to summarize key points from this vast\nliterature here. The material from Appendices B.2.1 to B.2.4 is typically covered in\nan introductory course on numerical analysis. Appendix B.2.5 mentions important\ntopics which might not be covered in such a course.\nRemark B.2.1. We have focused this appendix strongly on basic least squares problems (overdetermined and underdetermined) to keep it a reasonable length.\nB.2.1\nConcepts: forward and backward error\nThe forward error of an approximate solution to a computational problem is its\ndistance to the problem’s exact solution. Forward error is easy to interpret, but it\nis not without its limitations. First, it can rarely be computed in practice, since\nit is presumed that we do not have access to the problem’s exact solution. This\nmeans that substantial eﬀort is needed to approximate or bound forward error in\ndiﬀerent contexts. Second, even if one algorithm’s behavior with respect to forward\nerror has been analyzed, it may not be feasible to repurpose the analysis for another\nalgorithm. These shortcomings motivate the ideas of backward error and sensitivity\nanalysis, wherein one asks the following questions, respectively.\nPage 146\narXiv Version 2\n\nB.2. Basic error analysis\nLeast Squares and Optimization\n• How much do we need to perturb the problem data so that the computed\nsolution exactly solves the perturbed problem?\n• How does a small perturbation to a given problem change that problem’s\nexact solution?\nThe connection between the two concepts is clear: any bound on backward error\ncan be combined with sensitivity analysis to obtain an estimate of forward error.\nThe idea of sensitivity analysis is especially powerful since it is agnostic to the source\nof the problem’s perturbation; the perturbations might be due to rounding errors\nfrom ﬁnite-precision arithmetic, uncertainty in data (as might arise from experimental observations), or deliberate choices to only compute approximate solutions.\nIn any of these cases one can combine knowledge of an algorithm’s backward-error\nguarantees to obtain forward error estimates.\nThis reasoning can be carried further to arrive at two major beneﬁts of the\n“backward error plus sensitivity analysis” approach.\n• A large portion of algorithm-speciﬁc error analysis can be accomplished purely\nby understanding the algorithm’s behavior with respect to backward error.\n• For many problems one can cheaply compute upper bounds on a solution’s\nbackward error at runtime.\nThe combination of backward error and sensitivity analysis can therefore be used\nto establish a priori guarantees on algorithm numerical behavior and a posteriori\nguarantees on the quality of an approximate solution. However, we do note that\nsensitivity analysis results require knowledge of problem data that may not be\navailable, such as extreme singular values of a data matrix in a least squares problem.\nTherefore it is still diﬃcult to compute forward error bounds at runtime.\nB.2.2\nBasic sensitivity analysis for unregularized least squares\nproblems\nHere we paraphrase facts from [GV13, §5.3 and §5.6] on sensitivity analysis of\nbasic least squares problems.\nOur restatements adopt the notation we used for\nsaddle point problems, wherein both overdetermined and underdetermined problems\ninvolve a tall m × n matrix A. The overdetermined problem induced by A and an\nm-vector b is\nmin\nx∈Rn ∥Ax −b∥2\n2,\nwhile the underdetermined problem induced by A and an n-vector c is\nmin\ny∈Rm\n\b\n∥y∥2\n2 : A∗y = c\n\n.\nIn the following theorem statements, the reader should bear in mind that a perturbation δA can only satisfy ∥δA∥2 < σn(A) if σn(A) is positive. Therefore these\ntheorem statements only apply when A is full-rank.\nTheorem B.2.2. Suppose b is neither in the range of A nor the kernel of A∗, and\nlet x = A†b be the optimal solution of the overdetermined least squares problem with\ndata (A, b). Consider perturbations δb and δA where ∥δA∥2 < σn(A). Deﬁne\nϵ = max\n\u001a∥δA∥2\n∥A∥2\n, ∥δb∥2\n∥b∥2\n\u001b\n(B.8)\narXiv Version 2\nPage 147\n\nLeast Squares and Optimization\nB.2. Basic error analysis\ntogether with some auxiliary quantities\nsin θ = ∥b −Ax∥2\n∥b∥2\nand\nν =\n∥Ax∥2\nσn(A)∥x∥2\n.\n(B.9)\nThe perturbation δx necessary for x + δx to solve the least squares problem with\ndata (A + δA, b + δb) satisﬁes\n∥δx∥2\n∥x∥2\n≤ϵ\nn\nν\ncos θ + κ(A)(1 + ν tan θ)\no\n+ O(ϵ2).\n(B.10)\nTheorem B.2.2 restates part of [GV13, Theorem 5.3.1]; following the proof of\nthis result, the source material presents some simpliﬁed estimates for these bounds.\nThe ﬁrst step in producing the simpliﬁed estimate is to note that ν ≤κ(A) holds\nfor all nonzero x. Then, under the modest geometric assumption that θ is bounded\naway from π/2, (B.10) suggests that\n∥δx∥2\n∥x∥2\n≲ϵ\n\u001a\nκ(A) + ∥b −Ax∥2\n∥b∥2\nκ(A)2\n\u001b\n.\n(B.11)\nThe signiﬁcance of this bound is that it shows the dependence of ∥δx∥2 on the square\nof the condition number of A. This dependence is a fundamental obstacle to solving\nleast squares problems to a high degree of accuracy when measured by forward error.\nThe situation is diﬀerent if we try to bound the perturbation ∥A(δx)∥. We provide\nthe following result (which completes the restatement of [GV13, Theorem 5.3.1]) as\na step towards explaining why.\nTheorem B.2.3. Under the hypothesis and notation of Theorem B.2.2, we have\n∥A(δx)∥2\n∥b −Ax∥2\n≤ϵ\n\u001a\n1\nsin θ + κ(A)\n\u0012\n1\nν tan θ + 1\n\u0013\u001b\n+ O(ϵ2).\n(B.12)\nTheorem B.2.3 can be seen as a sensitivity analysis result for a very speciﬁc\nclass of dual saddle point problems. Speciﬁcally, since we have assumed that A is\nfull rank, y solves the dual problem if and only if y = b −Ax where x solves the\nprimal problem. In the same vein, if x + δx solves a perturbed primal problem and\nwe set δy = −A(δx), then y + δy solves the perturbed dual problem.\nAs with the bound for δx, (B.12) can be estimated under mild geometric assumptions; [GV13, pg. 267] points out that if θ is suﬃciently bounded away from\n0 and π/2, then we should have\n∥δy∥2\n∥y∥2\n≲ϵ κ(A).\n(B.13)\nThis shows there is more hope for solving dual saddle point problems to a high\ndegree of forward error accuracy, at least by comparison to primal saddle point\nproblems. Indeed, the following adaptation of [GV13, Theorem 5.6.1] provides an\neven more favorable sensitivity analysis result for underdetermined least squares.\nTheorem B.2.4. Let y = (A∗)†c solve the underdetermined least squares problem\nwith data (A, c) for a nonzero vector c. Consider perturbations δc and δA where\nϵ = max\n\u001a∥δc∥2\n∥c∥2\n, ∥δA∥2\n∥A∥2\n\u001b\n< σn(A).\nPage 148\narXiv Version 2\n\nB.2. Basic error analysis\nLeast Squares and Optimization\nThe perturbation δy needed for y + δy to solve the underdetermined least squares\nproblem with data (A + δA, c + δc) satisﬁes\n∥δy∥2\n∥y∥2\n≤3 ϵ κ(A) + O(ϵ2).\n(B.14)\nB.2.3\nSharper sensitivity analysis for overdetermined problems\nThe analysis results in Appendix B.2.2 have notable limitations: they hide constants\nin O(ϵ2) terms. Luckily there are a wealth of more precise results in the literature\nthat work with diﬀerent notions of error. One good example along these lines for\noverdetermined least squares is given in [Ips09, Fact 5.14], which obtains a relative\nerror bound normalized by the solution of the perturbed problem rather than the\noriginal problem. We paraphrase this fact below.\nTheorem B.2.5. Consider an overdetermined least squares problem with data\n(Ao, bo) that is solved by xo = (Ao)†(bo); consider also perturbed problem data\nA = Ao + δAo and b = bo + δbo together with solution x = A†b.\nIf we have\nrank(Ao) = rank(A) = n and deﬁne\nϵA = ∥δAo∥2\n∥Ao∥2\nand\nϵb =\n∥δbo∥\n∥Ao∥2∥x∥2\n,\nthen we have\n∥xo −x∥2\n∥x∥2\n≤(ϵA + ϵb)κ(Ao) + ϵA\n[κ(Ao)]2 ∥y∥2\n∥Ao∥2∥x∥2\n(B.15)\nfor y = b −Ax.\nBounds of similar character are given for y on [Ips09, pg. 101]. These bounds are\nuseful for understanding how solutions exhibit diﬀerent sensitivity for perturbations\nto the data matrix compared to perturbations to the right-hand-side. Even better\nbounds can be obtained by assuming structured perturbations.\nFor example, if\nrange(Ao) = range(A), then the sensitivity of the overdetermined least squares\nsolution depends only linearly on κ(Ao) [Ips09, Exercise 5.1].\nOur discussion of sensitivity analysis has only considered normwise perturbations to the problem data. More informative bounds can be had by considering\ncomponentwise perturbations. For example, one can measure a perturbation of an\ninitial matrix A by the smallest α for which |δAij| ≤α|Aij| for all i, j. We refer the reader to [Hig02, §20.1] for a componentwise sensitivity analysis result on\noverdetermined least squares.\nB.2.4\nSimple constructions to bound backward error\nHere we describe two methods for constructing perturbations to problem data that\nrender an approximate solution exact. By computing the norms of these perturbations, we can obtain upper bounds on (normwise) backward error. Such bounds are\nuseful as termination criteria for iterative solvers.\nThe notation here matches that of Theorem B.2.5. That is, we say our original\nleast squares problem has data (Ao, bo) and that x is an approximate solution to\nthis problem.\nRemark B.2.6. For discussion on componentwise backward error bounds for overdetermined least squares we again refer the reader to [Hig02, §20.1].\narXiv Version 2\nPage 149\n\nLeast Squares and Optimization\nB.2. Basic error analysis\nInconsistent overdetermined problems\nLetting r = bo −Aox, we deﬁne\nδAo = rr∗Ao\n∥r∥2\n2\nand\nA = Ao + δAo,\n(B.16)\nand subsequently\nδbo = −(δAo)x\nand\nb = bo + δbo.\n(B.17)\nSome simple algebra shows that x satisﬁes the normal equations\nA∗(b −Ax) = 0,\ntherefore it solves the overdetermined least squares problem with data (A, b).\nThis construction was ﬁrst given in [Ste77, Theorem 3.2]. It is especially nice\nsince the perturbation is rank-1, and so its spectral norm\n∥δAo∥2 = ∥A∗\nor∥2\n∥r∥2\nis easily computed at runtime by an iterative solver for overdetermined least squares.\nFurthermore, if the iterative solver in question is LSQR, and if we assume exact\narithmetic, then the perturbation will satisfy δAox = 0 [PS82, §6.2]. Therefore\nthe vector δbo in (B.17) is zero when running LSQR (or any equivalent method) in\nexact arithmetic.\nConsistent overdetermined problems: a word of warning\nThe perturbations given in (B.16) – (B.17) are not suitable for least squares problems where the optimal residual, (I −AoA†\no)bo, is zero or nearly zero.\nIn these\nsituations one should use a perturbation designed for consistent linear systems. We\ndescribe such a construction here based on termination criteria used in LSQR.\nLet δbo be an arbitrary m-vector. Suppose we set δAo as a function of δbo in\nthe following way:\nδAo = (δbo + bo −Aox) x∗\n∥x∥2\n2\n.\nIt can be seen that Ax = b upon taking b = bo + δbo and A = Ao + δAo, and so x\ntrivially solves the perturbed least squares problem with data (A, b).\nOne can obtain many backward-error constructions by considering diﬀerent\nchoices for δbo as a function of x, the problem data (Ao, bo), and desired error tolerances. The construction for LSQR considers two tolerance parameters ϵA, ϵb ∈[0, 1),\nand sets δbo as follows [PS82, §6.1]:\nδbo =\n\u0012\nϵb∥bo∥2\nϵb∥bo∥2 + ϵA∥Ao∥∥x∥2\n\u0013\n(Aox −bo).\n(B.18)\nThe parameters ϵA and ϵb indicate the (relative) sizes of perturbations to (Ao, bo)\nthat a user deems allowable. The authors of [PS82] suggest that “allowable” be\nbased on the extent to which (Ao, bo) are not known exactly in applications.\nIt is natural to want to reduce the two tolerance parameters (ϵA, ϵb) to a single\ntolerance parameter. For example, one might take ϵA = ϵb. Unfortunately, our\nPage 150\narXiv Version 2\n\nB.2. Basic error analysis\nLeast Squares and Optimization\nexperience is that taking ϵA = ϵb can produce unreliable algorithm behavior for\noverdetermined problems. Therefore we recommend that one sets ϵA = 0 if one\nwants to think only in terms of a single tolerance for consistent overdetermined\nproblems.\nWhile this may seem like a blunt solution, it ensures that δAo = 0,\nwhich is useful in applying Theorem B.2.5. If setting ϵA = 0 still feels too extreme\nthen one might consider setting ϵA = (ϵb)2 ≪ϵb.\nRemark B.2.7. As a minor detail, we point out that the norm of Ao in (B.18) is deliberately ambiguous. While the spectral norm would probably be most natural, the\nformal LSQR algorithm replaces ∥Ao∥by an estimate of ∥Ao∥F that monotonically\nincreases from one iteration to the next; see [PS82, §5.3].\nB.2.5\nMore advanced concepts\nSome of the earliest work on backward-error analysis for solutions to linear systems\nfocused on componentwise backward error for direct methods [OP64]. A principal\nshortcoming of componentwise error metrics is that they are expensive to compute,\nespecially as stopping criteria for iterative solvers. [ADR92] investigates metrics for\ncomponentwise backward error suitable for iterative solvers.\nThe “backward error plus sensitivity analysis” approach may overestimate forward error. Alternative estimates are available for some Krylov subspace methods\nsuch as PCG, wherein one uses algorithm-speciﬁc recurrences to estimate forward\nerror in the Euclidean norm or the norm induced by Aµ := [A; √µ]. See, for example, [AK01; ST02; ST05].\nThese error bounds are more accurate when used\nwith a good preconditioner, which we can generally expect to have when using the\nrandomized algorithms described herein.\nIt is not easy to apply sensitivity analysis results to compute forward error\nbounds at runtime. A primary obstacle in this regard is the need to have accurate\nestimates for the extreme singular values of Ao or the perturbation A (depending\non the sensitivity analysis result in question).\nOn this topic we note that if M\nis an SVD-based preconditioner then we will have computed the singular values\nand right singular vectors of a sketch SAo. Those singular values can be used as\napproximations to the reciprocals of the singular values of Ao. It is conceivable\nthat more accurate approximations could be obtained by applying iterative preconditioned eigenvalue estimation methods for (Ao)∗Ao. Such iterative methods\ntypically require initialization with an approximate eigenvector. On this front one\ncan use the leading (resp. trailing) left singular vector of M to approximate the\ntrailing (resp. leading) right singular vector of Ao. One should not expect too much\nof such estimates, however.1\nFinally, we note that some Krylov-subspace iterative methods can estimate condition numbers. For example, when LSQR is applied to a problem with data matrix L, it can estimate the Frobenius condition number ∥L∥F∥L†∥F. Bear in mind\nthat in our context we call LSQR with the preconditioned augmented data matrix,\nL = AµM. It would be useful to embed estimators for componentwise condition\nnumbers (which are known to be computable in polynomial time [Dem92]) into\nKrylov subspace solvers.\n1Any “cheap” method for estimating the smallest singular value even of triangular matrices\ncan return substantial overestimates and underestimates [DDM01].\narXiv Version 2\nPage 151\n\nLeast Squares and Optimization\nB.3. Ill-posed saddle point problems\nB.3\nIll-posed saddle point problems\nOur saddle point formulations of least squares problems can be problematic when A\nis rank-deﬁcient and µ is zero, in which case our problems can actually be infeasible\nor unbounded below. This appendix uses a limiting analysis to deﬁne canonical\nsolutions to saddle point problems in these settings.\nWe begin by recalling\nmin\nx∈Rn ∥Ax −b∥2\n2 + µ∥x∥2\n2 + 2c∗x,\n((3.2), revisited)\nmin\ny∈Rm ∥A∗y −c∥2\n2 + µ∥y −b∥2\n2,\n((3.3), revisited)\nand\nmin\ny∈Rm{∥y −b∥2\n2 : A∗y = c}.\n((3.4), revisited)\nWe also note the following form of solutions to (3.3), when µ is positive\ny(µ) = (AA∗+ µI)−1 (Ac + µb) .\n(B.19)\nProposition B.3.1. For any tall m×n matrix A, any m-vector b, and any n-vector\nc, we have\nlim\nµ↓0 y(µ) = (A∗)†c + (I −AA†)b.\n(B.20)\nProof. Let k = rank(A). If k = 0 then the claim is trivial since (B.19) reduces to\ny(µ) = b for all µ > 0. Henceforth, we assume k > 1. To establish the claim,\nconsider how the compact SVD A = UΣV∗lets us express\nAA∗+ µIm = Hµ + Gµ\nin terms of the Hermitian matrices\nHµ = U\n\u0000Σ2 + µIk\n\u0001\nU∗\nand\nGµ = µ(Im −UU∗).\nSince HµGµ = GµHµ = 0, the following identity holds for all positive µ:\n(AA∗+ µI)−1 = H†\nµ + G†\nµ.\nFurthermore, by expressing\nH†\nµAc = U\n\u0000Σ2 + µIk\n\u0001−1 ΣV∗c\nG†\nµAc = 0\nµH†\nµb = U\n\u0000Σ2/µ + Ik\n\u0001−1 U∗b\nµG†\nµb = (Im −UU∗)b\nwe ﬁnd that\ny(µ) = H†\nµ(Ac + µb) + G†\nµ(Ac + µb)\n→UΣ−1V∗c + (Im −UU∗)b.\nThis is equivalent to the desired claim since (A∗)† = UΣ−1V∗and AA† = UU∗.\nPage 152\narXiv Version 2\n\nB.4. Minimizing regularized quadratics\nLeast Squares and Optimization\nIn light of the above proposition, we take y(0) = (A∗)†c + (I −AA†)b as our\ncanonical solution to the dual problem when µ = 0.\nNow we let x(µ) denote the solution to (3.2) parameterized by µ > 0. It is clear\nthat this is given by\nx(µ) = (A∗A + µI)−1 (A∗b −c).\nIt is easy to show that if c is not orthogonal to the kernel of A, then the norms\n∥x(µ)∥will diverge to inﬁnity as µ tends to zero. However, if c is orthogonal to the\nkernel of A, then we have\nlim\nµ↓0 x(µ) = (A∗A)†(A∗b −c) =: x(0).\n(B.21)\nWe actually take the limit above as our canonical solution to the primal problem\n(3.2) regardless of whether or not c is orthogonal to the kernel of A. Our reasons\nfor this are two-fold. First, the values x(0), y(0) given above are unchanged when\nc is replaced by its orthogonal projection onto range of A∗. Second, the value y(0)\nis always the limiting solution to the dual problem. Meanwhile, the proposed value\nfor x(0) relates to y(0) by y(0) = b −Ax(0).\nB.4\nMinimizing regularized quadratics\nAppendix B.4.1 provides a brief introduction to kernel ridge regression (KRR).\nIt covers the ﬁnite-dimensional linear algebraic formulation and the Hilbert space\nformulation of this regression model, and it explains how ridge regression can be\nunderstood in the KRR framework. Appendix B.4.2 presents a novel preconditionergeneration procedure for solving a sketch of the regularized quadratic minimization\nproblem (3.1).\nB.4.1\nA primer on kernel ridge regression\nKernel ridge regression (KRR) is a type of nonparametric regression for learning\nreal-valued nonlinear functions f : X →R. It can be formulated as a linear algebra\nproblem as follows: we are given λ > 0, an m × m psd “kernel matrix” K, and a\nvector of observations h in Rm; we want to solve\nargmin\nα∈Rm\n1\nm∥Kα −h∥2\n2 + λ α∗Kα.\n(B.22)\nEquivalently, we want to solve the KRR normal equations (K + mλ I)α = h. The\nnormal equations formulation makes it clear that KRR is an instance of (3.1).\nA standard library for RandNLA would be well-served to not dwell on how\nK is deﬁned; it should instead only focus on how K can be accessed. However,\nstrictly speaking, (B.22) only encodes a KRR problem when the entries of K are\ngiven by pairwise evaluations of a suitable two-argument kernel function on some\ndatapoints {xi}m\ni=1 ⊂X.\nLetting k : X × X →R denote this kernel function,\nthe user will take α that approximately solves (B.22) to deﬁne the learned model\ng(z) = Pm\ni=1 αik(xi, z).\narXiv Version 2\nPage 153\n\nLeast Squares and Optimization\nB.4. Minimizing regularized quadratics\nA more technical description\nThe kernel function k induces a reproducing kernel Hilbert space, H, of real-valued\nfunctions on X. This space is (up to closure) equal to the set of real-linear combinations of functions y 7→ku(y) := k(y, u) parameterized by u ∈X. Additionally,\nif the function\ny 7→f(y) =\nm\nX\ni=1\nαik(y, xi)\nis parameterized by α ∈Rm and {xi}m\ni=1 ⊂X, then its squared norm is given by\n∥f∥2\nH =\nm\nX\ni=1\nm\nX\nj=1\nαiαjk(xi, xj).\nUsing the kernel matrix K with entries Kij = k(xi, xj), we can express that squared\nnorm as ∥f∥2\nH = α∗Kα. Furthermore, for any u ∈X and any f ∈H we have\nf(u) = ⟨f, ku⟩H.\nFor details on reproducing kernel Hilbert spaces we refer the\nreader to [Aro50].\nKRR problem data consists of observations {(xi, hi)}m\ni=1 ⊂X ×R and a positive\nregularization parameter λ.\nWe presume there are functions g in H for which\ng(xi) ≈hi, and we try to obtain such a function by solving\nmin\ng∈H\n1\nm\nm\nX\ni=1\n(g(xi) −hi)2 + λ∥g∥2\nH.\n(B.23)\nIt follows from [KW70] that the solution to (B.23) is in the span of the functions\n{kxi}m\ni=1. Speciﬁcally, the solution is g⋆= Pm\ni=1 αikxi where α solves (B.22).\nWhy is ridge regression a special case of kernel ridge regression?\nSuppose we have an m × n matrix X = [x1, . . . , xn] with linearly independent\ncolumns, and that we want to estimate a linear functional ˆg : Rm →R given access\nto the n point evaluations (xi, ˆg(xi))n\ni=1.\nGiven a regularization parameter λ > 0, ridge regression concerns ﬁnding the\nlinear function g : Rm →R that minimizes\nL(g) =\n\n\n\ng(x1)\n...\ng(xn)\n\n−\n\n\nˆg(x1)\n...\nˆg(xn)\n\n\n\n2\n2\n+ nλ∥g∥2.\nTo make this concrete, let us represent ˆg and g by m-vectors ˆg and g respectively,\nand set h = X∗ˆg. We also adopt a slight abuse of notation to write L(g) = L(g),\nso that the task of ridge regression can be framed as minimizing\nL(g) = ∥X∗g −h∥2\n2 + λn∥g∥2\n2.\nRemark B.4.1. We pause to emphasize that this is a KRR problem with n datapoints\nthat deﬁne functions on X = Rm. The parameter “m” here has nothing to do with\nthe number of datapoints in the problem; our notational choices for (m, n) here are\nfor consistency with Section 3.\nPage 154\narXiv Version 2\n\nB.4. Minimizing regularized quadratics\nLeast Squares and Optimization\nThe essential part of framing ridge regression as a type of KRR is showing that\nthe optimal estimate g is in the range of X. To see why this is the case, let P denote\nthe orthogonal projector onto the range of X. Using X∗Pg = X∗g, we have that\nL(g) = ∥X∗Pg −h∥2\n2 + λn\n\u0000∥Pg∥2\n2 + ∥(I −P)g∥2\n2\n\u0001\n≥∥X∗Pg −h∥2\n2 + λn∥Pg∥2\n2\n= L(Pg),\nand so g minimizes L only if L(Pg) = L(g). Since L(Pg) = L(g) holds if and only\nif Pg = g, we have that g = Xα for some α in Rn. Therefore, under our stated\nassumption that the columns of X are linearly independent, the following problems\nare equivalent\narg min{L(g) : g is a linear functional on Rm},\narg min{∥X∗Xα −h∥2\n2 + λn∥Xα∥2\n2 : α ∈Rn}, and\narg min{∥Xα −ˆg∥2\n2 + λn∥α∥2\n2 : α ∈Rn}.\nThe second of these problems is KRR with a scaled objective and the n × n kernel\nmatrix K = X∗X. The last of these problems is ridge regression in the familiar form.\nGiven this description of ridge regression, one obtains KRR by applying the\nso-called “kernel trick” (see, e.g., [Mur12, §14]). That is, one replaces hj = x∗\nj ˆg by\nhj = ⟨kxj, ˆg⟩H = ˆg(xj)\nand expresses the point evaluation of g = Pn\ni=1 αikxi at xj by\ng(xj) =\nn\nX\ni=1\nαi⟨kxi, kxj⟩H.\nWe note that within the KRR formalism it is allowed for K to be singular, so long\nas it is psd. This is because if β is any vector in the kernel of K then the function\nu 7→Pn\ni=1 βik(xi, u) is identically equal to zero.\nB.4.2\nEﬃcient sketch-and-solve for regularized quadratics\nLet G be an m × m psd matrix and µ be a positive regularization parameter. The\nsketch-and-solve approach to KRR from [AM15] can be considered generically as a\nsketch-and-solve approach to the regularized quadratic minimization problem (3.1).\nThe generic formulation is to approximate G ≈AA∗with an m × n matrix A\n(m ≫n) and then solve\n(AA∗+ µI) z = h.\n(B.24)\nIdentifying b = h/µ, c = 0, and y = z shows that this amounts to a dual saddle\npoint problem of the form (3.3). Here we explain how the sketch-and-precondition\nparadigm can eﬃciently be applied to solve (B.24) under the assumption that AA∗\ndeﬁnes a Nystr¨om approximation of G.\nLet So be an initial m × n sketching operator. The resulting sketch Y = GSo\nand factor R = chol(S∗\noY) together deﬁne A = YR−1. This deﬁnes a Nystr¨om\napproximation since\nAA∗= (KSo) (S∗\noKSo)† (KSo)∗.\narXiv Version 2\nPage 155\n\nLeast Squares and Optimization\nB.4. Minimizing regularized quadratics\nRecall that the problem of preconditioner generation entails ﬁnding an orthogonalizer of a sketch of Aµ = [A; √µI]. The fact that A is only represented implicitly\nmakes this delicate, but it remains doable, as we explain below.\nFor the sketching phase of preconditioner generation, we sample a d×m operator\nS (with d ≳n) and set\nAsk\nµ =\n\u0014\nS\n0\n0\nI\n\u0015\nAµ =\n\u0014\nSY\n√µR\n\u0015\nR−1.\nWe then compute the SVD of the augmented matrix\n\u0014 SY\n√µR\n\u0015\n= UΣV∗\nand ﬁnd that setting M = RVΣ−1 satisﬁes Ask\nµ M = U. The preconditioned linear\noperator AµM (and its adjoint) should be applied in the iterative solver by noting\nthe identity\n\u0014 A\n√µI\n\u0015\nM =\n\u0014 Y\n√µR\n\u0015\nVΣ−1.\nThis identity is important since it shows that R−1 need never be applied at any\npoint in the sketch-and-precondition approach to (B.24).\nPage 156\narXiv Version 2\n\nAppendix C\nDetails on Low-Rank\nApproximation\nC.1 Theory for submatrix-oriented decompositions ........... 157\nC.1.1 Approximation quality in low-rank ID .......................... 157\nC.1.2 Truncation in column-pivoted matrix decompositions ..... 158\nC.2 Computational routines .............................................. 160\nC.2.1 Power iteration for data-aware sketching ....................... 161\nC.2.2 RangeFinders and QB decompositions .......................... 162\nC.2.3 ID and subset selection .............................................. 166\nC.1\nTheory for submatrix-oriented decompositions\nC.1.1\nApproximation quality in low-rank ID\nProposition C.1.1 (Restatement of Proposition 4.1.3). Let ˜A be any rank-k approximation of A that satisﬁes the spectral norm error bound ∥A −˜A∥2 ≤ϵ. If\n˜A = ˜A[:, J]X for some k × n matrix X and an index vector J, then ˆA = A[:, J]X is\na low-rank column ID that satisﬁes\n∥A −ˆA∥2 ≤(1 + ∥X∥2)ϵ.\n((4.16), restated)\nFurthermore, if |Xij| ≤M for all (i, j), then\n∥X∥2 ≤\np\n1 + M 2k(n −k).\n((4.17), restated)\nProof. Proceeding in the grand tradition of adding zero and applying the triangle\ninequality, we have the bound\n∥A −ˆA∥2 = ∥(A −˜A) + (˜A −ˆA)∥2\n≤∥A −˜A∥2 + ∥˜A −ˆA∥2.\n(C.1)\nWe prove (4.16) by bounding the two terms in (C.1). The ﬁrst term is trivial since\nwe have already assumed ∥A −˜A∥2 ≤ϵ. We bound the second term by using the\n157\n\nLow-rank Approximation\nC.1. Theory for submatrix-oriented decompositions\nidentity ˜A −ˆA = (˜A[:, J] −A[:, J])X and then invoking submultiplicivity of the\nspectral norm:\n∥˜A −ˆA∥2 ≤∥˜A[:, J] −A[:, J]∥2∥X∥2.\n(C.2)\nFinally, since the spectral norm of a matrix is at least as large as the spectral norm\nof any of its submatrices, we obtain ∥˜A[:, J] −A[:, J]∥2 ≤∥˜A −A∥2 ≤ϵ. Combining\nthis with (C.2) shows that (C.1) implies (4.16).\nNow we address (4.17). For this, consider the m×k matrix C = ˜A[:, J]. Because\nwe have assumed ˜A = CX has rank k and that X is k × n, we can infer that C is\nfull column-rank. We can also extract the columns from both sides of ˜A = CX at\nindices J to ﬁnd the identity C = CX[:, J]. Multiplying this identity through on the\nleft by C†, we can use C†C = Ik to obtain X[:, J] = Ik.\nNow if |Xij| ≤M for all (i, j) in addition to X[:, J] = Ik, then there exists a\npermutation P of the column index set JnK where X[:, P] = [Ik, V] for a k × (n −k)\nmatrix V satisfying |Vij| ≤M. Since permuting the columns of X does not change\nits spectral norm, it suﬃces to bound ∥[Ik, V]∥2. Towards this end, we claim without\nproof that for any block matrix W = [U, V], one has\n∥W∥2 ≤\nq\n∥U∥2\n2 + ∥V∥2\nF.\nThis, combined with ∥Ik∥2 = 1 and ∥V∥2\nF ≤M 2k(n −k), gives (4.17).\nRemark C.1.2. The bound (4.17) is not the best possible. Indeed, looking at the\nﬁnal steps in the proposition’s proof, we see that it suﬃces for M to bound the\nentries of X that are not part of the identity submatrix.\nC.1.2\nTruncation in column-pivoted matrix decompositions\nThis part of the appendix follows up on Section 4.3.3. It examines how changing\nbasis of a column-pivoted decomposition can aﬀect approximation quality when\ntruncating these decompositions. To begin, we set forth some deﬁnitions.\nOur matrix of interest, G, is r × c and given through a decomposition GP = FT\nfor a permutation matrix P and upper-triangular T. Let us say that F has w =\nmin{c, r} columns (and hence that T has as many rows). We use U to denote the set\ninvertible upper-triangular matrices of order w. For a positive integer k < rank(G),\nwe consider the matrix-valued function gk that is deﬁned on U according to\ngk(U) = F(U−1)[:, :k]U[:k, :]TP∗.\nNote that for every diagonal D ∈U we have gk(D) = (F[:, :k])(T[:k, :])P∗.\nProposition C.1.3. Partition the factors F and T into blocks [F1, F2] and [T1; T2]\nso that F1 has k columns and T1 has k rows. If U⋆is an optimal solution to\nmin\nU∈U ∥G −gk(U)∥F.\n(C.3)\nthen the following identity holds in any unitarily invariant norm:\n∥G −gk(U⋆)∥=\n\n\u0010\nI −F1F†\n1\n\u0011\nF2T2\n\n.\nFurthermore, we have ∥G−gk(Iw×w)∥= ∥F2T2∥, and the identity matrix is optimal\nfor (C.3) if and only if range(F2) and range(F1) are orthogonal.\nPage 158\narXiv Version 2\n\nC.1. Theory for submatrix-oriented decompositions\nLow-rank Approximation\nProof. Our proof requires working with several block matrices. First, the matrices\nT1 and T2 are further partitioned so that\nT =\n\u0014T1\nT2\n\u0015\n=\n\u0014T11\nT12\n0\nT22\n\u0015\nwhere T11 is k × k and T22 is (w −k) × (c −k). Next, we introduce U ∈U and\npartition it twice:\nU =\n\u0014U1\nU2\n\u0015\n=\n\u0014U11\nU12\n0\nU22\n\u0015\n.\nIn the expressions above, U1 is a wide matrix of shape k×w, U11 is a square matrix\nof order k, and U22 a square matrix of order w −k. Note that since U is uppertriangular, the same is true of V = U−1. We partition V = [V1, V2] into a leading\nblock of k columns and a trailing block of w −k columns.\nThe point of all this notation is to help us ﬁnd useful expressions for gk(U). Our\nﬁrst such expression is\ngk(U) = FV1U1TP∗.\n(C.4)\nAs a step towards ﬁnding the next expression, we apply block a matrix-inversion\nidentity to compute\nFV1 = F1U−1\n11 .\nMeanwhile, a simple block matrix multiply gives\nU1T = U11T1 + U12T2.\nWe plug these two identities into (C.4) to obtain\ngk(U) = F1\n\u0000T1 + U−1\n11 U12T2\n\u0001\nP∗.\nThis expression is just what we need. By combining it with the identity G = FTP∗,\nwe easily compute the diﬀerence\nG −gk(U) = (F2T2 −F1U−1\n11 U12T2)P∗.\n(C.5)\nHaving access to (C.5) marks a checkpoint in our proof. With it, we obtain the\nfollowing identity for the distance from G to gk(U) in any unitarily invariant norm:\n∥G −gk(U)∥= ∥F2T2 −F1U−1\n11 U12T2∥.\nThis implies our claim about gk(Iw×w), since if U is diagonal, then U12 = 0. Therefore all that remains is our claim about matrices that solve (C.3). The truth of this\nclaim is easier to see after a change variables. Upon replacing U−1\n11 U12 by a general\nk × (w −k) matrix, we can express\nmin\nU∈U ∥G −gk(U)∥F = min\nn\n∥F2T2 −F1BT2∥F\n\nM ∈Rk×(w−k)o\n,\nand it is easily shown that the matrix M⋆= F†\n1F2 is an optimal solution to the\nproblem on the right-hand side of this equation.\narXiv Version 2\nPage 159\n\nLow-rank Approximation\nC.2. Computational routines\nC.2\nComputational routine interfaces and implementations\nAs we explained in Section 4, the design space for low-rank approximation algorithms is quite large. Here we illustrate the breadth and depth of that design space\nwith pseudocode for computational routines needed for four drivers: SVD1, EVD1,\nEVD2, and CURD1 (Algorithms 3 through 6, respectively). All pseudocode here uses\nPython-style zero-based indexing.\nThe dependency structure of these drivers and their supporting functions is\ngiven in Figure C.1. From the ﬁgure we see that the following three interfaces are\ncentral to low-rank approximation.\n• Y = Orth(X) returns an orthonormal basis for the range of a tall input matrix;\nthe number of columns in Y will never be larger than that of X and may be\nsmaller. The simplest implementation of Orth is to return the orthogonal\nfactor from an economic QR decomposition of X.\n• S = SketchOpGen(ℓ, k) returns an ℓ× k oblivious sketching operator sampled from some predetermined distribution. The most common distributions\nused for low-rank approximation were covered in Section 2.3. In actual implementations, this function would accept an input representing the state of\nthe random number generator.\n• Y = Stabilizer(X) has similar semantics Orth. It diﬀers in that it only\nrequires Y to be better-conditioned than X while preserving its range. The\nrelaxed semantics open up the possibility of methods that are less expensive\nthan computing an orthonormal basis, such as taking the lower-triangular\nfactor from an LU decomposition with column pivoting.\nWe explain the remaining interfaces as they arise in our implementations.\nPage 160\narXiv Version 2\n\nC.2. Computational routines\nLow-rank Approximation\nFigure C.1:\nDependency illustration for low-rank approximation functionality.\nLighter gray boxes correspond to abstract interfaces which specify semantics. Any\ninterface can have many diﬀerent implementations. To keep things at a reasonable\nlength the only interface with multiple implementations is QBDecomposer.\nSection 4.3 describes many algorithms that could be used for any such interface.\nThe computational routines represented in Figure C.1 include Algorithms 9\nthrough 14. This appendix provides pseudocode for one additional function that is\nnot reﬂected in the ﬁgure: Algorithm 15 shows one way to perform row or column\nsubset selection. We note that while Algorithm 15 is not used in the drivers mentioned above, it could easily have been used in a diﬀerent implementation of CURD1.\nC.2.1\nPower iteration for data-aware sketching\nWhen a TallSketchOpGen is called with parameters (A, k), it produces an n × k\nsketching operator where range(S) is reasonably well-aligned with the subspace\nspanned by the k leading right singular vectors of A. Here, “reasonably” is assessed\nwith respect to the computational cost incurred by running TallSketchOpGen. One\nextreme case of interest is to return an oblivious sketching operator without reading\nany entries of A.\nThis method uses a p-step power iteration technique. When p = 0, the method\nreturns an oblivious sketching operator. It is recommended that one use p > 0 (e.g.,\np ∈{2, 3}) when the singular values of A exhibit “slow” decay.\narXiv Version 2\nPage 161\n\nLow-rank Approximation\nC.2. Computational routines\nAlgorithm 8 TSOG1 : a TallSketchOpGen based on a power method, conceptually\nfollowing [ZM20]. The returned sketching operator is suitable for sketching A from\nthe right for purposes of low-rank approximation.\n1: function TSOG1(A, k)\nInputs:\nA is m × n, and k ≪min{m, n} is a positive integer.\nOutput:\nS is n × k, intended for later use in computing Y = AS.\nAbstract subroutines:\nSketchOpGen and Stabilizer\nTuning parameters:\np ≥0 controls the number of steps in the power method. It is equal\nto the total number of matrix-matrix multiplications that will involve\neither A or A∗.\nIf p = 0 then this function returns an oblivious\nsketching operator.\nq ≥1 is the number of matrix-matrix multiplications with A or A∗\nthat accumulate before the stabilizer is called.\n2:\npdone = 0\n3:\nif p is even then\n4:\nS = SketchOpGen(n, k)\n5:\nelse\n6:\nS = A∗SketchOpGen(m, k)\n7:\npdone = pdone + 1\n8:\nif pdone mod q = 0 then\n9:\nS = stabilizer(S)\n10:\nwhile p −pdone ≥2 do\n11:\nS = AS\n12:\npdone = pdone + 1\n13:\nif pdone mod q = 0 then\n14:\nS = stabilizer(S)\n15:\nS = A∗S\n16:\npdone = pdone + 1\n17:\nif pdone mod q = 0 then\n18:\nS = stabilizer(S)\n19:\nreturn S\nC.2.2\nRangeFinders and QB decompositions\nA general RangeFinder takes in a matrix A and a target rank parameter k, and\nreturns a matrix Q of rank d = min{k, rank(A)} such that the range of Q is an\napproximation to the space spanned by A’s top d left singular vectors.\nThe rangeﬁnder problem may also be viewed in the following way: given a\nmatrix A ∈Rm×n and a target rank k ≪min(m, n), ﬁnd a matrix Q with k\ncolumns such that the error ∥A −QQ∗A∥is “reasonably” small. Some RangeFinder\nimplementations are iterative and can accept a target accuracy as a third argument.\nPage 162\narXiv Version 2\n\nC.2. Computational routines\nLow-rank Approximation\nThe RangeFinder below, RF1, is very simple. It relies on an implementation of\nthe TallSketchOpGen interface (e.g., TSOG1) as well as the Orth interface.\nAlgorithm 9 RF1 : a RangeFinder that orthogonalizes a single row sketch\n1: function RF1(A, k)\nInputs:\nA is m × n, and k ≪min{m, n} is a positive integer\nOutput:\nQ is a column-orthonormal matrix with d = min{k, rank A} columns.\nWe have range(Q) ⊂range(A); it is intended that range(Q) is an\napproximation to the space spanned by A’s top d left singular vectors.\nAbstract subroutines and tuning parameters:\nTallSketchOpGen\n2:\nS = TallSketchOpGen(A, k)\n# S is n × k\n3:\nY = AS\n4:\nQ = orth(Y)\n5:\nreturn Q\nThe conceptual goal of QB decomposition algorithms is to produce an approximation ∥A −QB∥≤ϵ (for some unitarily-invariant norm), where rank(QB) ≤\nmin{k, rank(A)}. Our next three algorithms are diﬀerent implementations of the\nQBDecomposer interface. The ﬁrst two of these algorithms require an implementation of the RangeFinder interface. The ability of the implementation QB1 to control\naccuracy is completely dependent on that of the underlying rangeﬁnder.\nAlgorithm 10 QB1 : a QBDecomposer that falls back on an abstract rangeﬁnder\n1: function QB1(A, k, ϵ)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\nϵ is a target for the relative error ∥A −QB∥/∥A∥measured in some\nunitarily-invariant norm.\nThis parameter is passed directly to the\nRangeFinder, which determines its precise interpretation.\nOutput:\nQ an m × d matrix returned by the underlying RangeFinder and\nB = Q∗A is d × n; we can be certain that d ≤min{k, rank(A)}. The\nmatrix QB is a low-rank approximation of A.\nAbstract subroutines and tuning parameters:\nRangeFinder\n2:\nQ = RangeFinder(A, k, ϵ)\n3:\nB = Q∗A\n4:\nreturn Q, B\nThe following algorithm builds up a QB decomposition incrementally. It’s said to\nbe fully-adaptive because it has ﬁne-grained control over the error ∥A−QB∥F. If the\nalgorithm is called with k = min{m, n}, then its output will satisfy ∥A−QB∥F ≤ϵ.\narXiv Version 2\nPage 163\n\nLow-rank Approximation\nC.2. Computational routines\nAlgorithm 11 QB2 : a QBDecomposer that’s fully-adaptive\n(see [YGL18, Algorithm 2])\n1: function QB2(A, k, ϵ)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\nϵ is a target for the relative error ∥A −QB∥F/∥A∥F. This parameter\nis used as a termination criterion upon reaching the desired accuracy.\nOutput:\nQ an m×d matrix combined of successive outputs from the underlying\nRangeFinder and B = Q∗A is d × n; we can be certain that d ≤\nmin{k, rank(A)}. The matrix QB is a low-rank approximation of A.\nAbstract subroutines:\nRangeFinder\nTuning parameters:\nblock size ≥1 - at every iteration (except possibly for the ﬁnal\niteration), block size columns are added to the matrix Q.\n2:\nd = 0\n3:\nQ = [ ] ∈Rm×d # Preallocation is dangerous; k = min{m, n} is allowed.\n4:\nB = [ ] ∈Rd×n\n5:\nsquared error = ∥A∥2\nF\n6:\nwhile k > d do\n7:\nblock size = min{block size, k −d}\n8:\nQi = RangeFinder(A, block size)\n9:\nQi = orth(Qi −Q (Q∗Qi)) # for numerical stability\n10:\nBi = Q∗\ni A # original matrix A is valid here\n11:\nB =\n\u0014 B\nBi\n\u0015\n12:\nQ =\n\u0002\nQ\nQi\n\u0003\n13:\nd = d + block size\n14:\nA = A −QiBi # modiﬁcation can be implicit, but is required by Line 8\n15:\nsquared error = squared error −∥Bi∥2\nF # compute by a stable method\n16:\nif squared error ≤ϵ2 then\n17:\nbreak\n18:\nreturn Q, B\nOur third and ﬁnal QB algorithm also builds up its approximation incrementally.\nIt is called pass-eﬃcient because it does not access the data matrix A within its\nmain loop (see [DKM06a] for the original deﬁnition of the pass-eﬃcient model).\nThe algorithm can use a requested error tolerance as an early-stopping criterion.\nThis function should never be called with k = min{m, n}. We note that it takes a\nfair amount of algebra to prove that this algorithm produces a correct result.\nPage 164\narXiv Version 2\n\nC.2. Computational routines\nLow-rank Approximation\nAlgorithm 12 QB3 : a QBDecomposer that’s pass-eﬃcient and partially adaptive\n(based on [YGL18, Algorithm 4])\n1: function QB3(A, k, ϵ)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\nϵ is a target for the relative error ∥A −QB∥F/∥A∥F. This parameter\nis used as a termination criterion upon reaching the desired accuracy.\nOutput:\nQ an m × d matrix combined of successively-computed orthonormal bases Qi and B = Q∗A is d × n; we can be certain that\nd ≤min{k, rank(A)}. The matrix QB is a low-rank approximation\nof A.\nAbstract subroutines:\nTallSketchOpGen\nTuning parameters:\nblock size is a positive integer; at every iteration (except possibly for\nthe last), we add block size columns to Q.\n2:\nQ = [ ] ∈Rm×0 # It would be preferable to preallocate.\n3:\nB = [ ] ∈R0×n\n4:\nsquared error = ∥A∥2\nF\n5:\nS = TallSketchOpGen(A, k)\n6:\nG = AS, H = A∗G # Can be done in one pass over A\n7:\nmax blocks = ⌈k/block size⌉\n8:\ni = 0\n9:\nwhile i < max blocks do\n10:\nbstart = i · block size + 1\n11:\nbend = min{(i + 1) · block size, k}\n12:\nSi = S[: , bstart : bend]\n13:\nYi = G[: , bstart : bend] −Q(BSi)\n14:\nQi, Ri = qr(Yi) # the next three lines are for numerical stability\n15:\nQi = Qi −Q(Q∗Qi)\n16:\nQi, ˆRi = qr(Qi)\n17:\nRi = ˆRiRi\n18:\nBi = (H[: , bstart : bend])∗−(YiQ)B −(BSi)∗B\n19:\nBi = (R∗\ni )−1 Bi # in-place triangular solve\n20:\nB =\n\u0014 B\nBi\n\u0015\n21:\nQ =\n\u0002\nQ\nQi\n\u0003\n22:\nsquared error = squared error −∥Bi∥2\nF # compute by a stable method\n23:\ni = i + 1\n24:\nif squared error ≤ϵ2 then\n25:\nbreak\n26:\nreturn Q, B\narXiv Version 2\nPage 165\n\nLow-rank Approximation\nC.2. Computational routines\nC.2.3\nID and subset selection\nAs we indicated in Sections 4.2.3 and 4.3.3, the collective design space of algorithms\nfor ID, subset selection, and CUR is very large. This appendix presents one randomized algorithm for one-sided ID (Algorithm 14) and an analogous randomized\nalgorithm for subset selection (Algorithm 15). These algorithms are implemented in\nour Python prototype. The Python prototype has two more randomized algorithms\nwhich are not reproduced here (one for one-sided and one for two-sided ID).\nWe need two deterministic functions in order to state these algorithms. The\nﬁrst deterministic function – called as Q, R, J = qrcp(F, k) – returns data for an\neconomic QR decomposition with column pivoting, where the decomposition is restricted to rank k and may be incomplete. The second deterministic function (Algorithm 13, below) is the canonical way to use QRCP for one-sided ID. It produces\na column ID when the ﬁnal argument “axis” is set to one; otherwise, it produces a\nrow ID. When used for column ID, it’s typical for Y ∈Rℓ×w to be (very) wide and\nfor k to be only slightly smaller than ℓ(say, ℓ/2 ≤k ≤ℓ).\nAlgorithm 13 deterministic one-sided ID based on QRCP\n1: function osid qrcp(Y, k, axis)\nInputs:\nY is an ℓ× w matrix, typically a sketch of some larger matrix.\nk is an integer, typically close to min{ℓ, w}.\naxis is an integer, equals 1 for row ID and 2 for column ID.\nOutputs:\nWhen axis = 1:\nZ is ℓ× k and I is a length-k index vector.\nTogether, they satisfy Y[I, :] = (ZY[I, :])[I, :].\nWhen axis = 2:\nX is k × w and J is a length-k index vector.\nTogether, they satisfy Y[:, J] = (Y[:, J]X)[:, J].\nAbstract subroutines:\nqrcp\n2:\nif axis == 2 then\n3:\n(ℓ, w) = the number of (rows, columns) in Y\n4:\nassert k ≤min{ℓ, w}\n5:\nQ, R, J = qrcp(Y, k)\n6:\nT = (R[:k, :k])−1 R[:k, k + 1:] # use trsm from BLAS 3\n7:\nX = zeros(k, w)\n8:\nX[:, J] = [Ik×k, T]\n9:\nJ = J[:k]\n10:\nreturn X, J\n11:\nelse\n12:\nX, I = osid qrcp(Y∗, k, axis = 1)\n13:\nZ = X∗\n14:\nreturn Z, I\nPage 166\narXiv Version 2\n\nC.2. Computational routines\nLow-rank Approximation\nThe one-sided ID interface is\nM, P = OneSidedID(A, k, s, axis).\nThe output value M is the interpolation matrix and P is the length-k vector of\nskeleton indices. When axis = 1 we are considering a row ID and so obtain the\napproximation ˆA = MA[P, :] to A. When axis = 2, we are considering the lowrank column ID ˆA = A[:, P]M. Implementations of this interface perform internal\ncalculations with sketches of rank k + s.\nAlgorithm 14 OSID1 : implements OneSidedID by re-purposing an ID of a sketch.\nBesides the original source [VM16, §5.1], more information on this algorithm can\nbe found in [Mar18, §10.4] and [MT20, §13.4].\n1: function OSID1(A, k, axis)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\naxis is an integer, equal to 1 for row ID or 2 for column ID.\nOutput:\nA matrix Z and vector I satisfying Y[I, :] = (ZY[I, :])[I, :]\nor\na matrix X and vector J satisfying Y[:, J] = (Y[:, J]X)[:, J].\nAbstract subroutines:\nTallSketchOpGen and osid qrcp\nTuning parameters:\ns is a nonnegative integer. The algorithm internally works with a\nsketch of rank k + s.\n2:\nif axis == 1 then # row ID\n3:\nS = TallSketchOpGen(A, k + s)\n4:\nY = AS\n5:\nZ, I = osid qrcp(Y, k, axis = 0)\n6:\nreturn Z, I\n7:\nelse\n8:\nS = TallSketchOpGen(A∗, k + s)∗\n9:\nY = SA\n10:\nX, J = osid qrcp(Y, k, axis = 1)\n11:\nreturn X, J\nConsider the following interface for (randomized) row and column subset selection algorithms\nP = RowOrColSelection(A, k, s, axis).\nThe index vector P and oversampling parameter is understood in the same way as\nthe OneSidedID interface. That is, P is a partial permutation of the row index set\nJmK (when axis = 1) or the column index set JnK (when axis = 2). Implementations\nare supposed to perform internal calculations with sketches of rank k + s.\narXiv Version 2\nPage 167\n\nLow-rank Approximation\nC.2. Computational routines\nAlgorithm 15 ROCS1 : implements RowOrColSelection by QRCP on a sketch\n1: function ROCS1(A, k, s, axis)\nInputs:\nA is an m × n matrix and k ≪min{m, n} is a positive integer.\naxis is an integer, equal to 1 for row selection or 2 for column selection.\nOutput:\nI: a row selection vector of length k\nor\nJ: a column selection vector of length k.\nAbstract subroutines:\nTallSketchOpGen\nTuning parameters:\ns is a nonnegative integer. The algorithm internally works with a\nsketch of rank k + s.\n2:\nif axis == 1 then\n3:\nS = TallSketchOpGen(A, k + s)\n4:\nY = AS\n5:\nQ, R, I = qrcp(Y∗)\n6:\nreturn I[: k]\n7:\nelse\n8:\nS = TallSketchOpGen(A∗, k + s)\n9:\nY = SA\n10:\nQ, R, J = qrcp(Y)\n11:\nreturn J[: k]\nPage 168\narXiv Version 2\n\nAppendix D\nCorrectness of Preconditioned\nCholesky QRCP\nIn this appendix we prove Proposition 5.1.2. Since this would involve a fair amount\nof bookkeeping if we used the notation of Algorithm 7, we begin with a more detailed\nstatement of the algorithm.\nLet A be m × n and S be d × m with n ≤d ≪m.\n1. Compute the sketch Ask = SA\n2. Decompose [Qsk, Rsk, J] = qrcp(Ask)\n(a) J is a permutation vector for the index set JnK.\n(b) Abbreviating Ask\nJ = Ask[:, J], we have Ask\nJ = QskRsk.\n(c) Let k = rank(Ask).\n(d) Qsk is m × k and column-orthonormal.\n(e) Rsk = [Rsk\n1 , Rsk\n2 ] is k × n upper-triangular.\n(f) Rsk\n1 is k × k and nonsingular.\n3. Abbreviate AJ = A[:, J] and explicitly from Apre = AJ[:, :k](Rsk\n1 )−1.\n4. Compute an unpivoted QR decomposition Apre = QRpre.\n(a) If rank(A) = k then Q is an orthonormal basis for the range of A.\n(b) For the purposes of this appendix, it does not matter what algorithm\nwe use to compute this decomposition. We assume the decomposition\nis exact.\n5. Explicitly form R = RpreRsk\nThe goal of this proof is to show that the equality A[:, J] = QR holds under the\nassumption that rank(SA) = rank(A). Let us ﬁrst establish some useful identities.\nBy steps 3 and 4 of the algorithm description above, we know that\nRpre = Q∗AJ[:, :k](Rsk\n1 )−1.\n169\n\nCorrectness of Preconditioned Cholesky QRCP\nCombining this with the characterization of R from Steps 2e and 5, we have\nR = Q∗AJ[:, :k](Rsk\n1 )−1[Rsk\n1 , Rsk\n2 ].\nWe may further expand this expression as such:\nR = Q∗AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ].\nSince Q is an orthonormal basis for the range of A and, consequently, AJ, we\nhave that\nQR = AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ].\n(D.1)\nWe use (D.1) to establish the claim by a columnwise argument. That is, we show\nthat QR[:, ℓ] = AJ[:, ℓ] for all 1 ≤ℓ≤n.\nFirst, consider the case when ℓ≤k. Let δn\nℓbe the ℓth standard basis vector in\nRn. Then, consider the following series of identities:\nQR[:, ℓ] = QRδn\nℓ\n= AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ]δn\nℓ\n= AJ[:, :k]δk\nℓ= AJ[:, ℓ],\nhence the desired statement holds for ℓ≤k.\nIt remains to show that QR[:, ℓ] = AJ[:, ℓ] for ℓ> k. Note that\nQR[:, ℓ] = AJ[:, :k][Ik×k, (Rsk\n1 )−1Rsk\n2 ]δn\nℓ\n= AJ[:, :k]((Rsk\n1 )−1Rsk\n2 )[:, ℓ−k].\nLet γ = ((Rsk\n1 )−1Rsk\n2 )[:, ℓ−k]. Therefore, in order to obtain the desired identity for\nℓ> k, we will need to show that\nAJ[:, k]γ = AJ[:, ℓ].\nProposition D.0.1. If Ask\nJ [:, ℓ] = Ask\nJ [:, :k]u for some u ∈Rk, then\nAJ[:, ℓ] = AJ[:, :k]u.\nProof. To simplify notation, deﬁne the m×k matrix X = AJ[:, :k] and the m-vector\ny = AJ[:, ℓ].\nSuppose to the contrary that y ̸= Xu and Sy = SXu. Then, SXu −Sy = 0.\nDeﬁne U = ker(S[X, y]) and V = ker([X, y]). Clearly, U contains V . Additionally,\nif U contains a nonzero vector that is not in V , then dim(U) > dim(V ). This would\nfurther imply that rank(S[X, y]) < rank([X, y]).\nIf SXu−Sy = 0, then (u, −1) is a nonzero vector in U that is not in V . However,\nby our assumption, the sketch does not drop rank. Consequently, no such vector\n(u, −1) can exist, and we must have y = Xu.\nWe now prove that Ask\nJ [:, :k]γ = Ask\nJ [:, ℓ].\nTo do this, start by noting that\nAsk\nJ [:, :k] = QskRsk\n1 . Plugging in the deﬁnition of γ, we have\nAsk\nJ [:, :k]γ = QskRsk\n1 (Rsk\n1 )−1(Rsk\n2 )[:, ℓ−k] = Qsk(Rsk\n2 )[:, ℓ−k].\nThe next step is to use the simple observation that Rsk\n2 [:, ℓ−k] = Rsk[:, ℓ] to ﬁnd\nAsk\nJ [:, :k]γ = (QskRsk)[:, ℓ] = Ask\nJ [:, ℓ].\nCombining the above results and Proposition D.0.1 proves Proposition 5.1.2.\nPage 170\narXiv Version 2\n\nAppendix E\nBootstrap Methods\nfor Error Estimation\nE.1 Bootstrap methods in a nutshell ................................. 172\nE.2 Sketch-and-solve least squares .................................... 173\nE.3 Sketch-and-solve one-sided SVD ................................. 174\nWhenever a randomized algorithm produces a solution, a question immediately\narises: Is the solution suﬃciently accurate? In many situations, it is possible to\nestimate numerically the error of the solution using the available problem data —\na process that is often referred to as (a posteriori) error estimation.1 In addition\nto resolving uncertainty about the quality of a solution, another key beneﬁt of error estimation is that it enables computations to be done more adaptively. For\ninstance, error estimates can be used to determine if additional iterations should be\nperformed, or if tuning parameters should be modiﬁed. In this way, error estimates\ncan help to incrementally reﬁne a rough initial solution so that “just enough” work\nis done to reach a desired level of accuracy.\nIn this appendix, we provide a brief overview of bootstrap methods for error\nestimation in RandNLA. Up to now, these tools (which are common in statistics\nand statistical data analysis) have been designed for a handful of sketch-and-solve\ntype algorithms, and the development of bootstrap methods for a wider range of\nrandomized algorithms is an open direction of research. Our main purpose in writing\nthis appendix is to record the consideration we have given to bootstrap methods.\nOur secondary purpose is to provide a starting point for non-experts to survey this\nliterature as it evolves.\n1This should be contrasted with (a priori) error bounds often used in theoretical development\nof RandNLA algorithms, in which one bounds rather than estimates the error, and does so in a\nworst-case way that does not depend on the problem data.\n171\n\nBootstrap Methods for Error Estimation\nE.1. Bootstrap methods in a nutshell\nE.1\nBootstrap methods in a nutshell\nBootstrap methods have been studied extensively in the statistics literature for\nmore than four decades, and they comprise a very general framework for quantifying uncertainty [ET94; ST12]. One of the most common uses of these methods in\nstatistics is to assess the accuracy of parameter estimates. This use-case provides\nthe connection between bootstrap methods and error estimation in RandNLA. Indeed, an exact solution to a linear algebra problem can be viewed as an “unknown\nparameter,” and a randomized algorithm can be viewed as providing an “estimate”\nof that parameter. Taking the analogy a step further, a random sketch of a matrix can also be viewed as a “dataset” from which the estimate of the “population”\nquantity is computed. Likewise, when bootstrap methods are applied in RandNLA,\nthe rows or columns of a sketched matrix often play the role of “data vectors”.\nWe now formulate the task of error estimation in a way that is convenient for\ndiscussion of bootstrap methods. First, suppose the existence of some ﬁxed but\nunknown “true parameter” θ ∈R. Suppose we estimate this parameter by a value\nˆθ depending on random samples from some probability distribution. The error of ˆθ\nis deﬁned as ˆϵ = |ˆθ −θ|, which we emphasize is both random and unknown. From\nthis standpoint, it is natural to seek the tightest upper bound on ˆϵ that holds with\na speciﬁed probability, say 1−α. This ideal bound is known as the (1−α)-quantile\nof ˆϵ, and is deﬁned more formally as\nq1−α = inf{t ∈[0, ∞) | P(ˆϵ ≤t) ≥1 −α}.\nAn error estimation problem is considered solved if it is possible to construct a\nquantile estimate ˆq1−α such that the inequality ˆϵ ≤ˆq1−α holds with probability\nthat is close to 1 −α.\nThe bootstrap approach to estimating q1−α is based on imagining a scenario\nwhere it is possible to generate many independent samples ˆˆϵ1, . . . ,ˆˆϵN of the random\nvariable ˆϵ. Of course, this is not possible in practice, but if it were, then an estimate\nof q1−α could be easily obtained using the empirical (1−α)-quantile of the samples\nˆˆϵ1, . . . ,ˆˆϵN. The key idea that bootstrap methods use to circumvent the diﬃculty is\nto generate “approximate samples” of ˆϵ, which can be done in practice.\nTo illustrate how approximate samples of ˆϵ can be constructed, consider a generic\nsituation where the estimate ˆθ is computed as a function of a dataset X1, . . . , Xn.\nThat is, suppose ˆθ = f(X1, . . . , Xn) for some function f. Then, a bootstrap sample\nof ˆϵ, denoted ˆˆϵ, is computed as follows:\n• Sample n points { ˆˆXi}n\ni=1 with replacement from the original dataset {Xi}n\ni=1.\n• Compute ˆˆθ := f( ˆˆX1, . . . , ˆˆXn)\n• Compute ˆˆϵ := |ˆˆθ −ˆθ|.\nBy performing N independent iterations of this process, a collection of bootstrap\nsamples ˆˆϵ1, . . . ,ˆˆϵN can be generated. Then, the desired quantile estimate ˆq1−α can\nbe computed as the smallest number t ≥0 for which the inequality\n1\nN\nN\nX\ni=1\nI{ˆˆϵi ≤t} ≥1 −α\nPage 172\narXiv Version 2\n\nE.2. Sketch-and-solve least squares\nBootstrap Methods for Error Estimation\nis satisﬁed, where I{·} refers to the {0, 1}-valued indicator function. This quantity\nis also known as the empirical (1 −α)-quantile of ˆˆϵ1, . . . ,ˆˆϵn. We will sometimes\ndenote it by quantile[ˆˆϵ1, . . . ,ˆˆϵn; 1 −α].\nTo provide some intuition for the bootstrap, the random variable ˆˆθ can be viewed\nas a “perturbed version” of ˆθ, where the perturbing mechanism is designed so that\nthe deviations of ˆˆθ around ˆθ are statistically similar to the deviations of ˆθ around\nθ [ET94]. Equivalently, this means that the histogram of ˆˆϵ1, . . . ,ˆˆϵN will serve as\na good approximation to the distribution of the actual random error variable ˆϵ.\nFurthermore, it turns out that this approximation is asymptotically valid (i.e., n →\n∞) and supported by quantitative guarantees in a broad range of situations [ST12].\nE.2\nSketch-and-solve least squares\nThere is a direct analogy between the discussion above and the setting of sketchand-solve algorithms for least squares. First, the “true parameter” θ is the exact\nsolution x⋆= argminx∈Rn∥Ax −b∥2\n2. Second, the dataset X1, . . . , Xn corresponds\nto the sketches [ˆA, ˆb] = S[A, b]. Third, the estimate ˆθ corresponds to the sketchand-solve solution ˆx = argminx∈Rn∥ˆAx −ˆb∥2\n2. Fourth, the error variable can be\ndeﬁned as ˆϵ = ρ(ˆx, x⋆), for a preferred metric ρ, such as that induced by the ℓ2 or\nℓ∞norms.\nOnce these correspondences are recognized, the previous bootstrap sampling\nscheme can be applied. For further background, as well as extensions to error estimation for iterative randomized algorithms for least squares, we refer to [LWM18].\nMethod 1 (Bootstrap error estimation for sketch-and-solve least squares).\nInput: A positive integer B, the sketches ˆA ∈Rd×n, ˆb ∈Rd, and ˆx ∈Rn.\nFor ℓ∈JBK do in parallel\n1. Draw a vector I := (i1, . . . , id) by sampling d numbers with replacement from\nJdK.\n2. Form the matrix ˆˆA := ˆA[I, :], and vector ˆˆb := ˆb[I].\n3. Compute the following vector and scalar,\nˆˆx := arg min\nx∈Rn\n∥ˆˆAx −ˆˆb∥2\nand\nˆˆϵℓ:= ∥ˆˆx −ˆx∥.\n(E.1)\nReturn: The estimate quantile[ˆˆϵ1, . . . ,ˆˆϵB; 1−α] for the (1−α)-quantile of ∥ˆx−x⋆∥.\nTo brieﬂy comment on some of the computational characteristics of this method,\nit should be emphasized that the for loop can be implemented in an embarrassingly\nparallel manner, which is typical of most bootstrap methods. Second, the method\nonly relies on access to sketched quantities, and hence does not require any access\nto the full matrix A. Likewise, the computational cost of the method is independent\nof the number of rows of A.\narXiv Version 2\nPage 173\n\nBootstrap Methods for Error Estimation\nE.3. Sketch-and-solve one-sided SVD\nE.3\nSketch-and-solve one-sided SVD\nWe call the problem of computing the singular values and right singular vectors of\na matrix a “one-sided SVD.” We further use the term “sketch-and-solve one-sided\nSVD” for an algorithm that approximates the top k singular values and singular\nvectors of A by those of a sketch ˆA = SA. Here we consider estimating the error\nincurred by such an algorithm. As matters of notation, we let {(σj, vj)}k\nj=1 denote\nthe top k singular values and right singular vectors of A and {(ˆσj, ˆvj)}k\nj=1 the\ncorresponding quantities for ˆA. We suppose that error is measured uniformly over\nj ∈JkK, which leads us to consider error variables of the form\nϵΣ := max\nj∈JkK |ˆσj −σj|\nand\nϵV := max\nj∈JkK ρ(ˆvj, vj).\nThe following bootstrap method, developed in [LEM20], provides estimates for the\n(1 −α)-quantiles of ϵΣ and ϵV .\nMethod 2 (Bootstrap error estimation for sketch-and-solve SVD).\nInput: The sketch ˆA ∈Rd×n and its top k singular values and right singular\nvectors (ˆσ1, ˆv1), . . . , (ˆσk, ˆvk), a number of samples B, a parameter α ∈(0, 1).\n• For ℓ∈JBK do in parallel\n1. Form ˆˆA ∈Rd×n by sampling d rows from ˆA with replacement.\n2. Compute the top k singular values and right singular vectors of ˆˆA, denoted\nas ˆˆσ1, . . . , ˆˆσk and ˆˆv1, . . . , ˆˆvk. Then, compute the bootstrap samples\nˆˆϵΣ,ℓ:= max\nj∈JkK |ˆˆσj −ˆσj|\n(E.2)\nˆˆϵV,ℓ:= max\nj∈JkK ρ(ˆˆvj, ˆvj).\n(E.3)\nReturn: The estimates quantile[ˆˆϵΣ,1, . . . ,ˆˆϵΣ,B; 1−α] and quantile[ˆˆϵV,1, . . . ,ˆˆϵV,B; 1−α]\nfor the (1 −α)-quantiles of ϵΣ and ϵV .\nAlthough this method is only presented with regard to singular values and right\nsingular vectors, it is also possible to apply a variant of it to estimate the errors\nof approximate left singular vectors.\nHowever, a few extra technical details are\ninvolved, which may be found in [LEM20].\nAnother technique to estimate error in the setting of sketch-and-solve one-sided\nSVD is through the spectral norm ∥ˆA\n∗ˆA−A∗A∥2. Due to the Weyl and Davis-Kahan\ninequalities, an upper bound on ∥ˆA\n∗ˆA −A∗A∥2 directly implies upper bounds on\nthe errors of all the sketched singular values ˆσ1, . . . , ˆσn and sketched right singular\nvectors ˆv1, . . . , ˆvn. Furthermore, the quantiles of the error variable ∥ˆA\n∗ˆA −A∗A∥2\ncan be estimated via the bootstrap, as shown in [LEM23].\nPage 174\narXiv Version 2\n\nBibliography\n[AAB+17]\nA. Abdelfattah, H. Anzt, A. Bouteiller, A. Danalis, J. Dongarra, M. Gates,\nA. Haidar, J. Kurzak, P. Luszczek, S. Tomov, S. Wood, P. Wu, I. Yamazaki,\nand A. YarKhan. Roadmap for the Development of a Linear Algebra Library\nfor Exascale Computing: SLATE: Software for Linear Algebra Targeting\nExascale. SLATE Working Notes 01, ICL-UT-17-02. June 2017.\n[ABB+99]\nE. Anderson, Z. Bai, C. Bischof, L. S. Blackford, J. Demmel, J. Dongarra,\nJ. D. Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. Sorensen.\nLAPACK Users’ Guide. Society for Industrial and Applied Mathematics,\nJan. 1999.\n[AC06]\nN. Ailon and B. Chazelle. “Approximate nearest neighbors and the Fast\nJohnson-Lindenstrauss Transform”. In: Proceedings of the Thirty-Eighth\nAnnual ACM Symposium on Theory of Computing (STOC). STOC ’06.\nSeattle, WA, USA: Association for Computing Machinery, 2006, pp. 557–\n563. isbn: 1595931341.\n[AC09]\nN. Ailon and B. Chazelle. “The Fast Johnson–Lindenstrauss Transform\nand approximate nearest neighbors”. In: SIAM Journal on Computing 39.1\n(Jan. 2009), pp. 302–322.\n[Ach03]\nD. Achlioptas. “Database-friendly random projections: Johnson-Lindenstrauss\nwith binary coins”. In: Journal of Computer and System Sciences 66.4\n(2003), pp. 671–687.\n[ACW17a]\nH. Avron, K. L. Clarkson, and D. P. Woodruﬀ. “Sharper bounds for regularized data ﬁtting”. In: Approximation, Randomization, and Combinatorial\nOptimization. Algorithms and Techniques (2017).\n[ACW17b]\nH. Avron, K. L. Clarkson, and D. P. Woodruﬀ. “Faster kernel ridge regression using sketching and preconditioning”. In: SIAM Journal on Matrix\nAnalysis and Applications 38.4 (2017), pp. 1116–1138.\n[AD20]\nN. Anari and M. Derezi´nski. “Isotropy and log-concave polynomials: accelerated sampling and high-precision counting of matroid bases”. In: 2020\nIEEE 61st Annual Symposium on Foundations of Computer Science (FOCS).\nIEEE. 2020, pp. 1331–1344.\n[ADD+09]\nE. Agullo, J. Demmel, J. Dongarra, B. Hadri, J. Kurzak, J. Langou, H.\nLtaief, P. Luszczek, and S. Tomov. “Numerical linear algebra on emerging\narchitectures: the PLASMA and MAGMA projects”. In: Journal of Physics:\nConference Series 180 (July 2009), p. 012037.\n[ADN20]\nP. Ahren, J. Demmel, and H.-D. Nguyen. “Algorithms for eﬃcient reproducible ﬂoating point summation”. In: ACM Transactions on Mathematical\nSoftware 46.3 (2020).\n175\n\nBibliography\n[ADR92]\nM. Arioli, I. Duﬀ, and D. Ruiz. “Stopping criteria for iterative solvers”.\nIn: SIAM Journal on Matrix Analysis and Applications 13.1 (Jan. 1992),\npp. 138–144.\n[ADV+22]\nN. Anari, M. Derezi´nski, T.-D. Vuong, and E. Yang. “Domain sparsiﬁcation\nof discrete distributions using entropic independence”. In: ACM Symposium\non Discrete Algorithms (SODA). 2022.\n[AGL98]\nC. Ashcraft, R. G. Grimes, and J. G. Lewis. “Accurate symmetric indefinite linear equation solvers”. In: SIAM Journal on Matrix Analysis and\nApplications 20.2 (Jan. 1998), pp. 513–561.\n[AGR16]\nN. Anari, S. O. Gharan, and A. Rezaei. “Monte carlo markov chain algorithms for sampling strongly rayleigh distributions and determinantal\npoint processes”. In: Conference on Learning Theory (COLT). PMLR. 2016,\npp. 103–115.\n[AK01]\nO. Axelsson and I. Kaporin. “Error norm estimation and stopping criteria in\npreconditioned conjugate gradient iterations”. In: Numerical Linear Algebra\nwith Applications 8.4 (2001), pp. 265–286.\n[AKK+20]\nT. Ahle, M. Kapralov, J. Knudsen, R. Pagh, A. Velingker, D. Woodruﬀ, and\nA. Zandieh. “Oblivious sketching of high-degree polynomial kernels”. In:\nProceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete\nAlgorithms. SIAM, 2020, pp. 141–160.\n[AM15]\nA. E. Alaoui and M. W. Mahoney. “Fast randomized kernel methods with\nstatistical guarantees”. In: Annual Advances in Neural Information Processing Systems. 2015, pp. 775–783.\n[Ame22]\nS. Ameli. IMATE, a high-performance python package for implicit matrix\ntrace estimation. https://pypi.org/project/imate/. 2022.\n[AMT10]\nH. Avron, P. Maymounkov, and S. Toledo. “Blendenpik: Supercharging LAPACK’s Least-Squares Solver”. In: SIAM Journal on Scientiﬁc Computing\n32.3 (Jan. 2010), pp. 1217–1236.\n[ANW14]\nH. Avron, H. Nguyen, and D. Woodruﬀ. “Subspace embeddings for the\npolynomial kernel”. In: Advances in Neural Information Processing Systems. Vol. 27. Curran Associates, Inc., 2014.\n[Aro50]\nN. Aronszajn. “Theory of reproducing kernels”. In: Transactions of the\nAmerican mathematical society 68.3 (1950), pp. 337–404.\n[Bac13]\nF. Bach. “Sharp analysis of low-rank kernel matrix approximations”. In:\nProceedings of the 26th Annual Conference on Learning Theory (COLT).\n2013, pp. 185–209.\n[Bal22a]\nO. Balabanov. randKrylov, a MATLAB library for linear systems and eigenvalue problems. https://github.com/obalabanov/randKrylov. 2022.\n[Bal22b]\nO. Balabanov. Randomized Cholesky QR factorizations. 2022. arXiv: 2210.\n09953.\n[BBB+14]\nM. Baboulin, D. Becker, G. Bosilca, A. Danalis, and J. Dongarra. “An efﬁcient distributed randomized algorithm for solving large dense symmetric\nindeﬁnite linear systems”. In: Parallel Computing 40.7 (July 2014), pp. 213–\n223.\n[BBB15]\nD. J. Biagioni, D. Beylkin, and G. Beylkin. “Randomized interpolative\ndecomposition of separated representations”. In: Journal of Computational\nPhysics 281 (2015), pp. 116–134.\nPage 176\narXiv Version 2\n\nBibliography\n[BBG+22]\nO. Balabanov, M. Beaupere, L. Grigori, and V. Lederer. Block subsampled\nrandomized Hadamard transform for low-rank approximation on distributed\narchitectures. 2022. arXiv: 2210.11295.\n[BBK18]\nC. Battaglino, G. Ballard, and T. G. Kolda. “A practical randomized CP\ntensor decomposition”. In: SIAM Journal on Matrix Analysis and Applications 39.2 (2018), pp. 876–901.\n[BDH+13]\nM. Baboulin, J. Dongarra, J. Herrmann, and S. Tomov. “Accelerating linear\nsystem solutions using randomization techniques”. In: ACM Trans. Math.\nSoftw. 39.2 (Feb. 2013).\n[BDN15]\nJ. Bourgain, S. Dirksen, and J. Nelson. “Toward a uniﬁed theory of sparse\ndimensionality reduction in Euclidean space”. In: Geometric and Functional\nAnalysis 25.4 (July 2015), pp. 1009–1088.\n[BDR+17]\nM. Baboulin, J. Dongarra, A. R´emy, S. Tomov, and I. Yamazaki. “Solving dense symmetric indeﬁnite systems using GPUs”. In: Concurrency and\nComputation: Practice and Experience 29.9 (2017). e4055 cpe.4055, e4055.\n[BFG96]\nZ. Bai, G. Fahey, and G. Golub. “Some large-scale matrix computation\nproblems”. In: Journal of Computational and Applied Mathematics 74.1\n(1996), pp. 71–89.\n[BG13]\nC. Boutsidis and A. Gittens. “Improved matrix algorithms via the subsampled randomized Hadamard transform”. In: SIAM Journal on Matrix\nAnalysis and Applications 34.3 (Jan. 2013), pp. 1301–1340.\n[BG21]\nO. Balabanov and L. Grigori. Randomized block Gram-Schmidt process for\nsolution of linear systems and eigenvalue problems. 2021. arXiv: 2111 .\n14641.\n[BG22]\nO. Balabanov and L. Grigori. “Randomized Gram–Schmidt process with\napplication to GMRES”. In: SIAM Journal on Scientiﬁc Computing 44.3\n(2022), A1450–A1474.\n[BG65]\nP. Businger and G. H. Golub. “Linear least squares solutions by householder\ntransformations”. In: Numerische Mathematik 7.3 (June 1965), pp. 269–\n276.\n[BGL05]\nM. Benzi, G. H. Golub, and J. Liesen. “Numerical solution of saddle point\nproblems”. In: Acta Numerica 14 (2005), pp. 1–137.\n[Bha97]\nR. Bhatia. Matrix Analysis. Springer New York, 1997.\n[Bja19]\nE. K. Bjarkason. “Pass-eﬃcient randomized algorithms for low-rank matrix\napproximation using any number of views”. In: SIAM Journal on Scientiﬁc\nComputing 41.4 (Jan. 2019), A2355–A2383.\n[Bj¨o15]\n˚A. Bj¨orck. Numerical Methods in Matrix Computations. Vol. 59. 2015. isbn:\n978-3-319-05088-1.\n[Bj¨o96]\n˚A. Bj¨orck. Numerical Methods for Least Squares Problems. Society for Industrial and Applied Mathematics, Jan. 1996.\n[BK21]\nZ. Bujanovic and D. Kressner. “Norm and trace estimation with random\nrank-one vectors”. In: SIAM Journal on Matrix Analysis and Applications\n42.1 (2021), pp. 202–223.\n[BK77]\nJ. R. Bunch and L. Kaufman. “Some stable methods for calculating inertia\nand solving symmetric linear systems”. In: Mathematics of Computation\n31.137 (1977), pp. 163–179.\n[BKW21]\nS. Bamberger, F. Krahmer, and R. Ward. Johnson–Lindenstrauss Embeddings with Kronecker Structure. 2021. arXiv: 2106.13349.\narXiv Version 2\nPage 177\n\nBibliography\n[BLR14]\nM. Baboulin, X. S. Li, and F. Rouet. “Using random butterﬂy transformations to avoid pivoting in sparse direct methods”. In: High Performance\nComputing for Computational Science - VECPAR. Ed. by M. J. Dayd´e, O.\nMarques, and K. Nakajima. Vol. 8969. Lecture Notes in Computer Science.\nSpringer, 2014, pp. 135–144.\n[BM58]\nG. E. P. Box and M. E. Muller. “A note on the generation of random\nnormal deviates”. In: The Annals of Mathematical Statistics 29.2 (1958),\npp. 610–611.\n[BMD09]\nC. Boutsidis, M. W. Mahoney, and P. Drineas. “An improved approximation algorithm for the column subset selection problem”. In: Proceedings of\nthe 20th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA).\n2009, pp. 968–977.\n[BMM+22]\nV. Bharadwaj, O. A. Malik, R. Murray, A. Bulu¸c, and J. Demmel. DistributedMemory Randomized Algorithms for Sparse Tensor CP Decomposition. 2022.\narXiv: 2210.05105.\n[Boh07]\nM. Bohr. “A 30 year retrospective on Dennard’s MOSFET scaling paper”.\nIn: Solid-State Circuits Newsletter, IEEE 12.1 (2007), pp. 11–13.\n[BV21]\nD. Blackman and S. Vigna. “Scrambled linear pseudorandom number generators”. In: ACM Trans. Math. Softw. 47.4 (Sept. 2021). Software available\nat https://prng.di.unimi.it/.\n[CDD+96]\nJ. Choi, J. Demmel, I. Dhillon, J. Dongarra, S. Ostrouchov, A. Petitet,\nK. Stanley, D. Walker, and R. C. Whaley. “ScaLAPACK: a portable linear algebra library for distributed memory computers—design issues and\nperformance”. In: Computer Physics Communications 97.1-2 (1996), pp. 1–\n15.\n[CDO+95]\nJ. Choi, J. Dongarra, S. Ostrouchov, A. Petitet, D. W. Walker, and R. C.\nWhaley. “A proposal for a set of parallel basic linear algebra subprograms”.\nIn: Proceedings of the Second International Workshop on Applied Parallel\nComputing, Computations in Physics, Chemistry and Engineering Science.\nPARA ’95. Berlin, Heidelberg: Springer-Verlag, 1995, pp. 107–114. isbn:\n3540609024.\n[CDV20]\nD. Calandriello, M. Derezi´nski, and M. Valko. “Sampling from a k-DPP\nwithout looking at all items”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 6889–6899.\n[CET+22]\nY. Chen, E. N. Epperly, J. A. Tropp, and R. J. Webber. Randomly pivoted Cholesky: Practical approximation of a kernel matrix with few entry\nevaluations. 2022. arXiv: 2207.06503.\n[CFG95]\nM. T. Chu, R. E. Funderlic, and G. H. Golub. “A rank–one reduction\nformula and its applications to matrix factorizations”. In: SIAM Review\n37.4 (Dec. 1995), pp. 512–530.\n[CFS21]\nC. Cartis, J. Fiala, and Z. Shao. Hashing embeddings of optimal dimension,\nwith applications to linear least squares. 2021. arXiv: 2105.11815.\n[CH22]\nT. Chen and E. Hallman. Krylov-aware stochastic trace estimation. 2022.\narXiv: 2205.01736.\n[CH88]\nS. Chatterjee and A. Hadi. Sensitivity Analysis in Linear Regression. New\nYork: John Wiley & Sons, 1988.\n[CKN22]\nA. Cortinovis, D. Kressner, and Y. Nakatsukasa. Speeding up Krylov subspace methods for computing f(A)b via randomization. 2212.12758. 2022.\nPage 178\narXiv Version 2\n\nBibliography\n[CLA+20]\nA. Chowdhury, P. London, H. Avron, and P. Drineas. “Faster randomized\ninfeasible interior point methods for tall/wide linear programs”. In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle,\nM. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020, pp. 8704–8715.\n[CLN+20]\nK. Chen, Q. Li, K. Newton, and S. J. Wright. “Structured random sketching for PDE inverse problems”. In: SIAM Journal on Matrix Analysis and\nApplications 41.4 (2020), pp. 1742–1770.\n[CLV17]\nD. Calandriello, A. Lazaric, and M. Valko. “Distributed adaptive sampling\nfor kernel matrix approximation”. In: Artiﬁcial Intelligence and Statistics.\nPMLR. 2017, pp. 1421–1429.\n[C¸M09]\nA. C¸ivril and M. Magdon-Ismail. “On selecting a maximum volume submatrix of a matrix and related problems”. In: Theoretical Computer Science\n410.47-49 (Nov. 2009), pp. 4801–4811.\n[CMD+15]\nA. Cichocki, D. Mandic, L. De Lathauwer, G. Zhou, Q. Zhao, C. Caiafa,\nand H. A. Phan. “Tensor decompositions for signal processing applications:\nfrom two-way to multiway component analysis”. In: IEEE Signal Processing\nMagazine 32.2 (2015), pp. 145–163.\n[CMX+22]\nN. Cheng, O. A. Malik, Y. Xu, S. Becker, A. Doostan, and A. Narayan.\nQuadrature Sampling of Parametric Models with Bi-ﬁdelity Boosting. 2022.\narXiv: 2209.05705.\n[Coh16]\nM. B. Cohen. “Nearly tight oblivious subspace embeddings by trace inequalities”. In: Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms (SODA). Society for Industrial and Applied\nMathematics, Dec. 2016.\n[CP15]\nM. B. Cohen and R. Peng. “Lp row sampling by lewis weights”. In: Proceedings of the forty-seventh annual ACM Symposium on Theory of Computing\n(STOC). 2015, pp. 183–192.\n[CPL+16]\nD. Cheng, R. Peng, Y. Liu, and I. Perros. “SPALS: fast alternating least\nsquares via implicit leverage scores sampling”. In: Advances in Neural Information Processing Systems. 2016, pp. 721–729.\n[CTU22]\nT. Chen, T. Trogdon, and S. Ubaru. Randomized matrix-free quadrature\nfor spectrum and spectral sum approximation. 2022. arXiv: 2204.01941.\n[CW09]\nK. L. Clarkson and D. P. Woodruﬀ. “Numerical linear algebra in the\nstreaming model”. In: Proceedings of the Forty-First Annual ACM Symposium on Theory of Computing (STOC). STOC ’09. Bethesda, MD, USA:\nAssociation for Computing Machinery, 2009, pp. 205–214. isbn: 9781605585062.\n[CW13]\nK. L. Clarkson and D. P. Woodruﬀ. “Low rank approximation and regression in input sparsity time”. In: Proceedings of the Forty-Fifth Annual\nACM Symposium on Theory of Computing (STOC). Palo Alto, California, USA: Association for Computing Machinery, 2013, pp. 81–90. isbn:\n9781450320290.\n[CW17]\nK. L. Clarkson and D. P. Woodruﬀ. “Low-rank approximation and regression in input sparsity time”. In: J. ACM 63.6 (Jan. 2017). This is the\njournal version of a 2013 STOC article by the same name.\n[DB08]\nZ. Drmaˇc and Z. Bujanovi´c. “On the failure of rank-revealing qr factorization software – a case study”. In: ACM Trans. Math. Softw. 35.2 (July\n2008).\narXiv Version 2\nPage 179\n\nBibliography\n[DCM+19]\nM. Derezi´nski, K. L. Clarkson, M. W. Mahoney, and M. K. Warmuth. “Minimax experimental design: bridging the gap between statistical and worstcase approaches to least squares regression”. In: Conference on Learning\nTheory (COLT). PMLR. 2019, pp. 1050–1069.\n[DCV19]\nM. Derezi´nski, D. Calandriello, and M. Valko. “Exact sampling of determinantal point processes with sublinear time preprocessing”. In: Advances in\nNeural Information Processing Systems 32 (2019).\n[DDD+87]\nJ. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Hammarling, and\nD. Sorensen. Prospectus for the development of a linear algebra library for\nhigh-performance computers. https://netlib.org/lapack/lawns/. LAPACK\nWorking Note 01. Sept. 1987.\n[DDG+22]\nJ. Demmel, J. Dongarra, M. Gates, G. Henry, J. Langou, X. Li, P. Luszczek,\nW. Pereira, J. Riedy, and C. Rubio-Gonz´alez. Proposed Consistent Exception Handling for the BLAS and LAPACK. 2022. arXiv: 2207.09281.\n[DDH+09]\nA. Dasgupta, P. Drineas, B. Harb, R. Kumar, and M. W. Mahoney. “Sampling algorithms and coresets for ℓp regression”. In: SIAM Journal on Computing 38 (2009), pp. 2060–2078.\n[DDH+88]\nJ. J. Dongarra, J. Du Croz, S. Hammarling, and R. J. Hanson. “An extended set of fortran basic linear algebra subprograms”. In: ACM Trans.\nMath. Softw. 14.1 (Mar. 1988), pp. 1–17.\n[DDH+90]\nJ. J. Dongarra, J. Du Croz, S. Hammarling, and I. S. Duﬀ. “A set of level\n3 basic linear algebra subprograms”. In: ACM Trans. Math. Softw. 16.1\n(Mar. 1990), pp. 1–17.\n[DDH07]\nJ. Demmel, I. Dumitriu, and O. Holtz. “Fast linear algebra is stable”. In:\nNumerische Mathematik 108.1 (Oct. 2007), pp. 59–91.\n[DDL+20]\nJ. Demmel, J. Dongarra, J. Langou, J. Langou, P. Luszczek, and M. W.\nMahoney. Prospectus for the Next LAPACK and ScaLAPACK Libraries:\nBasic ALgebra LIbraries for Sustainable Technology with Interdisciplinary\nCollaboration (BALLISTIC). http://www.netlib.org/lapack/lawnspdf/\nlawn297.pdf. July 2020.\n[DDM01]\nJ. Demmel, B. Diament, and G. Malajovich. “On the complexity of computing error bounds”. In: Foundations of Computational Mathematics 1.1\n(Jan. 2001), pp. 101–125.\n[Dem92]\nJ. Demmel. “The componentwise distance to the nearest singular matrix”.\nIn: SIAM Journal on Matrix Analysis and Applications 13.1 (Jan. 1992),\npp. 10–19.\n[Der19]\nM. Derezi´nski. “Fast determinantal point processes via distortion-free intermediate sampling”. In: Conference on Learning Theory (COLT). PMLR.\n2019, pp. 1029–1049.\n[Der22a]\nM. Derezi´nski. Algorithmic Gaussianization through Sketching: Converting\nData into Sub-gaussian Random Designs. 2022. eprint: 2206.10291.\n[Der22b]\nM. Derezi´nski. Stochastic Variance-Reduced Newton: Accelerating FiniteSum Minimization with Large Batches. 2022. arXiv: 2206.02702.\n[DG03]\nS. Dasgupta and A. Gupta. “An elementary proof of a theorem of Johnson\nand Lindenstrauss”. In: Random Structures and Algorithms 22.1 (2003),\npp. 60–65.\n[DG17]\nJ. A. Duersch and M. Gu. “Randomized QR with column pivoting”. In:\nSIAM Journal on Scientiﬁc Computing 39.4 (Jan. 2017), pp. C263–C291.\nPage 180\narXiv Version 2\n\nBibliography\n[DGG+15]\nJ. Demmel, L. Grigori, M. Gu, and H. Xiang. “Communication-avoiding\nrank-revealing QR decomposition”. In: SIAM Journal on Matrix Analysis\nand its Applications 36.1 (2015), pp. 55–89.\n[DGH+19]\nJ. Dongarra, M. Gates, A. Haidar, J. Kurzak, P. Luszczek, P. Wu, I. Yamazaki, A. Yarkhan, M. Abalenkovs, N. Bagherpour, S. Hammarling, J.\nˇS´ıstek, D. Stevens, M. Zounon, and S. D. Relton. “PLASMA”. In: ACM\nTransactions on Mathematical Software 45.2 (June 2019), pp. 1–35.\n[DGR+74]\nR. Dennard, F. Gaensslen, V. Rideout, E. Bassous, and A. LeBlanc. “Design\nof ion-implanted mosfet’s with very small physical dimensions”. In: SolidState Circuits, IEEE Journal of 9.5 (Oct. 1974), pp. 256–268.\n[DGR19]\nJ. Demmel, L. Grigori, and A. Rusciano. An improved analysis and uniﬁed\nperspective on deterministic and randomized low rank matrix approximations. 2019. arXiv: 1910.00223.\n[DHK+06]\nJ. Demmel, Y. Hida, W. Kahan, X. S. Li, S. Mukherjee, and E. J. Riedy.\n“Error bounds from extra-precise iterative reﬁnement”. In: ACM Trans.\nMath. Softw. 32.2 (June 2006), pp. 325–351.\n[Dix83]\nJ. D. Dixon. “Estimating extremal eigenvalues and condition numbers of\nmatrices”. In: SIAM Journal on Numerical Analysis 20.4 (1983), pp. 812–\n814.\n[DJS+19]\nH. Diao, R. Jayaram, Z. Song, W. Sun, and D. P. Woodruﬀ. Optimal Sketching for Kronecker Product Regression and Low Rank Approximation. 2019.\narXiv: 1909.13384.\n[DKM06a]\nP. Drineas, R. Kannan, and M. W. Mahoney. “Fast Monte Carlo algorithms\nfor matrices I: approximating matrix multiplication”. In: SIAM Journal on\nComputing 36 (2006), pp. 132–157.\n[DKM06b]\nP. Drineas, R. Kannan, and M. W. Mahoney. “Fast Monte Carlo algorithms\nfor matrices II: computing a low-rank approximation to a matrix”. In: SIAM\nJournal on Computing 36 (2006), pp. 158–183.\n[DKM20]\nM. Derezi´nski, R. Khanna, and M. W. Mahoney. “Improved guarantees\nand a multiple-descent curve for Column Subset Selection and the Nystr¨om\nmethod”. In: Annual Advances in Neural Information Processing Systems.\n2020, pp. 4953–4964.\n[DKS10]\nA. Dasgupta, R. Kumar, and T. Sarlos. “A sparse Johnson-Lindenstrauss\ntransform”. In: Proceedings of the Forty-Second ACM Symposium on Theory of Computing (STOC). STOC ’10. Cambridge, Massachusetts, USA:\nAssociation for Computing Machinery, 2010, pp. 341–350. isbn: 9781450300506.\n[DLD+21]\nM. Derezi´nski, Z. Liao, E. Dobriban, and M. Mahoney. “Sparse sketches\nwith small inversion bias”. In: Conference on Learning Theory (COLT).\nPMLR. 2021, pp. 1467–1510.\n[DLL+20]\nM. Derezi´nski, F. T. Liang, Z. Liao, and M. W. Mahoney. “Precise expressions for random projections: low-rank approximation and randomized Newton”. In: Advances in Neural Information Processing Systems 33\n(2020), pp. 18272–18283.\n[DLP+21]\nM. Derezi´nski, J. Lacotte, M. Pilanci, and M. W. Mahoney. “Newton-LESS:\nsparsiﬁcation without trade-oﬀs for the sketched Newton update”. In: Advances in Neural Information Processing Systems 34 (2021).\n[DM05]\nP. Drineas and M. W. Mahoney. “On the Nystr¨om method for approximating a Gram matrix for improved kernel-based learning”. In: Journal of\nMachine Learning Research 6 (2005), pp. 2153–2175.\narXiv Version 2\nPage 181\n\nBibliography\n[DM10]\nP. Drineas and M. Mahoney. Eﬀective Resistances, Statistical Leverage, and\nApplications to Linear Equation Solving. 2010. arXiv: 1005.3097.\n[DM16]\nP. Drineas and M. W. Mahoney. “RandNLA: randomized numerical linear\nalgebra”. In: Communications of the ACM 59 (2016), pp. 80–90.\n[DM18]\nP. Drineas and M. W. Mahoney. “Lectures on randomized numerical linear\nalgebra”. In: The Mathematics of Data. Ed. by M. W. Mahoney, J. C.\nDuchi, and A. C. Gilbert. IAS/Park City Mathematics Series. Available at\nhttps://arxiv.org/abs/1712.08880. AMS/IAS/SIAM, 2018, pp. 1–48.\n[DM21a]\nM. Derezi´nski and M. W. Mahoney. “Determinantal point processes in\nrandomized numerical linear algebra”. In: Notices of the AMS 68.1 (2021),\npp. 34–45.\n[DM21b]\nY. Dong and P.-G. Martinsson. Simpler is better: A comparative study of\nrandomized algorithms for computing the CUR decomposition. 2021. arXiv:\n2104.05877.\n[DMB+79]\nJ. J. Dongarra, C. B. Moler, J. R. Bunch, and G. W. Stewart. LINPACK\nusers guide. SIAM, 1979.\n[DMM+11]\nP. Drineas, M. W. Mahoney, S. Muthukrishnan, and T. Sarl´os. “Faster\nleast squares approximation”. In: Numerische Mathematik 117.2 (2011),\npp. 219–249.\n[DMM+12]\nP. Drineas, M. Magdon-Ismail, M. W. Mahoney, and D. P. Woodruﬀ. “Fast\napproximation of matrix coherence and statistical leverage”. In: Journal of\nMachine Learning Research 13 (2012), pp. 3475–3506.\n[DMM06]\nP. Drineas, M. W. Mahoney, and S. Muthukrishnan. “Sampling algorithms\nfor ℓ2 regression and applications”. In: Proceedings of the 17th Annual\nACM-SIAM Symposium on Discrete Algorithms (SODA). 2006, pp. 1127–\n1136.\n[DMM08]\nP. Drineas, M. W. Mahoney, and S. Muthukrishnan. “Relative-error CUR\nmatrix decompositions”. In: SIAM Journal on Matrix Analysis and Applications 30.2 (Jan. 2008). This is a longer journal version of two conference\npapers from 2006., pp. 844–881.\n[Drm22]\nZ. Drmac. A LAPACK implementation of the Dynamic Mode Decomposition I. https://netlib.org/lapack/lawns/. LAPACK Working Note 298. Oct.\n2022.\n[DRV+06]\nA. Deshpande, L. Rademacher, S. S. Vempala, and G. Wang. “Matrix approximation and projective clustering via volume sampling”. In: Theory of\nComputing 2.1 (2006), pp. 225–247.\n[DSS+18]\nH. Diao, Z. Song, W. Sun, and D. Woodruﬀ. “Sketching for Kronecker\nproduct regression and P-splines”. In: Proceedings of the 21st International\nConference on Artiﬁcial Intelligence and Statistics. 2018, pp. 1299–1308.\n[DV06]\nA. Deshpande and S. Vempala. “Adaptive sampling and fast low-rank matrix approximation”. In: Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Ed. by J. D´ıaz, K. Jansen,\nJ. D. P. Rolim, and U. Zwick. Berlin, Heidelberg: Springer Berlin Heidelberg, 2006, pp. 292–303. isbn: 978-3-540-38045-0.\n[EBK19]\nN. B. Erichson, S. L. Brunton, and J. N. Kutz. “Compressed dynamic mode\ndecomposition for background modeling”. In: Journal of Real-Time Image\nProcessing 16.5 (2019), pp. 1479–1492.\n[ED16]\nN. B. Erichson and C. Donovan. “Randomized low-rank dynamic mode\ndecomposition for motion detection”. In: Computer Vision and Image Understanding 146 (2016), pp. 40–50.\nPage 182\narXiv Version 2\n\nBibliography\n[EMB+20]\nN. B. Erichson, K. Manohar, S. L. Brunton, and J. N. Kutz. “Randomized\nCP tensor decomposition”. In: Machine Learning: Science and Technology\n1.2 (2020), p. 025012.\n[EMK+19]\nN. B. Erichson, L. Mathelin, J. N. Kutz, and S. L. Brunton. “Randomized\ndynamic mode decomposition”. In: SIAM Journal on Applied Dynamical\nSystems 18.4 (2019), pp. 1867–1891.\n[EMW+18]\nN. B. Erichson, A. Mendible, S. Wihlborn, and N. J. Kutz. “Randomized nonnegative matrix factorization”. In: Pattern Recognition Letters 104\n(2018), pp. 1–7.\n[Epp23]\nE. Epperly. Stochastic Trace Estimation. https://www.ethanepperly.\ncom/index.php/2023/01/26/stochastic-trace-estimation/. Accessed:\n2023-03-27. Jan. 2023.\n[ET94]\nB. Efron and R. J. Tibshirani. An introduction to the bootstrap. CRC press,\n1994.\n[ETW23]\nE. N. Epperly, J. A. Tropp, and R. J. Webber. XTrace: Making the most\nof every sample in stochastic trace estimation. 2023. arXiv: 2301.07825.\n[EVB+19]\nN. B. Erichson, S. Voronin, S. L. Brunton, and J. N. Kutz. “Randomized\nmatrix decompositions using R”. In: Journal of Statistical Software 89.11\n(2019).\n[EZM+20]\nN. B. Erichson, P. Zheng, K. Manohar, S. L. Brunton, J. N. Kutz, and A. Y.\nAravkin. “Sparse principal component analysis via variable projection”. In:\nSIAM Journal on Applied Mathematics 80.2 (2020), pp. 977–1002.\n[FFG22]\nM. Fahrbach, T. Fu, and M. Ghadiri. Subquadratic Kronecker Regression\nwith Applications to Tensor Decomposition. 2022. arXiv: 2209.04876.\n[FGL21]\nY. Fan, Y. Guo, and T. Lin. A Novel Randomized XR-Based Preconditioned\nCholeskyQR Algorithm. 2021. arXiv: 2111.11148.\n[FHH99]\nR. D. Fierro, P. C. Hansen, and P. S. K. Hansen. “UTV tools: Matlab templates for rank-revealing UTV decompositions”. In: Numerical Algorithms\n20.2 (1999), pp. 165–194.\n[FKV04]\nA. Frieze, R. Kannan, and S. Vempala. “Fast Monte-Carlo algorithms for\nﬁnding low-rank approximations”. In: Journal of the ACM 51.6 (2004),\npp. 1025–1041.\n[FS11]\nD. C.-L. Fong and M. Saunders. “LSMR: an iterative algorithm for sparse\nleast-squares problems”. In: 33.5 (Jan. 2011), pp. 2950–2971.\n[FTU21]\nZ. Frangella, J. A. Tropp, and M. Udell. Randomized Nystr¨om Preconditioning. 2021. arXiv: 2110.02820 [math.NA].\n[FXG18]\nY. Feng, J. Xiao, and M. Gu. “Randomized complete pivoting for solving\nsymmetric indeﬁnite linear systems”. In: SIAM Journal on Matrix Analysis\nand Applications 39.4 (Jan. 2018), pp. 1616–1641.\n[FXG19]\nY. Feng, J. Xiao, and M. Gu. “Flip-ﬂop spectrum-revealing QR factorization and its applications to singular value decomposition”. In: ETNA Electronic Transactions on Numerical Analysis 51 (2019), pp. 469–494.\n[GCG+19]\nC. Gorman, G. Ch´avez, P. Ghysels, T. Mary, F.-H. Rouet, and X. S. Li.\n“Robust and accurate stopping criteria for adaptive randomized sampling\nin matrix-free hierarchically semiseparable construction”. In: SIAM Journal\non Scientiﬁc Computing 41.5 (2019), S61–S85.\n[GDX11]\nL. Grigori, J. Demmel, and H. Xiang. “CALU: a communication optimal\nLU factorization algorithm”. In: SIAM Journal on Matrix Analysis and\nApplications 32 (2011), pp. 1317–1350.\narXiv Version 2\nPage 183\n\nBibliography\n[GE95]\nM. Gu and S. C. Eisenstat. “A divide-and-conquer algorithm for the bidiagonal SVD”. In: SIAM Journal on Matrix Analysis and Applications 16.1\n(Jan. 1995), pp. 79–92.\n[GE96]\nM. Gu and S. C. Eisenstat. “Eﬃcient algorithms for computing a strong\nrank-revealing QR factorization”. In: SIAM Journal on Scientiﬁc Computing 17.4 (July 1996), pp. 848–869.\n[Gem80]\nS. Geman. “A limit theorem for the norm of random matrices”. In: The\nAnnals of Probability 8.2 (1980), pp. 252–261.\n[GIG21]\nN. Gazagnadou, M. Ibrahim, and R. M. Gower. RidgeSketch: A Fast\nsketching based solver for large scale ridge regression. 2021. arXiv: 2105.\n05565 [math.OC].\n[Gir87]\nD. Girard. Un algorithme simple et rapid pour la validation croisee g´een´eralis´ee\nsur des probl´ems de grande taille. 1987.\n[Gir89]\nA. Girard. “A fast ‘Monte-Carlo cross-validation’ procedure for large least\nsquares problems with noisy data”. In: Numerische Mathematik 56.1 (1989),\npp. 1–23.\n[GLA+17]\nM. Gates, P. Luszczek, A. Abdelfattah, J. Kurzak, J. Dongarra, K. Arturov,\nC. Cecka, and C. Freitag. C++ API for BLAS and LAPACK. Tech. rep.\n02, ICL-UT-17-03. Revision 02-21-2018. June 2017.\n[GM10]\nG. H. Golub and G. Meurant. Matrices, Moments and Quadrature with\nApplications. Princeton University Press, 2010. isbn: 9780691143415.\n[GM16]\nA. Gittens and M. W. Mahoney. “Revisiting the Nystr¨om method for improved large-scale machine learning”. In: Journal of Machine Learning Research 17.117 (2016), pp. 1–65.\n[GM18]\nA. Gopal and P.-G. Martinsson. The PowerURV algorithm for computing\nrank-revealing full factorizations. 2018. arXiv: 1812.06007.\n[GR15]\nR. M. Gower and P. Richt´arik. “Randomized iterative methods for linear systems”. In: SIAM Journal on Matrix Analysis and Applications 36.4\n(2015), pp. 1660–1690.\n[Gri16]\nO. Grisel. SciKit-Learn PR #5299: [MRG+3] Collapsing PCA and RandomizedPCA. https://github.com/scikit-learn/scikit-learn/pull/\n5299. Released in SciPy 0.18.0. Website accessed: 2023-04-03. 2016.\n[GS12]\nV. Guruswami and A. K. Sinop. “Optimal column-based low-rank matrix reconstruction”. In: Proceedings of the twenty-third annual ACM-SIAM\nSymposium on Discrete Algorithms (SODA). SIAM. 2012, pp. 1207–1214.\n[GS22]\nS. G¨uttel and M. Schweitzer. Randomized sketching for Krylov approximations of large-scale matrix functions. 2022. arXiv: 2208.11447.\n[GSO17]\nA. S. Gambhir, A. Stathopoulos, and K. Orginos. “Deﬂation as a method of\nvariance reduction for estimating the trace of a matrix inverse”. In: SIAM\nJournal on Scientiﬁc Computing 39.2 (Jan. 2017), A532–A558.\n[GTZ97]\nS. Goreinov, E. Tyrtyshnikov, and N. Zamarashkin. “A theory of pseudoskeleton approximations”. In: Linear Algebra and its Applications 261.1-3\n(Aug. 1997), pp. 1–21.\n[GV13]\nG. H. Golub and C. F. Van Loan. Matrix Computations. en. 4th ed. Johns\nHopkins Studies in the Mathematical Sciences. Baltimore, MD: Johns Hopkins University Press, Feb. 2013.\n[GV61]\nG. H. Golub and R. S. Varga. “Chebyshev semi-iterative methods, successive overrelaxation iterative methods, and second order Richardson iterative\nmethods”. In: Numerische Mathematik 3.1 (Dec. 1961), pp. 157–168.\nPage 184\narXiv Version 2\n\nBibliography\n[GZT95]\nS. A. Gore˘ınov, N. L. Zamarashkin, and E. E. Tyrtyshnikov. “Pseudoskeleton approximations of matrices”. In: Dokl. Akad. Nauk 343.2 (1995),\npp. 151–152.\n[Hig02]\nN. J. Higham. Accuracy and Stability of Numerical Algorithms. Second.\nPhiladelphia, PA, USA: Society for Industrial and Applied Mathematics,\n2002, pp. xxx+680. isbn: 0-89871-521-0.\n[Hig08]\nN. J. Higham. Functions of Matrices. Society for Industrial and Applied\nMathematics, Jan. 2008.\n[Hig97]\nN. J. Higham. “Iterative reﬁnement for linear systems and LAPACK”. In:\nIMA Journal of Numerical Analysis 17.4 (1997), pp. 495–509.\n[HL69]\nR. J. Hanson and C. L. Lawson. “Extensions and applications of the householder algorithm for solving linear least squares problems”. In: Mathematics\nof Computation 23.108 (1969), pp. 787–812.\n[HMT11]\nN. Halko, P. G. Martinsson, and J. A. Tropp. “Finding structure with\nrandomness: probabilistic algorithms for constructing approximate matrix\ndecompositions”. In: SIAM Review 53.2 (Jan. 2011), pp. 217–288.\n[HS52]\nM. R. Hestenes and E. Stiefel. “Methods of conjugate gradients for solving\nlinear systems”. In: Journal of Research of the National Bureau of Standards 49.1 (1952).\n[Hut90]\nM. Hutchinson. “A stochastic estimator of the trace of the inﬂuence matrix for Laplacian smoothing splines”. In: Communications in Statistics Simulation and Computation 19.2 (Jan. 1990), pp. 433–450.\n[IEE19]\nIEEE. “IEEE Standard for Floating-Point Arithmetic”. In: IEEE Std 7542019 (Revision of IEEE 754-2008) (2019), pp. 1–84.\n[IM98]\nP. Indyk and R. Motwani. “Approximate nearest neighbors: towards removing the curse of dimensionality”. In: Proceedings of the 30th Annual\nACM Symposium on Theory of Computing (STOC). 1998, pp. 604–613.\n[INR+20]\nM. A. Iwen, D. Needell, E. Rebrova, and A. Zare. Lower Memory Oblivious (Tensor) Subspace Embeddings with Fewer Random Bits: Modewise\nMethods for Least Squares. 2020. arXiv: 1912.08294.\n[Int19]\nIntel. Notes for oneMKL Vector Statistics. Tech. rep. Intel Corporation,\n2019, p. 120.\n[Ips09]\nI. C. F. Ipsen. Numerical Matrix Analysis: Linear Systems and Least Squares.\nSociety for Industrial and Applied Mathematics (SIAM), Philadelphia, PA,\n2009, pp. xiv+128. isbn: 978-0-898716-76-4.\n[JKW20]\nR. Jin, T. G. Kolda, and R. Ward. “Faster Johnson–Lindenstrauss transforms via Kronecker products”. In: Information and Inference: A Journal\nof the IMA 10.4 (Oct. 2020), pp. 1533–1562.\n[JL84]\nW. Johnson and J. Lindenstrauss. “Extensions of Lipshitz mapping into\nHilbert space”. In: Contemporary Mathematics 26 (1984), pp. 189–206.\n[JZ13]\nR. Johnson and T. Zhang. “Accelerating stochastic gradient descent using\npredictive variance reduction”. In: Advances in Neural Information Processing Systems 26 (2013).\n[KAI+15]\nG. Kollias, H. Avron, Y. Ineichen, C. Bekas, A. Curioni, V. Sindhwani,\nand K. Clarkson. libSkylark: A Framework for High-Performance Matrix\nSketching for Statistical Computing. http://sc15.supercomputing.org/\nsites/all/themes/SC15images/tech_poster/poster_files/post213s2file3.pdf. 2015.\narXiv Version 2\nPage 185\n\nBibliography\n[KB09]\nT. G. Kolda and B. W. Bader. “Tensor decompositions and applications”.\nIn: SIAM Review 51.3 (Aug. 2009), pp. 455–500.\n[KC21]\nM. F. Kaloorazi and J. Chen. “Projection-based QLP algorithm for eﬃciently computing low-rank approximation of matrices”. In: IEEE Transactions on Signal Processing 69 (2021), pp. 2218–2232.\n[KCL21]\nM. F. Kaloorazi, J. Chen, and R. C. de Lamare. A QLP Decomposition via\nRandomization. 2021. arXiv: 2110.01011.\n[KMT09a]\nS. Kumar, M. Mohri, and A. Talwalkar. “Ensemble Nystr¨om method”. In:\nAnnual Advances in Neural Information Processing Systems. 2009.\n[KMT09b]\nS. Kumar, M. Mohri, and A. Talwalkar. “Sampling techniques for the\nNystr¨om method”. In: Proceedings of the 12th Tenth International Workshop on Artiﬁcial Intelligence and Statistics. 2009, pp. 304–311.\n[KN12]\nD. M. Kane and J. Nelson. “Sparser Johnson-Lindenstrauss Transforms”.\nIn: Proceedings of the 2012 Annual ACM-SIAM Symposium on Discrete\nAlgorithms (SODA). 2012, pp. 1195–1206.\n[KN14]\nD. M. Kane and J. Nelson. “Sparser Johnson-Lindenstrauss Transforms”.\nIn: J. ACM 61.1 (Jan. 2014). Notes: journal version of a 2012 SODA paper\nby the same name; called “OSNAPs” in a related 2013 paper.\n[KRS+10]\nS. P. Kasiviswanathan, M. Rudelson, A. Smith, and J. Ullman. “The price\nof privately releasing contingency tables and the spectra of random matrices\nwith correlated rows”. In: Proceedings of the Forty-Second ACM Symposium\non Theory of Computing. 2010, pp. 775–784.\n[KT12]\nA. Kulesza and B. Taskar. “Determinantal point processes for machine\nlearning”. In: Foundations and Trends® in Machine Learning 5.2–3 (2012),\npp. 123–286.\n[KV17a]\nR. Kannan and S. Vempala. “Randomized algorithms in numerical linear\nalgebra”. In: Acta Numerica 26 (2017), pp. 95–135.\n[KV17b]\nW. Kong and G. Valiant. “Spectrum estimation from samples”. In: The\nAnnals of Statistics 45.5 (2017), pp. 2218–2247.\n[KW70]\nG. S. Kimeldorf and G. Wahba. “A correspondence between Bayesian estimation on stochastic processes and smoothing by splines”. In: The Annals\nof Mathematical Statistics 41.2 (1970), pp. 495–502.\n[KW92]\nJ. Kuczy´nski and H. Wo´zniakowski. “Estimating the largest eigenvalue by\nthe power and Lanczos algorithms with a random start”. In: SIAM Journal\non Matrix Analysis and Applications 13.4 (1992), pp. 1094–1122.\n[KWG+17]\nJ. Kurzak, P. Wu, M. Gates, I. Yamazaki, P. Luszczek, G. Ragghianti,\nand J. Dongarra. Designing SLATE: Software for Linear Algebra Targeting\nExascale. SLATE Working Notes 03, ICL-UT-17-06. Oct. 2017.\n[LEM20]\nM. E. Lopes, N. B. Erichson, and M. Mahoney. “Error estimation for\nsketched SVD via the bootstrap”. In: Proceedings of the 37thInternational\nConference on Machine Learning (ICML). Ed. by H. D. III and A. Singh.\nVol. 119. Proceedings of Machine Learning Research. PMLR, July 2020,\npp. 6382–6392.\n[LEM23]\nM. E. Lopes, N. B. Erichson, and M. W. Mahoney. “Bootstrapping the\noperator norm in high dimensions: Error estimation for covariance matrices\nand sketching”. In: Bernoulli 29.1 (2023), pp. 428–450.\n[LHK+79]\nC. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. “Basic linear\nalgebra subprograms for fortran usage”. In: ACM Trans. Math. Softw. 5.3\n(Sept. 1979), pp. 308–323.\nPage 186\narXiv Version 2\n\nBibliography\n[Li92]\nK.-h. Li. “Generation of random matrices with orthonormal columns and\nmultivariate normal variates with given sample mean and covariance”.\nIn: Journal of Statistical Computation and Simulation 43.1-2 (Oct. 1992),\npp. 11–18.\n[Lib09]\nE. Liberty. “Accelerated dense random projections”. PhD thesis. Yale University, May 2009.\n[Lin16]\nL. Lin. “Randomized estimation of spectral densities of large matrices made\naccurate”. In: Numerische Mathematik 136.1 (Aug. 2016), pp. 183–213.\n[LK20]\nB. W. Larsen and T. G. Kolda. Practical Leverage-Based Sampling for LowRank Tensor Decomposition. v3 released in 2022. 2020. arXiv: 2006.16438.\n[LKL10]\nM. Li, J. Kwok, and B.-L. Lu. “Making large-scale Nystr¨om approximation\npossible”. In: Proceedings of the 27th International Conference on Machine\nLearning (ICML). 2010, pp. 631–638.\n[LLD20]\nN. Lindquist, P. Luszczek, and J. Dongarra. “Replacing pivoting in distributed Gaussian elimination with randomized techniques”. In: 2020 IEEE/ACM\n11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale\nSystems (ScalA). 2020, pp. 35–43.\n[LLS+17]\nH. Li, G. C. Linderman, A. Szlam, K. P. Stanton, Y. Kluger, and M. Tygert.\n“Algorithm 971: an implementation of a randomized algorithm for principal\ncomponent analysis”. In: ACM Trans. Math. Softw. 43.3 (Jan. 2017).\n[LP19]\nJ. Lacotte and M. Pilanci. Faster least squares optimization. 2019.\n[LSS13]\nQ. Le, T. Sarl´os, and A. Smola. “Fastfood-computing Hilbert space expansions in loglinear time”. In: International Conference on Machine Learning.\nPMLR. 2013, pp. 244–252.\n[LWM+07]\nE. Liberty, F. Woolfe, P. G. Martinsson, V. Rokhlin, and M. Tygert. “Randomized algorithms for the low-rank approximation of matrices”. In: Proceedings of the National Academy of Sciences 104.51 (2007), pp. 20167–\n20172.\n[LWM18]\nM. E. Lopes, S. Wang, and M. Mahoney. “Error estimation for randomized\nleast-squares algorithms via the bootstrap”. In: Proceedings of the 35th\nInternational Conference on Machine Learning (ICML). Ed. by J. Dy and\nA. Krause. Vol. 80. Proceedings of Machine Learning Research. PMLR,\nJuly 2018, pp. 3217–3226.\n[Mah11]\nM. W. Mahoney. Randomized algorithms for matrices and data. Foundations and Trends in Machine Learning. Boston: NOW Publishers, 2011.\n[Mah16]\nM. W. Mahoney. Lecture Notes on Randomized Linear Algebra. 2016.\n[Mal22]\nO. A. Malik. “More eﬃcient sampling for tensor decomposition with worstcase guarantees”. In: Proceedings of the 39th International Conference on\nMachine Learning. Vol. 162. Proceedings of Machine Learning Research.\nPMLR, 2022, pp. 14887–14917.\n[Mar15]\nP. G. Martinsson. Blocked rank-revealing QR factorizations: How randomized sampling can be used to avoid single-vector pivoting. 2015. arXiv: 1505.\n08115.\n[Mar18]\nP.-G. Martinsson. “Randomized methods for matrix computations”. In:\nThe Mathematics of Data 25.4 (2018). Note: preprint arXiv:1607.01649\npublished in 2016, updated in 2019., pp. 187–239.\n[Mar22a]\nP. G. Martinsson. A remark on the precision of random number generation\nfor RandNLA. Personal communication. 2022.\narXiv Version 2\nPage 187\n\nBibliography\n[Mar22b]\nP. G. Martinsson. A remark on pivoting methods in randomized algorithms\nfor low-rank interpolative decomposition. Personal communication. 2022.\n[MB18]\nO. A. Malik and S. Becker. “Low-rank Tucker decomposition of large tensors using TensorSketch”. In: Advances in Neural Information Processing\nSystems. Vol. 31. Curran Associates, Inc., 2018.\n[MB20]\nO. A. Malik and S. Becker. “Guarantees for the Kronecker fast Johnson–\nLindenstrauss transform using a coherence and sampling argument”. In:\nLinear Algebra and its Applications 602 (Oct. 2020), pp. 120–137.\n[MB21]\nO. A. Malik and S. Becker. “A sampling-based method for tensor ring decomposition”. In: International Conference on Machine Learning. PMLR,\n2021, pp. 7400–7411.\n[MBM22]\nO. A. Malik, V. Bharadwaj, and R. Murray. Sampling-Based Decomposition\nAlgorithms for Arbitrary Tensor Networks. 2022. arXiv: 2210.03828.\n[MCD+22]\nG. Meanti, L. Carratino, E. De Vito, and L. Rosasco. “Eﬃcient hyperparameter tuning for large scale kernel ridge regression”. In: (to appear in)\nProceedings of The 25th International Conference on Artiﬁcial Intelligence\nand Statistics. 2022.\n[MCR+20]\nG. Meanti, L. Carratino, L. Rosasco, and A. Rudi. “Kernel methods through\nthe roof: handling billions of points eﬃciently”. In: Advances in Neural\nInformation Processing Systems. Ed. by H. Larochelle, M. Ranzato, R.\nHadsell, M. Balcan, and H. Lin. Vol. 33. Curran Associates, Inc., 2020,\npp. 14410–14422.\n[MD09]\nM. W. Mahoney and P. Drineas. “CUR matrix decompositions for improved\ndata analysis”. In: Proceedings of the National Academy of Sciences 106.3\n(2009), pp. 697–702.\n[MD16]\nM. W. Mahoney and P. Drineas. “Structural properties underlying highquality randomized numerical linear algebra algorithms”. In: Handbook of\nBig Data. Ed. by P. B¨uhlmann, P. Drineas, M. Kane, and M. van de Laan.\nCRC Press, 2016, pp. 137–154.\n[Mez07]\nF. Mezzadri. “How to generate random matrices from the classical compact\ngroups”. In: Notices of the AMS 54.5 (2007), pp. 592–604.\n[MG15]\nC. Melgaard and M. Gu. Gaussian Elimination with Randomized Complete\nPivoting. 2015.\n[MHG17]\nP.-G. Martinsson, G. Q. O. N. Heavner, and R. van de Geijn. “Householder\nQR factorization with randomization for column pivoting (HQRRP)”. In:\nSIAM Journal on Scientiﬁc Computing 39.2 (Jan. 2017), pp. C96–C115.\n[MM13]\nX. Meng and M. W. Mahoney. “Low-distortion subspace embeddings in\ninput-sparsity time and applications to robust linear regression”. In: Proceedings of the 45th Annual ACM Symposium on Theory of Computing.\n2013, pp. 91–100.\n[MM15]\nC. Musco and C. Musco. “Randomized block Krylov methods for stronger\nand faster approximate singular value decomposition”. In: Neural Information Processing Systems. 2015, pp. 1396–1404.\n[MM17]\nC. Musco and C. Musco. “Recursive sampling for the Nystr¨om method”.\nIn: Advances in Neural Information Processing Systems. Ed. by I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and\nR. Garnett. Vol. 30. Curran Associates, Inc., 2017.\nPage 188\narXiv Version 2\n\nBibliography\n[MMM+21]\nR. A. Meyer, C. Musco, C. Musco, and D. P. Woodruﬀ. “Hutch++: optimal stochastic trace estimation”. In: Symposium on Simplicity in Algorithms (SOSA). Society for Industrial and Applied Mathematics, Jan. 2021,\npp. 142–155.\n[MMY15]\nP. Ma, M. W. Mahoney, and B. Yu. “A statistical perspective on algorithmic leveraging”. In: Journal of Machine Learning Research 16 (2015),\npp. 861–911.\n[Moo65]\nG. E. Moore. “Cramming more components onto integrated circuits”. In:\nElectronics 38.8 (1965), pp. 114–117.\n[MQH19]\nP. G. Martinsson, G. Quintana-Ort´ı, and N. Heavner. “RandUTV: A blocked\nrandomized algorithm for computing a rank-revealing UTV factorization”.\nIn: ACM Trans. Math. Softw. 45.1 (Mar. 2019).\n[MRS+14]\nP.-G. Martinsson, V. Rokhlin, Y. Shkolinsky, and M. Tygert. ID: A software\npackage for low-rank approximation of matrices via interpolative decompositions, Version 0.4. http://www.tygert.com/id_doc.4.pdf. Available in\nSciPy. See also https://github.com/klho/PyMatrixID. Mar. 2014.\n[MS22]\nL. Ma and E. Solomonik. Cost-eﬃcient Gaussian Tensor Network Embeddings for Tensor-structured Inputs. 2022. arXiv: 2205.13163.\n[MSM14]\nX. Meng, M. A. Saunders, and M. W. Mahoney. “LSRN: a parallel iterative\nsolver for strongly over- or underdetermined systems”. In: SIAM Journal on\nScientiﬁc Computing 36.2 (Jan. 2014). Software at https://web.stanford.\nedu/group/SOL/software/lsrn/, pp. C95–C118.\n[MT00]\nG. Marsaglia and W. W. Tsang. “The ziggurat method for generating random variables”. In: Journal of Statistical Software 5.8 (2000), pp. 1–7.\n[MT20]\nP.-G. Martinsson and J. A. Tropp. “Randomized numerical linear algebra:\nFoundations and Algorithms”. In: Acta Numerica 29 (2020), pp. 403–572.\n[Mur12]\nK. P. Murphy. Machine Learning: A Probabilistic Perspective (Adaptive\nComputation and Machine Learning series). English. Hardcover. The MIT\nPress, Aug. 24, 2012, p. 1104.\n[MV16]\nP.-G. Martinsson and S. Voronin. “A randomized blocked algorithm for\neﬃciently computing rank-revealing factorizations of matrices”. In: SIAM\nJournal on Scientiﬁc Computing 38.5 (Jan. 2016), S485–S507.\n[MXC+22]\nO. A. Malik, Y. Xu, N. Cheng, S. Becker, A. Doostan, and A. Narayan.\nFast Algorithms for Monotone Lower Subsets of Kronecker Least Squares\nProblems. 2022. arXiv: 2209.05662.\n[MZX+22]\nP. Ma, X. Zhang, X. Xing, J. Ma, and M. W. Mahoney. “Asymptotic analysis of sampling estimators for randomized numerical linear algebra algorithms”. In: Journal of Machine Learning Research 23.177 (2022). Journal\nversion of a 2020 PMLR paper of the same name., pp. 1–45.\n[Nak20]\nY. Nakatsukasa. Fast and stable randomized low-rank matrix approximation. 2020. arXiv: 2009.11392.\n[NDM22]\nS. Na, M. Derezi´nski, and M. W. Mahoney. Hessian Averaging in Stochastic\nNewton Methods Achieves Superlinear Convergence. 2022. arXiv: 2204 .\n09266.\n[NDT09]\nN. H. Nguyen, T. T. Do, and T. D. Tran. “A fast and eﬃcient algorithm\nfor low-rank approximation of a matrix”. In: Proceedings of the Forty-First\nAnnual ACM Symposium on Theory of Computing (STOC). STOC ’09.\nBethesda, MD, USA: Association for Computing Machinery, 2009, pp. 215–\n224. isbn: 9781605585062.\narXiv Version 2\nPage 189\n\nBibliography\n[Ngu07]\nH. Nguyen, ed. GPU Gems 3. First. Addison-Wesley Professional, 2007.\nisbn: 9780321545428.\n[NN13]\nJ. Nelson and H. L. Nguyen. “OSNAP: faster numerical linear algebra algorithms via sparser subspace embeddings”. In: 2013 IEEE 54th Annual\nSymposium on Foundations of Computer Science. IEEE, Oct. 2013.\n[NT14]\nD. Needell and J. A. Tropp. “Paved with good intentions: Analysis of a randomized block Kaczmarz method”. In: Linear Algebra and its Applications\n441 (Jan. 2014), pp. 199–221.\n[NT21]\nY. Nakatsukasa and J. A. Tropp. Fast & Accurate Randomized Algorithms\nfor Linear Systems and Eigenvalue Problems. 2021. arXiv: 2111 . 00113\n[math.NA].\n[NTD10]\nR. Nath, S. Tomov, and J. Dongarra. “Accelerating GPU kernels for dense\nlinear algebra”. In: Proceedings of the 2009 International Meeting on High\nPerformance Computing for Computational Science, VECPAR’10. Berkeley, CA: Springer, June 2010.\n[OA17]\nD. Orban and M. Arioli. Iterative Solution of Symmetric Quasi-Deﬁnite\nLinear Systems. Society for Industrial and Applied Mathematics, Apr. 2017.\n[OP64]\nW. Oettli and W. Prager. “Compatibility of approximate solution of linear\nequations with given error bounds for coeﬃcients and right-hand sides”. In:\nNumerische Mathematik 6.1 (Dec. 1964), pp. 405–409.\n[OPA19]\nI. K. Ozaslan, M. Pilanci, and O. Arikan. “Iterative Hessian sketch with\nmomentum”. In: 2019 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). 2019, pp. 7470–7474.\n[OT17]\nS. Oymak and J. A. Tropp. “Universality laws for randomized dimension\nreduction, with applications”. In: Information and Inference: A Journal of\nthe IMA 7.3 (2017), pp. 337–446.\n[Pag13]\nR. Pagh. “Compressed matrix multiplication”. In: ACM Transactions on\nComputation Theory 5.3 (Aug. 2013), 9:1–9:17.\n[Pan00]\nC.-T. Pan. “On the existence and computation of rank-revealing lu factorizations”. In: Linear Algebra and its Applications 316.1 (2000). Special\nIssue: Conference celebrating the 60th birthday of Robert J. Plemmons,\npp. 199–222.\n[Par95]\nD. S. Parker. Random Butterﬂy Transformations with Applications in Computational Linear Algebra. Tech. rep. University of California, Los Angeles,\n1995.\n[PCK21]\nD. Persson, A. Cortinovis, and D. Kressner. Improved variants of the Hutch++\nalgorithm for trace estimation. 2021. arXiv: 2109.10659.\n[Pec21]\nJ. Peca-Medlin. “Numerical, spectral, and group properties of random butterﬂy matrices”. PhD thesis. University of California, Irvine, 2021.\n[PJM22]\nV. Patel, M. Jahangoshahi, and D. A. Maldonado. Randomized Block Adaptive Linear System Solvers. 2022. arXiv: 2204.01653.\n[PK22]\nD. Persson and D. Kressner. Randomized low-rank approximation of monotone matrix functions. 2022. arXiv: 2209.11023.\n[Pla05]\nJ. Platt. “FastMap, MetricMap, and Landmark MDS are all Nystr¨om algorithms”. In: Proceedings of the 10th International Workshop on Artiﬁcial\nIntelligence and Statistics. 2005, pp. 261–268.\n[PMG+13]\nJ. Poulson, B. Marker, R. A. van de Geijn, J. R. Hammond, and N. A.\nRomero. “Elemental”. In: 39.2 (Feb. 2013). https://github.com/elemental/\nElemental, pp. 1–24.\nPage 190\narXiv Version 2\n\nBibliography\n[Pou20]\nJ. Poulson. “High-performance sampling of generic determinantal point\nprocesses”. In: Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 378.2166 (Jan. 2020), p. 20190059.\n[PP13]\nN. Pham and R. Pagh. “Fast and scalable polynomial kernels via explicit\nfeature maps”. In: Proceedings of the 19th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining. KDD ’13. New York,\nNY, USA: ACM, 2013, pp. 239–247. isbn: 978-1-4503-2174-7.\n[PS82]\nC. C. Paige and M. A. Saunders. “LSQR: an algorithm for sparse linear\nequations and sparse least squares”. In: ACM Trans. Math. Softw. 8.1 (Mar.\n1982), pp. 43–71.\n[PW16]\nM. Pilanci and M. J. Wainwright. “Iterative Hessian Sketch: fast and accurate solution approximation for constrained least-squares”. In: J. Mach.\nLearn. Res. 17.1 (Jan. 2016), pp. 1842–1879.\n[PW17]\nM. Pilanci and M. J. Wainwright. “Newton Sketch: a near linear-time optimization algorithm with linear-quadratic convergence”. In: SIAM Journal\non Optimization 27.1 (Jan. 2017), pp. 205–245.\n[RB20]\nH. Ren and Z.-J. Bai. Single-pass randomized QLP decomposition for lowrank approximation. 2020. arXiv: 2011.06855.\n[RCC+18]\nA. Rudi, D. Calandriello, L. Carratino, and L. Rosasco. “On fast leverage\nscore sampling and optimal learning”. In: Advances in Neural Information\nProcessing Systems 31 (2018).\n[RCR15]\nA. Rudi, R. Camoriano, and L. Rosasco. Less is More: Nystr¨om Computational Regularization. 2015. arXiv: 1507.04717.\n[RDA18]\nJ. Riedy, J. Demmel, and P. Ahrens. “Reproducible BLAS: Make Addition\nAssociative Again!” In: SIAM News (Oct. 2018).\n[RM19]\nF. Roosta-Khorasani and M. W. Mahoney. “Sub-sampled Newton methods”. In: Mathematical Programming 174.1-2 (2019), pp. 293–326.\n[RR07]\nA. Rahimi and B. Recht. “Random features for large-scale kernel machines”.\nIn: Advances in Neural Information Processing Systems. Ed. by J. Platt,\nD. Koller, Y. Singer, and S. Roweis. Vol. 20. Curran Associates, Inc., 2007.\n[RR20]\nB. Rakhshan and G. Rabusseau. “Tensorized random projections”. In: Proceedings of the Twenty Third International Conference on Artiﬁcial Intelligence and Statistics. Ed. by S. Chiappa and R. Calandra. Vol. 108. Proceedings of Machine Learning Research. PMLR, Aug. 2020, pp. 3306–3316.\n[RR21]\nB. T. Rakhshan and G. Rabusseau. “Rademacher random projections with\ntensor networks”. In: NeurIPS Workshop on Quantum Tensor Networks in\nMachine Learning. 2021.\n[RST10]\nV. Rokhlin, A. Szlam, and M. Tygert. “A randomized algorithm for principal component analysis”. In: SIAM Journal on Matrix Analysis and Applications 31.3 (Jan. 2010), pp. 1100–1124.\n[RT08]\nV. Rokhlin and M. Tygert. “A fast randomized algorithm for overdetermined linear least-squares regression”. In: Proceedings of the National Academy\nof Sciences 105.36 (Sept. 2008), pp. 13212–13217.\n[Rud12]\nM. Rudelson. “Row products of random matrices”. In: Advances in Mathematics 231.6 (2012), pp. 3199–3231.\n[SAI17]\nA. K. Saibaba, A. Alexanderian, and I. C. F. Ipsen. “Randomized matrixfree trace and log-determinant estimators”. In: Numerische Mathematik\n137.2 (Apr. 2017), pp. 353–395.\narXiv Version 2\nPage 191\n\nBibliography\n[Sar06]\nT. Sarlos. “Improved approximation algorithms for large matrices via random projections”. In: Proceedings of the 47th Annual IEEE Symposium on\nFoundations of Computer Science (FOCS). FOCS ’06. USA: IEEE Computer Society, 2006, pp. 143–152. isbn: 0769527205.\n[SCS10]\nY. Saad, J. Chelikowsky, and S. Shontz. “Numerical methods for electronic\nstructure calculations of materials”. In: SIAM Review 52.1 (2010), pp. 3–\n54.\n[SDF+17]\nN. D. Sidiropoulos, L. De Lathauwer, X. Fu, K. Huang, E. E. Papalexakis,\nand C. Faloutsos. “Tensor decomposition for signal processing and machine learning”. In: IEEE Transactions on Signal Processing 65.13 (2017),\npp. 3551–3582.\n[SG21]\nA. Sobczyk and E. Gallopoulos. “Estimating leverage scores via rank revealing methods and randomization”. In: SIAM Journal on Matrix Analysis\nand Applications 42.3 (2021), pp. 1199–1228.\n[SG22]\nA. Sobczyk and E. Gallopoulos. pylspack: Parallel algorithms and data\nstructures for sketching, column subset selection, regression and leverage\nscores. 2022. arXiv: 2203.02798.\n[SGT+18]\nY. Sun, Y. Guo, J. A. Tropp, and M. Udell. “Tensor random projection for\nlow memory dimension reduction”. In: NeurIPS Workshop on Relational\nRepresentation Learning. 2018.\n[Sil85]\nJ. W. Silverstein. “The smallest eigenvalue of a large dimensional Wishart\nmatrix”. In: The Annals of Probability 13.4 (1985), pp. 1364–1368.\n[SMD+11]\nJ. K. Salmon, M. A. Moraes, R. O. Dror, and D. E. Shaw. “Parallel random numbers: as easy as 1, 2, 3”. In: SC ’11: Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage\nand Analysis. 2011, pp. 1–12.\n[SNM17]\nP. Seshadri, A. Narayan, and S. Mahadevan. “Eﬀectively subsampled quadratures for least squares polynomial approximations”. In: SIAM/ASA Journal\non Uncertainty Quantiﬁcation 5.1 (2017), pp. 1003–1023.\n[SSA+18]\nG. Shabat, Y. Shmueli, Y. Aizenbud, and A. Averbuch. “Randomized LU\ndecomposition”. In: Applied and Computational Harmonic Analysis 44.2\n(Mar. 2018). Available on arXiv in 2013., pp. 246–272.\n[ST02]\nZ. Strakoˇs and P. Tich´y. “On error estimation in the conjugate gradient\nmethod and why it works in ﬁnite precision computations”. In: Electron.\nTrans. Numer. Anal. 13 (2002), pp. 56–80.\n[ST05]\nZ. Strakoˇs and P. Tich´y. “Error estimation in preconditioned conjugate\ngradients”. In: BIT Numerical Mathematics 45.4 (Dec. 2005). Extends a\nrelated 2002 paper by the same authors., pp. 789–817.\n[ST12]\nJ. Shao and D. Tu. The jackknife and bootstrap. Springer Science & Business\nMedia, 2012.\n[Ste77]\nG. Stewart. “Research, development, and LINPACK”. In: Mathematical\nSoftware. Elsevier, 1977, pp. 1–14.\n[Ste80]\nG. W. Stewart. “The eﬃcient generation of random orthogonal matrices\nwith an application to condition estimators”. In: SIAM Journal on Numerical Analysis 17.3 (1980), pp. 403–409.\n[Ste92]\nG. Stewart. “An updating algorithm for subspace tracking”. In: IEEE\nTransactions on Signal Processing 40.6 (June 1992), pp. 1535–1541.\nPage 192\narXiv Version 2\n\nBibliography\n[Ste93]\nG. Stewart. “Updating a rank-revealing ULV decomposition”. In: SIAM\nJournal on Matrix Analysis and Applications 14.2 (Apr. 1993), pp. 494–\n499.\n[Ste99]\nG. W. Stewart. “The QLP approximation to the singular value decomposition”. In: SIAM J. Sci. Comput. 20.4 (Jan. 1999), pp. 1336–1348.\n[SV08]\nT. Strohmer and R. Vershynin. “A randomized Kaczmarz algorithm with\nexponential convergence”. In: Journal of Fourier Analysis and Applications\n15.2 (Apr. 2008), pp. 262–278.\n[SWY+21]\nZ. Song, D. Woodruﬀ, Z. Yu, and L. Zhang. “Fast sketching of polynomial\nkernels of polynomial degree”. In: Proceedings of the 38th International\nConference on Machine Learning (ICML). Vol. 139. Proceedings of Machine Learning Research. PMLR, 2021, pp. 9812–9823.\n[TDB10]\nS. Tomov, J. Dongarra, and M. Baboulin. “Towards dense linear algebra for\nhybrid GPU accelerated manycore systems”. In: Parallel Computing 36.5-6\n(June 2010), pp. 232–240.\n[TNX15]\nTao, A. Narayan, and D. Xiu. “Weighted discrete least-squares polynomial\napproximation using randomized quadratures”. In: Journal of Computational Physics 298 (2015), pp. 787–800.\n[TRL+14]\nJ. H. Tu, C. W. Rowley, D. M. Luchtenburg, S. L. Brunton, and J. N. Kutz.\n“On dynamic mode decomposition: theory and applications”. In: Journal\nof Computational Dynamics 1.2 (2014), pp. 391–421.\n[Tro11]\nJ. A. Tropp. “Improved analysis of the subsampled randomized Hadamard\ntransform”. In: Advances in Adaptive Data Analysis 03.01n02 (Apr. 2011),\npp. 115–126.\n[Tro15]\nJ. A. Tropp. “An introduction to matrix concentration inequalities”. In:\nFoundations and Trends® in Machine Learning 8.1-2 (2015), pp. 1–230.\n[Tro19]\nJ. A. Tropp. Matrix Concentration & Computational Linear Algebra. Lecture notes for a course at ´Ecole Normale Sup´erieure, Paris. July 2019.\n[Tro20]\nJ. A. Tropp. Randomized Algorithms for Matrix Computations. Lecture\nnotes (available online in April 2021). Mar. 2020.\n[Tyg22]\nM. Tygert. A suggestion for sparse sketching operators. Personal communication. 2022.\n[TYU+17a]\nJ. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher. “Fixed-rank approximation of a positive-semideﬁnite matrix from streaming data”. In: Advances\nin Neural Information Processing Systems. Ed. by I. Guyon, U. V. Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30.\nCurran Associates, Inc., 2017.\n[TYU+17b]\nJ. A. Tropp, A. Yurtsever, M. Udell, and V. Cevher. “Practical sketching algorithms for low-rank matrix approximation”. In: SIAM Journal on\nMatrix Analysis and Applications 38.4 (Jan. 2017), pp. 1454–1485.\n[UCS17]\nS. Ubaru, J. Chen, and Y. Saad. “Fast estimation of tr(f(A)) via stochastic\nLanczos quadrature”. In: SIAM Journal on Matrix Analysis and Applications 38.4 (Jan. 2017), pp. 1075–1099.\n[Ura13]\nY. Urano. “A fast randomized algorithm for linear least-squares regression\nvia sparse transforms”. MA thesis. New York University, Jan. 2013.\narXiv Version 2\nPage 193\n\nBibliography\n[VBG+18]\nJ. S. Vetter, R. Brightwell, M. Gokhale, P. McCormick, R. Ross, J. Shalf,\nK. Antypas, D. Donofrio, T. Humble, C. Schuman, B. Van Essen, S. Yoo,\nA. Aiken, D. Bernholdt, S. Byna, K. Cameron, F. Cappello, B. Chapman,\nA. Chien, M. Hall, R. Hartman-Baker, Z. Lan, M. Lang, J. Leidel, S. Li,\nR. Lucas, J. Mellor-Crummey, P. Peltz Jr., T. Peterka, M. Strout, and\nJ. Wilke. Extreme Heterogeneity 2018 – Productive Computational Science\nin the Era of Extreme Heterogeneity: Report for DOE ASCR Workshop\non Extreme Heterogeneity. English. Tech. rep. https://www.osti.gov/\nservlets/purl/1473756. US DOE Oﬃce of Science (SC), Washington,\nD.C. (United States), Dec. 2018.\n[VEK+19]\nM. Velegar, N. B. Erichson, C. A. Keller, and J. N. Kutz. “Scalable diagnostics for global atmospheric chemistry using ristretto library (version\n1.0)”. In: Geoscientiﬁc Model Development 12.4 (2019), pp. 1525–1539.\n[Ver18]\nR. Vershynin. High-dimensional probability: An introduction with applications in data science. Cambridge, United Kingdom New York, NY: Cambridge University Press, 2018. isbn: 9781108231596.\n[VM15]\nS. Voronin and P.-G. Martinsson. RSVDPACK: An implementation of randomized algorithms for computing the singular value, interpolative, and\nCUR decompositions of matrices on multi-core and GPU architectures.\n2015.\n[VM16]\nS. Voronin and P.-G. Martinsson. “Eﬃcient algorithms for CUR and interpolative matrix decompositions”. In: Advances in Computational Mathematics 43.3 (Nov. 2016), pp. 495–516.\n[Wan15]\nS. Wang. A Practical Guide to Randomized Matrix Computations with\nMATLAB Implementations. 2015. arXiv: 1505.07570.\n[WGM18]\nS. Wang, A. Gittens, and M. W. Mahoney. “Sketched ridge regression:\noptimization perspective, statistical perspective, and model averaging”. In:\nJournal of Machine Learning Research 18 (2018), pp. 1–50.\n[WLR+08]\nF. Woolfe, E. Liberty, V. Rokhlin, and M. Tygert. “A fast randomized algorithm for the approximation of matrices”. In: Applied and Computational\nHarmonic Analysis 25.3 (2008), pp. 335–366.\n[Woo14]\nD. P. Woodruﬀ. “Sketching as a tool for numerical linear algebra”. In:\nFound. Trends Theor. Comput. Sci. 10.1–2 (Oct. 2014), pp. 1–157.\n[WS00]\nC. Williams and M. Seeger. “Using the nystr¨om method to speed up kernel\nmachines”. In: Advances in Neural Information Processing Systems. Ed. by\nT. Leen, T. Dietterich, and V. Tresp. Vol. 13. MIT Press, 2000.\n[WX20]\nN. Wu and H. Xiang. “Randomized QLP decomposition”. In: Linear Algebra and its Applications 599 (Aug. 2020), pp. 18–35.\n[WZ20]\nD. Woodruﬀand A. Zandieh. “Near input sparsity time kernel embeddings\nvia adaptive sampling”. In: Proceedings of the 37th International Conference on Machine Learning. Vol. 119. Proceedings of Machine Learning Research. PMLR, 2020, pp. 10324–10333.\n[WZ22]\nD. Woodruﬀand A. Zandieh. “Leverage score sampling for tensor product\nmatrices in input sparsity time”. In: Proceedings of the 39th International\nConference on Machine Learning. Vol. 162. Proceedings of Machine Learning Research. PMLR, 2022, pp. 23933–23964.\n[XG16]\nJ. Xiao and M. Gu. “Spectrum-revealing Cholesky factorization for kernel\nmethods”. In: 2016 IEEE 16th International Conference on Data Mining\n(ICDM). IEEE, Dec. 2016.\nPage 194\narXiv Version 2\n\nBibliography\n[XGL17]\nJ. Xiao, M. Gu, and J. Langou. “Fast parallel randomized QR with column pivoting algorithms for reliable low-rank matrix approximations”. In:\n2017 IEEE 24th International Conference on High Performance Computing\n(HiPC). 2017, pp. 233–242.\n[XRM17]\nP. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Newton-Type Methods\nfor Non-Convex Optimization Under Inexact Hessian Information. 2017.\narXiv: 1708.07164.\n[YCR+18]\nJ. Yang, Y.-L. Chow, C. Re, and M. W. Mahoney. “Weighted SGD for\nLp regression with randomized preconditioning”. In: Journal of Machine\nLearning Research 18.211 (2018), pp. 1–43.\n[YGL+17]\nW. Yu, Y. Gu, J. Li, S. Liu, and Y. Li. “Single-pass PCA of large highdimensional data”. In: Proceedings of the Twenty-Sixth International Joint\nConference on Artiﬁcial Intelligence, IJCAI-17. 2017, pp. 3350–3356.\n[YGL18]\nW. Yu, Y. Gu, and Y. Li. “Eﬃcient randomized algorithms for the ﬁxedprecision low-rank matrix approximation”. In: SIAM Journal on Matrix\nAnalysis and Applications 39.3 (Jan. 2018), pp. 1339–1359.\n[YMM16]\nJ. Yang, X. Meng, and M. W. Mahoney. “Implementing randomized matrix\nalgorithms in parallel and distributed environments”. In: Proceedings of the\nIEEE 104.1 (2016), pp. 58–92.\n[YPW17]\nY. Yang, M. Pilanci, and M. J. Wainwright. “Randomized sketches for kernels: fast and optimal nonparametric regression”. In: The Annals of Statistics 45.3 (2017), pp. 991–1023.\n[YXR+18]\nZ. Yao, P. Xu, F. Roosta-Khorasani, and M. W. Mahoney. Inexact NonConvex Newton-Type Methods. 2018. arXiv: 1802.06925.\n[ZM20]\nB. Zhang and M. Mascagni. Pass-Eﬃcient Randomized LU Algorithms for\nComputing Low-Rank Matrix Approximation. 2020. arXiv: 2002.07138.\narXiv Version 2\nPage 195",
  "stats": {
    "raw_length": 526903,
    "normalized_length": 525341,
    "raw_lines": 11310,
    "normalized_lines": 10462
  }
}