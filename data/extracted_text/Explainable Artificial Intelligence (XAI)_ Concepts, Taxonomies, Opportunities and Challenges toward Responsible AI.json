{
  "pdf_path": "C:\\Users\\hp\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\Explainable Artificial Intelligence (XAI)_ Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.pdf",
  "pdf_name": "Explainable Artificial Intelligence (XAI)_ Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI.pdf",
  "file_hash": "e21978b17d8328db143b15e08bd99ac6b0757c0f07cd93671408970a87c78b76",
  "metadata": {
    "title": "",
    "author": "",
    "subject": "",
    "keywords": "",
    "creator": "TeX",
    "producer": "MiKTeX pdfTeX-1.40.20",
    "creation_date": "D:20191226090326+01'00'",
    "modification_date": "D:20191226090326+01'00'"
  },
  "raw_text": "Explainable Artiﬁcial Intelligence (XAI): Concepts, Taxonomies,\nOpportunities and Challenges toward Responsible AI\nAlejandro Barredo Arrietaa, Natalia D´ıaz-Rodr´ıguezb, Javier Del Sera,c,d, Adrien Bennetotb,e,f,\nSiham Tabikg, Alberto Barbadoh, Salvador Garciag, Sergio Gil-Lopeza, Daniel Molinag,\nRichard Benjaminsh, Raja Chatilaf, and Francisco Herrerag\naTECNALIA, 48160 Derio, Spain\nbENSTA, Institute Polytechnique Paris and INRIA Flowers Team, Palaiseau, France\ncUniversity of the Basque Country (UPV/EHU), 48013 Bilbao, Spain\ndBasque Center for Applied Mathematics (BCAM), 48009 Bilbao, Bizkaia, Spain\neSegula Technologies, Parc d’activit´e de Pissaloup, Trappes, France\nfInstitut des Syst`emes Intelligents et de Robotique, Sorbonne Universit`e, France\ngDaSCI Andalusian Institute of Data Science and Computational Intelligence, University of Granada, 18071 Granada, Spain\nhTelefonica, 28050 Madrid, Spain\nAbstract\nIn the last few years, Artiﬁcial Intelligence (AI) has achieved a notable momentum that, if harnessed\nappropriately, may deliver the best of expectations over many application sectors across the ﬁeld. For this\nto occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability,\nan inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural\nNetworks) that were not present in the last hype of AI (namely, expert systems and rule based models).\nParadigms underlying this problem fall within the so-called eXplainable AI (XAI) ﬁeld, which is widely\nacknowledged as a crucial feature for the practical deployment of AI models. The overview presented in\nthis article examines the existing literature and contributions already done in the ﬁeld of XAI, including a\nprospect toward what is yet to be reached. For this purpose we summarize previous efforts made to deﬁne\nexplainability in Machine Learning, establishing a novel deﬁnition of explainable Machine Learning that\ncovers such prior conceptual propositions with a major focus on the audience for which the explainability\nis sought. Departing from this deﬁnition, we propose and discuss about a taxonomy of recent contributions\nrelated to the explainability of different Machine Learning models, including those aimed at explaining\nDeep Learning methods for which a second dedicated taxonomy is built and examined in detail. This\ncritical literature analysis serves as the motivating background for a series of challenges faced by XAI,\nsuch as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept\nof Responsible Artiﬁcial Intelligence, namely, a methodology for the large-scale implementation of AI\nmethods in real organizations with fairness, model explainability and accountability at its core. Our\nultimate goal is to provide newcomers to the ﬁeld of XAI with a thorough taxonomy that can serve\nas reference material in order to stimulate future research advances, but also to encourage experts and\nprofessionals from other disciplines to embrace the beneﬁts of AI in their activity sectors, without any\nprior bias for its lack of interpretability.\nKeywords: Explainable Artiﬁcial Intelligence, Machine Learning, Deep Learning, Data Fusion,\nInterpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible\nArtiﬁcial Intelligence.\n∗Corresponding author. TECNALIA. P. Tecnologico, Ed. 700. 48170 Derio (Bizkaia), Spain. E-mail: javier.delser@tecnalia.com\nPreprint submitted to Information Fusion\nDecember 26, 2019\n\n\n1. Introduction\nArtiﬁcial Intelligence (AI) lies at the core of many activity sectors that have embraced new information\ntechnologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on the\nparamount importance featured nowadays by intelligent machines endowed with learning, reasoning and\nadaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented\nlevels of performance when learning to solve increasingly complex computational tasks, making them\npivotal for the future development of the human society [2]. The sophistication of AI-powered systems\nhas lately increased to such an extent that almost no human intervention is required for their design\nand deployment. When decisions derived from such systems ultimately affect humans’ lives (as in e.g.\nmedicine, law or defense), there is an emerging need for understanding how such decisions are furnished\nby AI methods [3].\nWhile the very ﬁrst AI systems were easily interpretable, the last years have witnessed the rise of\nopaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning\n(DL) models such as DNNs stems from a combination of efﬁcient learning algorithms and their huge\nparametric space. The latter space comprises hundreds of layers and millions of parameters, which makes\nDNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency,\ni.e., the search for a direct understanding of the mechanism by which a model works [5].\nAs black-box Machine Learning (ML) models are increasingly being employed to make important\npredictions in critical contexts, the demand for transparency is increasing from the various stakeholders in\nAI [6]. The danger is on creating and using decisions that are not justiﬁable, legitimate, or that simply do\nnot allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of a\nmodel are crucial, e.g., in precision medicine, where experts require far more information from the model\nthan a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomous\nvehicles in transportation, security, and ﬁnance, among others.\nIn general, humans are reticent to adopt techniques that are not directly interpretable, tractable and\ntrustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusing\nsolely on performance, the systems will be increasingly opaque. This is true in the sense that there is a\ntrade-off between the performance of a model and its transparency [10]. However, an improvement in the\nunderstanding of a system can lead to the correction of its deﬁciencies. When developing a ML model,\nthe consideration of interpretability as an additional design driver can improve its implementability for 3\nreasons:\n• Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct\nfrom bias in the training dataset.\n• Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations\nthat could change the prediction.\n• Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing\nthat an underlying truthful causality exists in the model reasoning.\nAll these means that the interpretation of the system should, in order to be considered practical,\nprovide either an understanding of the model mechanisms and predictions, a visualization of the model’s\ndiscrimination rules, or hints on what could perturb the model [11].\nIn order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI\n(XAI) [7] proposes creating a suite of ML techniques that 1) produce more explainable models while\nmaintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to\nunderstand, appropriately trust, and effectively manage the emerging generation of artiﬁcially intelligent\npartners. XAI draws as well insights from the Social Sciences [12] and considers the psychology of\nexplanation.\n2\n\n\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n(December 10th)\n0\n25\n50\n75\n100\n125\n150\n175\n200\n# of contributed works in the literature\nInterpretable Artiﬁcial Intelligence\nXAI\nExplainable Artiﬁcial Intelligence\nFigure 1: Evolution of the number of total publications whose title, abstract and/or keywords refer to the ﬁeld of XAI during\nthe last years. Data retrieved from Scopus R\n⃝(December 10th, 2019) by using the search terms indicated in the legend when\nquerying this database. It is interesting to note the latent need for interpretable AI models over time (which conforms to intuition, as\ninterpretability is a requirement in many scenarios), yet it has not been until 2017 when the interest in techniques to explain AI\nmodels has permeated throughout the research community.\nFigure 1 displays the rising trend of contributions on XAI and related concepts. This literature\noutbreak shares its rationale with the research agendas of national governments and agencies. Although\nsome recent surveys [8, 13, 10, 14, 15, 16, 17] summarize the upsurge of activity in XAI across sectors\nand disciplines, this overview aims to cover the creation of a complete uniﬁed framework of categories\nand concepts that allow for scrutiny and understanding of the ﬁeld of XAI methods. Furthermore, we pose\nintriguing thoughts around the explainability of AI models in data fusion contexts with regards to data\nprivacy and model conﬁdentiality. This, along with other research opportunities and challenges identiﬁed\nthroughout our study, serve as the pull factor toward Responsible Artiﬁcial Intelligence, term by which\nwe refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we\nwill later show in detail, model explainability is among the most crucial aspects to be ensured within this\nmethodological framework. All in all, the novel contributions of this overview can be summarized as\nfollows:\n1. Grounded on a ﬁrst elaboration of concepts and terms used in XAI-related research, we propose a\nnovel deﬁnition of explainability that places audience (Figure 2) as a key aspect to be considered when\nexplaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques,\nfrom trustworthiness to privacy awareness, which round up the claimed importance of purpose and\ntargeted audience in model explainability.\n2. We deﬁne and examine the different levels of transparency that a ML model can feature by itself, as\nwell as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that\nare not transparent by design.\n3. We thoroughly analyze the literature on XAI and related concepts published to date, covering ap-\nproximately 400 contributions arranged into two different taxonomies. The ﬁrst taxonomy addresses\nthe explainability of ML models using the previously made distinction between transparency and\npost-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e.,\n3\n\n\nshallow) learning models. The second taxonomy deals with XAI methods suited for the explanation of\nDeep Learning models, using classiﬁcation criteria closely linked to this family of ML methods (e.g.\nlayerwise explanations, representation vectors, attention).\n4. We enumerate a series of challenges of XAI that still remain insufﬁciently addressed to date. Speciﬁ-\ncally, we identify research needs around the concepts and metrics to evaluate the explainability of ML\nmodels, and outline research directions toward making Deep Learning models more understandable.\nWe further augment the scope of our prospects toward the implications of XAI techniques in regards\nto conﬁdentiality, robustness in adversarial settings, data diversity, and other areas intersecting with\nexplainability.\n5. After the previous prospective discussion, we arrive at the concept of Responsible Artiﬁcial Intelligence,\na manifold concept that imposes the systematic adoption of several AI principles for AI models to\nbe of practical use. In addition to explainability, the guidelines behind Responsible AI establish that\nfairness, accountability and privacy should also be considered when implementing AI models in real\nenvironments.\n6. Since Responsible AI blends together model explainability and privacy/security by design, we call\nfor a profound reﬂection around the beneﬁts and risks of XAI techniques in scenarios dealing with\nsensitive information and/or conﬁdential ML models. As we will later show, the regulatory push\ntoward data privacy, quality, integrity and governance demands more efforts to assess the role of XAI\nin this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy and\nsecurity under different data fusion paradigms.\nThe remainder of this overview is structured as follows: ﬁrst, Section 2 and subsections therein open a\ndiscussion on the terminology and concepts revolving around explainability and interpretability in AI,\nending up with the aforementioned novel deﬁnition of interpretability (Subsections 2.1 and 2.2), and a\ngeneral criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceed\nby reviewing recent ﬁndings on XAI for ML models (on transparent models and post-hoc techniques\nrespectively) that comprise the main division in the aforementioned taxonomy. We also include a review\non hybrid approaches among the two, to attain XAI. Beneﬁts and caveats of the synergies among the\nfamilies of methods are discussed in Section 5, where we present a prospect of general challenges and\nsome consequences to be cautious about. Finally, Section 6 elaborates on the concept of Responsible\nArtiﬁcial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the community\naround this vibrant research area, which has the potential to impact society, in particular those sectors that\nhave progressively embraced ML as a core technology of their activity.\n2. Explainability: What, Why, What For and How?\nBefore proceeding with our literature study, it is convenient to ﬁrst establish a common point of\nunderstanding on what the term explainability stands for in the context of AI and, more speciﬁcally,\nML. This is indeed the purpose of this section, namely, to pause at the numerous deﬁnitions that have\nbeen done in regards to this concept (what?), to argue why explainability is an important issue in AI and\nML (why? what for?) and to introduce the general classiﬁcation of XAI approaches that will drive the\nliterature study thereafter (how?).\n2.1. Terminology Clariﬁcation\nOne of the issues that hinders the establishment of common grounds is the interchangeable misuse of\ninterpretability and explainability in the literature. There are notable differences among these concepts.\nTo begin with, interpretability refers to a passive characteristic of a model referring to the level at which\na given model makes sense for a human observer. This feature is also expressed as transparency. By\n4\n\n\ncontrast, explainability can be viewed as an active characteristic of a model, denoting any action or\nprocedure taken by a model with the intent of clarifying or detailing its internal functions.\nTo summarize the most commonly used nomenclature, in this section we clarify the distinction and\nsimilarities among terms often used in the ethical AI and XAI communities.\n• Understandability (or equivalently, intelligibility) denotes the characteristic of a model to make a\nhuman understand its function – how the model works – without any need for explaining its internal\nstructure or the algorithmic means by which the model processes data internally [18].\n• Comprehensibility: when conceived for ML models, comprehensibility refers to the ability of a\nlearning algorithm to represent its learned knowledge in a human understandable fashion [19, 20, 21].\nThis notion of model comprehensibility stems from the postulates of Michalski [22], which stated that\n“the results of computer induction should be symbolic descriptions of given entities, semantically and\nstructurally similar to those a human expert might produce observing the same entities. Components of\nthese descriptions should be comprehensible as single ‘chunks’ of information, directly interpretable in\nnatural language, and should relate quantitative and qualitative concepts in an integrated fashion”.\nGiven its difﬁcult quantiﬁcation, comprehensibility is normally tied to the evaluation of the model\ncomplexity [17].\n• Interpretability: it is deﬁned as the ability to explain or to provide the meaning in understandable\nterms to a human.\n• Explainability: explainability is associated with the notion of explanation as an interface between\nhumans and a decision maker that is, at the same time, both an accurate proxy of the decision maker\nand comprehensible to humans [17].\n• Transparency: a model is considered to be transparent if by itself it is understandable. Since a model\ncan feature different degrees of understandability, transparent models in Section 3 are divided into three\ncategories: simulatable models, decomposable models and algorithmically transparent models [5].\nIn all the above deﬁnitions, understandability emerges as the most essential concept in XAI. Both\ntransparency and interpretability are strongly tied to this concept: while transparency refers to the\ncharacteristic of a model to be, on its own, understandable for a human, understandability measures the\ndegree to which a human can understand a decision made by a model. Comprehensibility is also connected\nto understandability in that it relies on the capability of the audience to understand the knowledge contained\nin the model. All in all, understandability is a two-sided matter: model understandability and human\nunderstandability. This is the reason why the deﬁnition of XAI given in Section 2.2 refers to the concept\nof audience, as the cognitive skills and pursued goal of the users of the model have to be taken into\naccount jointly with the intelligibility and comprehensibility of the model in use. This prominent role\ntaken by understandability makes the concept of audience the cornerstone of XAI, as we next elaborate in\nfurther detail.\n2.2. What?\nAlthough it might be considered to be beyond the scope of this paper, it is worth noting the discussion\nheld around general theories of explanation in the realm of philosophy [23]. Many proposals have been\ndone in this regard, suggesting the need for a general, uniﬁed theory that approximates the structure and\nintent of an explanation. However, nobody has stood the critique when presenting such a general theory.\nFor the time being, the most agreed-upon thought blends together different approaches to explanation\ndrawn from diverse knowledge disciplines. A similar problem is found when addressing interpretability\nin AI. It appears from the literature that there is not yet a common point of understanding on what\ninterpretability or explainability are. However, many contributions claim the achievement of interpretable\nmodels and techniques that empower explainability.\n5\n\n\nTo shed some light on this lack of consensus, it might be interesting to place the reference starting\npoint at the deﬁnition of the term Explainable Artiﬁcial Intelligence (XAI) given by D. Gunning in [7]:\n“XAI will create a suite of machine learning techniques that enables human users to understand,\nappropriately trust, and effectively manage the emerging generation of artiﬁcially intelligent partners”\nThis deﬁnition brings together two concepts (understanding and trust) that need to be addressed in\nadvance. However, it misses to consider other purposes motivating the need for interpretable AI models,\nsuch as causality, transferability, informativeness, fairness and conﬁdence [5, 24, 25, 26]. We will later\ndelve into these topics, mentioning them here as a supporting example of the incompleteness of the above\ndeﬁnition.\nAs exempliﬁed by the deﬁnition above, a thorough, complete deﬁnition of explainability in AI\nstill slips from our ﬁngers. A broader reformulation of this deﬁnition (e.g. “An explainable Artiﬁcial\nIntelligence is one that produces explanations about its functioning”) would fail to fully characterize the\nterm in question, leaving aside important aspects such as its purpose. To build upon the completeness, a\ndeﬁnition of explanation is ﬁrst required.\nAs extracted from the Cambridge Dictionary of English Language, an explanation is “the details or\nreasons that someone gives to make something clear or easy to understand” [27]. In the context of an\nML model, this can be rephrased as: ”the details or reasons a model gives to make its functioning clear\nor easy to understand”. It is at this point where opinions start to diverge. Inherently stemming from the\nprevious deﬁnitions, two ambiguities can be pointed out. First, the details or the reasons used to explain,\nare completely dependent of the audience to which they are presented. Second, whether the explanation\nhas left the concept clear or easy to understand also depends completely on the audience. Therefore, the\ndeﬁnition must be rephrased to reﬂect explicitly the dependence of the explainability of the model on the\naudience. To this end, a reworked deﬁnition could read as:\nGiven a certain audience, explainability refers to the details and reasons a model gives to make its\nfunctioning clear or easy to understand.\nSince explaining, as argumenting, may involve weighting, comparing or convincing an audience with\nlogic-based formalizations of (counter) arguments [28], explainability might convey us into the realm of\ncognitive psychology and the psychology of explanations [7], since measuring whether something has\nbeen understood or put clearly is a hard task to be gauged objectively. However, measuring to which\nextent the internals of a model can be explained could be tackled objectively. Any means to reduce the\ncomplexity of the model or to simplify its outputs should be considered as an XAI approach. How big\nthis leap is in terms of complexity or simplicity will correspond to how explainable the resulting model\nis. An underlying problem that remains unsolved is that the interpretability gain provided by such XAI\napproaches may not be straightforward to quantify: for instance, a model simpliﬁcation can be evaluated\nbased on the reduction of the number of architectural elements or number of parameters of the model\nitself (as often made, for instance, for DNNs). On the contrary, the use of visualization methods or natural\nlanguage for the same purpose does not favor a clear quantiﬁcation of the improvements gained in terms\nof interpretability. The derivation of general metrics to assess the quality of XAI approaches remain as\nan open challenge that should be under the spotlight of the ﬁeld in forthcoming years. We will further\ndiscuss on this research direction in Section 5.\nExplainability is linked to post-hoc explainability since it covers the techniques used to convert a\nnon-interpretable model into a explainable one. In the remaining of this manuscript, explainability will be\nconsidered as the main design objective, since it represents a broader concept. A model can be explained,\nbut the interpretability of the model is something that comes from the design of the model itself. Bearing\nthese observations in mind, explainable AI can be deﬁned as follows:\nGiven an audience, an explainable Artiﬁcial Intelligence is one that produces details or reasons to\nmake its functioning clear or easy to understand.\n6\n\n\nThis deﬁnition is posed here as a ﬁrst contribution of the present overview, implicitly assumes that the\nease of understanding and clarity targeted by XAI techniques for the model at hand reverts on different\napplication purposes, such as a better trustworthiness of the model’s output by the audience.\n2.3. Why?\nAs stated in the introduction, explainability is one of the main barriers AI is facing nowadays in\nregards to its practical implementation. The inability to explain or to fully understand the reasons by\nwhich state-of-the-art ML algorithms perform as well as they do, is a problem that ﬁnd its roots in two\ndifferent causes, which are conceptually illustrated in Figure 2.\nWithout a doubt, the ﬁrst cause is the gap between the research community and business sectors,\nimpeding the full penetration of the newest ML models in sectors that have traditionally lagged behind\nin the digital transformation of their processes, such as banking, ﬁnances, security and health, among\nmany others. In general this issue occurs in strictly regulated sectors with some reluctance to implement\ntechniques that may put at risk their assets.\nThe second axis is that of knowledge. AI has helped research across the world with the task of\ninferring relations that were far beyond the human cognitive reach. Every ﬁeld dealing with huge amounts\nof reliable data has largely beneﬁted from the adoption of AI and ML techniques. However, we are\nentering an era in which results and performance metrics are the only interest shown up in research\nstudies. Although for certain disciplines this might be the fair case, science and society are far from being\nconcerned just by performance. The search for understanding is what opens the door for further model\nimprovement and its practical utility.\nTarget audience\nin XAI\nWho? Domain experts/users of the model (e.g. medical doctors, insurance agents)\nWhy? Trust the model itself, gain scientiﬁc knowledge\nWho? Regulatory entities/agencies\nWhy? Certify model compliance with the\nWho? Users aﬀected by model decisions\nWhy? Understand their situation, verify\nWho? Managers and executive board members\nWhy? Assess regulatory compliance, understand\nWho? Data scientists, developers, product owners...\nWhy? Ensure/improve product eﬃciency, research,\ncorporate AI applications...\nnew functionalities...\nfair decisions...\nlegislation in force, audits, ...\n?\n?\n< / >\n?\n$ $ $\n?\n?\n?\nFigure 2: Diagram showing the different purposes of explainability in ML models sought by different audience proﬁles. Two goals\noccur to prevail across them: need for model understanding, and regulatory compliance. Image partly inspired by the one presented\nin [29], used with permission from IBM.\nThe following section develops these ideas further by analyzing the goals motivating the search for\nexplainable AI models.\n2.4. What for?\nThe research activity around XAI has so far exposed different goals to draw from the achievement\nof an explainable model. Almost none of the papers reviewed completely agrees in the goals required\nto describe what an explainable model should compel. However, all these different goals might help\ndiscriminate the purpose for which a given exercise of ML explainability is performed. Unfortunately,\nscarce contributions have attempted to deﬁne such goals from a conceptual perspective [5, 13, 24, 30].\nWe now synthesize and enumerate deﬁnitions for these XAI goals, so as to settle a ﬁrst classiﬁcation\ncriteria for the full suit of papers covered in this review:\n7\n\n\nXAI Goal\nMain target audience (Fig. 2)\nReferences\nTrustworthiness\nDomain experts, users of the model\naffected by decisions\n[5, 10, 24, 32, 33, 34, 35, 36, 37]\nCausality\nDomain experts, managers and\nexecutive board members,\nregulatory entities/agencies\n[35, 38, 39, 40, 41, 42, 43]\nTransferability\nDomain experts, data scientists\n[5, 44, 21, 26, 45, 30, 32, 37, 38, 39, 46, 47, 48, 49,\n50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n78, 79, 80, 81, 82, 83, 84, 85]\nInformativeness\nAll\n[5, 44, 21, 25, 26, 45, 30, 32, 34, 35, 37, 38, 41, 46, 49,\n50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65,\n66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 86,\n87, 88, 89, 59, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,\n100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]\nConﬁdence\nDomain experts, developers,\nmanagers, regulatory\nentities/agencies\n[5, 45, 35, 46, 48, 54, 61, 72, 88, 89, 96, 108, 117,\n119, 155]\nFairness\nUsers affected by model decisions,\nregulatory entities/agencies\n[5, 24, 45, 35, 47, 99, 100, 101, 120, 121, 128, 156,\n157, 158]\nAccessibility\nProduct owners, managers, users\naffected by model decisions\n[21, 26, 30, 32, 37, 50, 53, 55, 62, 67, 68, 69, 70, 71,\n74, 75, 76, 86, 93, 94, 103, 105, 107, 108, 111, 112,\n113, 114, 115, 124, 129]\nInteractivity\nDomain experts, users affected by\nmodel decisions\n[37, 50, 59, 65, 67, 74, 86, 124]\nPrivacy awareness\nUsers affected by model decisions,\nregulatory entities/agencies\n[89]\nTable 1: Goals pursued in the reviewed literature toward reaching explainability, and their main target audience.\n• Trustworthiness: several authors agree upon the search for trustworthiness as the primary aim of an\nexplainable AI model [31, 32]. However, declaring a model as explainable as per its capabilities of\ninducing trust might not be fully compliant with the requirement of model explainability. Trustwor-\nthiness might be considered as the conﬁdence of whether a model will act as intended when facing a\ngiven problem. Although it should most certainly be a property of any explainable model, it does not\nimply that every trustworthy model can be considered explainable on its own, nor is trustworthiness\na property easy to quantify. Trust might be far from being the only purpose of an explainable model\nsince the relation among the two, if agreed upon, is not reciprocal. Part of the reviewed papers mention\nthe concept of trust when stating their purpose for achieving explainability. However, as seen in Table\n1, they do not amount to a large share of the recent contributions related to XAI.\n• Causality: another common goal for explainability is that of ﬁnding causality among data variables.\nSeveral authors argue that explainable models might ease the task of ﬁnding relationships that, should\nthey occur, could be tested further for a stronger causal link between the involved variables [159, 160].\nThe inference of causal relationships from observational data is a ﬁeld that has been broadly studied\nover time [161]. As widely acknowledged by the community working on this topic, causality requires a\nwide frame of prior knowledge to prove that observed effects are causal. A ML model only discovers\ncorrelations among the data it learns from, and therefore might not sufﬁce for unveiling a cause-effect\nrelationship. However, causation involves correlation, so an explainable ML model could validate\nthe results provided by causality inference techniques, or provide a ﬁrst intuition of possible causal\n8\n\n\nrelationships within the available data. Again, Table 1 reveals that causality is not among the most\nimportant goals if we attend to the amount of papers that state it explicitly as their goal.\n• Transferability: models are always bounded by constraints that should allow for their seamless\ntransferability. This is the main reason why a training-testing approach is used when dealing with\nML problems [162, 163]. Explainability is also an advocate for transferability, since it may ease the\ntask of elucidating the boundaries that might affect a model, allowing for a better understanding and\nimplementation. Similarly, the mere understanding of the inner relations taking place within a model\nfacilitates the ability of a user to reuse this knowledge in another problem. There are cases in which the\nlack of a proper understanding of the model might drive the user toward incorrect assumptions and\nfatal consequences [44, 164]. Transferability should also fall between the resulting properties of an\nexplainable model, but again, not every transferable model should be considered as explainable. As\nobserved in Table 1, the amount of papers stating that the ability of rendering a model explainable is to\nbetter understand the concepts needed to reuse it or to improve its performance is the second most used\nreason for pursuing model explainability.\n• Informativeness: ML models are used with the ultimate intention of supporting decision making [92].\nHowever, it should not be forgotten that the problem being solved by the model is not equal to that\nbeing faced by its human counterpart. Hence, a great deal of information is needed in order to be able\nto relate the user’s decision to the solution given by the model, and to avoid falling in misconception\npitfalls. For this purpose, explainable ML models should give information about the problem being\ntackled. Most of the reasons found among the papers reviewed is that of extracting information about\nthe inner relations of a model. Almost all rule extraction techniques substantiate their approach on\nthe search for a simpler understanding of what the model internally does, stating that the knowledge\n(information) can be expressed in these simpler proxies that they consider explaining the antecedent.\nThis is the most used argument found among the reviewed papers to back up what they expect from\nreaching explainable models.\n• Conﬁdence: as a generalization of robustness and stability, conﬁdence should always be assessed\non a model in which reliability is expected. The methods to maintain conﬁdence under control are\ndifferent depending on the model. As stated in [165, 166, 167], stability is a must-have when drawing\ninterpretations from a certain model. Trustworthy interpretations should not be produced by models\nthat are not stable. Hence, an explainable model should contain information about the conﬁdence of its\nworking regime.\n• Fairness: from a social standpoint, explainability can be considered as the capacity to reach and\nguarantee fairness in ML models. In a certain literature strand, an explainable ML model suggests a\nclear visualization of the relations affecting a result, allowing for a fairness or ethical analysis of the\nmodel at hand [3, 100]. Likewise, a related objective of XAI is highlighting bias in the data a model\nwas exposed to [168, 169]. The support of algorithms and models is growing fast in ﬁelds that involve\nhuman lives, hence explainability should be considered as a bridge to avoid the unfair or unethical use\nof algorithm’s outputs.\n• Accessibility: a minor subset of the reviewed contributions argues for explainability as the property\nthat allows end users to get more involved in the process of improving and developing a certain ML\nmodel [37, 86] . It seems clear that explainable models will ease the burden felt by non-technical or\nnon-expert users when having to deal with algorithms that seem incomprehensible at ﬁrst sight. This\nconcept is expressed as the third most considered goal among the surveyed literature.\n• Interactivity: some contributions [50, 59] include the ability of a model to be interactive with the user\nas one of the goals targeted by an explainable ML model. Once again, this goal is related to ﬁelds in\n9\n\n\nwhich the end users are of great importance, and their ability to tweak and interact with the models is\nwhat ensures success.\n• Privacy awareness: almost forgotten in the reviewed literature, one of the byproducts enabled by ex-\nplainability in ML models is its ability to assess privacy. ML models may have complex representations\nof their learned patterns. Not being able to understand what has been captured by the model [4] and\nstored in its internal representation may entail a privacy breach. Contrarily, the ability to explain the\ninner relations of a trained model by non-authorized third parties may also compromise the differential\nprivacy of the data origin. Due to its criticality in sectors where XAI is foreseen to play a crucial role,\nconﬁdentiality and privacy issues will be covered further in Subsections 5.4 and 6.3, respectively.\nThis subsection has reviewed the goals encountered among the broad scope of the reviewed papers.\nAll these goals are clearly under the surface of the concept of explainability introduced before in this\nsection. To round up this prior analysis on the concept of explainability, the last subsection deals with\ndifferent strategies followed by the community to address explainability in ML models.\n2.5. How?\nThe literature makes a clear distinction among models that are interpretable by design, and those\nthat can be explained by means of external XAI techniques. This duality could also be regarded as the\ndifference between interpretable models and model interpretability techniques; a more widely accepted\nclassiﬁcation is that of transparent models and post-hoc explainability. This same duality also appears\nin the paper presented in [17] in which the distinction its authors make refers to the methods to solve\nthe transparent box design problem against the problem of explaining the black-box problem. This\nwork, further extends the distinction made among transparent models including the different levels of\ntransparency considered.\nWithin transparency, three levels are contemplated: algorithmic transparency, decomposability and\nsimulatability1. Among post-hoc techniques we may distinguish among text explanations, visualizations,\nlocal explanations, explanations by example, explanations by simpliﬁcation and feature relevance. In this\ncontext, there is a broader distinction proposed by [24] discerning between 1) opaque systems, where\nthe mappings from input to output are invisible to the user; 2) interpretable systems, in which users can\nmathematically analyze the mappings; and 3) comprehensible systems, in which the models should output\nsymbols or rules along with their speciﬁc output to aid in the understanding process of the rationale\nbehind the mappings being made. This last classiﬁcation criterion could be considered included within\nthe one proposed earlier, hence this paper will attempt at following the more speciﬁc one.\n2.5.1. Levels of Transparency in Machine Learning Models\nTransparent models convey some degree of interpretability by themselves. Models belonging to\nthis category can be also approached in terms of the domain in which they are interpretable, namely,\nalgorithmic transparency, decomposability and simulatability. As we elaborate next in connection to\nFigure 3, each of these classes contains its predecessors, e.g. a simulatable model is at the same time a\nmodel that is decomposable and algorithmically transparent:\n• Simulatability denotes the ability of a model of being simulated or thought about strictly by a human,\nhence complexity takes a dominant place in this class. This being said, simple but extensive (i.e., with\ntoo large amount of rules) rule based systems fall out of this characteristic, whereas a single perceptron\nneural network falls within. This aspect aligns with the claim that sparse linear models are more\ninterpretable than dense ones [170], and that an interpretable model is one that can be easily presented\n1The alternative term simulability is also used in the literature to refer to the capacity of a system or process to be simulated.\nHowever, we note that this term does not appear in current English dictionaries.\n10\n\n\nto a human by means of text and visualizations [32]. Again, endowing a decomposable model with\nsimulatability requires that the model has to be self-contained enough for a human to think and reason\nabout it as a whole.\n• Decomposability stands for the ability to explain each of the parts of a model (input, parameter and\ncalculation). It can be considered as intelligibility as stated in [171]. This characteristic might empower\nthe ability to understand, interpret or explain the behavior of a model. However, as occurs with\nalgorithmic transparency, not every model can fulﬁll this property. Decomposability requires every\ninput to be readily interpretable (e.g. cumbersome features will not ﬁt the premise). The added\nconstraint for an algorithmically transparent model to become decomposable is that every part of the\nmodel must be understandable by a human without the need for additional tools.\n• Algorithmic Transparency can be seen in different ways. It deals with the ability of the user to\nunderstand the process followed by the model to produce any given output from its input data. Put\nit differently, a linear model is deemed transparent because its error surface can be understood and\nreasoned about, allowing the user to understand how the model will act in every situation it may\nface [163]. Contrarily, it is not possible to understand it in deep architectures as the loss landscape\nmight be opaque [172, 173] since it cannot be fully observed and the solution has to be approximated\nthrough heuristic optimization (e.g. through stochastic gradient descent). The main constraint for\nalgorithmically transparent models is that the model has to be fully explorable by means of mathematical\nanalysis and methods.\nMϕ\nx1\nx2\nx3\ny\n(b)\nIf x2 > 180 then y = 1\nElse if x1 + x3 > 150 then y = 1\nElse y = 0\nMϕ\nx1\nx2\nx3\ny\n(a)\nIf g(fA(x1), fB(x2)) > 5\nthen y = 1, else y = 0\nfA(x1) = 1/x2\n1, fB(x2) = log x2\ng(f, g) = 1/(f + g)\nx1: weight, x2: height, x3: age\nMϕ\nx1\nx2\nx3\ny\n(c)\n(c)\n95% of the positive training samples\nhave x2 > 180 7→Rule 1\n90% of the positive training samples\nhave x1 + x3 > 150 7→Rule 2\n?\n?\n?\nFigure 3: Conceptual diagram exemplifying the different levels of transparency characterizing a ML model Mϕ, with ϕ denoting\nthe parameter set of the model at hand: (a) simulatability; (b) decomposability; (c) algorithmic transparency. Without loss of\ngenerality, the example focuses on the ML model as the explanation target. However, other targets for explainability may include a\ngiven example, the output classes or the dataset itself.\n2.5.2. Post-hoc Explainability Techniques for Machine Learning Models\nPost-hoc explainability targets models that are not readily interpretable by design by resorting to\ndiverse means to enhance their interpretability, such as text explanations, visual explanations, local\nexplanations, explanations by example, explanations by simpliﬁcation and feature relevance explanations\ntechniques. Each of these techniques covers one of the most common ways humans explain systems and\nprocesses by themselves.\nFurther along this river, actual techniques, or better put, actual group of techniques are speciﬁed\nto ease the future work of any researcher that intends to look up for an speciﬁc technique that suits its\nknowledge. Not ending there, the classiﬁcation also includes the type of data in which the techniques has\nbeen applied. Note that many techniques might be suitable for many different types of data, although\nthe categorization only considers the type used by the authors that proposed such technique. Overall,\npost-hoc explainability techniques are divided ﬁrst by the intention of the author (explanation technique\ne.g. Explanation by simpliﬁcation), then, by the method utilized (actual technique e.g. sensitivity analysis)\nand ﬁnally by the type of data in which it was applied (e.g. images).\n11\n\n\n• Text explanations deal with the problem of bringing explainability for a model by means of learning to\ngenerate text explanations that help explaining the results from the model [169]. Text explanations also\ninclude every method generating symbols that represent the functioning of the model. These symbols\nmay portrait the rationale of the algorithm by means of a semantic mapping from model to symbols.\n• Visual explanation techniques for post-hoc explainability aim at visualizing the model’s behavior.\nMany of the visualization methods existing in the literature come along with dimensionality reduction\ntechniques that allow for a human interpretable simple visualization. Visualizations may be coupled\nwith other techniques to improve their understanding, and are considered as the most suitable way to\nintroduce complex interactions within the variables involved in the model to users not acquainted to\nML modeling.\n• Local explanations tackle explainability by segmenting the solution space and giving explanations\nto less complex solution subspaces that are relevant for the whole model. These explanations can be\nformed by means of techniques with the differentiating property that these only explain part of the\nwhole system’s functioning.\n• Explanations by example consider the extraction of data examples that relate to the result generated by\na certain model, enabling to get a better understanding of the model itself. Similarly to how humans\nbehave when attempting to explain a given process, explanations by example are mainly centered in\nextracting representative examples that grasp the inner relationships and correlations found by the\nmodel being analyzed.\n• Explanations by simpliﬁcation collectively denote those techniques in which a whole new system is\nrebuilt based on the trained model to be explained. This new, simpliﬁed model usually attempts at\noptimizing its resemblance to its antecedent functioning, while reducing its complexity, and keeping a\nsimilar performance score. An interesting byproduct of this family of post-hoc techniques is that the\nsimpliﬁed model is, in general, easier to be implemented due to its reduced complexity with respect to\nthe model it represents.\n• Finally, feature relevance explanation methods for post-hoc explainability clarify the inner functioning\nof a model by computing a relevance score for its managed variables. These scores quantify the affection\n(sensitivity) a feature has upon the output of the model. A comparison of the scores among different\nvariables unveils the importance granted by the model to each of such variables when producing its\noutput. Feature relevance methods can be thought to be an indirect method to explain a model.\nThe above classiﬁcation (portrayed graphically in Figure 4) will be used when reviewing spe-\nciﬁc/agnostic XAI techniques for ML models in the following sections (Table 2). For each ML model, a\ndistinction of the propositions to each of these categories is presented in order to pose an overall image of\nthe ﬁeld’s trends.\n3. Transparent Machine Learning Models\nThe previous section introduced the concept of transparent models. A model is considered to be\ntransparent if by itself it is understandable. The models surveyed in this section are a suit of transparent\nmodels that can fall in one or all of the levels of model transparency described previously (namely,\nsimulatability, decomposability and algorithmic transparency). In what follows we provide reasons for\nthis statement, with graphical support given in Figure 5.\n12\n\n\nBlack-box\nmodel\nx\ny\nMϕ\nx = (x1, ... , xn)\nFeature\nrelevance\n...\n“Feature x2 has a\n90% importance in y”\n...\nLocal\nexplanations\nxi\nyi\nMϕ\n“What happens with the prediction yi if\nwe change slightly the features of xi?”\nxi: input instance\nVisualization\nx\ny\nMϕ\nx1 x2 x3 x4\nxn\nx1\nx3\nModel\nsimpliﬁcation\nx\ny\nMϕ\nx1\nx3\nx7\nx13\ny′\nx\nF\nG\nText\nexplanations\nxi\nyi\nMϕ\n“The output for xi is\nyi because x3 > γ ”\nby example\nExplanations\nxi\nyi\nMϕ\n”Explanatory examples\nfor the model:”\n- xA 7→yA\n- xB 7→yB\n- xC 7→yC\nFigure 4: Conceptual diagram showing the different post-hoc explainability approaches available for a ML model Mϕ.\n3.1. Linear/Logistic Regression\nLogistic Regression (LR) is a classiﬁcation model to predict a dependent variable (category) that is\ndichotomous (binary). However, when the dependent variable is continuous, linear regression would\nbe its homonym. This model takes the assumption of linear dependence between the predictors and the\npredicted variables, impeding a ﬂexible ﬁt to the data. This speciﬁc reason (stiffness of the model) is the\none that maintains the model under the umbrella of transparent methods. However, as stated in Section 2,\nexplainability is linked to a certain audience, which makes a model fall under both categories depending\nwho is to interpret it. This way, logistic and linear regression, although clearly meeting the characteristics\nof transparent models (algorithmic transparency, decomposability and simulatability), may also demand\npost-hoc explainability techniques (mainly, visualization), particularly when the model is to be explained\nto non-expert audiences.\nThe usage of this model has been largely applied within Social Sciences for quite a long time,\nwhich has pushed researchers to create ways of explaining the results of the models to non-expert\nusers. Most authors agree on the different techniques used to analyze and express the soundness of LR\n[174, 175, 176, 177], including the overall model evaluation, statistical tests of individual predictors,\ngoodness-of-ﬁt statistics and validation of the predicted probabilities. The overall model evaluation\nshows the improvement of the applied model over a baseline, showing if it is in fact improving the model\nwithout predictions. The statistical signiﬁcance of single predictors is shown by calculating the Wald\nchi-square statistic. The goodness-of-ﬁt statistics show the quality of ﬁtness of the model to the data\nand how signiﬁcant this is. This can be achieved by resorting to different techniques e.g. the so-called\nHosmer-Lemeshow (H-L) statistic. The validation of predicted probabilities involves testing whether the\noutput of the model corresponds to what is shown by the data. These techniques show mathematical ways\nof representing the ﬁtness of the model and its behavior.\nOther techniques from other disciplines besides Statistics can be adopted for explaining these re-\n13\n\n\nModel\nTransparent ML Models\nPost-hoc\nanalysis\nSimulatability\nDecomposability\nAlgorithmic Transparency\nLinear/Logistic Regression\nPredictors are human readable and\ninteractions among them are kept to a\nminimum\nVariables are still readable, but the number\nof interactions and predictors involved in\nthem have grown to force decomposition\nVariables and interactions are too complex\nto be analyzed without mathematical tools\nNot needed\nDecision Trees\nA human can simulate and obtain the\nprediction of a decision tree on his/her own,\nwithout requiring any mathematical\nbackground\nThe model comprises rules that do not alter\ndata whatsoever, and preserves their\nreadability\nHuman-readable rules that explain the\nknowledge learned from data and allows\nfor a direct understanding of the prediction\nprocess\nNot needed\nK-Nearest Neighbors\nThe complexity of the model (number of\nvariables, their understandability and the\nsimilarity measure under use) matches\nhuman naive capabilities for simulation\nThe amount of variables is too high and/or\nthe similarity measure is too complex to be\nable to simulate the model completely, but\nthe similarity measure and the set of\nvariables can be decomposed and analyzed\nseparately\nThe similarity measure cannot be\ndecomposed and/or the number of\nvariables is so high that the user has to rely\non mathematical and statistical tools to\nanalyze the model\nNot needed\nRule Based Learners\nVariables included in rules are readable,\nand the size of the rule set is manageable\nby a human user without external help\nThe size of the rule set becomes too large\nto be analyzed without decomposing it into\nsmall rule chunks\nRules have become so complicated (and\nthe rule set size has grown so much) that\nmathematical tools are needed for\ninspecting the model behaviour\nNot needed\nGeneral Additive Models\nVariables and the interaction among them\nas per the smooth functions involved in the\nmodel must be constrained within human\ncapabilities for understanding\nInteractions become too complex to be\nsimulated, so decomposition techniques are\nrequired for analyzing the model\nDue to their complexity, variables and\ninteractions cannot be analyzed without the\napplication of mathematical and statistical\ntools\nNot needed\nBayesian Models\nStatistical relationships modeled among\nvariables and the variables themselves\nshould be directly understandable by the\ntarget audience\nStatistical relationships involve so many\nvariables that they must be decomposed in\nmarginals so as to ease their analysis\nStatistical relationships cannot be\ninterpreted even if already decomposed,\nand predictors are so complex that model\ncan be only analyzed with mathematical\ntools\nNot needed\nTree Ensembles\n\u0017\n\u0017\n\u0017\nNeeded: Usually Model simpliﬁcation or\nFeature relevance techniques\nSupport Vector Machines\n\u0017\n\u0017\n\u0017\nNeeded: Usually Model simpliﬁcation or\nLocal explanations techniques\nMulti–layer Neural Network\n\u0017\n\u0017\n\u0017\nNeeded: Usually Model simpliﬁcation,\nFeature relevance or Visualization\ntechniques\nConvolutional Neural Network\n\u0017\n\u0017\n\u0017\nNeeded: Usually Feature relevance or\nVisualization techniques\nRecurrent Neural Network\n\u0017\n\u0017\n\u0017\nNeeded: Usually Feature relevance\ntechniques\nTable 2: Overall picture of the classiﬁcation of ML models attending to their level of explainability.\ngression models. Visualization techniques are very powerful when presenting statistical conclusions to\nusers not well-versed in statistics. For instance, the work in [178] shows that the usage of probabilities to\ncommunicate the results, implied that the users where able to estimate the outcomes correctly in 10% of\nthe cases, as opposed to 46% of the cases when using natural frequencies. Although logistic regression is\namong the simplest classiﬁcation models in supervised learning, there are concepts that must be taken\ncare of.\nIn this line of reasoning, the authors of [179] unveil some concerns with the interpretations derived\nfrom LR. They ﬁrst mention how dangerous it might be to interpret log odds ratios and odd ratios as\nsubstantive effects, since they also represent unobserved heterogeneity. Linked to this ﬁrst concern,\n[179] also states that a comparison between these ratios across models with different variables might be\nproblematic, since the unobserved heterogeneity is likely to vary, thereby invalidating the comparison.\nFinally they also mention that the comparison of these odds across different samples, groups and time is\nalso risky, since the variation of the heterogeneity is not known across samples, groups and time points.\nThis last paper serves the purpose of visualizing the problems a model’s interpretation might entail, even\nwhen its construction is as simple as that of LR.\nAlso interesting is to note that, for a model such as logistic or linear regression to maintain decompos-\nability and simulatability, its size must be limited, and the variables used must be understandable by their\nusers. As stated in Section 2, if inputs to the model are highly engineered features that are complex or\ndifﬁcult to understand, the model at hand will be far from being decomposable. Similarly, if the model is\nso large that a human cannot think of the model as a whole, its simulatability will be put to question.\n3.2. Decision Trees\nDecision trees are another example of a model that can easily fulﬁll every constraint for transparency.\nDecision trees are hierarchical structures for decision making used to support regression and classiﬁcation\nproblems [132, 180]. In the simplest of their ﬂavors, decision trees are simulatable models. However,\ntheir properties can render them decomposable or algorithmically transparent.\n14\n\n\nx2\nx1\nx1 ≥γ\nx2 ≥γ′\nx2 ≥γ′′\nYes\nNo\nx1 ≥γ′′′\nYes\nNo\nYes\nNo\nClass\nSupport: 70%\nImpurity: 0.1\nStraightforward what-if testing\nSimple univariate thresholds\nSimulatable, decomposable\nDirect support and impurity measures\nwi: increase in y if xi\nw0 (intercept): y for a test instance\nwith average normalized features\nincreases by one unit\nx1\nx2\nxtest\n2\nxtest\n1\nYes\nPrediction by majority voting\nK similar training instances\nSimulatable, decomposable\nAlgorithmic transparency (lazy training)\nLinguistic rules: easy to interpret\nTraining\ndataset\n−If x1 is high then y =\n−If x1 is low and x2 is\nhigh then y =\n−If x2 is low then y =\nSimulatable if ruleset coverage and\nspeciﬁty are kept constrained\nFuzziness improves interpretability\ny = w1x1 + w2x2 + w0\ng(E(y)) = w1f1(x1) + w2f2(x2)\nTraining\ndataset\ng(z)\nz\nfi(xi)\nxi\nE(y): expected value\nSimulatable, decomposable\nInterpretability depends on link\nfunction g(z), the selected fi(xi)\nand the sparseness of [w1, . . . , wN]\ny\nTraining\ndataset\nTraining\ndataset\nTraining\ndataset\nTraining\ndataset\nx1\nx2\ny\np(y|x1, x2) ∝p(y|x1)p(y|x2)\nxi\np(y|xi)\nto assess the contribution of each variable\nThe independence assumption permits\nSimulatable, decomposable\nAlgorithmic transparency (distribution ﬁtting)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nClass\nClass\nClass\nClass\n?\n?\n?\n?\n?\n?\nFigure 5: Graphical illustration of the levels of transparency of different ML models considered in this overview: (a) Linear\nregression; (b) Decision trees; (c) K-Nearest Neighbors; (d) Rule-based Learners; (e) Generalized Additive Models; (f) Bayesian\nModels.\nDecision trees have always lingered in between the different categories of transparent models. Their\nutilization has been closely linked to decision making contexts, being the reason why their complexity\nand understandability have always been considered a paramount matter. A proof of this relevance can\nbe found in the upsurge of contributions to the literature dealing with decision tree simpliﬁcation and\ngeneration [132, 180, 181, 182]. As noted above, although being capable of ﬁtting every category within\ntransparent models, the individual characteristics of decision trees can push them toward the category of\nalgorithmically transparent models. A simulatable decision tree is one that is manageable by a human\nuser. This means its size is somewhat small and the amount of features and their meaning are easily\nunderstandable. An increment in size transforms the model into a decomposable one since its size impedes\nits full evaluation (simulation) by a human. Finally, further increasing its size and using complex feature\nrelations will make the model algorithmically transparent loosing the previous characteristics.\nDecision trees have long been used in decision support contexts due to their off-the-shelf transparency.\nMany applications of these models fall out of the ﬁelds of computation and AI (even information\ntechnologies), meaning that experts from other ﬁelds usually feel comfortable interpreting the outputs of\nthese models [183, 184, 185]. However, their poor generalization properties in comparison with other\nmodels make this model family less interesting for their application to scenarios where a balance between\npredictive performance is a design driver of utmost importance. Tree ensembles aim at overcoming such\na poor performance by aggregating the predictions performed by trees learned on different subsets of\ntraining data. Unfortunately, the combination of decision trees looses every transparent property, calling\nfor the adoption of post-hoc explainability techniques as the ones reviewed later in the manuscript.\n15\n\n\n3.3. K-Nearest Neighbors\nAnother method that falls within transparent models is that of K-Nearest Neighbors (KNN), which\ndeals with classiﬁcation problems in a methodologically simple way: it predicts the class of a test sample\nby voting the classes of its K nearest neighbors (where the neighborhood relation is induced by a measure\nof distance between samples). When used in the context of regression problems, the voting is replaced by\nan aggregation (e.g. average) of the target values associated with the nearest neighbors.\nIn terms of model explainability, it is important to observe that predictions generated by KNN models\nrely on the notion of distance and similarity between examples, which can be tailored depending on the\nspeciﬁc problem being tackled. Interestingly, this prediction approach resembles that of experience-based\nhuman decision making, which decides upon the result of past similar cases. There lies the rationale\nof why KNN has also been adopted widely in contexts in which model interpretability is a requirement\n[186, 187, 188, 189]. Furthermore, aside from being simple to explain, the ability to inspect the reasons\nby which a new sample has been classiﬁed inside a group and to examine how these predictions evolve\nwhen the number of neighbors K is increased or decreased empowers the interaction between the users\nand the model.\nOne must keep in mind that as mentioned before, KNN’s class of transparency depends on the features,\nthe number of neighbors and the distance function used to measure the similarity between data instances.\nA very high K impedes a full simulation of the model performance by a human user. Similarly, the usage\nof complex features and/or distance functions would hinder the decomposability of the model, restricting\nits interpretability solely to the transparency of its algorithmic operations.\n3.4. Rule-based Learning\nRule-based learning refers to every model that generates rules to characterize the data it is intended to\nlearn from. Rules can take the form of simple conditional if-then rules or more complex combinations of\nsimple rules to form their knowledge. Also connected to this general family of models, fuzzy rule based\nsystems are designed for a broader scope of action, allowing for the deﬁnition of verbally formulated\nrules over imprecise domains. Fuzzy systems improve two main axis relevant for this paper. First, they\nempower more understandable models since they operate in linguistic terms. Second, they perform better\nthat classic rule systems in contexts with certain degrees of uncertainty. Rule based learners are clearly\ntransparent models that have been often used to explain complex models by generating rules that explain\ntheir predictions [126, 127, 190, 191].\nRule learning approaches have been extensively used for knowledge representation in expert systems\n[192]. However, a central problem with rule generation approaches is the coverage (amount) and the\nspeciﬁcity (length) of the rules generated. This problem relates directly to the intention for their use in\nthe ﬁrst place. When building a rule database, a typical design goal sought by the user is to be able to\nanalyze and understand the model. The amount of rules in a model will clearly improve the performance\nof the model at the stake of compromising its intepretability. Similarly, the speciﬁcity of the rules plays\nalso against interpretability, since a rule with a high number of antecedents an/or consequences might\nbecome difﬁcult to interpret. In this same line of reasoning, these two features of a rule based learner\nplay along with the classes of transparent models presented in Section 2. The greater the coverage or\nthe speciﬁcity is, the closer the model will be to being just algorithmically transparent. Sometimes, the\nreason to transition from classical rules to fuzzy rules is to relax the constraints of rule sizes, since a\ngreater range can be covered with less stress on interpretability.\nRule based learners are great models in terms of interpretability across ﬁelds. Their natural and\nseamless relation to human behaviour makes them very suitable to understand and explain other models.\nIf a certain threshold of coverage is acquired, a rule wrapper can be thought to contain enough information\nabout a model to explain its behavior to a non-expert user, without forfeiting the possibility of using the\ngenerated rules as an standalone prediction model.\n16\n\n\n3.5. General Additive Models\nIn statistics, a Generalized Additive Model (GAM) is a linear model in which the value of the\nvariable to be predicted is given by the aggregation of a number of unknown smooth functions deﬁned\nfor the predictor variables. The purpose of such model is to infer the smooth functions whose aggregate\ncomposition approximates the predicted variable. This structure is easily interpretable, since it allows the\nuser to verify the importance of each variable, namely, how it affects (through its corresponding function)\nthe predicted output.\nSimilarly to every other transparent model, the literature is replete with case studies where GAMs\nare in use, specially in ﬁelds related to risk assessment. When compared to other models, these are\nunderstandable enough to make users feel conﬁdent on using them for practical applications in ﬁnance\n[193, 194, 195], environmental studies [196], geology [197], healthcare [44], biology [198, 199] and\nenergy [200]. Most of these contributions use visualization methods to further ease the interpretation of\nthe model. GAMs might be also considered as simulatable and decomposable models if the properties\nmentioned in its deﬁnitions are fulﬁlled, but to an extent that depends roughly on eventual modiﬁcations\nto the baseline GAM model, such as the introduction of link functions to relate the aggregation with the\npredicted output, or the consideration of interactions between predictors.\nAll in all, applications of GAMs like the ones exempliﬁed above share one common factor: under-\nstandability. The main driver for conducting these studies with GAMs is to understand the underlying\nrelationships that build up the cases for scrutiny. In those cases the research goal is not accuracy for its\nown sake, but rather the need for understanding the problem behind and the relationship underneath the\nvariables involved in data. This is why GAMs have been accepted in certain communities as their de facto\nmodeling choice, despite their acknowledged misperforming behavior when compared to more complex\ncounterparts.\n3.6. Bayesian Models\nA Bayesian model usually takes the form of a probabilistic directed acyclic graphical model whose\nlinks represent the conditional dependencies between a set of variables. For example, a Bayesian network\ncould represent the probabilistic relationships between diseases and symptoms. Given symptoms, the\nnetwork can be used to compute the probabilities of the presence of various diseases. Similar to GAMs,\nthese models also convey a clear representation of the relationships between features and the target, which\nin this case are given explicitly by the connections linking variables to each other.\nOnce again, Bayesian models fall below the ceiling of Transparent models. Its categorization leaves\nit under simulatable, decomposable and algorithmically transparent. However, it is worth noting that\nunder certain circumstances (overly complex or cumbersome variables), a model may loose these ﬁrst\ntwo properties. Bayesian models have been shown to lead to great insights in assorted applications such\nas cognitive modeling [201, 202], ﬁshery [196, 203], gaming [204], climate [205], econometrics [206] or\nrobotics [207]. Furthermore, they have also been utilized to explain other models, such as averaging tree\nensembles [208].\n4. Post-hoc Explainability Techniques for Machile Learning Models: Taxonomy, Shallow Models\nand Deep Learning\nWhen ML models do not meet any of the criteria imposed to declare them transparent, a separate\nmethod must be devised and applied to the model to explain its decisions. This is the purpose of post-hoc\nexplainability techniques (also referred to as post-modeling explainability), which aim at communicating\nunderstandable information about how an already developed model produces its predictions for any given\ninput. In this section we categorize and review different algorithmic approaches for post-hoc explainability,\ndiscriminating among 1) those that are designed for their application to ML models of any kind; and 2)\nthose that are designed for a speciﬁc ML model and thus, can not be directly extrapolated to any other\n17\n\n\nlearner. We now elaborate on the trends identiﬁed around post-hoc explainability for different ML models,\nwhich are illustrated in Figure 6 in the form of hierarchical bibliographic categories and summarized next:\n• Model-agnostic techniques for post-hoc explainability (Subsection 4.1), which can be applied seam-\nlessly to any ML model disregarding its inner processing or internal representations.\n• Post-hoc explainability that are tailored or speciﬁcally designed to explain certain ML models. We\ndivide our literature analysis into two main branches: contributions dealing with post-hoc explainability\nof shallow ML models, which collectively refers to all ML models that do not hinge on layered\nstructures of neural processing units (Subsection 4.2); and techniques devised for deep learning models,\nwhich correspondingly denote the family of neural networks and related variants, such as convolutional\nneural networks, recurrent neural networks (Subsection 4.3) and hybrid schemes encompassing deep\nneural networks and transparent models. For each model we perform a thorough review of the latest\npost-hoc methods proposed by the research community, along with a identiﬁcation of trends followed\nby such contributions.\n• We end our literature analysis with Subsection 4.4, where we present a second taxonomy that com-\nplements the more general one in Figure 6 by classifying contributions dealing with the post-hoc\nexplanation of Deep Learning models. To this end we focus on particular aspects related to this family\nof black-box ML methods, and expose how they link to the classiﬁcation criteria used in the ﬁrst\ntaxonomy.\n4.1. Model-agnostic Techniques for Post-hoc Explainability\nModel-agnostic techniques for post-hoc explainability are designed to be plugged to any model\nwith the intent of extracting some information from its prediction procedure. Sometimes, simpliﬁcation\ntechniques are used to generate proxies that mimic their antecedents with the purpose of having something\ntractable and of reduced complexity. Other times, the intent focuses on extracting knowledge directly\nfrom the models or simply visualizing them to ease the interpretation of their behavior. Following the\ntaxonomy introduced in Section 2, model-agnostic techniques may rely on model simpliﬁcation, feature\nrelevance estimation and visualization techniques:\n• Explanation by simpliﬁcation. They are arguably the broadest technique under the category of model\nagnostic post-hoc methods. Local explanations are also present within this category, since sometimes,\nsimpliﬁed models are only representative of certain sections of a model. Almost all techniques taking\nthis path for model simpliﬁcation are based on rule extraction techniques. Among the most known\ncontributions to this approach we encounter the technique of Local Interpretable Model-Agnostic\nExplanations (LIME) [32] and all its variations [214, 216]. LIME builds locally linear models around\nthe predictions of an opaque model to explain it. These contributions fall under explanations by\nsimpliﬁcation as well as under local explanations. Besides LIME and related ﬂavors, another approach\nto rule extraction is G-REX [212]. Although it was not originally intended for extracting rules from\nopaque models, the generic proposition of G-REX has been extended to also account for model\nexplainability purposes [190, 211]. In line with rule extraction methods, the work in [215] presents a\nnovel approach to learn rules in CNF (Conjunctive Normal Form) or DNF (Disjunctive Normal Form)\nto bridge from a complex model to a human-interpretable model. Another contribution that falls off the\nsame branch is that in [218], where the authors formulate model simpliﬁcation as a model extraction\nprocess by approximating a transparent model to the complex one. Simpliﬁcation is approached from a\ndifferent perspective in [120], where an approach to distill and audit black box models is presented. In\nit, two main ideas are exposed: a method for model distillation and comparison to audit black-box risk\nscoring models; and an statistical test to check if the auditing data is missing key features it was trained\nwith. The popularity of model simpliﬁcation is evident, given it temporally coincides with the most\n18\n\n\nXAI in ML\nTransparent Models\nLogistic / Linear Regression\nDecision Trees\nK-Nearest Neighbors\nRule-base Learners\nGeneral Additive Models:\n[44]\nBayesian Models:\n[31, 49, 209, 210]\nPost-Hoc Explainability\nModel-Agnostic\nExplanation by simpliﬁcation\nRule-based learner:\n[32, 51, 120, 190, 211, 212, 213, 214, 215, 216]\nDecision Tree:\n[21, 119, 133, 135, 149, 217, 218]\nOthers:\n[56, 219]\nFeature relevance explanation\nInﬂuence functions:\n[173, 220, 221]\nSensitivity:\n[222, 223]\nGame theory inspired:\n[224, 225] [226]\nSaliency:\n[85, 227]\nInteraction based:\n[123, 228]\nOthers:\n[140, 141, 229, 230, 231]\nLocal Explanations\nRule-based learner:\n[32, 216]\nDecision Tree:\n[232, 233]\nOthers:\n[67, 224, 230, 234, 235, 236, 237]\nVisual explanation\nConditional / Dependence / Shapley plots:\n[56, 224, 238, 239]\nSensitivity / Saliency:\n[85, 227] [222, 223]\nOthers:\n[117, 123, 140, 178, 234]\nModel-Speciﬁc\nEnsembles and Multiple Classiﬁer Systems\nExplanation by simpliﬁcation\nDecision Tree/Prototype:\n[84, 118, 122]\nFeature relevance explanation\nFeature importance / contribution:\n[103, 104, 240, 241]\nVisual explanation\nVariable importance / attribution:\n[104, 241] [242, 243]\nSupport Vector Machines\nExplanation by simpliﬁcation\nRule-based learner:\n[57, 93, 94, 98, 106, 134, 244, 245, 246]\nProbabilistic:\n[247, 248]\nOthers:\n[102]\nFeature relevance explanation\nFeature Contribution / Statistics:\n[249] [116, 249]\nVisual explanation\nInternal visualization:\n[68, 77, 250]\nMulti-Layer Neural Networks\nExplanation by simpliﬁcation\nRule-based learner:\n[82, 83, 147, 148, 251, 252, 253, 254, 255, 256]\nDecision Tree:\n[21, 56, 79, 81, 97, 135, 257, 258, 259]\nOthers:\n[80]\nFeature relevance explanation\nImportance/Contribution:\n[60, 61, 110, 260, 261]\nSensitivity / Saliency:\n[260]\n[262]\nLocal explanation\nDecision Tree / Sensitivity:\n[233] [263]\nExplanation by Example\nActivation clusters:\n[264, 144]\nText explanation\nCaption generation:\n[111] [150]\nVisual explanation\nSaliency / Weights:\n[265]\nArchitecture modiﬁcation\nOthers:\n[264] [266]\n[267]\nConvolutional Neural Networks\nExplanation by simpliﬁcation\nDecision Tree:\n[78]\nFeature relevance explanation\nActivations:\n[72, 268] [46]\nFeature Extraction:\n[72, 268]\nVisual explanation\nFilter / Activation:\n[63, 136, 137, 142, 152, 269, 270, 271]\nSensitivity / Saliency:\n[131, 272] [46]\nOthers:\n[273]\nArchitecture modiﬁcation\nLayer modiﬁcation:\n[143, 274, 275]\nModel combination:\n[91, 274, 276]\nAttention networks:\n[107, 114, 277, 278] [91]\nLoss modiﬁcation:\n[276] [113]\nOthers:\n[279]\nRecurrent Neural Networks\nExplanation by simpliﬁcation\nRule-based learner:\n[146]\nFeature relevance explanation\nActivation propagation:\n[280]\nVisual explanation\nActivations:\n[281]\nArquitecture modiﬁcation\nLoss / Layer modiﬁcation:\n[276, 282] [274]\nOthers:\n[151, 283, 284] [285]\nFigure 6: Taxonomy of the reviewed literature and trends identiﬁed for explainability techniques related to different ML models.\nReferences boxed in blue, green and red correspond to XAI techniques using image, text or tabular data, respectively. In order\nto build this taxonomy, the literature has been analyzed in depth to discriminate whether a post-hoc technique can be seamlessly\napplied to any ML model, even if, e.g., explicitly mentions Deep Learning in its title and/or abstract.\nrecent literature on XAI, including techniques such as LIME or G-REX. This symptomatically reveals\nthat this post-hoc explainability approach is envisaged to continue playing a central role on XAI.\n• Feature relevance explanation techniques aim to describe the functioning of an opaque model by\n19\n\n\nranking or measuring the inﬂuence, relevance or importance each feature has in the prediction output by\nthe model to be explained. An amalgam of propositions are found within this category, each resorting\nto different algorithmic approaches with the same targeted goal. One fruitful contribution to this path\nis that of [224] called SHAP (SHapley Additive exPlanations). Its authors presented a method to\ncalculate an additive feature importance score for each particular prediction with a set of desirable\nproperties (local accuracy, missingness and consistency) that its antecedents lacked. Another approach\nto tackle the contribution of each feature to predictions has been coalitional Game Theory [225] and\nlocal gradients [234]. Similarly, by means of local gradients [230] test the changes needed in each\nfeature to produce a change in the output of the model. In [228] the authors analyze the relations and\ndependencies found in the model by grouping features, that combined, bring insights about the data.\nThe work in [173] presents a broad variety of measures to tackle the quantiﬁcation of the degree of\ninﬂuence of inputs on outputs of systems. Their QII (Quantitative Input Inﬂuence) measures account\nfor correlated inputs while measuring inﬂuence. In contrast, in [222] the authors build upon the existing\nSA (Sensitivity Analysis) to construct a Global SA which extends the applicability of the existing\nmethods. In [227] a real-time image saliency method is proposed, which is applicable to differentiable\nimage classiﬁers. The study in [123] presents the so-called Automatic STRucture IDentiﬁcation method\n(ASTRID) to inspect which attributes are exploited by a classiﬁer to generate a prediction. This method\nﬁnds the largest subset of features such that the accuracy of a classiﬁer trained with this subset of\nfeatures cannot be distinguished in terms of accuracy from a classiﬁer built on the original feature set.\nIn [221] the authors use inﬂuence functions to trace a model’s prediction back to the training data, by\nonly requiring an oracle version of the model with access to gradients and Hessian-vector products.\nHeuristics for creating counterfactual examples by modifying the input of the model have been also\nfound to contribute to its explainability [236, 237]. Compared to those attempting explanations by\nsimpliﬁcation, a similar amount of publications were found tackling explainability by means of feature\nrelevance techniques. Many of the contributions date from 2017 and some from 2018, implying that as\nwith model simpliﬁcation techniques, feature relevance has also become a vibrant subject study in the\ncurrent XAI landscape.\n• Visual explanation techniques are a vehicle to achieve model-agnostic explanations. Representative\nworks in this area can be found in [222], which present a portfolio of visualization techniques to help in\nthe explanation of a black-box ML model built upon the set of extended techniques mentioned earlier\n(Global SA). Another set of visualization techniques is presented in [223]. The authors present three\nnovel SA methods (data based SA, Monte-Carlo SA, cluster-based SA) and one novel input importance\nmeasure (Average Absolute Deviation). Finally, [238] presents ICE (Individual Conditional Expecta-\ntion) plots as a tool for visualizing the model estimated by any supervised learning algorithm. Visual\nexplanations are less common in the ﬁeld of model-agnostic techniques for post-hoc explainability.\nSince the design of these methods must ensure that they can be seamlessly applied to any ML model\ndisregarding its inner structure, creating visualizations from just inputs and outputs from an opaque\nmodel is a complex task. This is why almost all visualization methods falling in this category work\nalong with feature relevance techniques, which provide the information that is eventually displayed to\nthe end user.\nSeveral trends emerge from our literature analysis. To begin with, rule extraction techniques prevail\nin model-agnostic contributions under the umbrella of post-hoc explainability. This could have been\nintuitively expected if we bear in mind the wide use of rule based learning as explainability wrappers\nanticipated in Section 3.4, and the complexity imposed by not being able to get into the model itself.\nSimilarly, another large group of contributions deals with feature relevance. Lately these techniques are\ngathering much attention by the community when dealing with DL models, with hybrid approaches that\nutilize particular aspects of this class of models and therefore, compromise the independence of the feature\nrelevance method on the model being explained. Finally, visualization techniques propose interesting\n20\n\n\nways for visualizing the output of feature relevance techniques to ease the task of model’s interpretation.\nBy contrast, visualization techniques for other aspects of the trained model (e.g. its structure, operations,\netc) are tightly linked to the speciﬁc model to be explained.\n4.2. Post-hoc Explainability in Shallow ML Models\nShallow ML covers a diversity of supervised learning models. Within these models, there are strictly\ninterpretable (transparent) approaches (e.g. KNN and Decision Trees, already discussed in Section 3).\nHowever, other shallow ML models rely on more sophisticated learning algorithms that require additional\nlayers of explanation. Given their prominence and notable performance in predictive tasks, this section\nconcentrates on two popular shallow ML models (tree ensembles and Support Vector Machines, SVMs)\nthat require the adoption of post-hoc explainability techniques for explaining their decisions.\n4.2.1. Tree Ensembles, Random Forests and Multiple Classiﬁer Systems\nTree ensembles are arguably among the most accurate ML models in use nowadays. Their advent\ncame as an efﬁcient means to improve the generalization capability of single decision trees, which are\nusually prone to overﬁtting. To circumvent this issue, tree ensembles combine different trees to obtain an\naggregated prediction/regression. While it results to be effective against overﬁtting, the combination of\nmodels makes the interpretation of the overall ensemble more complex than each of its compounding tree\nlearners, forcing the user to draw from post-hoc explainability techniques. For tree ensembles, techniques\nfound in the literature are explanation by simpliﬁcation and feature relevance techniques; we next examine\nrecent advances in these techniques.\nTo begin with, many contributions have been presented to simplify tree ensembles while maintaining\npart of the accuracy accounted for the added complexity. The author from [119] poses the idea of training\na single albeit less complex model from a set of random samples from the data (ideally following the real\ndata distribution) labeled by the ensemble model. Another approach for simpliﬁcation is that in [118], in\nwhich authors create a Simpliﬁed Tree Ensemble Learner (STEL). Likewise, [122] presents the usage\nof two models (simple and complex) being the former the one in charge of interpretation and the latter\nof prediction by means of Expectation-Maximization and Kullback-Leibler divergence. As opposed to\nwhat was seen in model-agnostic techniques, not that many techniques to board explainability in tree\nensembles by means of model simpliﬁcation. It derives from this that either the proposed techniques are\ngood enough, or model-agnostic techniques do cover the scope of simpliﬁcation already.\nFollowing simpliﬁcation procedures, feature relevance techniques are also used in the ﬁeld of tree\nensembles. Breiman [286] was the ﬁrst to analyze the variable importance within Random Forests. His\nmethod is based on measuring MDA (Mean Decrease Accuracy) or MIE (Mean Increase Error) of the\nforest when a certain variable is randomly permuted in the out-of-bag samples. Following this contribution\n[241] shows, in an real setting, how the usage of variable importance reﬂects the underlying relationships\nof a complex system modeled by a Random Forest. Finally, a crosswise technique among post-hoc\nexplainability, [240] proposes a framework that poses recommendations that, if taken, would convert\nan example from one class to another. This idea attempts to disentangle the variables importance in a\nway that is further descriptive. In the article, the authors show how these methods can be used to elevate\nrecommendations to improve malicious online ads to make them rank higher in paying rates.\nSimilar to the trend shown in model-agnostic techniques, for tree ensembles again, simpliﬁcation and\nfeature relevance techniques seem to be the most used schemes. However, contrarily to what was observed\nbefore, most papers date back from 2017 and place their focus mostly on bagging ensembles. When\nshifting the focus towards other ensemble strategies, scarce activity has been recently noted around the\nexplainability of boosting and stacking classiﬁers. Among the latter, it is worth highlighting the connection\nbetween the reason why a compounding learner of the ensemble produces an speciﬁc prediction on a given\ndata, and its contribution to the output of the ensemble. The so-called Stacking With Auxiliary Features\n(SWAF) approach proposed in [242] points in this direction by harnessing and integrating explanations in\n21\n\n\nstacking ensembles to improve their generalization. This strategy allows not only relying on the output\nof the compounding learners, but also on the origin of that output and its consensus across the entire\nensemble. Other interesting studies on the explainability of ensemble techniques include model-agnostic\nschemes such as DeepSHAP [226], put into practice with stacking ensembles and multiple classiﬁer\nsystems in addition to Deep Learning models; the combination of explanation maps of multiple classiﬁers\nto produce improved explanations of the ensemble to which they belong [243]; and recent insights dealing\nwith traditional and gradient boosting ensembles [287, 288].\n4.2.2. Support Vector Machines\nAnother shallow ML model with historical presence in the literature is the SVM. SVM models are\nmore complex than tree ensembles, with a much opaquer structure. Many implementations of post-hoc\nexplainability techniques have been proposed to relate what is mathematically described internally in\nthese models, to what different authors considered explanations about the problem at hand. Technically,\nan SVM constructs a hyper-plane or set of hyper-planes in a high or inﬁnite-dimensional space, which\ncan be used for classiﬁcation, regression, or other tasks such as outlier detection. Intuitively, a good\nseparation is achieved by the hyperplane that has the largest distance (so-called functional margin) to the\nnearest training-data point of any class, since in general, the larger the margin, the lower the generalization\nerror of the classiﬁer. SVMs are among the most used ML models due to their excellent prediction\nand generalization capabilities. From the techniques stated in Section 2, post-hoc explainability applied\nto SVMs covers explanation by simpliﬁcation, local explanations, visualizations and explanations by\nexample.\nAmong explanation by simpliﬁcation, four classes of simpliﬁcations are made. Each of them dif-\nferentiates from the other by how deep they go into the algorithm inner structure. First, some authors\npropose techniques to build rule based models only from the support vectors of a trained model. This is\nthe approach of [93], which proposes a method that extracts rules directly from the support vectors of a\ntrained SVM using a modiﬁed sequential covering algorithm. In [57] the same authors propose eclectic\nrule extraction, still considering only the support vectors of a trained model. The work in [94] generates\nfuzzy rules instead of classical propositional rules. Here, the authors argue that long antecedents reduce\ncomprehensibility, hence, a fuzzy approach allows for a more linguistically understandable result. The\nsecond class of simpliﬁcations can be exempliﬁed by [98], which proposed the addition of the SVM’s\nhyperplane, along with the support vectors, to the components in charge of creating the rules. His method\nrelies on the creation of hyper-rectangles from the intersections between the support vectors and the\nhyper-plane. In a third approach to model simpliﬁcation, another group of authors considered adding\nthe actual training data as a component for building the rules. In [126, 244, 246] the authors proposed a\nclustering method to group prototype vectors for each class. By combining them with the support vectors,\nit allowed deﬁning ellipsoids and hyper-rectangles in the input space. Similarly in [106], the authors\nproposed the so-called Hyper-rectangle Rule Extraction, an algorithm based on SVC (Support Vector\nClustering) to ﬁnd prototype vectors for each class and then deﬁne small hyper-rectangles around. In\n[105], the authors formulate the rule extraction problem as a multi-constrained optimization to create a\nset of non-overlapping rules. Each rule conveys a non-empty hyper-cube with a shared edge with the\nhyper-plane. In a similar study conducted in [245], extracting rules for gene expression data, the authors\npresented a novel technique as a component of a multi-kernel SVM. This multi-kernel method consists\nof feature selection, prediction modeling and rule extraction. Finally, the study in [134] makes use of a\ngrowing SVC to give an interpretation to SVM decisions in terms of linear rules that deﬁne the space in\nVoronoi sections from the extracted prototypes.\nLeaving aside rule extraction, the literature has also contemplated some other techniques to contribute\nto the interpretation of SVMs. Three of them (visualization techniques) are clearly used toward explaining\nSVM models when used for concrete applications. For instance, [77] presents an innovative approach to\nvisualize trained SVM to extract the information content from the kernel matrix. They center the study\n22\n\n\non Support Vector Regression models. They show the ability of the algorithm to visualize which of the\ninput variables are actually related with the associated output data. In [68] a visual way combines the\noutput of the SVM with heatmaps to guide the modiﬁcation of compounds in late stages of drug discovery.\nThey assign colors to atoms based on the weights of a trained linear SVM that allows for a much more\ncomprehensive way of debugging the process. In [116] the authors argue that many of the presented\nstudies for interpreting SVMs only account for the weight vectors, leaving the margin aside. In their study\nthey show how this margin is important, and they create an statistic that explicitly accounts for the SVM\nmargin. The authors show how this statistic is speciﬁc enough to explain the multivariate patterns shown\nin neuroimaging.\nNoteworthy is also the intersection between SVMs and Bayesian systems, the latter being adopted\nas a post-hoc technique to explain decisions made by the SVM model. This is the case of [248] and\n[247], which are studies where SVMs are interpreted as MAP (Maximum A Posteriori) solutions to\ninference problems with Gaussian Process priors. This framework makes tuning the hyper-parameters\ncomprehensible and gives the capability of predicting class probabilities instead of the classical binary\nclassiﬁcation of SVMs. Interpretability of SVM models becomes even more involved when dealing\nwith non-CPD (Conditional Positive Deﬁnite) kernels that are usually harder to interpret due to missing\ngeometrical and theoretical understanding. The work in [102] revolves around this issue with a geometrical\ninterpretation of indeﬁnite kernel SVMs, showing that these do not classify by hyper-plane margin\noptimization. Instead, they minimize the distance between convex hulls in pseudo-Euclidean spaces.\nA difference might be appreciated between the post-hoc techniques applied to other models and those\nnoted for SVMs. In previous models, model simpliﬁcation in a broad sense was the prominent method\nfor post-hoc explainability. In SVMs, local explanations have started to take some weight among the\npropositions. However, simpliﬁcation based methods are, on average, much older than local explanations.\nAs a ﬁnal remark, none of the reviewed methods treating SVM explainability are dated beyond 2017,\nwhich might be due to the progressive proliferation of DL models in almost all disciplines. Another\nplausible reason is that these models are already understood, so it is hard to improve upon what has\nalready been done.\n4.3. Explainability in Deep Learning\nPost-hoc local explanations and feature relevance techniques are increasingly the most adopted\nmethods for explaining DNNs. This section reviews explainability studies proposed for the most used\nDL models, namely multi-layer neural networks, Convolutional Neural Networks (CNN) and Recurrent\nNeural Networks (RNN).\n4.3.1. Multi-layer Neural Networks\nFrom their inception, multi-layer neural networks (also known as multi-layer perceptrons) have been\nwarmly welcomed by the academic community due to their huge ability to infer complex relations among\nvariables. However, as stated in the introduction, developers and engineers in charge of deploying these\nmodels in real-life production ﬁnd in their questionable explainability a common reason for reluctance.\nThat is why neural networks have been always considered as black-box models. The fact that explainability\nis often a must for the model to be of practical value, forced the community to generate multiple\nexplainability techniques for multi-layer neural networks, including model simpliﬁcation approaches,\nfeature relevance estimators, text explanations, local explanations and model visualizations.\nSeveral model simpliﬁcation techniques have been proposed for neural networks with one single\nhidden layer, however very few works have been presented for neural networks with multiple hidden\nlayers. One of these few works is DeepRED algorithm [257], which extends the decompositional approach\nto rule extraction (splitting at neuron level) presented in [259] for multi-layer neural network by adding\nmore decision trees and rules.\nSome other works use model simpliﬁcation as a post-hoc explainability approach. For instance, [56]\npresents a simple distillation method called Interpretable Mimic Learning to extract an interpretable model\n23\n\n\nby means of gradient boosting trees. In the same direction, the authors in [135] propose a hierarchical\npartitioning of the feature space that reveals the iterative rejection of unlikely class labels, until association\nis predicted. In addition, several works addressed the distillation of knowledge from an ensemble of\nmodels into a single model [80, 289, 290] .\nGiven the fact that the simpliﬁcation of multi-layer neural networks is more complex as the number of\nlayers increases, explaining these models by feature relevance methods has become progressively more\npopular. One of the representative works in this area is [60], which presents a method to decompose the\nnetwork classiﬁcation decision into contributions of its input elements. They consider each neuron as an\nobject that can be decomposed and expanded then aggregate and back-propagate these decompositions\nthrough the network, resulting in a deep Taylor decomposition. In the same direction, the authors in [110]\nproposed DeepLIFT, an approach for computing importance scores in a multi-layer neural network. Their\nmethod compares the activation of a neuron to the reference activation and assigns the score according to\nthe difference.\nOn the other hand, some works try to verify the theoretical soundness of current explainability methods.\nFor example, the authors in [262], bring up a fundamental problem of most feature relevance techniques,\ndesigned for multi-layer networks. They showed that two axioms that such techniques ought to fulﬁll\nnamely, sensitivity and implementation invariance, are violated in practice by most approaches. Following\nthese axioms, the authors of [262] created integrated gradients, a new feature relevance method proven\nto meet the aforementioned axioms. Similarly, the authors in [61] analyzed the correctness of current\nfeature relevance explanation approaches designed for Deep Neural Networks, e,g., DeConvNet, Guided\nBackProp and LRP, on simple linear neural networks. Their analysis showed that these methods do not\nproduce the theoretically correct explanation and presented two new explanation methods PatternNet and\nPatternAttribution that are more theoretically sound for both, simple and deep neural networks.\n4.3.2. Convolutional Neural Networks\nCurrently, CNNs constitute the state-of-art models in all fundamental computer vision tasks, from\nimage classiﬁcation and object detection to instance segmentation. Typically, these models are built as\na sequence of convolutional layers and pooling layers to automatically learn increasingly higher level\nfeatures. At the end of the sequence, one or multiple fully connected layers are used to map the output\nfeatures map into scores. This structure entails extremely complex internal relations that are very difﬁcult\nto explain. Fortunately, the road to explainability for CNNs is easier than for other types of models, as the\nhuman cognitive skills favors the understanding of visual data.\nExisting works that aim at understanding what CNNs learn can be divided into two broad categories:\n1) those that try to understand the decision process by mapping back the output in the input space to\nsee which parts of the input were discriminative for the output; and 2) those that try to delve inside the\nnetwork and interpret how the intermediate layers see the external world, not necessarily related to any\nspeciﬁc input, but in general.\nOne of the seminal works in the ﬁrst category was [291]. When an input image runs feed-forward\nthrough a CNN, each layer outputs a number of feature maps with strong and soft activations. The authors\nin [291] used Deconvnet, a network designed previously by the same authors [142] that, when fed with a\nfeature map from a selected layer, reconstructs the maximum activations. These reconstructions can give\nan idea about the parts of the image that produced that effect. To visualize these strongest activations in\nthe input image, the same authors used the occlusion sensitivity method to generate a saliency map [136],\nwhich consists of iteratively forwarding the same image through the network occluding a different region\nat a time.\nTo improve the quality of the mapping on the input space, several subsequent papers proposed\nsimplifying both the CNN architecture and the visualization method. In particular, [96] included a global\naverage pooling layer between the last convolutional layer of the CNN and the fully-connected layer that\npredicts the object class. With this simple architectural modiﬁcation of the CNN, the authors built a class\n24\n\n\nactivation map that helps identify the image regions that were particularly important for a speciﬁc object\nclass by projecting back the weights of the output layer on the convolutional feature maps. Later, in [143],\nthe authors showed that max-pooling layers can be used to replace convolutional layers with a large stride\nwithout loss in accuracy on several image recognition benchmarks. They obtained a cleaner visualization\nthan Deconvnet by using a guided backpropagation method.\nTo increase the interpretability of classical CNNs, the authors in [113] used a loss for each ﬁlter in\nhigh level convolutional layers to force each ﬁlter to learn very speciﬁc object components. The obtained\nactivation patterns are much more interpretable for their exclusiveness with respect to the different labels\nto be predicted. The authors in [72] proposed visualizing the contribution to the prediction of each single\npixel of the input image in the form of a heatmap. They used a Layer-wise Relevance Propagation (LRP)\ntechnique, which relies on a Taylor series close to the prediction point rather than partial derivatives at\nthe prediction point itself. To further improve the quality of the visualization, attribution methods such\nas heatmaps, saliency maps or class activation methods (GradCAM [292]) are used (see Figure 7). In\nparticular, the authors in [292] proposed a Gradient-weighted Class Activation Mapping (Grad-CAM),\nwhich uses the gradients of any target concept, ﬂowing into the ﬁnal convolutional layer to produce a\ncoarse localization map, highlighting the important regions in the image for predicting the concept.\n(a) Heatmap [168]\n(b) Attribution [293]\n(c) Grad-CAM [292]\nFigure 7: Examples of rendering for different XAI visualization techniques on images.\nIn addition to the aforementioned feature relevance and visual explanation methods, some works\nproposed generating text explanations of the visual content of the image. For example, the authors in [91]\ncombined a CNN feature extractor with an RNN attention model to automatically learn to describe the\ncontent of images. In the same line, [278] presented a three-level attention model to perform a ﬁne-grained\nclassiﬁcation task. The general model is a pipeline that integrates three types of attention: the object\nlevel attention model proposes candidate image regions or patches from the input image, the part-level\nattention model ﬁlters out non-relevant patches to a certain object, and the last attention model localizes\ndiscriminative patches. In the task of video captioning, the authors in [111] use a CNN model combined\nwith a bi-directional LSTM model as encoder to extract video features and then feed these features to an\nLSTM decoder to generate textual descriptions.\nOne of the seminal works in the second category is [137]. In order to analyse the visual information\ncontained inside the CNN, the authors proposed a general framework that reconstruct an image from the\nCNN internal representations and showed that several layers retain photographically accurate information\nabout the image, with different degrees of geometric and photometric invariance. To visualize the notion\nof a class captured by a CNN, the same authors created an image that maximizes the class score based on\ncomputing the gradient of the class score with respect to the input image [272]. In the same direction,\nthe authors in [268] introduced a Deep Generator Network (DGN) that generates the most representative\nimage for a given output neuron in a CNN.\nFor quantifying the interpretability of the latent representations of CNNs, the authors in [125] used a\ndifferent approach called network dissection. They run a large number of images through a CNN and then\nanalyze the top activated images by considering each unit as a concept detector to further evaluate each\n25\n\n\nunit for semantic segmentation. This paper also examines the effects of classical training techniques on\nthe interpretability of the learned model.\nAlthough many of the techniques examined above utilize local explanations to achieve an overall\nexplanation of a CNN model, others explicitly focus on building global explanations based on locally found\nprototypes. In [263, 294], the authors empirically showed how local explanations in deep networks are\nstrongly dominated by their lower level features. They demonstrated that deep architectures provide strong\npriors that prevent the altering of how these low-level representations are captured. All in all, visualization\nmixed with feature relevance methods are arguably the most adopted approach to explainability in CNNs.\nInstead of using one single interpretability technique, the framework proposed in [295] combines\nseveral methods to provide much more information about the network. For example, combining feature\nvisualization (what is a neuron looking for?) with attribution (how does it affect the output?) allows\nexploring how the network decides between labels. This visual interpretability interface displays different\nblocks such as feature visualization and attribution depending on the visualization goal. This interface\ncan be thought of as a union of individual elements that belong to layers (input, hidden, output), atoms (a\nneuron, channel, spatial or neuron group), content (activations – the amount a neuron ﬁres, attribution –\nwhich classes a spatial position most contributes to, which tends to be more meaningful in later layers), and\npresentation (information visualization, feature visualization). Figure 8 shows some examples. Attribution\nmethods normally rely on pixel association, displaying what part of an input example is responsible for\nthe network activating in a particular way [293].\n(a) Neuron\n(b) Channel\n(c) Layer\nFigure 8: Feature visualization at different levels of a certain network [293].\n(a) Original image\n(b) Explaining electric guitar\n(c) Explaining acoustic guitar\nFigure 9: Examples of explanation when using LIME on images [71].\nA much simpler approach to all the previously cited methods was proposed in LIME framework\n[71], as was described in Subsection 4.1 LIME perturbs the input and sees how the predictions change.\nIn image classiﬁcation, LIME creates a set of perturbed instances by dividing the input image into\ninterpretable components (contiguous superpixels), and runs each perturbed instance through the model\n26\n\n\nto get a probability. A simple linear model learns on this data set, which is locally weighted. At the end of\nthe process, LIME presents the superpixels with highest positive weights as an explanation (see Figure 9).\nA completely different explainability approach is proposed in adversarial detection. To understand\nmodel failures in detecting adversarial examples, the authors in [264] apply the k-nearest neighbors\nalgorithm on the representations of the data learned by each layer of the CNN. A test input image is\nconsidered as adversarial if its representations are far from the representations of the training images.\n4.3.3. Recurrent Neural Networks\nAs occurs with CNNs in the visual domain, RNNs have lately been used extensively for predictive\nproblems deﬁned over inherently sequential data, with a notable presence in natural language processing\nand time series analysis. These types of data exhibit long-term dependencies that are complex to be\ncaptured by a ML model. RNNs are able to retrieve such time-dependent relationships by formulating the\nretention of knowledge in the neuron as another parametric characteristic that can be learned from data.\nFew contributions have been made for explaining RNN models. These studies can be divided into two\ngroups: 1) explainability by understanding what a RNN model has learned (mainly via feature relevance\nmethods); and 2) explainability by modifying RNN architectures to provide insights about the decisions\nthey make (local explanations).\nIn the ﬁrst group, the authors in [280] extend the usage of LRP to RNNs. They propose a speciﬁc\npropagation rule that works with multiplicative connections as those in LSTMs (Long Short Term Memory)\nunits and GRUs (Gated Recurrent Units). The authors in [281] propose a visualization technique based on\nﬁnite horizon n-grams that discriminates interpretable cells within LSTM and GRU networks. Following\nthe premise of not altering the architecture, [296] extends the interpretable mimic learning distillation\nmethod used for CNN models to LSTM networks, so that interpretable features are learned by ﬁtting\nGradient Boosting Trees to the trained LSTM network under focus.\nAside from the approaches that do not change the inner workings of the RNNs, [285] presents RETAIN\n(REverse Time AttentIoN) model, which detects inﬂuential past patterns by means of a two-level neural\nattention model. To create an interpretable RNN, the authors in [283] propose an RNN based on SISTA\n(Sequential Iterative Soft-Thresholding Algorithm) that models a sequence of correlated observations\nwith a sequence of sparse latent vectors, making its weights interpretable as the parameters of a principled\nstatistical model. Finally, [284] constructs a combination of an HMM (Hidden Markov Model) and an\nRNN, so that the overall model approach harnesses the interpretability of the HMM and the accuracy of\nthe RNN model.\n4.3.4. Hybrid Transparent and Black-box Methods\nThe use of background knowledge in the form of logical statements or constraints in Knowledge\nBases (KBs) has shown to not only improve explainability but also performance with respect to purely\ndata-driven approaches [297, 298, 299]. A positive side effect shown is that this hybrid approach provides\nrobustness to the learning system when errors are present in the training data labels. Other approaches\nhave shown to be able to jointly learn and reason with both symbolic and sub-symbolic representations\nand inference. The interesting aspect is that this blend allows for expressive probabilistic-logical reasoning\nin an end-to-end fashion [300]. A successful use case is on dietary recommendations, where explanations\nare extracted from the reasoning behind (non-deep but KB-based) models [301].\nFuture data fusion approaches may thus consider endowing DL models with explainability by external-\nizing other domain information sources. Deep formulation of classical ML models has been done, e.g. in\nDeep Kalman ﬁlters (DKFs) [302], Deep Variational Bayes Filters (DVBFs) [303], Structural Variational\nAutoencoders (SVAE) [304], or conditional random ﬁelds as RNNs [305]. These approaches provide\ndeep models with the interpretability inherent to probabilistic graphical models. For instance, SVAE\ncombines probabilistic graphical models in the embedding space with neural networks to enhance the\ninterpretability of DKFs. A particular example of classical ML model enhanced with its DL counterpart is\n27\n\n\nDeep Nearest Neighbors DkNN [264], where the neighbors constitute human-interpretable explanations\nof predictions. The intuition is based on the rationalization of a DNN prediction based on evidence.\nThis evidence consists of a characterization of conﬁdence termed credibility that spans the hierarchy of\nrepresentations within a DNN, that must be supported by the training data [264].\nMϕ\nBlack-box\nML model\nx\ny\nTransparent design methods\n• Decision Tree\n• (Fuzzy) rule-based learning\n• KNN\nPrediction\nExplanation\nFigure 10: Pictorial representation of a hybrid model. A neural network considered as a black-box can be explained by associating\nit to a more interpretable model such as a Decision Tree [306], a (fuzzy) rule-based system [19] or KNN [264].\nA different perspective on hybrid XAI models consists of enriching black-box models knowledge\nwith that one of transparent ones, as proposed in [24] and further reﬁned in [169] and [307]. In particular,\nthis can be done by constraining the neural network thanks to a semantic KB and bias-prone concepts\n[169], or by stacking ensembles jointly encompassing white- and black-box models [307].\nOther examples of hybrid symbolic and sub-symbolic methods where a knowledge-base tool or\ngraph-perspective enhances the neural (e.g., language [308]) model are in [309, 310]. In reinforcement\nlearning, very few examples of symbolic (graphical [311] or relational [75, 312]) hybrid models exist,\nwhile in recommendation systems, for instance, explainable autoencoders are proposed [313]. A speciﬁc\ntransformer architecture symbolic visualization method (applied to music) pictorially shows how soft-max\nattention works [314]. By visualizing self-reference, i.e., the last layer of attention weights, arcs show\nwhich notes in the past are informing the future and how attention is skip over less relevant sections.\nTransformers can also help explain image captions visually [315].\nAnother hybrid approach consists of mapping an uninterpretable black-box system to a white-box twin\nthat is more interpretable. For example, an opaque neural network can be combined with a transparent\nCase Based Reasoning (CBR) system [316, 317]. In [318], the DNN and the CBR (in this case a kNN) are\npaired in order to improve interpretability while keeping the same accuracy. The explanation by example\nconsists of analyzing the feature weights of the DNN which are then used in the CBR, in order to retrieve\nnearest-neighbor cases to explain the DNN’s prediction.\n4.4. Alternative Taxonomy of Post-hoc Explainability Techniques for Deep Learning\nDL is the model family where most research has been concentrated in recent times and they have\nbecome central for most of the recent literature on XAI. While the division between model-agnostic\nand model-speciﬁc is the most common distinction made, the community has not only relied on this\ncriteria to classify XAI methods. For instance, some model-agnostic methods such as SHAP [224] are\nwidely used to explain DL models. That is why several XAI methods can be easily categorized in\ndifferent taxonomy branches depending on the angle the method is looked at. An example is LIME\nwhich can also be used over CNNs, despite not being exclusive to deal with images. Searching within\nthe alternative DL taxonomy shows us that LIME can explicitly be used for Explaining a Deep Network\nProcessing, as a kind of Linear Proxy Model. Another type of classiﬁcation is indeed proposed in [13]\nwith a segmentation based on 3 categories. The ﬁrst category groups methods explaining the processing of\ndata by the network, thus answering to the question “why does this particular input leads to this particular\noutput?”. The second one concerns methods explaining the representation of data inside the network, i.e.,\nanswering to the question “what information does the network contain?”. The third approach concerns\n28\n\n\nmodels speciﬁcally designed to simplify the interpretation of their own behavior. Such a multiplicity of\nclassiﬁcation possibilities leads to different ways of constructing XAI taxonomies.\nXAI in DL\nExplanation of Deep\nNetwork Processing\nLinear Proxy Models\n[32]\nDecision Trees\n[82, 257, 258, 259]\nAutomatic-Rule Extraction\n[217, 251, 252, 253, 254, 255, 256, 319, 320, 321]\nSalience Mapping\n[96, 136, 261, 262, 272, 280, 322, 323]\nExplanation of Deep\nNetwork Representation\nRole of Layers\n[324, 325]\nRole of IndividualUnits\n[125, 326, 327, 328, 329]\nRole of RepresentationVectors\n[144]\nExplanation Producing\nSystems\nAttention Networks\n[267, 278, 330, 331, 332, 333, 334]\nRepresentation Disentanglement\n[113, 279, 335, 336, 337, 338, 339, 340, 341, 342]\nExplanation Generation\n[276, 343, 344, 345]\nHybrid Transparent\nand Black-box Methods\nNeural-symbolic Systems\n[297, 298, 299, 300]\nKB-enhanced Systems\n[24, 169, 301, 308, 309, 310]\nDeep Formulation\n[264, 302, 303, 304, 305]\nRelational Reasoning\n[75, 312, 313, 314]\nCase-base Reasoning\n[316, 317, 318]\nExplanation of Deep\nExplanation Producing\nLearning Representation\nExplanation of Deep\nNetwork Processing\nSystems\nSimpliﬁcation\nLocal Explanation\nText Explanation\nFeature Relevance\nArchitecture Modiﬁcation\nVisual explanation\nXAI in DL\nXAI in ML\nHybrid Transparent and\nBlack-box Methods\nTransparent Models\n(Fig. 11.a)\n(Fig. 6)\nExplanation by Example\n(a)\n(b)\nFigure 11: (a) Alternative Deep Learning speciﬁc taxonomy extended from the categorization from [13]; and (b) its connection to\nthe taxonomy in Figure 6.\nFigure 11 shows the alternative Deep Learning taxonomy inferred from [13]. From the latter, it can be\ndeduced the complementarity and overlapping of this taxonomy to Figure 6 as:\n• Some methods [272, 280] classiﬁed in distinct categories (namely feature relevance for CNN and\nfeature relevance for RNN) in Figure 6 are included in a single category (Explanation of Deep Network\nProcessing with Salience Mapping) when considering the classiﬁcation from [13].\n• Some methods [82, 144] are classiﬁed on a single category (Explanation by simpliﬁcation for Multi-\nLayer Neural Network) in Figure 6 while being in 2 different categories (namely, Explanation of Deep\nNetwork Processing with Decision Trees and Explanation of Deep Network Representation with the\nRole of Representation Vectors) in [13], as shown in Figure 11.\nA classiﬁcation based on explanations of model processing and explanations of model representation\nis relevant, as it leads to a differentiation between the execution trace of the model and its internal data\nstructure. This means that depending of the failure reasons of a complex model, it would be possible to\npick-up the right XAI method according to the information needed: the execution trace or the data structure.\nThis idea is analogous to testing and debugging methods used in regular programming paradigms [346].\n5. XAI: Opportunities, Challenges and Future Research Needs\nWe now capitalize on the performed literature review to put forward a critique of the achievements,\ntrends and challenges that are still to be addressed in the ﬁeld of explainability of ML and data fusion\nmodels. Actually our discussion on the advances taken so far in this ﬁeld has already anticipated some\nof these challenges. In this section we revisit them and explore new research opportunities for XAI,\nidentifying possible research paths that can be followed to address them effectively in years to come:\n29\n\n\n• When introducing the overview in Section 1 we already mentioned the existence of a tradeoff between\nmodel interpretability and performance, in the sense that making a ML model more understandable\ncould eventually degrade the quality of its produced decisions. In Subsection 5.1 we will stress on the\npotential of XAI developments to effectively achieve an optimal balance between the interpretability\nand performance of ML models.\n• In Subsection 2.2 we stressed on the imperative need for reaching a consensus on what explainability\nentails within the AI realm. Reasons for pursuing explainability are also assorted and, under our\nown assessment of the literature so far, not unambiguously mentioned throughout related works. In\nSubsection 5.2 we will further delve into this important issue.\n• Given its notable prevalence in the XAI literature, Subsections 4.3 and 4.4 revolved on the explainability\nof Deep Learning models, examining advances reported so far around a speciﬁc bibliographic taxonomy.\nWe go in this same direction with Subsection 5.3, which exposes several challenges that hold in regards\nto the explainability of this family of models.\n• Finally, we close up this prospective discussion with Subsections 5.4 to 5.8, which place on the table\nseveral research niches that despite its connection to model explainability, remain insufﬁciently studied\nby the community.\nBefore delving into these identiﬁed challenges, it is important to bear in mind that this prospective\nsection is complemented by Section 6, which enumerates research needs and open questions related to\nXAI within a broader context: the need for responsible AI.\n5.1. On the Tradeoff between Interpretability and Performance\nThe matter of interpretability versus performance is one that repeats itself through time, but as any\nother big statement, has its surroundings ﬁlled with myths and misconceptions.\nAs perfectly stated in [347], it is not necessarily true that models that are more complex are inherently\nmore accurate. This statement is false in cases in which the data is well structured and features at our\ndisposal are of great quality and value. This case is somewhat common in some industry environments,\nsince features being analyzed are constrained within very controlled physical problems, in which all of\nthe features are highly correlated, and not much of the possible landscape of values can be explored in\nthe data [348]. What can be hold as true, is that more complex models enjoy much more ﬂexibility than\ntheir simpler counterparts, allowing for more complex functions to be approximated. Now, returning to\nthe statement “models that are more complex are more accurate”, given the premise that the function\nto be approximated entails certain complexity, that the data available for study is greatly widespread\namong the world of suitable values for each variable and that there is enough data to harness a complex\nmodel, the statement presents itself as a true statement. It is in this situation that the trade-off between\nperformance and interpretability can be observed. It should be noted that the attempt at solving problems\nthat do not respect the aforementioned premises will fall on the trap of attempting to solve a problem that\ndoes not provide enough data diversity (variance). Hence, the added complexity of the model will only\nﬁght against the task of accurately solving the problem.\nIn this path toward performance, when the performance comes hand in hand with complexity, in-\nterpretability encounters itself on a downwards slope that until now appeared unavoidable. However,\nthe apparition of more sophisticated methods for explainability could invert or at least cancel that slope.\nFigure 12 shows a tentative representation inspired by previous works [7], in which XAI shows its\npower to improve the common trade-off between model interpretability and performance. Another aspect\nworth mentioning at this point due to its close link to model interpretability and performance is the\napproximation dilemma: explanations made for a ML model must be made drastic and approximate\nenough to match the requirements of the audience for which they are sought, ensuring that explanations\nare representative of the studied model and do not oversimplify its essential features.\n30\n\n\nModel interpretability\nModel\nSVM\nEnsembles\nBayesian Models\nDecision Trees\nGeneralized\nModels\nLinear/Logistic\nkNN\nAdditive\nRegression\nRule-based\nlearning\nLearning\nDeep\nModel accuracy\nPost-hoc explainability techniques\nInterpretability-driven model designs\nHybrid modelling approaches\nNew explainability-preserving modelling approaches\nInterpretable feature engineering\nXAI’s future\nresearch arena\nLow\nHigh\nLow\nHigh\nFigure 12: Trade-off between model interpretability and performance, and a representation of the area of improvement where the\npotential of XAI techniques and tools resides.\n5.2. On the Concept and Metrics\nThe literature clearly asks for an uniﬁed concept of explainability. In order for the ﬁeld to thrive,\nit is imperative to place a common ground upon which the community is enabled to contribute new\ntechniques and methods. A common concept must convey the needs expressed in the ﬁeld. It should\npropose a common structure for every XAI system. This paper attempted a new proposition of a concept\nof explainability that is built upon that from Gunning [7]. In that proposition and the following strokes to\ncomplete it (Subsection 2.2), explainability is deﬁned as the ability a model has to make its functioning\nclearer to an audience. To address it, post-hoc type methods exist. The concept portrayed in this survey\nmight not be complete but as it stands, allows for a ﬁrst common ground and reference point to sustain\na proﬁtable discussion in this matter. It is paramount that the ﬁeld of XAI reaches an agreement in this\nrespect combining the shattered efforts of a widespread ﬁeld behind the same banner.\nAnother key feature needed to relate a certain model to this concrete concept is the existence of a\nmetric. A metric, or group of them should allow for a meaningful comparison of how well a model ﬁts\nthe deﬁnition of explainable. Without such tool, any claim in this respect dilutes among the literature, not\nproviding a solid ground on which to stand. These metrics, as the classic ones (accuracy, F1, sensitivity...),\nshould express how well the model performs in a certain aspect of explainability. Some attempts have been\ndone recently around the measurement of XAI, as reviewed thoroughly in [349, 350]. In general, XAI\nmeasurements should evaluate the goodness, usefulness and satisfaction of explanations, the improvement\nof the mental model of the audience induced by model explanations, and the impact of explanations on the\nperformance of the model and on the trust and reliance of the audience. Measurement techniques surveyed\nin [349] and [350] (e.g., goodness checklist, explanation satisfaction scale, elicitation methods for mental\nmodels, computational measures for explainer ﬁdelity, explanation trustworthiness and model reliability)\nseem to be a good push in the direction of evaluating XAI techniques. Unfortunately, conclusions drawn\nfrom these overviews are aligned with our prospects on the ﬁeld: more quantiﬁable, general XAI metrics\nare really needed to support the existing measurement procedures and tools proposed by the community.\nThis survey does not tackle the problem of designing such a suite of metrics, since such a task should\nbe approached by the community as a whole prior acceptance of the broader concept of explainabil-\nity, which on the other hand, is one of the aims of the current work. Nevertheless, we advocate for\nfurther efforts towards new proposals to evaluate the performance of XAI techniques, as well as compar-\nison methodologies among XAI approaches that allow contrasting them quantitatively under different\n31\n\n\napplication context, models and purposes.\n5.3. Challenges to achieve Explainable Deep Learning\nWhile many efforts are currently being made in the area of XAI, there are still many challenges to be\nfaced before being able to obtain explainability in DL models. First, as explained in Subsection 2.2, there\nis a lack of agreement on the vocabulary and the different deﬁnitions surrounding XAI. As an example,\nwe often see the terms feature importance and feature relevance referring to the same concept. This is\neven more obvious for visualization methods, where there is absolutely no consistency behind what is\nknown as saliency maps, salient masks, heatmaps, neuron activations, attribution, and other approaches\nalike. As XAI is a relatively young ﬁeld, the community does not have a standardized terminology yet.\nAs it has been commented in Subsection 5.1, there is a trade-off between interpretability and accuracy\n[13], i.e., between the simplicity of the information given by the system on its internal functioning, and\nthe exhaustiveness of this description. Whether the observer is an expert in the ﬁeld, a policy-maker or a\nuser without machine learning knowledge, intelligibility does not have to be at the same level in order\nto provide the audience an understanding [6]. This is one of the reasons why, as mentioned above, a\nchallenge in XAI is establishing objective metrics on what constitutes a good explanation. A possibility\nto reduce this subjectivity is taking inspiration from experiments on human psychology, sociology or\ncognitive sciences to create objectively convincing explanations. Relevant ﬁndings to be considered when\ncreating an explainable AI model are highlighted in [12]: First, explanations are better when constrictive,\nmeaning that a prerequisite for a good explanation is that it does not only indicate why the model made a\ndecision X, but also why it made decision X rather than decision Y. It is also explained that probabilities\nare not as important as causal links in order to provide a satisfying explanation. Considering that black box\nmodels tend to process data in a quantitative manner, it would be necessary to translate the probabilistic\nresults into qualitative notions containing causal links. In addition, they state that explanations are\nselective, meaning that focusing solely on the main causes of a decision-making process is sufﬁcient. It\nwas also shown that the use of counterfactual explanations can help the user to understand the decision of\na model [40, 42, 351].\nCombining connectionist and symbolic paradigms seems a favourable way to address this challenge\n[169, 299, 312, 352, 353]. On one hand, connectionist methods are more precise but opaque. On the other\nhand, symbolic methods are popularly considered less efﬁcient, while they offer a greater explainability\nthus respecting the conditions mentioned above:\n• The ability to refer to established reasoning rules allows symbolic methods to be constrictive.\n• The use of a KB formalized e.g. by an ontology can allow data to be processed directly in a qualitative\nway.\n• Being selective is less straightforward for connectionist models than for symbolic ones.\nRecalling that a good explanation needs to inﬂuence the mental model of the user, i.e. the repre-\nsentation of the external reality using, among other things, symbols, it seems obvious that the use of\nthe symbolic learning paradigm is appropriate to produce an explanation. Therefore, neural-symbolic\ninterpretability could provide convincing explanations while keeping or improving generic performance\n[297].\nAs stated in [24], a truly explainable model should not leave explanation generation to the users as\ndifferent explanations may be deduced depending on their background knowledge. Having a semantic\nrepresentation of the knowledge can help a model to have the ability to produce explanations (e.g., in\nnatural language [169]) combining common sense reasoning and human-understandable features.\nFurthermore, until an objective metric has been adopted, it appears necessary to make an effort to\nrigorously formalize evaluation methods. One way may be drawing inspiration from the social sciences,\ne.g., by being consistent when choosing the evaluation questions and the population sample used [354].\n32\n\n\nA ﬁnal challenge XAI methods for DL need to address is providing explanations that are accessible\nfor society, policy makers and the law as a whole. In particular, conveying explanations that require\nnon-technical expertise will be paramount to both handle ambiguities, and to develop the social right to\nthe (not-yet available) right for explanation in the EU General Data Protection Regulation (GDPR) [355].\n5.4. Explanations for AI Security: XAI and Adversarial Machine Learning\nNothing has been said about conﬁdentiality concerns linked to XAI. One of the last surveys very\nbrieﬂy introduced the idea of algorithm property and trade secrets [14]. However, not much attention\nhas been payed to these concepts. If conﬁdential is the property that makes something secret, in the\nAI context many aspects involved in a model may hold this property. For example, imagine a model\nthat some company has developed through many years of research in a speciﬁc ﬁeld. The knowledge\nsynthesized in the model built might be considered to be conﬁdential, and it may be compromised even\nby providing only input and output access [356]. The latter shows that, under minimal assumptions,\ndata model functionality stealing is possible. An approach that has served to make DL models more\nrobust against intellectual property exposure based on a sequence of non accessible queries is in [357].\nThis recent work exposes the need for further research toward the development of XAI tools capable of\nexplaining ML models while keeping the model’s conﬁdentiality in mind.\nIdeally, XAI should be able to explain the knowledge within an AI model and it should be able to\nreason about what the model acts upon. However, the information revealed by XAI techniques can be used\nboth to generate more effective attacks in adversarial contexts aimed at confusing the model, at the same\ntime as to develop techniques to better protect against private content exposure by using such information.\nAdversarial attacks [358] try to manipulate a ML algorithm after learning what is the speciﬁc information\nthat should be fed to the system so as to lead it to a speciﬁc output. For instance, regarding a supervised\nML classiﬁcation model, adversarial attacks try to discover the minimum changes that should be applied\nto the input data in order to cause a different classiﬁcation. This has happened regarding computer vision\nsystems of autonomous vehicles; a minimal change in a stop signal, imperceptible to the human eye, led\nvehicles to detect it as a 45 mph signal [359]. For the particular case of DL models, available solutions\nsuch as Cleverhans [360] seek to detect adversarial vulnerabilities, and provide different approaches\nto harden the model against them. Other examples include AlfaSVMLib [361] for SVM models, and\nAdversarialLib [362] for evasion attacks. There are even available solutions for unsupervised ML, like\nclustering algorithms [363].\nWhile XAI techniques can be used to furnish more effective adversarial attacks or to reveal conﬁdential\naspects of the model itself, some recent contributions have capitalized on the possibilities of Generative\nAdversarial Networks (GANs [364]), Variational Autoencoders [365] and other generative models towards\nexplaining data-based decisions. Once trained, generative models can generate instances of what they\nhave learned based on a noise input vector that can be interpreted as a latent representation of the data at\nhand. By manipulating this latent representation and examining its impact on the output of the generative\nmodel, it is possible to draw insights and discover speciﬁc patterns related to the class to be predicted.\nThis generative framework has been adopted by several recent studies [366, 367] mainly as an attribution\nmethod to relate a particular output of a Deep Learning model to their input variables. Another interesting\nresearch direction is the use of generative models for the creation of counterfactuals, i.e., modiﬁcations\nto the input data that could eventually alter the original prediction of the model [368]. Counterfactual\nprototypes help the user understand the performance boundaries of the model under consideration for\nhis/her improved trust and informed criticism. In light of this recent trend, we deﬁnitely believe that there\nis road ahead for generative ML models to take their part in scenarios demanding understandable machine\ndecisions.\n5.5. XAI and Output Conﬁdence\nSafety issues have also been studied in regards to processes that depend on the output of AI models,\nsuch as vehicular perception and self-driving in autonomous vehicles, automated surgery, data-based\n33\n\n\nsupport for medical diagnosis, insurance risk assessment and cyber-physical systems in manufacturing,\namong others [369]. In all these scenarios erroneous model outputs can lead to harmful consequences,\nwhich has yielded comprehensive regulatory efforts aimed at ensuring that no decision is made solely on\nthe basis of data processing [3].\nIn parallel, research has been conducted towards minimizing both risk and uncertainty of harms\nderived from decisions made on the output of a ML model. As a result, many techniques have been\nreported to reduce such a risk, among which we pause at the evaluation of the model’s output conﬁdence\nto decide upon. In this case, the inspection of the share of epistemic uncertainty (namely, the uncertainty\ndue to lack of knowledge) of the input data and its correspondence with the model’s output conﬁdence\ncan inform the user and eventually trigger his/her rejection of the model’s output [370, 371]. To this end,\nexplaining via XAI techniques which region of the input data the model is focused on when producing a\ngiven output can discriminate possible sources of epistemic uncertainty within the input domain.\n5.6. XAI, Rationale Explanation, and Critical Data Studies\nWhen shifting the focus to the research practices seen in Data Science, it has been noted that\nreproducibility is stringently subject not only to the mere sharing of data, models and results to the\ncommunity, but also to the availability of information about the full discourse around data collection,\nunderstanding, assumptions held and insights drawn from model construction and results’ analyses [372].\nIn other words, in order to transform data into a valuable actionable asset, individuals must engage in\ncollaborative sense-making by sharing the context producing their ﬁndings, wherein context refers to sets\nof narrative stories around how data were processed, cleaned, modeled and analyzed. In this discourse\nwe ﬁnd also an interesting space for the adoption of XAI techniques due to their powerful ability to\ndescribe black-box models in an understandable, hence conveyable fashion towards colleagues from\nSocial Science, Politics, Humanities and Legal ﬁelds.\nXAI can effectively ease the process of explaining the reasons why a model reached a decision in an\naccessible way to non-expert users, i.e. the rationale explanation. This conﬂuence of multi-disciplinary\nteams in projects related to Data Science and the search for methodologies to make them appraise the\nethical implications of their data-based choices has been lately coined as Critical Data studies [373]. It\nis in this ﬁeld where XAI can signiﬁcantly boost the exchange of information among heterogeneous\naudiences about the knowledge learned by models.\n5.7. XAI and Theory-guided Data Science\nWe envision an exciting synergy between the XAI realm and Theory-guided Data Science, a paradigm\nexposed in [374] that merges both Data Science and the classic theoretical principles underlying the\napplication/context where data are produced. The rationale behind this rising paradigm is the need for data-\nbased models to generate knowledge that is the prior knowledge brought by the ﬁeld in which it operates.\nThis means that the model type should be chosen according to the type of relations we intend to encounter.\nThe structure should also follow what is previously known. Similarly, the training approach should not\nallow for the optimization process to enter regions that are not plausible. Accordingly, regularization\nterms should stand the prior premises of the ﬁeld, avoiding the elimination of badly represented true\nrelations for spurious and deceptive false relations. Finally, the output of the model should inform about\neverything the model has come to learn, allowing to reason and merge the new knowledge with what was\nalready known in the ﬁeld.\nMany examples of the implementation of this approach are currently available with promising results.\nThe studies in [375]-[382] were carried out in diverse ﬁelds, showcasing the potential of this new paradigm\nfor data science. Above all, it is relevant to notice the resemblance that all concepts and requirements of\nTheory-guided Data Science share with XAI. All the additions presented in [374] push toward techniques\nthat would eventually render a model explainable, and furthermore, knowledge consistent. The concept\nof knowledge from the beginning, central to Theory-guided Data Science, must also consider how\n34\n\n\nthe knowledge captured by a model should be explained for assessing its compliance with theoretical\nprinciples known beforehand. This, again, opens a magniﬁcent window of opportunity for XAI.\n5.8. Guidelines for ensuring Interpretable AI Models\nRecent surveys have emphasized on the multidisciplinary, inclusive nature of the process of making\nan AI-based model interpretable. Along this process, it is of utmost importance to scrutinize and take into\nproper account the interests, demands and requirements of all stakeholders interacting with the system to\nbe explained, from the designers of the system to the decision makers consuming its produced outputs\nand users undergoing the consequences of decisions made therefrom.\nGiven the conﬂuence of multiple criteria and the need for having the human in the loop, some\nattempts at establishing the procedural guidelines to implement and explain AI systems have been recently\ncontributed. Among them, we pause at the thorough study in [383], which suggests that the incorporation\nand consideration of explainability in practical AI design and deployment workﬂows should comprise\nfour major methodological steps:\n1. Contextual factors, potential impacts and domain-speciﬁc needs must be taken into account when\ndevising an approach to interpretability: These include a thorough understanding of the purpose for\nwhich the AI model is built, the complexity of explanations that are required by the audience, and the\nperformance and interpretability levels of existing technology, models and methods. The latter pose a\nreference point for the AI system to be deployed in lieu thereof.\n2. Interpretable techniques should be preferred when possible: when considering explainability in the\ndevelopment of an AI system, the decision of which XAI approach should be chosen should gauge\ndomain-speciﬁc risks and needs, the available data resources and existing domain knowledge, and the\nsuitability of the ML model to meet the requirements of the computational task to be addressed. It is in\nthe conﬂuence of these three design drivers where the guidelines postulated in [383] (and other studies\nin this same line of thinking [384]) recommend ﬁrst the consideration of standard interpretable models\nrather than sophisticated yet opaque modeling methods. In practice, the aforementioned aspects\n(contextual factors, impacts and domain-speciﬁc needs) can make transparent models preferable\nover complex modeling alternatives whose interpretability require the application of post-hoc XAI\ntechniques. By contrast, black-box models such as those reviewed in this work (namely, support\nvector machines, ensemble methods and neural networks) should be selected only when their superior\nmodeling capabilities ﬁt best the characteristics of the problem at hand.\n3. If a black-box model has been chosen, the third guideline establishes that ethics-, fairness- and safety-\nrelated impacts should be weighed. Speciﬁcally, responsibility in the design and implementation of\nthe AI system should be ensured by checking whether such identiﬁed impacts can be mitigated and\ncounteracted by supplementing the system with XAI tools that provide the level of explainability\nrequired by the domain in which it is deployed. To this end, the third guideline suggests 1) a detailed\narticulation, examination and evaluation of the applicable explanatory strategies, 2) the analysis of\nwhether the coverage and scope of the available explanatory approaches match the requirements of\nthe domain and application context where the model is to be deployed; and 3) the formulation of\nan interpretability action plan that sets forth the explanation delivery strategy, including a detailed\ntime frame for the execution of the plan, and a clearance of the roles and responsibilities of the team\ninvolved in the workﬂow.\n4. Finally, the fourth guideline encourages to rethink interpretability in terms of the cognitive skills,\ncapacities and limitations of the individual human. This is an important question on which studies\non measures of explainability are intensively revolving by considering human mental models, the\naccessibility of the audience to vocabularies of explanatory outcomes, and other means to involve the\nexpertise of the audience into the decision of what explanations should provide.\n35\n\n\nWe foresee that the set of guidelines proposed in [383] and summarized above will be complemented\nand enriched further by future methodological studies, ultimately heading to a more responsible use of AI.\nMethodological principles ensure that the purpose for which explainability is pursued is met by bringing\nthe manifold of requirements of all participants into the process, along with other universal aspects of\nequal relevance such as no discrimination, sustainability, privacy or accountability. A challenge remains\nin harnessing the potential of XAI to realize a Responsible AI, as we discuss in the next section.\n6. Toward Responsible AI: Principles of Artiﬁcial Intelligence, Fairness, Privacy and Data Fusion\nOver the years many organizations, both private and public, have published guidelines to indicate\nhow AI should be developed and used. These guidelines are commonly referred to as AI principles, and\nthey tackle issues related to potential AI threats to both individuals and to the society as a whole. This\nsection presents some of the most important and widely recognized principles in order to link XAI –\nwhich normally appears inside its own principle – to all of them. Should a responsible implementation\nand use of AI models be sought in practice, it is our ﬁrm claim that XAI does not sufﬁce on its own. Other\nimportant principles of Artiﬁcial Intelligence such as privacy and fairness must be carefully addressed\nin practice. In the following sections we elaborate on the concept of Responsible AI, along with the\nimplications of XAI and data fusion in the fulﬁllment of its postulated principles.\n6.1. Principles of Artiﬁcial Intelligence\nA recent review of some of the main AI principles published since 2016 appears in [385]. In this\nwork, the authors show a visual framework where different organizations are classiﬁed according to the\nfollowing parameters:\n• Nature, which could be private sector, government, inter-governmental organization, civil society or\nmultistakeholder.\n• Content of the principles: eight possible principles such as privacy, explainability, or fairness. They\nalso consider the coverage that the document grants for each of the considered principles.\n• Target audience: to whom the principles are aimed. They are normally for the organization that\ndeveloped them, but they could also be destined for another audience (see Figure 2).\n• Whether or not they are rooted on the International Human Rights, as well as whether they explicitly\ntalk about them.\nFor instance, [386] is an illustrative example of a document of AI principles for the purpose of\nthis overview, since it accounts for some of the most common principles, and deals explicitly with\nexplainability. Here, the authors propose ﬁve principles mainly to guide the development of AI within\ntheir company, while also indicating that they could also be used within other organizations and businesses.\nThe authors of those principles aim to develop AI in a way that it directly reinforces inclusion, gives\nequal opportunities for everyone, and contributes to the common good. To this end, the following aspects\nshould be considered:\n• The outputs after using AI systems should not lead to any kind of discrimination against individuals\nor collectives in relation to race, religion, gender, sexual orientation, disability, ethnic, origin or any\nother personal condition. Thus, a fundamental criteria to consider while optimizing the results of an AI\nsystem is not only their outputs in terms of error optimization, but also how the system deals with those\ngroups. This deﬁnes the principle of Fair AI.\n36\n\n\n• People should always know when they are communicating with a person, and when they are commu-\nnicating with an AI system. People should also be aware if their personal information is being used\nby the AI system and for what purpose. It is crucial to ensure a certain level of understanding about\nthe decisions taken by an AI system. This can be achieved through the usage of XAI techniques. It\nis important that the generated explanations consider the proﬁle of the user that will receive those\nexplanations (the so-called audience as per the deﬁnition given in Subsection 2.2) in order to adjust the\ntransparency level, as indicated in [45]. This deﬁnes the principle of Transparent and Explainable AI.\n• AI products and services should always be aligned with the United Nation’s Sustainable Development\nGoals [387] and contribute to them in a positive and tangible way. Thus, AI should always generate\na beneﬁt for humanity and the common good. This deﬁnes the principle of Human-centric AI (also\nreferred to as AI for Social Good [388]).\n• AI systems, specially when they are fed by data, should always consider privacy and security standards\nduring all of its life cycle. This principle is not exclusive of AI systems since it is shared with many\nother software products. Thus, it can be inherited from processes that already exist within a company.\nThis deﬁnes the principle of Privacy and Security by Design, which was also identiﬁed as one of\nthe core ethical and societal challenges faced by Smart Information Systems under the Responsible\nResearch and Innovation paradigm (RRI, [389]). RRI refers to a package of methodological guidelines\nand recommendations aimed at considering a wider context for scientiﬁc research, from the perspective\nof the lab to global societal challenges such as sustainability, public engagement, ethics, science\neducation, gender equality, open access, and governance. Interestingly, RRI also requires openness and\ntransparency to be ensured in projects embracing its principles, which links directly to the principle of\nTransparent and Explainable AI mentioned previously.\n• The authors emphasize that all these principles should always be extended to any third-party (providers,\nconsultants, partners...).\nGoing beyond the scope of these ﬁve AI principles, the European Commission (EC) has recently\npublished ethical guidelines for Trustworthy AI [390] through an assessment checklist that can be\ncompleted by different proﬁles related to AI systems (namely, product managers, developers and other\nroles). The assessment is based in a series of principles: 1) human agency and oversight; 2) technical\nrobustness and safety; 3) privacy and data governance; 4) transparency, diversity, non-discrimination and\nfairness; 5) societal and environmental well-being; 6) accountability. These principles are aligned with\nthe ones detailed in this section, though the scope for the EC principles is more general, including any\ntype of organization involved in the development of AI.\nIt is worth mentioning that most of these AI principles guides directly approach XAI as a key aspect\nto consider and include in AI systems. In fact, the overview for these principles introduced before [385],\nindicates that 28 out of the 32 AI principles guides covered in the analysis, explicitly include XAI as a\ncrucial component. Thus, the work and scope of this article deals directly with one of the most important\naspects regarding AI at a worldwide level.\n6.2. Fairness and Accountability\nAs mentioned in the previous section, there are many critical aspects, beyond XAI, included within\nthe different AI principles guidelines published during the last decade. However, those aspects are not\ncompletely detached from XAI; in fact, they are intertwined. This section presents two key components\nwith a huge relevance within the AI principles guides, Fairness and Accountability. It also highlights how\nthey are connected to XAI.\n37\n\n\n6.2.1. Fairness and Discrimination\nIt is in the identiﬁcation of implicit correlations between protected and unprotected features where\nXAI techniques ﬁnd their place within discrimination-aware data mining methods. By analyzing how\nthe output of the model behaves with respect to the input feature, the model designer may unveil hidden\ncorrelations between the input variables amenable to cause discrimination. XAI techniques such as SHAP\n[224] could be used to generate counterfactual outcomes explaining the decisions of a ML model when\nfed with protected and unprotected variables.\nRecalling the Fair AI principle introduced in the previous section, [386] reminds that fairness is a\ndiscipline that generally includes proposals for bias detection within datasets regarding sensitive data that\naffect protected groups (through variables like gender, race...). Indeed, ethical concerns with black-box\nmodels arise from their tendency to unintentionally create unfair decisions by considering sensitive factors\nsuch as the individual’s race, age or gender [391]. Unfortunately, such unfair decisions can give rise to\ndiscriminatory issues, either by explicitly considering sensitive attributes or implicitly by using factors\nthat correlate with sensitive data. In fact, an attribute may implicitly encode a protected factor, as occurs\nwith postal code in credit rating [392]. The aforementioned proposals centered on fairness aspects permit\nto discover correlations between non-sensitive variables and sensitive ones, detect imbalanced outcomes\nfrom the algorithms that penalize a speciﬁc subgroup of people (discrimination), and mitigate the effect\nof bias on the model’s decisions. These approaches can deal with:\n• Individual fairness: here, fairness is analyzed by modeling the differences between each subject and the\nrest of the population.\n• Group fairness: it deals with fairness from the perspective of all individuals.\n• Counterfactual fairness: it tries to interpret the causes of bias using, for example, causal graphs.\nThe sources for bias, as indicated in [392], can be traced to:\n• Skewed data: bias within the data acquisition process.\n• Tainted data: errors in the data modelling deﬁnition, wrong feature labelling, and other possible causes.\n• Limited features: using too few features could lead to an inference of false feature relationships that\ncan lead to bias.\n• Sample size disparities: when using sensitive features, disparities between different subgroups can\ninduce bias.\n• Proxy features: there may be correlated features with sensitive ones that can induce bias even when the\nsensitive features are not present in the dataset.\nThe next question that can be asked is what criteria could be used to deﬁne when AI is not biased. For\nsupervised ML, [393] presents a framework that uses three criteria to evaluate group fairness when there\nis a sensitive feature present within the dataset:\n• Independence: this criterion is fulﬁlled when the model predictions are independent of the sensitive\nfeature. Thus, the proportion of positive samples (namely, those ones belonging to the class of interest)\ngiven by the model is the same for all the subgroups within the sensitive feature.\n• Separation: it is met when the model predictions are independent of the sensitive feature given the\ntarget variable. For instance, in classiﬁcation models, the True Positive (TP) rate and the False Positive\n(FP) rate are the same in all the subgroups within the sensitive feature. This criteria is also known as\nEqualized Odds.\n38\n\n\n• Sufﬁciency: it is accomplished when the target variable is independent of the sensitive feature given\nthe model output. Thus, the Positive Predictive Value is the same for all subgroups within the sensitive\nfeature. This criteria is also known as Predictive Rate Parity.\nAlthough not all of the criteria can be fulﬁlled at the same time, they can be optimized together in\norder to minimize the bias within the ML model.\nThere are two possible actions that could be used in order to achieve those criteria. On one hand,\nevaluation includes measuring the amount of bias present within the model (regarding one of the criteria\naforementioned). There are many different metrics that can be used, depending on the criteria considered.\nRegarding independence criterion, possible metrics are statistical parity difference or disparate impact.\nIn case of the separation criterion, possible metrics are equal opportunity difference and average odds\ndifference [393]. Another possible metric is the Theil index [394], which measures inequality both in\nterms of individual and group fairness.\nOn the other hand, mitigation refers to the process of ﬁxing some aspects in the model in order to\nremove the effect of the bias in terms of one or several sensitive features. Several techniques exist within\nthe literature, classiﬁed in the following categories:\n• Pre-processing: these groups of techniques are applied before the ML model is trained, looking to\nremove the bias at the ﬁrst step of the learning process. An example is Reweighing [395], which\nmodiﬁes the weights of the features in order to remove discrimination in sensitive attributes. Another\nexample is [396], which hinges on transforming the input data in order to ﬁnd a good representation\nthat obfuscates information about membership in sensitive features.\n• In-processing: these techniques are applied during the training process of the ML model. Normally,\nthey include Fairness optimization constraints along with cost functions of the ML model. An example\nis Adversarial Debiasing, [397]. This technique optimizes jointly the ability of predicting the target\nvariable while minimizing the ability of predicting sensitive features using a GAN.\n• Post-processing: these techniques are applied after the ML model is trained. They are less intrusive\nbecause they do not modify the input data or the ML model. An example is Equalized Odds [393]. This\ntechniques allows to adjust the thresholds in the classiﬁcation model in order to reduce the differences\nbetween the TP rate and the FP rate for each sensitive subgroup.\nEven though these references apparently address an AI principle that appears to be independent of\nXAI, the literature shows that they are intertwined. For instance, the survey in [385] evinces that 26 out\nof the 28 AI principles that deal with XAI, also talk about fairness explicitly. This fact elucidates that\norganizations usually consider both aspects together when implementing Responsible AI.\nThe literature also exploses that XAI proposals can be used for bias detection. For example, [398]\nproposes a framework to visually analyze the bias present in a model (both for individual and group\nfairness). Thus, the fairness report is shown just like the visual summaries used within XAI. This\nexplainability approach eases the understanding and measurement of bias. The system must report that\nthere is bias, justify it quantitatively, indicate the degree of fairness, and explain why a user or group\nwould be treated unfairly with the available data. Similarly, XAI techniques such as SHAP [224] could be\nused to generate counterfactual outcomes explaining the decisions of a ML model when fed with protected\nand unprotected variables. By identifying implicit correlations between protected and unprotected features\nthrough XAI techniques, the model designer may unveil hidden correlations between the input variables\namenable to cause discrimination.\nAnother example is [399], where the authors propose a fair-by-design approach in order to develop\nML models that jointly have less bias and include as explanations human comprehensible rules. The\nproposal is based in self-learning locally generative models that use only a small part of the whole\ndataset available (weak supervision). It ﬁrst ﬁnds recursively relevant prototypes within the dataset, and\n39\n\n\nextracts the empirical distribution and density of the points around them. Then it generates rules in an\nIF/THEN format that explain that a data point is classiﬁed within a speciﬁc category because it is similar\nto some prototypes. The proposal then includes an algorithm that both generates explanations and reduces\nbias, as it is demonstrated for the use case of recidivism using the Correctional Offender Management\nProﬁling for Alternative Sanctions (COMPAS) dataset [400]. The same goal has been recently pursued in\n[401], showing that post-hoc XAI techniques can forge fairer explanations from truly unfair black-box\nmodels. Finally, CERTIFAI (Counterfactual Explanations for Robustness, Transparency, Interpretability,\nand Fairness of Artiﬁcial Intelligence models) [402] uses a customized genetic algorithm to generate\ncounterfactuals that can help to see the robustness of a ML model, generate explanations, and examine\nfairness (both at the individual level and at the group level) at the same time.\nStrongly linked to the concept of fairness, much attention has been lately devoted to the concept of\ndata diversity, which essentially refers to the capability of an algorithmic model to ensure that all different\ntypes of objects are represented in its output [403]. Therefore, diversity can be thought to be an indicator\nof the quality of a collection of items that, when taking the form of a model’s output, can quantify the\nproneness of the model to produce diverse results rather than highly accurate predictions. Diversity comes\ninto play in human-centered applications with ethical restrictions that permeate to the AI modeling phase\n[404]. Likewise, certain AI problems (such as content recommendation or information retrieval) also\naim at producing diverse recommendations rather than highly-scoring yet similar results [405, 406]. In\nthese scenarios, dissecting the internals of a black-box model via XAI techniques can help identifying the\ncapability of the model to maintain the input data diversity at its output. Learning strategies to endow a\nmodel with diversity keeping capabilities could be complemented with XAI techniques in order to shed\ntransparency over the model internals, and assess the effectiveness of such strategies with respect to the\ndiversity of the data from which the model was trained. Conversely, XAI could help to discriminate which\nparts of the model are compromising its overall ability to preserve diversity.\n6.2.2. Accountability\nRegarding accountability, the EC [390] deﬁnes the following aspects to consider:\n• Auditability: it includes the assessment of algorithms, data and design processes, but preserving the\nintellectual property related to the AI systems. Performing the assessment by both internal and external\nauditors, and making the reports available, could contribute to the trustworthiness of the technology.\nWhen the AI system affects fundamental rights, including safety-critical applications, it should always\nbe audited by an external third party.\n• Minimization and reporting of negative impacts: it consists of reporting actions or decisions that yield\na certain outcome by the system. It also comprises the assessment of those outcomes and how to\nrespond to them. To address that, the development of AI systems should also consider the identiﬁcation,\nassessment, documentation and minimization of their potential negative impacts. In order to minimize\nthe potential negative impact, impact assessments should be carried out both prior to and during the\ndevelopment, deployment and use of AI systems. It is also important to guarantee protection for anyone\nwho raises concerns about an AI system (e.g., whistle-blowers). All assessments must be proportionate\nto the risk that the AI systems pose.\n• Trade-offs: in case any tension arises due to the implementation of the above requirements, trade-offs\ncould be considered but only if they are ethically acceptable. Such trade-offs should be reasoned,\nexplicitly acknowledged and documented, and they must be evaluated in terms of their risk to ethical\nprinciples. The decision maker must be accountable for the manner in which the appropriate trade-off\nis being made, and the trade-off decided should be continually reviewed to ensure the appropriateness\nof the decision. If there is no ethically acceptable trade-off, the development, deployment and use of\nthe AI system should not proceed in that form.\n40\n\n\n• Redress: it includes mechanisms that ensure an adequate redress for situations when unforeseen unjust\nadverse impacts take place. Guaranteeing a redress for those non-predicted scenarios is a key to ensure\ntrust. Special attention should be paid to vulnerable persons or groups.\nThese aspects addressed by the EC highlight different connections of XAI with accountability. First,\nXAI contributes to auditability as it can help explaining AI systems for different proﬁles, including\nregulatory ones. Also, since there is a connection between fairness and XAI as stated before, XAI can\nalso contribute to the minimization and report of negative impacts.\n6.3. Privacy and Data Fusion\nThe ever-growing number of information sources that nowadays coexist in almost all domains of\nactivity calls for data fusion approaches aimed at exploiting them simultaneously toward solving a learning\ntask. By merging heterogeneous information, data fusion has been proven to improve the performance of\nML models in many applications, such as industrial prognosis [348], cyber-physical social systems [407]\nor the Internet of Things [408], among others. This section speculates with the potential of data fusion\ntechniques to enrich the explainability of ML models, and to compromise the privacy of the data from\nwhich ML models are learned. To this end, we brieﬂy overview different data fusion paradigms, and later\nanalyze them from the perspective of data privacy. As we will later, despite its relevance in the context of\nResponsible AI, the conﬂuence between XAI and data fusion is an uncharted research area in the current\nresearch mainstream.\n6.3.1. Basic Levels of Data Fusion\nWe depart from the different levels of data fusion that have been identiﬁed in comprehensive surveys\non the matter [409, 410, 411, 412]. In the context of this subsection, we will distinguish among fusion at\ndata level, fusion at model level and fusion at knowledge level. Furthermore, a parallel categorization can\nbe established depending on where such data is processed and fused, yielding centralized and distributed\nmethods for data fusion. In a centralized approach, nodes deliver their locally captured data to a centralized\nprocessing system to merge them together. In contrast, in a distributed approach, each of the nodes merges\nits locally captured information, eventually sharing the result of the local fusion with its counterparts.\nFusion through the information generation process has properties and peculiarities depending on\nthe level at which the fusion is performed. At the so-called data level, fusion deals with raw data. As\nschematically shown in Figure 13, a fusion model at this stage receives raw data from different information\nsources, and combines them to create a more coherent, compliant, robust or simply representative data\nﬂow. On the other hand, fusion at the model level aggregates models, each learned from a subset of the\ndata sets that were to be fused. Finally, at the knowledge level the fusion approach deals with knowledge in\nthe form of rules, ontologies or other knowledge representation techniques with the intention of merging\nthem to create new, better or more complete knowledge from what was originally provided. Structured\nknowledge information is extracted from each data source and for every item in the data set using multiple\nknowledge extractors (e.g. a reasoning engine operating on an open semantic database). All produced\ninformation is then fused to further ensure the quality, correctness and manageability of the produced\nknowledge about the items in the data set.\nOther data fusion approaches exist beyonds the ones represented in Figure 13. As such, data-level\nfusion can be performed either by a technique speciﬁcally devoted to this end (as depicted in Figure 13.b)\nor, instead, performed along the learning process of the ML model (as done in e.g. DL models). Similarly,\nmodel-level data fusion can be made by combining the decisions of different models (as done in tree\nensembles).\n6.3.2. Emerging Data Fusion Approaches\nIn the next subsection we examine other data fusion approaches that have recently come into scene\ndue to their implications in terms of data privacy:\n41\n\n\nD1\nD2\nDN\n...\nD1\nD2\nDN\n...\n(d)\n(e)\n(f)\n...\n...\nW1\nSplit\nW2\nW3\nWN\nMap\nReduce\nR\n∼ML\nML1\nML2\nMLN\n...\nAggregation\nSecure\nClient-server\ndelivery\n: encrypted model\n: model update\ninformation (e.g.\nServer side\nRemote clients\ngradients)\nWn: n-th worker node\nR: reducer node\n...\nD1\nD2\nDN\n...\nMLN\n...\nView 1\nView 2\nView 3\nView V\n...\nML1\nML2\nML3\nMLV\n...\nJoint optimization\n+ Fusion\nD1\nD2\nDN\nDF\nD1\nD2\nDN\nML\nML\nML\nML\nData Fusion technique\nModel Fusion\n...\n...\n...\nD1\nD2\nDN\n...\nKnowledge\nextractor\nKnowledge\nextractor\nKnowledge\nextractor\nKnowledge Fusion\nKB\n...\nML\nKB\nDi\nDF\n...\n(a)\n(b)\n(c)\n: Predictions\n: Knowledge Base\n: Fused data\n: i-th dataset\nFigure 13: Diagrams showing different levels at which data fusion can be performed: (a) data level; (b) model level; (c) knowledge\nlevel; (d) Big Data fusion; (e) Federated Learning and (f) Multiview Learning.\n• In Big Data fusion (Figure 13.d), local models are learned on a split of the original data sources, each\nsubmitted to a Worker node in charge of performing this learning process (Map task). Then, a Reduce\nnode (or several Reduce nodes, depending on the application) combines the outputs produced by each\nMap task. Therefore, Big Data fusion can be conceived as a means to distribute the complexity of learn-\ning a ML model over a pool of Worker nodes, wherein the strategy to design how information/models\nare fused together between the Map and the Reduce tasks is what deﬁnes the quality of the ﬁnally\ngenerated outcome [413].\n• By contrast, in Federated Learning [414, 415, 416], the computation of ML models is made on data\ncaptured locally by remote client devices (Figure 13.e). Upon local model training, clients transmit\nencrypted information about their learned knowledge to a central server, which can take the form of\nlayer-wise gradients (in the case of neural ML models) or any other model-dependent content alike. The\ncentral server aggregates (fuses) the knowledge contributions received from all clients to yield a shared\nmodel harnessing the collected information from the pool of clients. It is important to observe that no\nclient data is delivered to the central server, which elicits the privacy-preserving nature of Federated\nLearning. Furthermore, computation is set closer to the collected data, which reduces the processing\nlatency and alleviates the computational burden of the central server.\n• Finally, Multiview Learning [417] constructs different views of the object as per the information\ncontained in the different data sources (Figure 13.f). These views can be produced from multiple\nsources of information and/or different feature subsets [418]. Multiview Learning devises strategies\nto jointly optimize ML models learned from the aforementioned views to enhance the generalization\nperformance, specially in those applications with weak data supervision and hence, prone to model\noverﬁtting. This joint optimization resorts to different algorithmic means, from co-training to co-\nregularization [419].\n6.3.3. Opportunities and Challenges in Privacy and Data Fusion under the Responsible AI Paradigm\nAI systems, specially when dealing with multiple data sources, need to explicitly include privacy\nconsiderations during the system’s life cycle. This is specially critical when working with personal data,\n42\n\n\nbecause respecting people’s right to privacy should always be addressed. The EC highlights that privacy\nshould also address data governance, covering the quality and integrity of the used data [390]. It should\nalso include the deﬁnition of access protocols and the capability to process data in a way that ensures\nprivacy. The EC guide breaks down the privacy principle into three aspects:\n• Privacy and data protection: they should be guaranteed in AI systems throughout its entire lifecycle. It\nincludes both information provided by users and information generated about those users derived from\ntheir interactions with the system. Since digital information about a user could be used in a negative\nway against them (discrimination due to sensitive features, unfair treatment...), it is crucial to ensure\nproper usage of all the data collected.\n• Quality and integrity of data: quality of data sets is fundamental to reach good performance with AI\nsystems that are fueled with data, like ML. However, sometimes the data collected contains socially\nconstructed biases, inaccuracies, errors and mistakes. This should be tackled before training any model\nwith the data collected. Additionally, the integrity of the data sets should be ensured.\n• Access to data: if there is individual personal data, there should always be data protocols for data\ngovernance. These protocols should indicate who may access data and under which circumstances.\nThe aforementioned examples from the EC shows how data fusion is directly intertwined with privacy\nand with fairness, regardless of the technique employed for it.\nNotwithstanding this explicit concern from regulatory bodies, loss of privacy has been compromised\nby DL methods in scenarios where no data fusion is performed. For instance, a few images are enough\nto threaten users’ privacy even in the presence of image obfuscation [420], and the model parameters of\na DNN can be exposed by simply performing input queries on the model [356, 357]. An approach to\nexplain loss of privacy is by using privacy loss and intent loss subjective scores. The former provides a\nsubjective measure of the severity of the privacy violation depending on the role of a face in the image,\nwhile the latter captures the intent of the bystanders to appear in the picture. These kind of explanations\nhave motivated, for instance, secure matching cryptographic protocols for photographer and bystanders to\npreserve privacy [356, 421, 422]. We deﬁnite advocate for more efforts invested in this direction, namely,\nin ensuring that XAI methods do not pose a threat in regards to the privacy of the data used for training\nthe ML model under target.\nWhen data fusion enters the picture, different implications arise with the context of explainability\ncovered in this survey. To begin with, classical techniques for fusion at the data level only deal with data\nand have no connection to the ML model, so they have little to do with explainability. However, the\nadvent of DL models has blurred the distinction between information fusion and predictive modeling. The\nﬁrst layers of DL architectures are in charge of learning high-level features from raw data that possess\nrelevance for the task at hand. This learning process can be thought to aim at solving a data level fusion\nproblem, yet in a directed learning fashion that makes the fusion process tightly coupled to the task to be\nsolved.\nIn this context, many techniques in the ﬁeld of XAI have been proposed to deal with the analysis of\ncorrelation between features. This paves the way to explaining how data sources are actually fused through\nthe DL model, which can yield interesting insights on how the predictive task at hand induces correlations\namong the data sources over the spatial and/or time domain. Ultimately, this gained information on the\nfusion could not only improve the usability of the model as a result of its enhanced understanding by the\nuser, but could also help identifying other data sources of potential interest that could be incorporated to\nthe model, or even contribute to a more efﬁcient data fusion in other contexts.\nUnfortunately, this previously mentioned concept of fusion at data level contemplates data under\ncertain constraints of known form and source origin. As presented in [423], the Big Data era presents\nan environment in which these premises cannot be taken for granted, and methods to board Big Data\nfusion (as that illustrated in Figure 13.d) have to be thought. Conversely, a concern with model fusion\n43\n\n\ncontext emerges in the possibility that XAI techniques could be explanatory enough to compromise the\nconﬁdentiality of private data. This could eventually occur if sensitive information (e.g. ownership) could\nbe inferred from the explained fusion among protected and unprotected features.\nWhen turning our prospects to data fusion at model level, we have already argued that the fusion of the\noutputs of several transparent models (as in tree ensembles) could make the overall model opaque, thereby\nmaking it necessary to resort to post-hoc explainability solutions. However, model fusion may entail other\ndrawbacks when endowed with powerful post-hoc XAI techniques. Let us imagine that relationships of\na model’s input features have been discovered by means of a post-hoc technique) and that one of those\nfeatures is hidden or unknown. Will it be possible to infer another model’s features if that previous feature\nwas known to be used in that model? Would this possibility uncover a problem as privacy breaches in\ncases in which related protected input variables are not even shared in the ﬁrst place?\nTo get the example clearer, in [424] a multiview perspective is utilized in which different single views\n(representing the sources they attend to) models are fused. These models contain among others, cell-phone\ndata, transportation data, etc. which might introduce the problem that information that is not even shared\ncan be discovered through other sources that are actually shared. In the example above, what if instead of\nfeatures, a model shares with another a layer or part of its architecture as in Federated Learning? Would\nthis sharing make possible to infer information from that exchanged part of its model, to the extent of\nallowing for the design of adversarial attacks with better success rate upon the antecedent model?\nIf focused at knowledge level fusion, a similar reasoning holds: XAI comprises techniques that extract\nknowledge from ML model(s). This ability to explain models could have an impact on the necessity of\ndiscovering new knowledge through the complex interactions formed within ML models. If so, XAI might\nenrich knowledge fusion paradigms, bringing the possibility of discovering new knowledge extractors\nof relevance for the task at hand. For this purpose, it is of paramount importance that the knowledge\nextracted from a model by means of XAI techniques can be understood and extrapolated to the domain\nin which knowledge extractors operate. The concept matches with ease with that of transfer learning\nportrayed in [425]. Although XAI is not contemplated in the surveyed processes of extracting knowledge\nfrom models trained in certain feature spaces and distributions, to then be utilized in environments where\nprevious conditions do not hold, when deployed, XAI can pose a threat if the explanations given about the\nmodel can be reversely engineered through the knowledge fusion paradigm to eventually compromise, for\ninstance, the differential privacy of the overall model.\nThe distinction between centralized and distributed data fusion also spurs further challenges in\nregards to privacy and explainability. The centralized approach does not bring any further concerns\nthat those presented above. However, distributed fusion does arise new problems. Distributed fusion\nmight be applied for different reasons, mainly due to environmental constraints or due to security or\nprivacy issues. The latter context may indulge some dangers. Among other goals (e.g. computational\nefﬁciency), model-level data fusion is performed in a distributed fashion to ensure that no actual data is\nactually shared, but rather parts of an ML model trained on local data. This rationale lies at the heart\nof Federated Learning, where models exchange locally learned information among nodes. Since data\ndo not leave the local device, only the transmission of model updates is required across distributed\ndevices. This lightens the training process for network-compromised settings and guarantees data privacy\n[416]. Upon the use of post-hoc explainability techniques, a node could disguise sensitive information\nabout the local context in which the received ML model part was trained. In fact, it was shown that a\nblack-box model based on a DNN from which an input/output query interface is given can be used to\naccurately predict every single hyperparameter value used for training, allowing for potential privacy-\nrelated consequences [357, 420, 421]. This relates to studies showing that blurring images does not\nguarantee privacy preservation.\nData fusion, privacy and model explainability are concepts that have not been analysed together so far.\nFrom the above discussion it is clear that there are unsolved concerns and caveats that demand further\nstudy by the community in forthcoming times.\n44\n\n\n6.4. Implementing Responsible AI Principles in an Organization\nWhile increasingly more organizations are publishing AI principles to declare that they care about\navoiding unintended negative consequences, there is much less experience on how to actually implement\nthe principles into an organization. Looking at several examples of principles declared by different\norganizations [385], we can divide them into two groups:\n• AI-speciﬁc principles that focus on aspects that are speciﬁc to AI, such as explainability, fairness and\nhuman agency.\n• End-to-end principles that cover all aspects involved in AI, including also privacy, security and safety.\nThe EC Guidelines for Trustworthy AI are an example of end-to-end principles [390], while those of\nTelefonica (a large Spanish ICT company operating worldwide) are more AI-speciﬁc [386]. For example,\nsafety and security are relevant for any connected IT system, and therefore also for AI systems. The\nsame holds for privacy, but it is probably true that privacy in the context of AI systems is even more\nimportant than for general IT systems, due to the fact that ML models need huge amounts of data and\nmost importantly, because XAI tools and data fusion techniques pose new challenges to preserve the\nprivacy of protected records.\nWhen it comes to implement the AI Principles into an organization, it is important to operationalize\nthe AI-speciﬁc parts and, at the same time, leverage the processes already existing for the more generic\nprinciples. Indeed, in many organizations there already exist norms and procedures for privacy, security\nand safety. Implementing AI principles requires a methodology such as that presented in [386] that breaks\ndown the process into different parts. The ingredients of such a methodology should include, at least:\n• AI principles (already discussed earlier), which set the values and boundaries.\n• Awareness and training about the potential issues, both technical and non-technical.\n• A questionnaire that forces people to think about certain impacts of the AI system (impact explanation).\nThis questionnaire should give concrete guidance on what to do if certain undesired impacts are\ndetected.\n• Tools that help answering some of the questions, and help mitigating any problems identiﬁed. XAI\ntools and fairness tools fall in this category, as well as other recent proposals such as model cards [426].\n• A governance model assigning responsibilities and accountabilities (responsibility explanation). There\nare two philosophies for governance: 1) based on committees that review and approve AI developments,\nand 2) based on the self-responsibility of the employees. While both are possible, given the fact\nthat agility is key for being successful in the digital world, it seems wiser to focus on awareness and\nemployee responsibility, and only use committees when there are speciﬁc, but important issues.\nFrom the above elaborations, it is clear that the implementation of Responsible AI principles in\ncompanies should balance between two requirements: 1) major cultural and organizational changes\nneeded to enforce such principles over processes endowed with AI functionalities; and 2) the feasibility\nand compliance of the implementation of such principles with the IT assets, policies and resources already\navailable at the company. It is in the gradual process of rising corporate awareness around the principles\nand values of Responsible AI where we envision that XAI will make its place and create huge impact.\n7. Conclusions and Outlook\nThis overview has revolved around eXplainable Artiﬁcial Intelligence (XAI), which has been identiﬁed\nin recent times as an utmost need for the adoption of ML methods in real-life applications. Our study\n45\n\n\nhas elaborated on this topic by ﬁrst clarifying different concepts underlying model explainability, as well\nas by showing the diverse purposes that motivate the search for more interpretable ML methods. These\nconceptual remarks have served as a solid baseline for a systematic review of recent literature dealing with\nexplainability, which has been approached from two different perspectives: 1) ML models that feature\nsome degree of transparency, thereby interpretable to an extent by themselves; and 2) post-hoc XAI\ntechniques devised to make ML models more interpretable. This literature analysis has yielded a global\ntaxonomy of different proposals reported by the community, classifying them under uniform criteria.\nGiven the prevalence of contributions dealing with the explainability of Deep Learning models, we have\ninspected in depth the literature dealing with this family of models, giving rise to an alternative taxonomy\nthat connects more closely with the speciﬁc domains in which explainability can be realized for Deep\nLearning models.\nWe have moved our discussions beyond what has been made so far in the XAI realm toward the concept\nof Responsible AI, a paradigm that imposes a series of AI principles to be met when implementing AI\nmodels in practice, including fairness, transparency, and privacy. We have also discussed the implications\nof adopting XAI techniques in the context of data fusion, unveiling the potential of XAI to compromise\nthe privacy of protected data involved in the fusion process. Implications of XAI in fairness have also\nbeen discussed in detail. This vision of XAI as a core concept to ensure the aforementioned principles for\nResponsible AI is summarized graphically in Figure 14.\nXAI\nInterpretability\nversus \nPerformance\nXAI \nConcepts\nand Metrics\nAchieving\nExplainability \nin Deep \nLearning\nXAI & \nSecurity: \nAdversarial \nML\nRationale\nExplanation\n& Critical\nData \nStudies\nTheory\nguided Data \nScience\nImplementation\n& Guidelines\nXAI and \nOutput \nConfidence\nXAI & Data \nFusion \nFairness\nPrivacy\nAccountability\nEthics\nTransparency\nSecurity & \nSafety\nResponsible\nAI\nFigure 14: Summary of XAI challenges discussed in this overview and its impact on the principles for Responsible AI.\nOur reﬂections about the future of XAI, conveyed in the discussions held throughout this work,\nagree on the compelling need for a proper understanding of the potentiality and caveats opened up by\nXAI techniques. It is our vision that model interpretability must be addressed jointly with requirements\nand constraints related to data privacy, model conﬁdentiality, fairness and accountability. A responsible\nimplementation and use of AI methods in organizations and institutions worldwide will be only guaranteed\nif all these AI principles are studied jointly.\nAcknowledgments\nAlejandro Barredo-Arrieta, Javier Del Ser and Sergio Gil-Lopez would like to thank the Basque\nGovernment for the funding support received through the EMAITEK and ELKARTEK programs. Javier\nDel Ser also acknowledges funding support from the Consolidated Research Group MATHMODE\n(IT1294-19) granted by the Department of Education of the Basque Government. Siham Tabik, Salvador\nGarcia, Daniel Molina and Francisco Herrera would like to thank the Spanish Government for its funding\nsupport (SMART-DaSCI project, TIN2017-89517-P), as well as the BBVA Foundation through its Ayudas\n46\n\n\nFundaci´on BBVA a Equipos de Investigaci´on Cient´ıﬁca 2018 call (DeepSCOP project). This work was\nalso funded in part by the European Union’s Horizon 2020 research and innovation programme AI4EU\nunder grant agreement 825619. We also thank Chris Olah, Alexander Mordvintsev and Ludwig Schubert\nfor borrowing images for illustration purposes. Part of this overview is inspired by a preliminary work of\nthe concept of Responsible AI: R. Benjamins, A. Barbado, D. Sierra, “Responsible AI by Design”, to\nappear in the Proceedings of the Human-Centered AI: Trustworthiness of AI Models & Data (HAI) track\nat AAAI Fall Symposium, DC, November 7-9, 2019 [386].\nReferences\n[1] S. J. Russell, P. Norvig, Artiﬁcial intelligence: a modern approach, Malaysia; Pearson Education\nLimited,, 2016.\n[2] D. M. West, The future of work: robots, AI, and automation, Brookings Institution Press, 2018.\n[3] B. Goodman, S. Flaxman, European union regulations on algorithmic decision-making and a “right\nto explanation”, AI Magazine 38 (3) (2017) 50–57.\n[4] D. Castelvecchi, Can we open the black box of AI?, Nature News 538 (7623) (2016) 20.\n[5] Z. C. Lipton, The mythos of model interpretability, Queue 16 (3) (2018) 30:31–30:57.\n[6] A. Preece, D. Harborne, D. Braines, R. Tomsett, S. Chakraborty, Stakeholders in Explainable AI\n(2018). arXiv:1810.00184.\n[7] D. Gunning, Explainable artiﬁcial intelligence (xAI), Tech. rep., Defense Advanced Research\nProjects Agency (DARPA) (2017).\n[8] E. Tjoa, C. Guan, A survey on explainable artiﬁcial intelligence (XAI): Towards medical XAI\n(2019). arXiv:1907.07374.\n[9] J. Zhu, A. Liapis, S. Risi, R. Bidarra, G. M. Youngblood, Explainable AI for designers: A human-\ncentered perspective on mixed-initiative co-creation, 2018 IEEE Conference on Computational\nIntelligence and Games (CIG) (2018) 1–8.\n[10] F. K. Do˜silovi´c, M. Br˜ci´c, N. Hlupi´c, Explainable artiﬁcial intelligence: A survey, in: 41st\nInternational Convention on Information and Communication Technology, Electronics and Micro-\nelectronics (MIPRO), 2018, pp. 210–215.\n[11] P. Hall, On the Art and Science of Machine Learning Explanations (2018). arXiv:1810.02909.\n[12] T. Miller, Explanation in artiﬁcial intelligence: Insights from the social sciences, Artif. Intell. 267\n(2019) 1–38.\n[13] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, L. Kagal, Explaining Explanations: An\nOverview of Interpretability of Machine Learning (2018). arXiv:1806.00069.\n[14] A. Adadi, M. Berrada, Peeking inside the black-box: A survey on explainable artiﬁcial intelligence\n(XAI), IEEE Access 6 (2018) 52138–52160.\n[15] O. Biran, C. Cotton, Explanation and justiﬁcation in machine learning: A survey, in: IJCAI-17\nworkshop on explainable AI (XAI), Vol. 8, 2017, p. 1.\n47\n\n\n[16] S. T. Shane T. Mueller, R. R. Hoffman, W. Clancey, G. Klein, Explanation in Human-AI Systems: A\nLiterature Meta-Review Synopsis of Key Ideas and Publications and Bibliography for Explainable\nAI, Tech. rep., Defense Advanced Research Projects Agency (DARPA) XAI Program (2019).\n[17] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods\nfor explaining black box models, ACM Computing Surveys 51 (5) (2018) 93:1–93:42.\n[18] G. Montavon, W. Samek, K.-R. M¨uller, Methods for interpreting and understanding deep neural\nnetworks, Digital Signal Processing 73 (2018) 1–15. doi:10.1016/j.dsp.2017.10.011.\n[19] A. Fernandez, F. Herrera, O. Cordon, M. Jose del Jesus, F. Marcelloni, Evolutionary fuzzy systems\nfor explainable artiﬁcial intelligence: Why, when, what for, and where to?, IEEE Computational\nIntelligence Magazine 14 (1) (2019) 69–81.\n[20] M. Gleicher, A framework for considering comprehensibility in modeling, Big data 4 (2) (2016)\n75–88.\n[21] M. W. Craven, Extracting comprehensible models from trained neural networks, Tech. rep., Univer-\nsity of Wisconsin-Madison Department of Computer Sciences (1996).\n[22] R. S. Michalski, A theory and methodology of inductive learning, in: Machine learning, Springer,\n1983, pp. 83–134.\n[23] J. D´ıez, K. Khalifa, B. Leuridan, General theories of explanation: buyer beware, Synthese 190 (3)\n(2013) 379–396.\n[24] D. Doran, S. Schulz, T. R. Besold, What does explainable AI really mean? a new conceptualization\nof perspectives (2017). arXiv:1710.00794.\n[25] F. Doshi-Velez, B. Kim, Towards a rigorous science of interpretable machine learning (2017).\narXiv:1702.08608.\n[26] A. Vellido, J. D. Mart´ın-Guerrero, P. J. Lisboa, Making machine learning models interpretable., in:\nEuropean Symposium on Artiﬁcial Neural Networks, Computational Intelligence and Machine\nLearning (ESANN), Vol. 12, Citeseer, 2012, pp. 163–172.\n[27] E. Walter, Cambridge advanced learner’s dictionary, Cambridge University Press, 2008.\n[28] P. Besnard, A. Hunter, Elements of Argumentation, The MIT Press, 2008.\n[29] F. Rossi, AI Ethics for Enterprise AI (2019).\nURL\nhttps://economics.harvard.edu/files/economics/files/rossi-\nfrancesca_4-22-19_ai-ethics-for-enterprise-ai_ec3118-hbs.pdf\n[30] A. Holzinger, C. Biemann, C. S. Pattichis, D. B. Kell, What do we need to build explainable Ai\nsystems for the medical domain? (2017). arXiv:1712.09923.\n[31] B. Kim, E. Glassman, B. Johnson, J. Shah, iBCM: Interactive bayesian case model empowering\nhumans via intuitive interaction, Tech. rep., MIT-CSAIL-TR-2015-010 (2015).\n[32] M. T. Ribeiro, S. Singh, C. Guestrin, Why should I trust you?: Explaining the predictions of any\nclassiﬁer, in: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\nACM, 2016, pp. 1135–1144.\n[33] M. Fox, D. Long, D. Magazzeni, Explainable planning (2017). arXiv:1709.10256.\n48\n\n\n[34] H. C. Lane, M. G. Core, M. Van Lent, S. Solomon, D. Gomboc, Explainable artiﬁcial intelligence\nfor training and tutoring, Tech. rep., University of Southern California (2005).\n[35] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, B. Yu, Interpretable machine learning:\ndeﬁnitions, methods, and applications (2019). arXiv:1901.04592.\n[36] J. Haspiel, N. Du, J. Meyerson, L. P. Robert Jr, D. Tilbury, X. J. Yang, A. K. Pradhan, Explana-\ntions and expectations: Trust building in automated vehicles, in: Companion of the ACM/IEEE\nInternational Conference on Human-Robot Interaction, ACM, 2018, pp. 119–120.\n[37] A. Chander, R. Srinivasan, S. Chelian, J. Wang, K. Uchino, Working with beliefs: AI transparency\nin the enterprise., in: Workshops of the ACM Conference on Intelligent User Interfaces, 2018.\n[38] A. B. Tickle, R. Andrews, M. Golea, J. Diederich, The truth will come to light: Directions and\nchallenges in extracting the knowledge embedded within trained artiﬁcial neural networks, IEEE\nTransactions on Neural Networks 9 (6) (1998) 1057–1068.\n[39] C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, M. Welling, Causal effect inference with\ndeep latent-variable models, in: Advances in Neural Information Processing Systems, 2017, pp.\n6446–6456.\n[40] O. Goudet, D. Kalainathan, P. Caillou, I. Guyon, D. Lopez-Paz, M. Sebag, Learning functional\ncausal models with generative neural networks, in: Explainable and Interpretable Models in\nComputer Vision and Machine Learning, Springer, 2018, pp. 39–80.\n[41] S. Athey, G. W. Imbens, Machine learning methods for estimating heterogeneous causal effects,\nstat 1050 (5) (2015).\n[42] D. Lopez-Paz, R. Nishihara, S. Chintala, B. Scholkopf, L. Bottou, Discovering causal signals in\nimages, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n2017, pp. 6979–6987.\n[43] C. Barabas, K. Dinakar, J. Ito, M. Virza, J. Zittrain, Interventions over predictions: Reframing the\nethical debate for actuarial risk assessment (2017). arXiv:1712.08238.\n[44] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, N. Elhadad, Intelligible models for healthcare:\nPredicting pneumonia risk and hospital 30-day readmission, in: Proceedings of the 21th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, 2015,\npp. 1721–1730.\n[45] A. Theodorou, R. H. Wortham, J. J. Bryson, Designing and implementing transparency for real\ntime inspection of autonomous robots, Connection Science 29 (3) (2017) 230–241.\n[46] W. Samek, T. Wiegand, K.-R. M¨uller, Explainable artiﬁcial intelligence: Understanding, visualizing\nand interpreting deep learning models (2017). arXiv:1708.08296.\n[47] C. Wadsworth, F. Vera, C. Piech, Achieving fairness through adversarial learning: an application to\nrecidivism prediction (2018). arXiv:1807.00199.\n[48] X. Yuan, P. He, Q. Zhu, X. Li, Adversarial examples: Attacks and defenses for deep learning, IEEE\nTransactions on Neural Networks and Learning Systems 30 (9) (2019) 2805–2824.\n[49] B. Letham, C. Rudin, T. H. McCormick, D. Madigan, et al., Interpretable classiﬁers using rules\nand bayesian analysis: Building a better stroke prediction model, The Annals of Applied Statistics\n9 (3) (2015) 1350–1371.\n49\n\n\n[50] M. Harbers, K. van den Bosch, J.-J. Meyer, Design and evaluation of explainable BDI agents, in:\nIEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,\nVol. 2, IEEE, 2010, pp. 125–132.\n[51] M. H. Aung, P. G. Lisboa, T. A. Etchells, A. C. Testa, B. Van Calster, S. Van Huffel, L. Valentin,\nD. Timmerman, Comparing analytical decision support models through boolean rule extraction:\nA case study of ovarian tumour malignancy, in: International Symposium on Neural Networks,\nSpringer, 2007, pp. 1177–1186.\n[52] A. Weller, Challenges for transparency (2017). arXiv:1708.01870.\n[53] A. A. Freitas, Comprehensible classiﬁcation models: a position paper, ACM SIGKDD explorations\nnewsletter 15 (1) (2014) 1–10.\n[54] V. Schetinin, J. E. Fieldsend, D. Partridge, T. J. Coats, W. J. Krzanowski, R. M. Everson, T. C.\nBailey, A. Hernandez, Conﬁdent interpretation of bayesian decision tree ensembles for clinical\napplications, IEEE Transactions on Information Technology in Biomedicine 11 (3) (2007) 312–319.\n[55] D. Martens, J. Vanthienen, W. Verbeke, B. Baesens, Performance of classiﬁcation models from a\nuser perspective, Decision Support Systems 51 (4) (2011) 782–793.\n[56] Z. Che, S. Purushotham, R. Khemani, Y. Liu, Interpretable deep models for ICU outcome prediction,\nin: AMIA Annual Symposium Proceedings, Vol. 2016, American Medical Informatics Association,\n2016, p. 371.\n[57] N. Barakat, J. Diederich, Eclectic rule-extraction from support vector machines, International\nJournal of Computer, Electrical, Automation, Control and Information Engineering 2 (5) (2008)\n1672–1675.\n[58] F. J. C. Garcia, D. A. Robb, X. Liu, A. Laskov, P. Patron, H. Hastie, Explain yourself: A natural\nlanguage interface for scrutable autonomous robots (2018). arXiv:1803.02088.\n[59] P. Langley, B. Meadows, M. Sridharan, D. Choi, Explainable agency for intelligent autonomous\nsystems, in: AAAI Conference on Artiﬁcial Intelligence, 2017, pp. 4762–4763.\n[60] G. Montavon, S. Lapuschkin, A. Binder, W. Samek, K.-R. M¨uller, Explaining nonlinear classiﬁca-\ntion decisions with deep taylor decomposition, Pattern Recognition 65 (2017) 211–222.\n[61] P.-J. Kindermans, K. T. Sch¨utt, M. Alber, K.-R. M¨uller, D. Erhan, B. Kim, S. D¨ahne, Learning how\nto explain neural networks: Patternnet and patternattribution (2017). arXiv:1705.05598.\n[62] G. Ras, M. van Gerven, P. Haselager, Explanation methods in deep learning: Users, values,\nconcerns and challenges, in: Explainable and Interpretable Models in Computer Vision and\nMachine Learning, Springer, 2018, pp. 19–36.\n[63] S. Bach, A. Binder, K.-R. M¨uller, W. Samek, Controlling explanatory heatmap resolution and\nsemantics via decomposition depth, in: IEEE International Conference on Image Processing (ICIP),\nIEEE, 2016, pp. 2271–2275.\n[64] G. J. Katuwal, R. Chen, Machine learning model interpretability for precision medicine (2016).\narXiv:1610.09045.\n[65] M. A. Neerincx, J. van der Waa, F. Kaptein, J. van Diggelen, Using perceptual and cognitive expla-\nnations for enhanced human-agent team performance, in: International Conference on Engineering\nPsychology and Cognitive Ergonomics, Springer, 2018, pp. 204–214.\n50\n\n\n[66] J. D. Olden, D. A. Jackson, Illuminating the “black box”: a randomization approach for under-\nstanding variable contributions in artiﬁcial neural networks, Ecological modelling 154 (1-2) (2002)\n135–150.\n[67] J. Krause, A. Perer, K. Ng, Interacting with predictions: Visual inspection of black-box machine\nlearning models, in: CHI Conference on Human Factors in Computing Systems, ACM, 2016, pp.\n5686–5697.\n[68] L. Rosenbaum, G. Hinselmann, A. Jahn, A. Zell, Interpreting linear support vector machine models\nwith heat map molecule coloring, Journal of Cheminformatics 3 (1) (2011) 11.\n[69] J. Tan, M. Ung, C. Cheng, C. S. Greene, Unsupervised feature construction and knowledge\nextraction from genome-wide assays of breast cancer with denoising autoencoders, in: Paciﬁc\nSymposium on Biocomputing Co-Chairs, World Scientiﬁc, 2014, pp. 132–143.\n[70] S. Krening, B. Harrison, K. M. Feigh, C. L. Isabell, M. Riedl, A. Thomaz, Learning from expla-\nnations using sentiment and advice in RL, IEEE Transactions on Cognitive and Developmental\nSystems 9 (1) (2017) 44–55.\n[71] M. T. Ribeiro, S. Singh, C. Guestrin, Model-agnostic interpretability of machine learning (2016).\narXiv:1606.05386.\n[72] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M¨uller, W. Samek, On pixel-wise expla-\nnations for non-linear classiﬁer decisions by layer-wise relevance propagation, PloS one 10 (7)\n(2015) e0130140.\n[73] T. A. Etchells, P. J. Lisboa, Orthogonal search-based rule extraction (OSRE) for trained neural\nnetworks: a practical and efﬁcient approach, IEEE Transactions on Neural Networks 17 (2) (2006)\n374–384.\n[74] Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, H. H. Zhuo, S. Kambhampati, Plan expli-\ncability and predictability for robot task planning, in: 2017 IEEE International Conference on\nRobotics and Automation (ICRA), IEEE, 2017, pp. 1313–1320.\n[75] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, T. Lillicrap,\nA simple neural network module for relational reasoning, in: Advances in Neural Information\nProcessing Systems, 2017, pp. 4967–4976.\n[76] C.-Y. J. Peng, T.-S. H. So, F. K. Stage, E. P. S. John, The use and interpretation of logistic regression\nin higher education journals: 1988–1999, Research in Higher Education 43 (3) (2002) 259–293.\n[77] B. ¨Ust¨un, W. Melssen, L. Buydens, Visualisation and interpretation of support vector regression\nmodels, Analytica Chimica Acta 595 (1-2) (2007) 299–309.\n[78] Q. Zhang, Y. Yang, H. Ma, Y. N. Wu, Interpreting CNNs via decision trees, in: IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 6261–6270.\n[79] M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth, F. Doshi-Velez, Beyond sparsity: Tree\nregularization of deep models for interpretability, in: AAAI Conference on Artiﬁcial Intelligence,\n2018, pp. 1670–1678.\n[80] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network (2015).\narXiv:1503.02531.\n[81] N. Frosst, G. Hinton, Distilling a neural network into a soft decision tree (2017). arXiv:1711.09784.\n51\n\n\n[82] M. G. Augasta, T. Kathirvalavakumar, Reverse engineering the neural networks for rule extraction\nin classiﬁcation problems, Neural Processing Letters 35 (2) (2012) 131–150.\n[83] Z.-H. Zhou, Y. Jiang, S.-F. Chen, Extracting symbolic rules from trained neural network ensembles,\nAI Communications 16 (1) (2003) 3–15.\n[84] H. F. Tan, G. Hooker, M. T. Wells, Tree space prototypes: Another look at making tree ensembles\ninterpretable (2016). arXiv:1611.07115.\n[85] R. C. Fong, A. Vedaldi, Interpretable explanations of black boxes by meaningful perturbation, in:\nIEEE International Conference on Computer Vision, 2017, pp. 3429–3437.\n[86] T. Miller, P. Howe, L. Sonenberg, Explainable AI: Beware of inmates running the asylum, in:\nInternational Joint Conference on Artiﬁcial Intelligence, Workshop on Explainable AI (XAI),\nVol. 36, 2017, pp. 36–40.\n[87] R. Goebel, A. Chander, K. Holzinger, F. Lecue, Z. Akata, S. Stumpf, P. Kieseberg, A. Holzinger,\nExplainable AI: the new 42?, in: International Cross-Domain Conference for Machine Learning\nand Knowledge Extraction, Springer, 2018, pp. 295–303.\n[88] V. Belle, Logic meets probability: Towards explainable AI systems for uncertain worlds, in:\nInternational Joint Conference on Artiﬁcial Intelligence, 2017, pp. 5116–5120.\n[89] L. Edwards, M. Veale, Slave to the algorithm: Why a right to an explanation is probably not the\nremedy you are looking for, Duke L. & Tech. Rev. 16 (2017) 18.\n[90] Y. Lou, R. Caruana, J. Gehrke, G. Hooker, Accurate intelligible models with pairwise interactions,\nin: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM,\n2013, pp. 623–631.\n[91] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, Y. Bengio, Show, attend\nand tell: Neural image caption generation with visual attention, in: International Conference on\nMachine Learning, 2015, pp. 2048–2057.\n[92] J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, B. Baesens, An empirical evaluation of the\ncomprehensibility of decision table, tree and rule based predictive models, Decision Support\nSystems 51 (1) (2011) 141–154.\n[93] N. H. Barakat, A. P. Bradley, Rule extraction from support vector machines: A sequential covering\napproach, IEEE Transactions on Knowledge and Data Engineering 19 (6) (2007) 729–741.\n[94] F. C. Adriana da Costa, M. M. B. Vellasco, R. Tanscheit, Fuzzy rule extraction from support vector\nmachines, in: International Conference on Hybrid Intelligent Systems, IEEE, 2005, pp. 335–340.\n[95] D. Martens, B. Baesens, T. Van Gestel, J. Vanthienen, Comprehensible credit scoring models using\nrule extraction from support vector machines, European Journal of Operational Research 183 (3)\n(2007) 1466–1476.\n[96] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for discriminative\nlocalization, in: IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2921–\n2929.\n[97] R. Krishnan, G. Sivakumar, P. Bhattacharya, Extracting decision trees from trained neural networks,\nPattern Recognition 32 (12) (1999) 1999–2009.\n52\n\n\n[98] X. Fu, C. Ong, S. Keerthi, G. G. Hung, L. Goh, Extracting the knowledge embedded in support\nvector machines, in: IEEE International Joint Conference on Neural Networks, Vol. 1, IEEE, 2004,\npp. 291–296.\n[99] B. Green, “Fair” risk assessments: A precarious approach for criminal justice reform, in: 5th\nWorkshop on Fairness, Accountability, and Transparency in Machine Learning, 2018.\n[100] A. Chouldechova, Fair prediction with disparate impact: A study of bias in recidivism prediction\ninstruments, Big Data 5 (2) (2017) 153–163.\n[101] M. Kim, O. Reingold, G. Rothblum, Fairness through computationally-bounded awareness, in:\nAdvances in Neural Information Processing Systems, 2018, pp. 4842–4852.\n[102] B. Haasdonk, Feature space interpretation of SVMs with indeﬁnite kernels, IEEE Transactions on\nPattern Analysis and Machine Intelligence 27 (4) (2005) 482–492.\n[103] A. Palczewska, J. Palczewski, R. M. Robinson, D. Neagu, Interpreting random forest classiﬁcation\nmodels using a feature contribution method, in: Integration of Reusable Systems, Springer, 2014,\npp. 193–218.\n[104] S. H. Welling, H. H. Refsgaard, P. B. Brockhoff, L. H. Clemmensen, Forest ﬂoor visualizations of\nrandom forests (2016). arXiv:1605.09196.\n[105] G. Fung, S. Sandilya, R. B. Rao, Rule extraction from linear support vector machines, in: ACM\nSIGKDD International Conference on Knowledge Discovery in Data Mining, ACM, 2005, pp.\n32–40.\n[106] Y. Zhang, H. Su, T. Jia, J. Chu, Rule extraction from trained support vector machines, in: Paciﬁc-\nAsia Conference on Knowledge Discovery and Data Mining, Springer, 2005, pp. 61–70.\n[107] D. Linsley, D. Shiebler, S. Eberhardt, T. Serre, Global-and-local attention networks for visual\nrecognition (2018). arXiv:1805.08819.\n[108] S.-M. Zhou, J. Q. Gan, Low-level interpretability and high-level interpretability: a uniﬁed view\nof data-driven interpretable fuzzy system modelling, Fuzzy Sets and Systems 159 (23) (2008)\n3091–3131.\n[109] J. Burrell, How the machine ‘thinks’: Understanding opacity in machine learning algorithms, Big\nData & Society 3 (1) (2016) 1–12.\n[110] A. Shrikumar, P. Greenside, A. Shcherbina, A. Kundaje, Not just a black box: Learning important\nfeatures through propagating activation differences (2016). arXiv:1605.01713.\n[111] Y. Dong, H. Su, J. Zhu, B. Zhang, Improving interpretability of deep neural networks with\nsemantic information, in: IEEE Conference on Computer Vision and Pattern Recognition, 2017,\npp. 4306–4314.\n[112] G. Ridgeway, D. Madigan, T. Richardson, J. O’Kane, Interpretable boosted na¨ıve bayes classi-\nﬁcation., in: ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 1998, pp.\n101–104.\n[113] Q. Zhang, Y. Nian Wu, S.-C. Zhu, Interpretable convolutional neural networks, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 8827–8836.\n53\n\n\n[114] S. Seo, J. Huang, H. Yang, Y. Liu, Interpretable convolutional neural networks with dual local and\nglobal attention for review rating prediction, in: Proceedings of the Eleventh ACM Conference on\nRecommender Systems, ACM, 2017, pp. 297–305.\n[115] K. Larsen, J. H. Petersen, E. Budtz-Jørgensen, L. Endahl, Interpreting parameters in the logistic\nregression model with random effects, Biometrics 56 (3) (2000) 909–914.\n[116] B. Gaonkar, R. T. Shinohara, C. Davatzikos, A. D. N. Initiative, et al., Interpreting support vector\nmachine models for multivariate group wise analysis in neuroimaging, Medical image analysis\n24 (1) (2015) 190–204.\n[117] K. Xu, D. H. Park, C. Yi, C. Sutton, Interpreting deep classiﬁer by visual distillation of dark\nknowledge (2018). arXiv:1803.04042.\n[118] H. Deng, Interpreting tree ensembles with intrees (2014). arXiv:1408.5456.\n[119] P. Domingos, Knowledge discovery via multiple models, Intelligent Data Analysis 2 (1-4) (1998)\n187–202.\n[120] S. Tan, R. Caruana, G. Hooker, Y. Lou, Distill-and-compare: Auditing black-box models using\ntransparent model distillation, in: AAAI/ACM Conference on AI, Ethics, and Society, ACM, 2018,\npp. 303–310.\n[121] R. A. Berk, J. Bleich, Statistical procedures for forecasting criminal behavior: A comparative\nassessment, Criminology & Public Policy 12 (3) (2013) 513–544.\n[122] S. Hara, K. Hayashi, Making tree ensembles interpretable (2016). arXiv:1606.05390.\n[123] A. Henelius, K. Puolam¨aki, A. Ukkonen, Interpreting classiﬁers through attribute interactions in\ndatasets (2017). arXiv:1707.07576.\n[124] H. Hastie, F. J. C. Garcia, D. A. Robb, P. Patron, A. Laskov, MIRIAM: a multimodal chat-based\ninterface for autonomous systems, in: ACM International Conference on Multimodal Interaction,\nACM, 2017, pp. 495–496.\n[125] D. Bau, B. Zhou, A. Khosla, A. Oliva, A. Torralba, Network dissection: Quantifying interpretability\nof deep visual representations, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017, pp. 6541–6549.\n[126] H. N´u˜nez, C. Angulo, A. Catal`a, Rule extraction from support vector machines., in: European\nSymposium on Artiﬁcial Neural Networks, Computational Intelligence and Machine Learning\n(ESANN), 2002, pp. 107–112.\n[127] H. N´u˜nez, C. Angulo, A. Catal`a, Rule-based learning systems for support vector machines, Neural\nProcessing Letters 24 (1) (2006) 1–18.\n[128] M. Kearns, S. Neel, A. Roth, Z. S. Wu, Preventing fairness gerrymandering: Auditing and learning\nfor subgroup fairness (2017). arXiv:1711.05144.\n[129] E. Akyol, C. Langbort, T. Basar, Price of transparency in strategic machine learning (2016).\narXiv:1610.08210.\n[130] D. Erhan, A. Courville, Y. Bengio, Understanding representations learned in deep architectures,\nDepartment dInformatique et Recherche Operationnelle, University of Montreal, QC, Canada,\nTech. Rep 1355 (2010) 1.\n54\n\n\n[131] Y. Zhang, B. Wallace, A sensitivity analysis of (and practitioners’ guide to) convolutional neural\nnetworks for sentence classiﬁcation (2015). arXiv:1510.03820.\n[132] J. R. Quinlan, Simplifying decision trees, International journal of man-machine studies 27 (3)\n(1987) 221–234.\n[133] Y. Zhou, G. Hooker, Interpreting models via single tree approximation (2016). arXiv:1610.09036.\n[134] A. Navia-V´azquez, E. Parrado-Hern´andez, Support vector machine interpretation, Neurocomputing\n69 (13-15) (2006) 1754–1759.\n[135] J. J. Thiagarajan, B. Kailkhura, P. Sattigeri, K. N. Ramamurthy, Treeview: Peeking into deep neural\nnetworks via feature-space partitioning (2016). arXiv:1611.07429.\n[136] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: European\nconference on computer vision, Springer, 2014, pp. 818–833.\n[137] A. Mahendran, A. Vedaldi, Understanding deep image representations by inverting them, in:\nProceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5188–\n5196.\n[138] J. Wagner, J. M. Kohler, T. Gindele, L. Hetzel, J. T. Wiedemer, S. Behnke, Interpretable and\nﬁne-grained visual explanations for convolutional neural networks, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp. 9097–9107.\n[139] A. Kanehira, T. Harada, Learning to explain with complemental examples, in: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 8603–8611.\n[140] D. W. Apley, Visualizing the effects of predictor variables in black box supervised learning models\n(2016). arXiv:1612.08468.\n[141] M. Staniak, P. Biecek, Explanations of Model Predictions with live and breakDown Packages, The\nR Journal 10 (2) (2018) 395–409.\n[142] M. D. Zeiler, D. Krishnan, G. W. Taylor, R. Fergus, Deconvolutional networks., in: CVPR, Vol. 10,\n2010, p. 7.\n[143] J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller, Striving for simplicity: The all\nconvolutional net (2014). arXiv:1412.6806.\n[144] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, R. Sayres, Interpretability be-\nyond feature attribution: Quantitative testing with concept activation vectors (TCAV) (2017).\narXiv:1711.11279.\n[145] A. Polino, R. Pascanu, D. Alistarh, Model compression via distillation and quantization (2018).\narXiv:1802.05668.\n[146] W. J. Murdoch, A. Szlam, Automatic rule extraction from long short term memory networks (2017).\narXiv:1702.02540.\n[147] M. W. Craven, J. W. Shavlik, Using sampling and queries to extract rules from trained neural\nnetworks, in: Machine learning proceedings 1994, Elsevier, 1994, pp. 37–45.\n[148] A. D. Arbatli, H. L. Akin, Rule extraction from trained neural networks using genetic algorithms,\nNonlinear Analysis: Theory, Methods & Applications 30 (3) (1997) 1639–1648.\n55\n\n\n[149] U. Johansson, L. Niklasson, Evolving decision trees using oracle guides, in: 2009 IEEE Symposium\non Computational Intelligence and Data Mining, IEEE, 2009, pp. 238–244.\n[150] T. Lei, R. Barzilay, T. Jaakkola, Rationalizing neural predictions (2016). arXiv:1606.04155.\n[151] A. Radford, R. Jozefowicz, I. Sutskever, Learning to generate reviews and discovering sentiment\n(2017). arXiv:1704.01444.\n[152] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, D. Batra, Grad-CAM: Why did\nyou say that? (2016).\n[153] R. Shwartz-Ziv, N. Tishby, Opening the black box of deep neural networks via information (2017).\narXiv:1703.00810.\n[154] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson, Understanding neural networks through\ndeep visualization (2015). arXiv:1506.06579.\n[155] P. E. Pope, S. Kolouri, M. Rostami, C. E. Martin, H. Hoffmann, Explainability methods for graph\nconvolutional neural networks, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2019, pp. 10772–10781.\n[156] P. Gajane, M. Pechenizkiy, On formalizing fairness in prediction with machine learning (2017).\narXiv:1710.03184.\n[157] C. Dwork, C. Ilvento, Composition of fairsystems (2018). arXiv:1806.06122.\n[158] S. Barocas, M. Hardt, A. Narayanan, Fairness and Machine Learning, fairmlbook.org, 2019,\nhttp://www.fairmlbook.org.\n[159] H.-X. Wang, L. Fratiglioni, G. B. Frisoni, M. Viitanen, B. Winblad, Smoking and the occurence of\nalzheimer’s disease: Cross-sectional and longitudinal data in a population-based study, American\njournal of epidemiology 149 (7) (1999) 640–644.\n[160] P. Rani, C. Liu, N. Sarkar, E. Vanman, An empirical study of machine learning techniques for affect\nrecognition in human–robot interaction, Pattern Analysis and Applications 9 (1) (2006) 58–69.\n[161] J. Pearl, Causality, Cambridge university press, 2009.\n[162] M. Kuhn, K. Johnson, Applied predictive modeling, Vol. 26, Springer, 2013.\n[163] G. James, D. Witten, T. Hastie, R. Tibshirani, An introduction to statistical learning, Vol. 112,\nSpringer, 2013.\n[164] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, R. Fergus, Intriguing\nproperties of neural networks (2013). arXiv:1312.6199.\n[165] D. Ruppert, Robust statistics: The approach based on inﬂuence functions, Taylor & Francis, 1987.\n[166] S. Basu, K. Kumbier, J. B. Brown, B. Yu, Iterative random forests to discover predictive and\nstable high-order interactions, Proceedings of the National Academy of Sciences 115 (8) (2018)\n1943–1948.\n[167] B. Yu, et al., Stability, Bernoulli 19 (4) (2013) 1484–1500.\n[168] K. Burns, L. A. Hendricks, K. Saenko, T. Darrell, A. Rohrbach, Women also Snowboard: Over-\ncoming Bias in Captioning Models (2018). arXiv:1803.09797.\n56\n\n\n[169] A. Bennetot, J.-L. Laurent, R. Chatila, N. D´ıaz-Rodr´ıguez, Towards explainable neural-symbolic\nvisual reasoning, in: NeSy Workshop IJCAI 2019, Macau, China, 2019.\n[170] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical\nSociety: Series B (Methodological) 58 (1) (1996) 267–288.\n[171] Y. Lou, R. Caruana, J. Gehrke, Intelligible models for classiﬁcation and regression, in: ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2012, pp.\n150–158.\n[172] K. Kawaguchi, Deep learning without poor local minima, in: Advances in neural information\nprocessing systems, 2016, pp. 586–594.\n[173] A. Datta, S. Sen, Y. Zick, Algorithmic transparency via quantitative input inﬂuence: Theory and\nexperiments with learning systems, in: 2016 IEEE symposium on security and privacy (SP), IEEE,\n2016, pp. 598–617.\n[174] Z. Bursac, C. H. Gauss, D. K. Williams, D. W. Hosmer, Purposeful selection of variables in logistic\nregression, Source code for biology and medicine 3 (1) (2008) 17.\n[175] J. Jaccard, Interaction effects in logistic regression: Quantitative applications in the social sciences,\nSage Thousand Oaks, CA, 2001.\n[176] D. W. Hosmer Jr, S. Lemeshow, R. X. Sturdivant, Applied logistic regression, Vol. 398, John Wiley\n& Sons, 2013.\n[177] C.-Y. J. Peng, K. L. Lee, G. M. Ingersoll, An introduction to logistic regression analysis and\nreporting, The journal of educational research 96 (1) (2002) 3–14.\n[178] U. Hoffrage, G. Gigerenzer, Using natural frequencies to improve diagnostic inferences, Academic\nmedicine 73 (5) (1998) 538–540.\n[179] C. Mood, Logistic regression: Why we cannot do what we think we can do, and what we can do\nabout it, European sociological review 26 (1) (2010) 67–82.\n[180] H. Laurent, R. L. Rivest, Constructing optimal binary decision trees is Np-complete, Information\nprocessing letters 5 (1) (1976) 15–17.\n[181] P. E. Utgoff, Incremental induction of decision trees, Machine learning 4 (2) (1989) 161–186.\n[182] J. R. Quinlan, Induction of decision trees, Machine learning 1 (1) (1986) 81–106.\n[183] L. Rokach, O. Z. Maimon, Data mining with decision trees: theory and applications, Vol. 69, World\nscientiﬁc, 2014.\n[184] S. Rovnyak, S. Kretsinger, J. Thorp, D. Brown, Decision trees for real-time transient stability\nprediction, IEEE Transactions on Power Systems 9 (3) (1994) 1417–1426.\n[185] H. Nefeslioglu, E. Sezer, C. Gokceoglu, A. Bozkir, T. Duman, Assessment of landslide suscep-\ntibility by decision trees in the metropolitan area of istanbul, turkey, Mathematical Problems in\nEngineering 2010 (2010) Article ID 901095.\n[186] S. B. Imandoust, M. Bolandraftar, Application of k-nearest neighbor (knn) approach for predicting\neconomic events: Theoretical background, International Journal of Engineering Research and\nApplications 3 (5) (2013) 605–610.\n57\n\n\n[187] L. Li, D. M. Umbach, P. Terry, J. A. Taylor, Application of the GA/KNN method to SELDI\nproteomics data, Bioinformatics 20 (10) (2004) 1638–1640.\n[188] G. Guo, H. Wang, D. Bell, Y. Bi, K. Greer, An KNN model-based approach and its application in\ntext categorization, in: International Conference on Intelligent Text Processing and Computational\nLinguistics, Springer, 2004, pp. 559–570.\n[189] S. Jiang, G. Pang, M. Wu, L. Kuang, An improved k-nearest-neighbor algorithm for text catego-\nrization, Expert Systems with Applications 39 (1) (2012) 1503–1509.\n[190] U. Johansson, R. K¨onig, L. Niklasson, The truth is in there-rule extraction from opaque models\nusing genetic programming., in: FLAIRS Conference, Miami Beach, FL, 2004, pp. 658–663.\n[191] J. R. Quinlan, Generating production rules from decision trees., in: ijcai, Vol. 87, Citeseer, 1987,\npp. 304–307.\n[192] P. Langley, H. A. Simon, Applications of machine learning and rule induction, Communications of\nthe ACM 38 (11) (1995) 54–64.\n[193] D. Berg, Bankruptcy prediction by generalized additive models, Applied Stochastic Models in\nBusiness and Industry 23 (2) (2007) 129–143.\n[194] R. Calabrese, et al., Estimating bank loans loss given default by generalized additive models, UCD\nGeary Institute Discussion Paper Series, WP2012/24 (2012).\n[195] P. Taylan, G.-W. Weber, A. Beck, New approaches to regression by generalized additive models and\ncontinuous optimization for modern applications in ﬁnance, science and technology, Optimization\n56 (5-6) (2007) 675–698.\n[196] H. Murase, H. Nagashima, S. Yonezaki, R. Matsukura, T. Kitakado, Application of a generalized\nadditive model (GAM) to reveal relationships between environmental factors and distributions of\npelagic ﬁsh and krill: a case study in sendai bay, Japan, ICES Journal of Marine Science 66 (6)\n(2009) 1417–1424.\n[197] N. Tomi´c, S. Boˇzi´c, A modiﬁed geosite assessment model (M-GAM) and its application on the\nlazar canyon area (serbia), International journal of environmental research 8 (4) (2014) 1041–1052.\n[198] A. Guisan, T. C. Edwards Jr, T. Hastie, Generalized linear and generalized additive models in\nstudies of species distributions: setting the scene, Ecological Modelling 157 (2-3) (2002) 89–100.\n[199] P. Rothery, D. B. Roy, Application of generalized additive models to butterﬂy transect count data,\nJournal of Applied Statistics 28 (7) (2001) 897–909.\n[200] A. Pierrot, Y. Goude, Short-term electricity load forecasting with generalized additive models, in:\n16th Intelligent System Applications to Power Systems Conference, ISAP 2011, IEEE, 2011, pp.\n410–415.\n[201] T. L. Grifﬁths, C. Kemp, J. B. Tenenbaum, Bayesian models of cognition.\n(4 2008).\ndoi:10.1184/R1/6613682.v1.\nURL\nhttps://kilthub.cmu.edu/articles/Bayesian_models_of_\ncognition/6613682\n[202] B. H. Neelon, A. J. O’Malley, S.-L. T. Normand, A bayesian model for repeated measures zero-\ninﬂated count data with application to outpatient psychiatric service use, Statistical modelling\n10 (4) (2010) 421–439.\n58\n\n\n[203] M. McAllister, G. Kirkwood, Bayesian stock assessment: a review and example application using\nthe logistic model, ICES Journal of Marine Science 55 (6) (1998) 1031–1060.\n[204] G. Synnaeve, P. Bessiere, A bayesian model for opening prediction in RTS games with application\nto starcraft, in: Computational Intelligence and Games (CIG), 2011 IEEE Conference on, IEEE,\n2011, pp. 281–288.\n[205] S.-K. Min, D. Simonis, A. Hense, Probabilistic climate change predictions applying bayesian model\naveraging, Philosophical transactions of the royal society of london a: mathematical, physical and\nengineering sciences 365 (1857) (2007) 2103–2116.\n[206] G. Koop, D. J. Poirier, J. L. Tobias, Bayesian econometric methods, Cambridge University Press,\n2007.\n[207] A. R. Cassandra, L. P. Kaelbling, J. A. Kurien, Acting under uncertainty: Discrete bayesian models\nfor mobile-robot navigation, in: Proceedings of IEEE/RSJ International Conference on Intelligent\nRobots and Systems. IROS’96, Vol. 2, IEEE, 1996, pp. 963–972.\n[208] H. A. Chipman, E. I. George, R. E. McCulloch, Bayesian cart model search, Journal of the\nAmerican Statistical Association 93 (443) (1998) 935–948.\n[209] B. Kim, C. Rudin, J. A. Shah, The bayesian case model: A generative approach for case-based\nreasoning and prototype classiﬁcation, in: Advances in Neural Information Processing Systems,\n2014, pp. 1952–1960.\n[210] B. Kim, R. Khanna, O. O. Koyejo, Examples are not enough, learn to criticize! criticism for\ninterpretability, in: Advances in Neural Information Processing Systems, 2016, pp. 2280–2288.\n[211] U. Johansson, L. Niklasson, R. K¨onig, Accuracy vs. comprehensibility in data mining models,\nin: Proceedings of the seventh international conference on information fusion, Vol. 1, 2004, pp.\n295–300.\n[212] R. Konig, U. Johansson, L. Niklasson, G-rex: A versatile framework for evolutionary data mining,\nin: 2008 IEEE International Conference on Data Mining Workshops, IEEE, 2008, pp. 971–974.\n[213] H. Lakkaraju, E. Kamar, R. Caruana, J. Leskovec, Interpretable & explorable approximations of\nblack box models (2017). arXiv:1707.01154.\n[214] S. Mishra, B. L. Sturm, S. Dixon, Local interpretable model-agnostic explanations for music\ncontent analysis., in: ISMIR, 2017, pp. 537–543.\n[215] G. Su, D. Wei, K. R. Varshney, D. M. Malioutov, Interpretable two-level boolean rule learning for\nclassiﬁcation (2015). arXiv:1511.07361.\n[216] M. T. Ribeiro, S. Singh, C. Guestrin, Nothing else matters: Model-agnostic explanations by\nidentifying prediction invariance (2016). arXiv:1611.05817.\n[217] M. W. Craven, Extracting comprehensible models from trained neural networks, Ph.D. thesis,\naAI9700774 (1996).\n[218] O. Bastani, C. Kim, H. Bastani, Interpretability via model extraction (2017). arXiv:1706.09773.\n[219] G. Hooker, Discovering additive structure in black box functions, in: Proceedings of the tenth\nACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2004,\npp. 575–580.\n59\n\n\n[220] P. Adler, C. Falk, S. A. Friedler, T. Nix, G. Rybeck, C. Scheidegger, B. Smith, S. Venkatasubra-\nmanian, Auditing black-box models for indirect inﬂuence, Knowledge and Information Systems\n54 (1) (2018) 95–122.\n[221] P. W. Koh, P. Liang, Understanding black-box predictions via inﬂuence functions, in: Proceedings\nof the 34th International Conference on Machine Learning-Volume 70, JMLR. org, 2017, pp.\n1885–1894.\n[222] P. Cortez, M. J. Embrechts, Opening black box data mining models using sensitivity analysis, in:\n2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2011, pp.\n341–348.\n[223] P. Cortez, M. J. Embrechts, Using sensitivity analysis and visualization techniques to open black\nbox data mining models, Information Sciences 225 (2013) 1–17.\n[224] S. M. Lundberg, S.-I. Lee, A uniﬁed approach to interpreting model predictions, in: Advances in\nNeural Information Processing Systems, 2017, pp. 4765–4774.\n[225] I. Kononenko, et al., An efﬁcient explanation of individual classiﬁcations using game theory,\nJournal of Machine Learning Research 11 (Jan) (2010) 1–18.\n[226] H. Chen, S. Lundberg, S.-I. Lee, Explaining models by propagating shapley values of local\ncomponents (2019). arXiv:arXiv:1911.11888.\n[227] P. Dabkowski, Y. Gal, Real time image saliency for black box classiﬁers, in: Advances in Neural\nInformation Processing Systems, 2017, pp. 6967–6976.\n[228] A. Henelius, K. Puolam¨aki, H. Bostr¨om, L. Asker, P. Papapetrou, A peek into the black box:\nexploring classiﬁers by randomization, Data mining and knowledge discovery 28 (5-6) (2014)\n1503–1529.\n[229] J. Moeyersoms, B. d’Alessandro, F. Provost, D. Martens, Explaining classiﬁcation models built on\nhigh-dimensional sparse data (2016). arXiv:1607.06280.\n[230] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, K.-R. M ˜Aˇzller, How to\nexplain individual classiﬁcation decisions, Journal of Machine Learning Research 11 (Jun) (2010)\n1803–1831.\n[231] J. Adebayo, L. Kagal, Iterative orthogonal feature projection for diagnosing bias in black-box\nmodels (2016). arXiv:1611.04967.\n[232] R. Guidotti, A. Monreale, S. Ruggieri, D. Pedreschi, F. Turini, F. Giannotti, Local rule-based\nexplanations of black box decision systems (2018). arXiv:1805.10820.\n[233] S. Krishnan, E. Wu, Palm: Machine learning explanations for iterative debugging, in: Proceedings\nof the 2nd Workshop on Human-In-the-Loop Data Analytics, ACM, 2017, p. 4.\n[234] M. Robnik-ˇSikonja, I. Kononenko, Explaining classiﬁcations for individual instances, IEEE\nTransactions on Knowledge and Data Engineering 20 (5) (2008) 589–600.\n[235] M. T. Ribeiro, S. Singh, C. Guestrin, Anchors: High-precision model-agnostic explanations, in:\nAAAI Conference on Artiﬁcial Intelligence, 2018, pp. 1527–1535.\n[236] D. Martens, F. Provost, Explaining data-driven document classiﬁcations, MIS Quarterly 38 (1)\n(2014) 73–100.\n60\n\n\n[237] D. Chen, S. P. Fraiberger, R. Moakler, F. Provost, Enhancing transparency and control when\ndrawing data-driven inferences about individuals, Big data 5 (3) (2017) 197–212.\n[238] A. Goldstein, A. Kapelner, J. Bleich, E. Pitkin, Peeking inside the black box: Visualizing statistical\nlearning with plots of individual conditional expectation, Journal of Computational and Graphical\nStatistics 24 (1) (2015) 44–65.\n[239] G. Casalicchio, C. Molnar, B. Bischl, Visualizing the feature importance for black box models,\nin: Joint European Conference on Machine Learning and Knowledge Discovery in Databases,\nSpringer, 2018, pp. 655–670.\n[240] G. Tolomei, F. Silvestri, A. Haines, M. Lalmas, Interpretable predictions of tree-based ensembles via\nactionable feature tweaking, in: Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, ACM, 2017, pp. 465–474.\n[241] L. Auret, C. Aldrich, Interpretation of nonlinear relationships between process variables by use of\nrandom forests, Minerals Engineering 35 (2012) 27–42.\n[242] N. F. Rajani, R. Mooney, Stacking with auxiliary features for visual question answering, in: Proceed-\nings of the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 2217–2226.\n[243] N. F. Rajani, R. J. Mooney, Ensembling visual explanations, in: Explainable and Interpretable\nModels in Computer Vision and Machine Learning, Springer, 2018, pp. 155–172.\n[244] H. N´u˜nez, C. Angulo, A. Catal`a, Rule-based learning systems for support vector machines, Neural\nProcessing Letters 24 (1) (2006) 1–18.\n[245] Z. Chen, J. Li, L. Wei, A multiple kernel support vector machine scheme for feature selection and\nrule extraction from gene expression data of cancer tissue, Artiﬁcial Intelligence in Medicine 41 (2)\n(2007) 161–175.\n[246] H. N´u˜nez, C. Angulo, A. Catal`a, Support vector machines with symbolic interpretation, in: VII\nBrazilian Symposium on Neural Networks, 2002. SBRN 2002. Proceedings., IEEE, 2002, pp.\n142–147.\n[247] P. Sollich, Bayesian methods for support vector machines: Evidence and predictive class probabili-\nties, Machine learning 46 (1-3) (2002) 21–52.\n[248] P. Sollich, Probabilistic methods for support vector machines, in: Advances in neural information\nprocessing systems, 2000, pp. 349–355.\n[249] W. Landecker, M. D. Thomure, L. M. Bettencourt, M. Mitchell, G. T. Kenyon, S. P. Brumby,\nInterpreting individual classiﬁcations of hierarchical networks, in: 2013 IEEE Symposium on\nComputational Intelligence and Data Mining (CIDM), IEEE, 2013, pp. 32–38.\n[250] A. Jakulin, M. Moˇzina, J. Demˇsar, I. Bratko, B. Zupan, Nomograms for visualizing support vector\nmachines, in: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge\ndiscovery in data mining, ACM, 2005, pp. 108–117.\n[251] L. Fu, Rule generation from neural networks, IEEE Transactions on Systems, Man, and Cybernetics\n24 (8) (1994) 1114–1124.\n[252] G. G. Towell, J. W. Shavlik, Extracting reﬁned rules from knowledge-based neural networks,\nMachine Learning 13 (1) (1993) 71–101.\n61\n\n\n[253] S. Thrun, Extracting rules from artiﬁcial neural networks with distributed representations, in:\nProceedings of the 7th International Conference on Neural Information Processing Systems,\nNIPS’94, 1994, pp. 505–512.\n[254] R. Setiono, W. K. Leow, FERNN: An algorithm for fast extraction of rules from neural networks,\nApplied Intelligence 12 (1) (2000) 15–25.\n[255] I. A. Taha, J. Ghosh, Symbolic interpretation of artiﬁcial neural networks, IEEE Transactions on\nKnowledge and Data Engineering 11 (3) (1999) 448–463.\n[256] H. Tsukimoto, Extracting rules from trained neural networks, IEEE Transactions on Neural\nNetworks 11 (2) (2000) 377–389.\n[257] J. R. Zilke, E. L. Menc´ıa, F. Janssen, Deepred–rule extraction from deep neural networks, in:\nInternational Conference on Discovery Science, Springer, 2016, pp. 457–473.\n[258] G. P. J. Schmitz, C. Aldrich, F. S. Gouws, ANN-DT: an algorithm for extraction of decision trees\nfrom artiﬁcial neural networks, IEEE Transactions on Neural Networks 10 (6) (1999) 1392–1401.\n[259] M. Sato, H. Tsukimoto, Rule extraction from neural networks via decision tree induction, in:\nIJCNN’01. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222),\nVol. 3, IEEE, 2001, pp. 1870–1875.\n[260] R. F´eraud, F. Cl´erot, A methodology to explain neural network classiﬁcation, Neural networks\n15 (2) (2002) 237–246.\n[261] A. Shrikumar, P. Greenside, A. Kundaje, Learning Important Features Through Propagating\nActivation Differences (2017). arXiv:1704.02685.\n[262] M. Sundararajan, A. Taly, Q. Yan, Axiomatic attribution for deep networks, in: International\nConference on Machine Learning, Vol. 70, JMLR. org, 2017, pp. 3319–3328.\n[263] J. Adebayo, J. Gilmer, I. Goodfellow, B. Kim, Local explanation methods for deep neural networks\nlack sensitivity to parameter values (2018). arXiv:1810.03307.\n[264] N. Papernot, P. McDaniel, Deep k-nearest neighbors: Towards conﬁdent, interpretable and robust\ndeep learning (2018). arXiv:1803.04765.\n[265] J. Li, X. Chen, E. Hovy, D. Jurafsky, Visualizing and understanding neural models in NLP (2015).\narXiv:1506.01066.\n[266] S. Tan, K. C. Sim, M. Gales, Improving the interpretability of deep neural networks with stimulated\nlearning, in: 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),\nIEEE, 2015, pp. 617–623.\n[267] L. Rieger, C. Singh, W. J. Murdoch, B. Yu, Interpretations are useful: penalizing explanations to\nalign neural networks with prior knowledge (2019). arXiv:arXiv:1909.13584.\n[268] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, J. Clune, Synthesizing the preferred inputs for\nneurons in neural networks via deep generator networks, in: Advances in Neural Information\nProcessing Systems, 2016, pp. 3387–3395.\n[269] Y. Li, J. Yosinski, J. Clune, H. Lipson, J. E. Hopcroft, Convergent learning: Do different neural\nnetworks learn the same representations?, in: ICLR, 2016.\n62\n\n\n[270] M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, S. Liu, Towards better analysis of deep convolutional neural\nnetworks, IEEE transactions on visualization and computer graphics 23 (1) (2016) 91–100.\n[271] Y. Goyal, A. Mohapatra, D. Parikh, D. Batra, Towards transparent AI systems: Interpreting visual\nquestion answering models (2016). arXiv:1608.08974.\n[272] K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualising image\nclassiﬁcation models and saliency maps (2013). arXiv:1312.6034.\n[273] A. Nguyen, J. Yosinski, J. Clune, Deep neural networks are easily fooled: High conﬁdence\npredictions for unrecognizable images, in: Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2015, pp. 427–436.\n[274] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko,\nT. Darrell, Long-term recurrent convolutional networks for visual recognition and description,\nin: Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp.\n2625–2634.\n[275] M. Lin, Q. Chen, S. Yan, Network in network (2013). arXiv:1312.4400.\n[276] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell, Generating Visual\nExplanations (2016). arXiv:1603.08507.\n[277] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, X. Tang, Residual attention\nnetwork for image classiﬁcation, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017, pp. 3156–3164.\n[278] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, Z. Zhang, The application of two-level attention models\nin deep convolutional neural network for ﬁne-grained image classiﬁcation, in: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 842–850.\n[279] Q. Zhang, R. Cao, Y. Nian Wu, S.-C. Zhu, Growing Interpretable Part Graphs on ConvNets via\nMulti-Shot Learning (2016). arXiv:1611.04246.\n[280] L. Arras, G. Montavon, K.-R. M¨uller, W. Samek, Explaining recurrent neural network predictions\nin sentiment analysis (2017). arXiv:1706.07206.\n[281] A. Karpathy, J. Johnson, L. Fei-Fei, Visualizing and understanding recurrent networks (2015).\narXiv:1506.02078.\n[282] J. Clos, N. Wiratunga, S. Massie, Towards explainable text classiﬁcation by jointly learning lexicon\nand modiﬁer terms, in: IJCAI-17 Workshop on Explainable AI (XAI), 2017, p. 19.\n[283] S. Wisdom, T. Powers, J. Pitton, L. Atlas, Interpretable recurrent neural networks using sequential\nsparse recovery (2016). arXiv:1611.07252.\n[284] V. Krakovna, F. Doshi-Velez, Increasing the interpretability of recurrent neural networks using\nhidden markov models (2016). arXiv:1606.05320.\n[285] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, W. Stewart, Retain: An interpretable\npredictive model for healthcare using reverse time attention mechanism, in: Advances in Neural\nInformation Processing Systems, 2016, pp. 3504–3512.\n[286] L. Breiman, Classiﬁcation and regression trees, Routledge, 2017.\n63\n\n\n[287] A. Lucic, H. Haned, M. de Rijke, Explaining predictions from tree-based boosting ensembles\n(2019). arXiv:arXiv:1907.02582.\n[288] S. M. Lundberg, G. G. Erion, S.-I. Lee, Consistent individualized feature attribution for tree\nensembles (2018). arXiv:arXiv:1802.03888.\n[289] C. Buciluˇa, R. Caruana, A. Niculescu-Mizil, Model compression, in: ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, ACM, 2006, pp. 535–541.\n[290] R. Traor´e, H. Caselles-Dupr´e, T. Lesort, T. Sun, G. Cai, N. D. Rodr´ıguez, D. Filliat, DisCoRL:\nContinual reinforcement learning via policy distillation (2019). arXiv:1907.05855.\n[291] M. D. Zeiler, G. W. Taylor, R. Fergus, et al., Adaptive deconvolutional networks for mid and high\nlevel feature learning., in: ICCV, Vol. 1, 2011, p. 6.\n[292] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-cam: Visual\nexplanations from deep networks via gradient-based localization, in: Proceedings of the IEEE\nInternational Conference on Computer Vision, 2017, pp. 618–626.\n[293] C. Olah, A. Mordvintsev, L. Schubert, Feature visualization., DistillHttps://distill.pub/2017/feature-\nvisualization (2017). doi:10.23915/distill.00007.\n[294] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, B. Kim, Sanity checks for saliency\nmaps, in: Advances in Neural Information Processing Systems, 2018, pp. 9505–9515.\n[295] C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, A. Mordvintsev, The building\nblocks of interpretability, Distill (2018).\nURL https://distill.pub/2018/building-blocks/\n[296] Z. Che, S. Purushotham, R. Khemani, Y. Liu, Distilling knowledge from deep networks with\napplications to healthcare domain (2015). arXiv:1512.03542.\n[297] I. Donadello, L. Seraﬁni, A. D. Garcez, Logic tensor networks for semantic image interpretation,\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence, IJCAI\n(2017) 1596–1602.\n[298] I. Donadello, Semantic image interpretation-integration of numerical data and logical knowledge\nfor cognitive vision, Ph.D. thesis, University of Trento (2018).\n[299] A. S. d’Avila Garcez, M. Gori, L. C. Lamb, L. Seraﬁni, M. Spranger, S. N. Tran, Neural-symbolic\ncomputing: An effective methodology for principled integration of machine learning and reasoning\n(2019). arXiv:1905.06088.\n[300] R. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester, L. De Raedt, DeepProbLog: Neural\nprobabilistic logic programming, in: Advances in Neural Information Processing Systems 31, 2018,\npp. 3749–3759.\n[301] I. Donadello, M. Dragoni, C. Eccher, Persuasive explanation of reasoning inferences on dietary\ndata, in: First Workshop on Semantic Explainability @ ISWC 2019, 2019.\n[302] R. G. Krishnan, U. Shalit, D. Sontag, Deep Kalman Filters (2015). arXiv:1511.05121.\n[303] M. Karl, M. Soelch, J. Bayer, P. van der Smagt, Deep Variational Bayes Filters: Unsupervised\nLearning of State Space Models from Raw Data (2016). arXiv:1605.06432.\n64\n\n\n[304] M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, S. R. Datta, Composing graphical\nmodels with neural networks for structured representations and fast inference, in: Advances in\nNeural Information Processing Systems 29, 2016, pp. 2946–2954.\n[305] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, P. H. Torr,\nConditional random ﬁelds as recurrent neural networks, in: Proceedings of the IEEE international\nconference on computer vision, 2015, pp. 1529–1537.\n[306] N. Narodytska, A. Ignatiev, F. Pereira, J. Marques-Silva, Learning optimal decision trees with SAT,\nin: Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence,\nIJCAI-18, 2018, pp. 1362–1368.\n[307] O. Loyola-Gonz´alez, Black-box vs. white-box: Understanding their advantages and weaknesses\nfrom a practical point of view, IEEE Access 7 (2019) 154096–154113.\n[308] F. Petroni, T. Rockt¨aschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, S. Riedel, Language models\nas knowledge bases? (2019). arXiv:1909.01066.\n[309] K. Bollacker, N. D´ıaz-Rodr´ıguez, X. Li, Extending knowledge graphs with subjective inﬂuence\nnetworks for personalized fashion, in: E. Portmann, M. E. Tabacchi, R. Seising, A. Habenstein\n(Eds.), Designing Cognitive Cities, Springer International Publishing, 2019, pp. 203–233.\n[310] W. Shang, A. Trott, S. Zheng, C. Xiong, R. Socher, Learning world graphs to accelerate hierarchical\nreinforcement learning (2019). arXiv:1907.00664.\n[311] M. Zolotas, Y. Demiris, Towards explainable shared control using augmented reality, 2019.\n[312] M. Garnelo, K. Arulkumaran, M. Shanahan, Towards deep symbolic reinforcement learning (2016).\narXiv:1609.05518.\n[313] V. Bellini, A. Schiavone, T. Di Noia, A. Ragone, E. Di Sciascio, Knowledge-aware autoencoders\nfor explainable recommender systems, in: Proceedings of the 3rd Workshop on Deep Learning for\nRecommender Systems, DLRS 2018, 2018, pp. 24–31.\n[314] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, C. Hawthorne, A. M. Dai, M. D. Hoffman,\nD. Eck, Music transformer: Generating music with long-term structure (2018). arXiv:1809.04281.\n[315] M. Cornia, L. Baraldi, R. Cucchiara, Smart: Training shallow memory-aware transformers for\nrobotic explainability (2019). arXiv:1910.02974.\n[316] A. Aamodt, E. Plaza, Case-based reasoning: Foundational issues, Methodological Variations, and\nSystem Approaches 7 (1) (1994) 39–59.\n[317] R. Caruana, Case-based explanation for artiﬁcial neural nets, in: Artiﬁcial Neural Networks in\nMedicine and Biology, Proceedings of the ANNIMAB-1 Conference, 2000, pp. 303–308.\n[318] M. T. Keane, E. M. Kenny, The Twin-System Approach as One Generic Solution for XAI: An\nOverview of ANN-CBR Twins for Explaining Deep Learning (2019). arXiv:1905.08069.\n[319] T. Hailesilassie, Rule extraction algorithm for deep neural networks:\nA review (2016).\narXiv:1610.05267.\n[320] J. M. Benitez, J. L. Castro, I. Requena, Are artiﬁcial neural networks black boxes?, IEEE Trans.\nNeural Networks 8 (5) (1997) 1156–1164.\n65\n\n\n[321] U. Johansson, R. K¨onig, L. Niklasson, Automatically balancing accuracy and comprehensibility in\npredictive modeling, in: Proceedings of the 8th International Conference on Information Fusion,\nVol. 2, 2005, p. 7 pp.\n[322] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, M. Wattenberg, SmoothGrad: removing noise by adding\nnoise (2017). arXiv:1706.03825.\n[323] M. Ancona, E. Ceolini, C. ¨Oztireli, M. Gross, Towards better understanding of gradient-based\nattribution methods for Deep Neural Networks (2017). arXiv:1711.06104.\n[324] J. Yosinski, J. Clune, Y. Bengio, H. Lipson, How transferable are features in deep neural networks?\n(2014). arXiv:1411.1792.\n[325] A. Sharif Razavian, H. Azizpour, J. Sullivan, S. Carlsson, CNN Features off-the-shelf: an Astound-\ning Baseline for Recognition (2014). arXiv:1403.6382.\n[326] S. Du, H. Guo, A. Simpson, Self-driving car steering angle prediction based on image recognition,\nTech. rep., Technical Report, Stanford University (2017).\n[327] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Object Detectors Emerge in Deep Scene\nCNNs (2014). arXiv:1412.6856.\n[328] Y. Zhang, X. Chen, Explainable Recommendation: A Survey and New Perspectives (2018).\narXiv:1804.11192.\n[329] J. Frankle, M. Carbin, The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n(2018). arXiv:1803.03635.\n[330] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\nAttention Is All You Need (2017). arXiv:1706.03762.\n[331] J. Lu, J. Yang, D. Batra, D. Parikh, Hierarchical question-image co-attention for visual question\nanswering, in: Proceedings of the 30th International Conference on Neural Information Processing\nSystems, NIPS’16, 2016, pp. 289–297.\n[332] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, D. Batra, Human Attention in Visual Question\nAnswering: Do Humans and Deep Networks Look at the Same Regions? (2016). arXiv:1606.03556.\n[333] D. Huk Park, L. A. Hendricks, Z. Akata, A. Rohrbach, B. Schiele, T. Darrell, M. Rohrbach, Multi-\nmodal Explanations: Justifying Decisions and Pointing to the Evidence (2018). arXiv:1802.08129.\n[334] A. Slavin Ross, M. C. Hughes, F. Doshi-Velez, Right for the Right Reasons: Training Differentiable\nModels by Constraining their Explanations (2017). arXiv:1703.03717.\n[335] I. T. Jolliffe, Principal Component Analysis and Factor Analysis, Springer New York, 1986, pp.\n115–128.\n[336] A. Hyv¨arinen, E. Oja, Oja, e.: Independent component analysis: Algorithms and applications.\nneural networks 13(4-5), 411-430, Neural networks 13 (2000) 411–430.\n[337] M. W. Berry, M. Browne, A. N. Langville, V. P. Pauca, R. J. Plemmons, Algorithms and applications\nfor approximate nonnegative matrix factorization, Computational Statistics & Data Analysis 52\n(2007) 155–173.\n[338] D. P. Kingma, M. Welling, Auto-Encoding Variational Bayes (2013). arXiv:1312.6114.\n66\n\n\n[339] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. M. Botvinick, S. Mohamed, A. Lerchner,\nbeta-vae: Learning basic visual concepts with a constrained variational framework, in: ICLR, 2017.\n[340] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, P. Abbeel, InfoGAN: Inter-\npretable Representation Learning by Information Maximizing Generative Adversarial Nets (2016).\narXiv:1606.03657.\n[341] Q. Zhang, Y. Yang, Y. Liu, Y. Nian Wu, S.-C. Zhu, Unsupervised Learning of Neural Networks to\nExplain Neural Networks (2018). arXiv:1805.07468.\n[342] S. Sabour, N. Frosst, G. E Hinton, Dynamic Routing Between Capsules (2017). arXiv:1710.09829.\n[343] A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Batra, D. Parikh, VQA: Visual Question\nAnswering (2015). arXiv:1505.00468.\n[344] A. Fukui, D. Huk Park, D. Yang, A. Rohrbach, T. Darrell, M. Rohrbach, Multimodal Compact\nBilinear Pooling for Visual Question Answering and Visual Grounding (2016). arXiv:1606.01847.\n[345] D. Bouchacourt, L. Denoyer, EDUCE: explaining model decisions through unsupervised concepts\nextraction (2019). arXiv:1905.11852.\n[346] C. Hofer, M. Denker, S. Ducasse, Design and Implementation of a Backward-In-Time Debugger,\nin: NODe 2006, Vol. P-88 of Lecture Notes in Informatics, 2006, pp. 17–32.\n[347] C. Rudin, Please stop explaining black box models for high stakes decisions (2018).\narXiv:1811.10154.\n[348] A. Diez-Olivan, J. Del Ser, D. Galar, B. Sierra, Data fusion and machine learning for industrial\nprognosis: Trends and perspectives towards Industry 4.0, Information Fusion 50 (2019) 92–111.\n[349] R. R. Hoffman, S. T. Mueller, G. Klein, J. Litman, Metrics for explainable ai: Challenges and\nprospects (2018). arXiv:arXiv:1812.04608.\n[350] S. Mohseni, N. Zarei, E. D. Ragan, A multidisciplinary survey and framework for design and\nevaluation of explainable ai systems (2018). arXiv:arXiv:1811.11839.\n[351] R. M. J. Byrne, Counterfactuals in explainable artiﬁcial intelligence (XAI): Evidence from human\nreasoning, in: Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-19, 2019, pp. 6276–6282.\n[352] M. Garnelo, M. Shanahan, Reconciling deep learning with symbolic artiﬁcial intelligence: repre-\nsenting objects and relations, Current Opinion in Behavioral Sciences 29 (2019) 17–23.\n[353] G. Marra, F. Giannini, M. Diligenti, M. Gori, Integrating learning and reasoning with deep logic\nmodels (2019). arXiv:1901.04195.\n[354] K. Kelley, B. Clark, V. Brown, J. Sitzia, Good practice in the conduct and reporting of survey\nresearch, International Journal for Quality in Health Care 15 (3) (2003) 261–266.\n[355] S. Wachter, B. Mittelstadt, L. Floridi, Why a right to explanation of automated decision-making\ndoes not exist in the general data protection regulation, International Data Privacy Law 7 (2) (2017)\n76–99.\n[356] T. Orekondy, B. Schiele, M. Fritz, Knockoff nets: Stealing functionality of black-box models\n(2018). arXiv:1812.02766.\n67\n\n\n[357] S. J. Oh, B. Schiele, M. Fritz, Towards reverse-engineering black-box neural networks, in: Explain-\nable AI: Interpreting, Explaining and Visualizing Deep Learning, Springer, 2019, pp. 121–144.\n[358] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing adversarial examples (2014).\narXiv:1412.6572.\n[359] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, D. Song,\nRobust physical-world attacks on deep learning models (2017). arXiv:1707.08945.\n[360] I. J. Goodfellow, N. Papernot, P. D. McDaniel, cleverhans v0.1: an adversarial machine learning\nlibrary (2016). arXiv:1610.00768.\n[361] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, F. Roli, Support vector machines under\nadversarial label contamination, Neurocomputing 160 (C) (2015) 53–62.\n[362] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov, G. Giacinto, F. Roli, Evasion\nattacks against machine learning at test time, in: Proceedings of the 2013th European Conference\non Machine Learning and Knowledge Discovery in Databases - Volume Part III, ECMLPKDD’13,\n2013, pp. 387–402.\n[363] B. Biggio, I. Pillai, S. R. Bul`o, D. Ariu, M. Pelillo, F. Roli, Is data clustering in adversarial settings\nsecure? (2018). arXiv:1811.09982.\n[364] Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan, Y. Zheng, Recent progress on generative adversarial\nnetworks (gans): A survey, IEEE Access 7 (2019) 36322–36333.\n[365] D. Charte, F. Charte, S. Garc´ıa, M. J. del Jesus, F. Herrera, A practical tutorial on autoencoders\nfor nonlinear feature fusion: Taxonomy, models, software and guidelines, Information Fusion 44\n(2018) 78–96.\n[366] C. F. Baumgartner, L. M. Koch, K. Can Tezcan, J. Xi Ang, E. Konukoglu, Visual feature attribution\nusing wasserstein gans, in: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 8309–8319.\n[367] C. Bifﬁ, O. Oktay, G. Tarroni, W. Bai, A. De Marvao, G. Doumou, M. Rajchl, R. Bedair, S. Prasad,\nS. Cook, et al., Learning interpretable anatomical features through deep generative models: Ap-\nplication to cardiac remodeling, in: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2018, pp. 464–471.\n[368] S. Liu, B. Kailkhura, D. Loveland, Y. Han, Generative counterfactual introspection for explainable\ndeep learning (2019). arXiv:arXiv:1907.03077.\n[369] K. R. Varshney, H. Alemzadeh, On the safety of machine learning: Cyber-physical systems,\ndecision sciences, and data products, Big data 5 (3) (2017) 246–255.\n[370] G. M. Weiss, Mining with rarity: a unifying framework, ACM Sigkdd Explorations Newsletter\n6 (1) (2004) 7–19.\n[371] J. Attenberg, P. Ipeirotis, F. Provost, Beat the machine: Challenging humans to ﬁnd a predictive\nmodel’s “unknown unknowns”, Journal of Data and Information Quality (JDIQ) 6 (1) (2015) 1.\n[372] G. Neff, A. Tanweer, B. Fiore-Gartland, L. Osburn, Critique and contribute: A practice-based\nframework for improving critical data studies and data science, Big data 5 (2) (2017) 85–97.\n[373] A. Iliadis, F. Russo, Critical data studies: An introduction, Big Data & Society 3 (2) (2016)\n2053951716674238.\n68\n\n\n[374] A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar,\nN. Samatova, V. Kumar, Theory-guided data science: A new paradigm for scientiﬁc discovery\nfrom data, IEEE Transactions on Knowledge and Data Engineering 29 (10) (2017) 2318–2331.\n[375] G. Hautier, C. C. Fischer, A. Jain, T. Mueller, G. Ceder, Finding nature’s missing ternary oxide\ncompounds using machine learning and density functional theory, Chemistry of Materials 22 (12)\n(2010) 3762–3767.\n[376] C. C. Fischer, K. J. Tibbetts, D. Morgan, G. Ceder, Predicting crystal structure by merging data\nmining with quantum mechanics, Nature materials 5 (8) (2006) 641.\n[377] S. Curtarolo, G. L. Hart, M. B. Nardelli, N. Mingo, S. Sanvito, O. Levy, The high-throughput\nhighway to computational materials design, Nature materials 12 (3) (2013) 191.\n[378] K. C. Wong, L. Wang, P. Shi, Active model with orthotropic hyperelastic material for cardiac image\nanalysis, in: International Conference on Functional Imaging and Modeling of the Heart, Springer,\n2009, pp. 229–238.\n[379] J. Xu, J. L. Sapp, A. R. Dehaghani, F. Gao, M. Horacek, L. Wang, Robust transmural electrophysi-\nological imaging: Integrating sparse and dynamic physiological models into ecg-based inference,\nin: International Conference on Medical Image Computing and Computer-Assisted Intervention,\nSpringer, 2015, pp. 519–527.\n[380] T. Lesort, M. Seurin, X. Li, N. D´ıaz-Rodr´ıguez, D. Filliat, Unsupervised state representation\nlearning with robotic priors: a robustness benchmark (2017). arXiv:arXiv:1709.05185.\n[381] J. Z. Leibo, Q. Liao, F. Anselmi, W. A. Freiwald, T. Poggio, View-tolerant face recognition and\nhebbian learning imply mirror-symmetric neural tuning to head orientation, Current Biology 27 (1)\n(2017) 62–67.\n[382] F. Schrodt, J. Kattge, H. Shan, F. Fazayeli, J. Joswig, A. Banerjee, M. Reichstein, G. B¨onisch,\nS. D´ıaz, J. Dickie, et al., Bhpmf–a hierarchical bayesian approach to gap-ﬁlling and trait prediction\nfor macroecology and functional biogeography, Global Ecology and Biogeography 24 (12) (2015)\n1510–1521.\n[383] D. Leslie, Understanding artiﬁcial intelligence ethics and safety (2019). arXiv:arXiv:1906.05684,\ndoi:10.5281/zenodo.3240529.\n[384] C. Rudin, Stop explaining black box machine learning models for high stakes decisions and use\ninterpretable models instead (2018). arXiv:arXiv:1811.10154.\n[385] J. Fjeld, H. Hilligoss, N. Achten, M. L. Daniel, J. Feldman, S. Kagay, Principled artiﬁcial intelli-\ngence: A map of ethical and rights-based approaches (2019).\nURL https://ai-hr.cyber.harvard.edu/images/primp-viz.pdf\n[386] R. Benjamins, A. Barbado, D. Sierra, Responsible AI by design (2019). arXiv:1909.12838.\n[387] United-Nations, Transforming our world: the 2030 agenda for sustainable development, Tech. rep.,\neSocialSciences (2015).\nURL https://EconPapers.repec.org/RePEc:ess:wpaper:id:7559\n[388] G. D. Hager, A. Drobnis, F. Fang, R. Ghani, A. Greenwald, T. Lyons, D. C. Parkes,\nJ. Schultz, S. Saria, S. F. Smith, M. Tambe, Artiﬁcial intelligence for social good (2019).\narXiv:arXiv:1901.05406.\n69\n\n\n[389] B. C. Stahl, D. Wright, Ethics and privacy in ai and big data: Implementing responsible research\nand innovation, IEEE Security & Privacy 16 (3) (2018) 26–33.\n[390] High Level Expert Group on Artiﬁcial Intelligence, Ethics guidelines for trustworthy ai, Tech. rep.,\nEuropean Commission (2019).\n[391] B. d’Alessandro, C. O’Neil, T. LaGatta, Conscientious classiﬁcation: A data scientist’s guide to\ndiscrimination-aware classiﬁcation, Big data 5 (2) (2017) 120–134.\n[392] S. Barocas, A. D. Selbst, Big data’s disparate impact, Calif. L. Rev. 104 (2016) 671.\n[393] M. Hardt, E. Price, N. Srebro, et al., Equality of opportunity in supervised learning, in: Advances\nin neural information processing systems, 2016, pp. 3315–3323.\n[394] T. Speicher, H. Heidari, N. Grgic-Hlaca, K. P. Gummadi, A. Singla, A. Weller, M. B. Zafar, A\nuniﬁed approach to quantifying algorithmic unfairness: Measuring individual group unfairness\nvia inequality indices, in: Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery Data Mining, ACM, 2018, pp. 2239–2248.\n[395] F. Kamiran, T. Calders, Data preprocessing techniques for classiﬁcation without discrimination,\nKnowledge and Information Systems 33 (1) (2012) 1–33.\n[396] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, C. Dwork, Learning fair representations, in: International\nConference on Machine Learning, 2013, pp. 325–333.\n[397] B. H. Zhang, B. Lemoine, M. Mitchell, Mitigating unwanted biases with adversarial learning, in:\nProceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, ACM, 2018, pp.\n335–340.\n[398] Y. Ahn, Y.-R. Lin, Fairsight: Visual analytics for fairness in decision making, IEEE transactions on\nvisualization and computer graphics (2019).\n[399] E. Soares, P. Angelov, Fair-by-design explainable models for prediction of recidivism, arXiv\npreprint arXiv:1910.02043 (2019).\n[400] J. Dressel, H. Farid, The accuracy, fairness, and limits of predicting recidivism, Science advances\n4 (1) (2018) eaao5580.\n[401] U. Aivodji, H. Arai, O. Fortineau, S. Gambs, S. Hara, A. Tapp, Fairwashing: the risk of rationaliza-\ntion, in: International Conference on Machine Learning, 2019, pp. 161–170.\n[402] S. Sharma, J. Henderson, J. Ghosh, Certifai:\nCounterfactual explanations for robustness,\ntransparency, interpretability, and fairness of artiﬁcial intelligence models, arXiv preprint\narXiv:1905.07857 (2019).\n[403] M. Drosou, H. Jagadish, E. Pitoura, J. Stoyanovich, Diversity in big data: A review, Big data 5 (2)\n(2017) 73–84.\n[404] J. Lerman, Big data and its exclusions, Stan. L. Rev. Online 66 (2013) 55.\n[405] R. Agrawal, S. Gollapudi, A. Halverson, S. Ieong, Diversifying search results, in: Proceedings of\nthe second ACM international conference on web search and data mining, ACM, 2009, pp. 5–14.\n[406] B. Smyth, P. McClave, Similarity vs. diversity, in: International conference on case-based reasoning,\nSpringer, 2001, pp. 347–361.\n70\n\n\n[407] P. Wang, L. T. Yang, J. Li, J. Chen, S. Hu, Data fusion in cyber-physical-social systems: State-of-\nthe-art and perspectives, Information Fusion 51 (2019) 42–57.\n[408] W. Ding, X. Jing, Z. Yan, L. T. Yang, A survey on data fusion in internet of things: Towards secure\nand privacy-preserving fusion, Information Fusion 51 (2019) 129–144.\n[409] A. Smirnov, T. Levashova, Knowledge fusion patterns: A survey, Information Fusion 52 (2019)\n31–40.\n[410] W. Ding, X. Jing, Z. Yan, L. T. Yang, A survey on data fusion in internet of things: Towards secure\nand privacy-preserving fusion, Information Fusion 51 (2019) 129–144.\n[411] P. Wang, L. T. Yang, J. Li, J. Chen, S. Hu, Data fusion in cyber-physical-social systems: State-of-\nthe-art and perspectives, Information Fusion 51 (2019) 42–57.\n[412] B. P. L. Lau, S. H. Marakkalage, Y. Zhou, N. U. Hassan, C. Yuen, M. Zhang, U.-X. Tan, A survey\nof data fusion in smart city applications, Information Fusion 52 (2019) 357–374.\n[413] S. Ram´ırez-Gallego, A. Fern´andez, S. Garc´ıa, M. Chen, F. Herrera, Big data: Tutorial and guidelines\non information and process fusion for analytics algorithms with mapreduce, Information Fusion 42\n(2018) 51–61.\n[414] J. Koneˇcn´y, H. B. McMahan, D. Ramage, P. Richt´arik, Federated optimization: Distributed machine\nlearning for on-device intelligence (2016). arXiv:1610.02527.\n[415] B. McMahan, E. Moore, D. Ramage, S. Hampson, B. A. y Arcas, Communication-efﬁcient\nlearning of deep networks from decentralized data, in: Artiﬁcial Intelligence and Statistics, 2017,\npp. 1273–1282.\n[416] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, D. Bacon, Federated learning:\nStrategies for improving communication efﬁciency (2016). arXiv:1610.05492.\n[417] S. Sun, A survey of multi-view machine learning, Neural computing and applications 23 (7-8)\n(2013) 2031–2038.\n[418] R. Zhang, F. Nie, X. Li, X. Wei, Feature selection with multi-view data: A survey, Information\nFusion 50 (2019) 158–167.\n[419] J. Zhao, X. Xie, X. Xu, S. Sun, Multi-view learning overview: Recent progress and new challenges,\nInformation Fusion 38 (2017) 43–54.\n[420] S. J. Oh, R. Benenson, M. Fritz, B. Schiele, Faceless person recognition: Privacy implications\nin social media, in: Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam,\nProceedings, Part III, 2016, pp. 19–35.\n[421] P. Aditya, R. Sen, P. Druschel, S. Joon Oh, R. Benenson, M. Fritz, B. Schiele, B. Bhattacharjee,\nT. T. Wu, I-pic: A platform for privacy-compliant image capture, in: Proceedings of the 14th annual\ninternational conference on mobile systems, applications, and services, ACM, 2016, pp. 235–248.\n[422] Q. Sun, A. Tewari, W. Xu, M. Fritz, C. Theobalt, B. Schiele, A hybrid model for identity obfuscation\nby face replacement, in: Proceedings of the European Conference on Computer Vision (ECCV),\n2018, pp. 553–569.\n[423] X. L. Dong, D. Srivastava, Big data integration, in: 2013 IEEE 29th international conference on\ndata engineering (ICDE), IEEE, 2013, pp. 1245–1248.\n71\n\n\n[424] D. Zhang, J. Zhao, F. Zhang, T. He, comobile: Real-time human mobility modeling at urban scale\nusing multi-view learning, in: Proceedings of the 23rd SIGSPATIAL International Conference on\nAdvances in Geographic Information Systems, ACM, 2015, p. 40.\n[425] S. J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on knowledge and data\nengineering 22 (10) (2009) 1345–1359.\n[426] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji,\nT. Gebru, Model cards for model reporting, in: Proceedings of the Conference on Fairness,\nAccountability, and Transparency, ACM, 2019, pp. 220–229.\n72\n",
  "normalized_text": "Explainable Artiﬁcial Intelligence (XAI): Concepts, Taxonomies,\nOpportunities and Challenges toward Responsible AI\nAlejandro Barredo Arrietaa, Natalia D´ıaz-Rodr´ıguezb, Javier Del Sera,c,d, Adrien Bennetotb,e,f,\nSiham Tabikg, Alberto Barbadoh, Salvador Garciag, Sergio Gil-Lopeza, Daniel Molinag,\nRichard Benjaminsh, Raja Chatilaf, and Francisco Herrerag\naTECNALIA, 48160 Derio, Spain\nbENSTA, Institute Polytechnique Paris and INRIA Flowers Team, Palaiseau, France\ncUniversity of the Basque Country (UPV/EHU), 48013 Bilbao, Spain\ndBasque Center for Applied Mathematics (BCAM), 48009 Bilbao, Bizkaia, Spain\neSegula Technologies, Parc d’activit´e de Pissaloup, Trappes, France\nfInstitut des Syst`emes Intelligents et de Robotique, Sorbonne Universit`e, France\ngDaSCI Andalusian Institute of Data Science and Computational Intelligence, University of Granada, 18071 Granada, Spain\nhTelefonica, 28050 Madrid, Spain\nAbstract\nIn the last few years, Artiﬁcial Intelligence (AI) has achieved a notable momentum that, if harnessed\nappropriately, may deliver the best of expectations over many application sectors across the ﬁeld. For this\nto occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability,\nan inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural\nNetworks) that were not present in the last hype of AI (namely, expert systems and rule based models).\nParadigms underlying this problem fall within the so-called eXplainable AI (XAI) ﬁeld, which is widely\nacknowledged as a crucial feature for the practical deployment of AI models. The overview presented in\nthis article examines the existing literature and contributions already done in the ﬁeld of XAI, including a\nprospect toward what is yet to be reached. For this purpose we summarize previous efforts made to deﬁne\nexplainability in Machine Learning, establishing a novel deﬁnition of explainable Machine Learning that\ncovers such prior conceptual propositions with a major focus on the audience for which the explainability\nis sought. Departing from this deﬁnition, we propose and discuss about a taxonomy of recent contributions\nrelated to the explainability of different Machine Learning models, including those aimed at explaining\nDeep Learning methods for which a second dedicated taxonomy is built and examined in detail. This\ncritical literature analysis serves as the motivating background for a series of challenges faced by XAI,\nsuch as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept\nof Responsible Artiﬁcial Intelligence, namely, a methodology for the large-scale implementation of AI\nmethods in real organizations with fairness, model explainability and accountability at its core. Our\nultimate goal is to provide newcomers to the ﬁeld of XAI with a thorough taxonomy that can serve\nas reference material in order to stimulate future research advances, but also to encourage experts and\nprofessionals from other disciplines to embrace the beneﬁts of AI in their activity sectors, without any\nprior bias for its lack of interpretability.\nKeywords: Explainable Artiﬁcial Intelligence, Machine Learning, Deep Learning, Data Fusion,\nInterpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible\nArtiﬁcial Intelligence.\n∗Corresponding author. TECNALIA. P. Tecnologico, Ed. 700. 48170 Derio (Bizkaia), Spain. E-mail: javier.delser@tecnalia.com\nPreprint submitted to Information Fusion\nDecember 26, 2019\n\n1. Introduction\nArtiﬁcial Intelligence (AI) lies at the core of many activity sectors that have embraced new information\ntechnologies [1]. While the roots of AI trace back to several decades ago, there is a clear consensus on the\nparamount importance featured nowadays by intelligent machines endowed with learning, reasoning and\nadaptation capabilities. It is by virtue of these capabilities that AI methods are achieving unprecedented\nlevels of performance when learning to solve increasingly complex computational tasks, making them\npivotal for the future development of the human society [2]. The sophistication of AI-powered systems\nhas lately increased to such an extent that almost no human intervention is required for their design\nand deployment. When decisions derived from such systems ultimately affect humans’ lives (as in e.g.\nmedicine, law or defense), there is an emerging need for understanding how such decisions are furnished\nby AI methods [3].\nWhile the very ﬁrst AI systems were easily interpretable, the last years have witnessed the rise of\nopaque decision systems such as Deep Neural Networks (DNNs). The empirical success of Deep Learning\n(DL) models such as DNNs stems from a combination of efﬁcient learning algorithms and their huge\nparametric space. The latter space comprises hundreds of layers and millions of parameters, which makes\nDNNs be considered as complex black-box models [4]. The opposite of black-box-ness is transparency,\ni.e., the search for a direct understanding of the mechanism by which a model works [5].\nAs black-box Machine Learning (ML) models are increasingly being employed to make important\npredictions in critical contexts, the demand for transparency is increasing from the various stakeholders in\nAI [6]. The danger is on creating and using decisions that are not justiﬁable, legitimate, or that simply do\nnot allow obtaining detailed explanations of their behaviour [7]. Explanations supporting the output of a\nmodel are crucial, e.g., in precision medicine, where experts require far more information from the model\nthan a simple binary prediction for supporting their diagnosis [8]. Other examples include autonomous\nvehicles in transportation, security, and ﬁnance, among others.\nIn general, humans are reticent to adopt techniques that are not directly interpretable, tractable and\ntrustworthy [9], given the increasing demand for ethical AI [3]. It is customary to think that by focusing\nsolely on performance, the systems will be increasingly opaque. This is true in the sense that there is a\ntrade-off between the performance of a model and its transparency [10]. However, an improvement in the\nunderstanding of a system can lead to the correction of its deﬁciencies. When developing a ML model,\nthe consideration of interpretability as an additional design driver can improve its implementability for 3\nreasons:\n• Interpretability helps ensure impartiality in decision-making, i.e. to detect, and consequently, correct\nfrom bias in the training dataset.\n• Interpretability facilitates the provision of robustness by highlighting potential adversarial perturbations\nthat could change the prediction.\n• Interpretability can act as an insurance that only meaningful variables infer the output, i.e., guaranteeing\nthat an underlying truthful causality exists in the model reasoning.\nAll these means that the interpretation of the system should, in order to be considered practical,\nprovide either an understanding of the model mechanisms and predictions, a visualization of the model’s\ndiscrimination rules, or hints on what could perturb the model [11].\nIn order to avoid limiting the effectiveness of the current generation of AI systems, eXplainable AI\n(XAI) [7] proposes creating a suite of ML techniques that 1) produce more explainable models while\nmaintaining a high level of learning performance (e.g., prediction accuracy), and 2) enable humans to\nunderstand, appropriately trust, and effectively manage the emerging generation of artiﬁcially intelligent\npartners. XAI draws as well insights from the Social Sciences [12] and considers the psychology of\nexplanation.\n2\n\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n(December 10th)\n0\n25\n50\n75\n100\n125\n150\n175\n200\n# of contributed works in the literature\nInterpretable Artiﬁcial Intelligence\nXAI\nExplainable Artiﬁcial Intelligence\nFigure 1: Evolution of the number of total publications whose title, abstract and/or keywords refer to the ﬁeld of XAI during\nthe last years. Data retrieved from Scopus R\n⃝(December 10th, 2019) by using the search terms indicated in the legend when\nquerying this database. It is interesting to note the latent need for interpretable AI models over time (which conforms to intuition, as\ninterpretability is a requirement in many scenarios), yet it has not been until 2017 when the interest in techniques to explain AI\nmodels has permeated throughout the research community.\nFigure 1 displays the rising trend of contributions on XAI and related concepts. This literature\noutbreak shares its rationale with the research agendas of national governments and agencies. Although\nsome recent surveys [8, 13, 10, 14, 15, 16, 17] summarize the upsurge of activity in XAI across sectors\nand disciplines, this overview aims to cover the creation of a complete uniﬁed framework of categories\nand concepts that allow for scrutiny and understanding of the ﬁeld of XAI methods. Furthermore, we pose\nintriguing thoughts around the explainability of AI models in data fusion contexts with regards to data\nprivacy and model conﬁdentiality. This, along with other research opportunities and challenges identiﬁed\nthroughout our study, serve as the pull factor toward Responsible Artiﬁcial Intelligence, term by which\nwe refer to a series of AI principles to be necessarily met when deploying AI in real applications. As we\nwill later show in detail, model explainability is among the most crucial aspects to be ensured within this\nmethodological framework. All in all, the novel contributions of this overview can be summarized as\nfollows:\n1. Grounded on a ﬁrst elaboration of concepts and terms used in XAI-related research, we propose a\nnovel deﬁnition of explainability that places audience (Figure 2) as a key aspect to be considered when\nexplaining a ML model. We also elaborate on the diverse purposes sought when using XAI techniques,\nfrom trustworthiness to privacy awareness, which round up the claimed importance of purpose and\ntargeted audience in model explainability.\n2. We deﬁne and examine the different levels of transparency that a ML model can feature by itself, as\nwell as the diverse approaches to post-hoc explainability, namely, the explanation of ML models that\nare not transparent by design.\n3. We thoroughly analyze the literature on XAI and related concepts published to date, covering approximately 400 contributions arranged into two different taxonomies. The ﬁrst taxonomy addresses\nthe explainability of ML models using the previously made distinction between transparency and\npost-hoc explainability, including models that are transparent by themselves, Deep and non-Deep (i.e.,\n3\n\nshallow) learning models. The second taxonomy deals with XAI methods suited for the explanation of\nDeep Learning models, using classiﬁcation criteria closely linked to this family of ML methods (e.g.\nlayerwise explanations, representation vectors, attention).\n4. We enumerate a series of challenges of XAI that still remain insufﬁciently addressed to date. Speciﬁcally, we identify research needs around the concepts and metrics to evaluate the explainability of ML\nmodels, and outline research directions toward making Deep Learning models more understandable.\nWe further augment the scope of our prospects toward the implications of XAI techniques in regards\nto conﬁdentiality, robustness in adversarial settings, data diversity, and other areas intersecting with\nexplainability.\n5. After the previous prospective discussion, we arrive at the concept of Responsible Artiﬁcial Intelligence,\na manifold concept that imposes the systematic adoption of several AI principles for AI models to\nbe of practical use. In addition to explainability, the guidelines behind Responsible AI establish that\nfairness, accountability and privacy should also be considered when implementing AI models in real\nenvironments.\n6. Since Responsible AI blends together model explainability and privacy/security by design, we call\nfor a profound reﬂection around the beneﬁts and risks of XAI techniques in scenarios dealing with\nsensitive information and/or conﬁdential ML models. As we will later show, the regulatory push\ntoward data privacy, quality, integrity and governance demands more efforts to assess the role of XAI\nin this arena. In this regard, we provide an insight on the implications of XAI in terms of privacy and\nsecurity under different data fusion paradigms.\nThe remainder of this overview is structured as follows: ﬁrst, Section 2 and subsections therein open a\ndiscussion on the terminology and concepts revolving around explainability and interpretability in AI,\nending up with the aforementioned novel deﬁnition of interpretability (Subsections 2.1 and 2.2), and a\ngeneral criterion to categorize and analyze ML models from the XAI perspective. Sections 3 and 4 proceed\nby reviewing recent ﬁndings on XAI for ML models (on transparent models and post-hoc techniques\nrespectively) that comprise the main division in the aforementioned taxonomy. We also include a review\non hybrid approaches among the two, to attain XAI. Beneﬁts and caveats of the synergies among the\nfamilies of methods are discussed in Section 5, where we present a prospect of general challenges and\nsome consequences to be cautious about. Finally, Section 6 elaborates on the concept of Responsible\nArtiﬁcial Intelligence. Section 7 concludes the survey with an outlook aimed at engaging the community\naround this vibrant research area, which has the potential to impact society, in particular those sectors that\nhave progressively embraced ML as a core technology of their activity.\n2. Explainability: What, Why, What For and How?\nBefore proceeding with our literature study, it is convenient to ﬁrst establish a common point of\nunderstanding on what the term explainability stands for in the context of AI and, more speciﬁcally,\nML. This is indeed the purpose of this section, namely, to pause at the numerous deﬁnitions that have\nbeen done in regards to this concept (what?), to argue why explainability is an important issue in AI and\nML (why? what for?) and to introduce the general classiﬁcation of XAI approaches that will drive the\nliterature study thereafter (how?).\n2.1. Terminology Clariﬁcation\nOne of the issues that hinders the establishment of common grounds is the interchangeable misuse of\ninterpretability and explainability in the literature. There are notable differences among these concepts.\nTo begin with, interpretability refers to a passive characteristic of a model referring to the level at which\na given model makes sense for a human observer. This feature is also expressed as transparency. By\n4\n\ncontrast, explainability can be viewed as an active characteristic of a model, denoting any action or\nprocedure taken by a model with the intent of clarifying or detailing its internal functions.\nTo summarize the most commonly used nomenclature, in this section we clarify the distinction and\nsimilarities among terms often used in the ethical AI and XAI communities.\n• Understandability (or equivalently, intelligibility) denotes the characteristic of a model to make a\nhuman understand its function – how the model works – without any need for explaining its internal\nstructure or the algorithmic means by which the model processes data internally [18].\n• Comprehensibility: when conceived for ML models, comprehensibility refers to the ability of a\nlearning algorithm to represent its learned knowledge in a human understandable fashion [19, 20, 21].\nThis notion of model comprehensibility stems from the postulates of Michalski [22], which stated that\n“the results of computer induction should be symbolic descriptions of given entities, semantically and\nstructurally similar to those a human expert might produce observing the same entities. Components of\nthese descriptions should be comprehensible as single ‘chunks’ of information, directly interpretable in\nnatural language, and should relate quantitative and qualitative concepts in an integrated fashion”.\nGiven its difﬁcult quantiﬁcation, comprehensibility is normally tied to the evaluation of the model\ncomplexity [17].\n• Interpretability: it is deﬁned as the ability to explain or to provide the meaning in understandable\nterms to a human.\n• Explainability: explainability is associated with the notion of explanation as an interface between\nhumans and a decision maker that is, at the same time, both an accurate proxy of the decision maker\nand comprehensible to humans [17].\n• Transparency: a model is considered to be transparent if by itself it is understandable. Since a model\ncan feature different degrees of understandability, transparent models in Section 3 are divided into three\ncategories: simulatable models, decomposable models and algorithmically transparent models [5].\nIn all the above deﬁnitions, understandability emerges as the most essential concept in XAI. Both\ntransparency and interpretability are strongly tied to this concept: while transparency refers to the\ncharacteristic of a model to be, on its own, understandable for a human, understandability measures the\ndegree to which a human can understand a decision made by a model. Comprehensibility is also connected\nto understandability in that it relies on the capability of the audience to understand the knowledge contained\nin the model. All in all, understandability is a two-sided matter: model understandability and human\nunderstandability. This is the reason why the deﬁnition of XAI given in Section 2.2 refers to the concept\nof audience, as the cognitive skills and pursued goal of the users of the model have to be taken into\naccount jointly with the intelligibility and comprehensibility of the model in use. This prominent role\ntaken by understandability makes the concept of audience the cornerstone of XAI, as we next elaborate in\nfurther detail.\n2.2. What?\nAlthough it might be considered to be beyond the scope of this paper, it is worth noting the discussion\nheld around general theories of explanation in the realm of philosophy [23]. Many proposals have been\ndone in this regard, suggesting the need for a general, uniﬁed theory that approximates the structure and\nintent of an explanation. However, nobody has stood the critique when presenting such a general theory.\nFor the time being, the most agreed-upon thought blends together different approaches to explanation\ndrawn from diverse knowledge disciplines. A similar problem is found when addressing interpretability\nin AI. It appears from the literature that there is not yet a common point of understanding on what\ninterpretability or explainability are. However, many contributions claim the achievement of interpretable\nmodels and techniques that empower explainability.\n5\n\nTo shed some light on this lack of consensus, it might be interesting to place the reference starting\npoint at the deﬁnition of the term Explainable Artiﬁcial Intelligence (XAI) given by D. Gunning in [7]:\n“XAI will create a suite of machine learning techniques that enables human users to understand,\nappropriately trust, and effectively manage the emerging generation of artiﬁcially intelligent partners”\nThis deﬁnition brings together two concepts (understanding and trust) that need to be addressed in\nadvance. However, it misses to consider other purposes motivating the need for interpretable AI models,\nsuch as causality, transferability, informativeness, fairness and conﬁdence [5, 24, 25, 26]. We will later\ndelve into these topics, mentioning them here as a supporting example of the incompleteness of the above\ndeﬁnition.\nAs exempliﬁed by the deﬁnition above, a thorough, complete deﬁnition of explainability in AI\nstill slips from our ﬁngers. A broader reformulation of this deﬁnition (e.g. “An explainable Artiﬁcial\nIntelligence is one that produces explanations about its functioning”) would fail to fully characterize the\nterm in question, leaving aside important aspects such as its purpose. To build upon the completeness, a\ndeﬁnition of explanation is ﬁrst required.\nAs extracted from the Cambridge Dictionary of English Language, an explanation is “the details or\nreasons that someone gives to make something clear or easy to understand” [27]. In the context of an\nML model, this can be rephrased as: ”the details or reasons a model gives to make its functioning clear\nor easy to understand”. It is at this point where opinions start to diverge. Inherently stemming from the\nprevious deﬁnitions, two ambiguities can be pointed out. First, the details or the reasons used to explain,\nare completely dependent of the audience to which they are presented. Second, whether the explanation\nhas left the concept clear or easy to understand also depends completely on the audience. Therefore, the\ndeﬁnition must be rephrased to reﬂect explicitly the dependence of the explainability of the model on the\naudience. To this end, a reworked deﬁnition could read as:\nGiven a certain audience, explainability refers to the details and reasons a model gives to make its\nfunctioning clear or easy to understand.\nSince explaining, as argumenting, may involve weighting, comparing or convincing an audience with\nlogic-based formalizations of (counter) arguments [28], explainability might convey us into the realm of\ncognitive psychology and the psychology of explanations [7], since measuring whether something has\nbeen understood or put clearly is a hard task to be gauged objectively. However, measuring to which\nextent the internals of a model can be explained could be tackled objectively. Any means to reduce the\ncomplexity of the model or to simplify its outputs should be considered as an XAI approach. How big\nthis leap is in terms of complexity or simplicity will correspond to how explainable the resulting model\nis. An underlying problem that remains unsolved is that the interpretability gain provided by such XAI\napproaches may not be straightforward to quantify: for instance, a model simpliﬁcation can be evaluated\nbased on the reduction of the number of architectural elements or number of parameters of the model\nitself (as often made, for instance, for DNNs). On the contrary, the use of visualization methods or natural\nlanguage for the same purpose does not favor a clear quantiﬁcation of the improvements gained in terms\nof interpretability. The derivation of general metrics to assess the quality of XAI approaches remain as\nan open challenge that should be under the spotlight of the ﬁeld in forthcoming years. We will further\ndiscuss on this research direction in Section 5.\nExplainability is linked to post-hoc explainability since it covers the techniques used to convert a\nnon-interpretable model into a explainable one. In the remaining of this manuscript, explainability will be\nconsidered as the main design objective, since it represents a broader concept. A model can be explained,\nbut the interpretability of the model is something that comes from the design of the model itself. Bearing\nthese observations in mind, explainable AI can be deﬁned as follows:\nGiven an audience, an explainable Artiﬁcial Intelligence is one that produces details or reasons to\nmake its functioning clear or easy to understand.\n6\n\nThis deﬁnition is posed here as a ﬁrst contribution of the present overview, implicitly assumes that the\nease of understanding and clarity targeted by XAI techniques for the model at hand reverts on different\napplication purposes, such as a better trustworthiness of the model’s output by the audience.\n2.3. Why?\nAs stated in the introduction, explainability is one of the main barriers AI is facing nowadays in\nregards to its practical implementation. The inability to explain or to fully understand the reasons by\nwhich state-of-the-art ML algorithms perform as well as they do, is a problem that ﬁnd its roots in two\ndifferent causes, which are conceptually illustrated in Figure 2.\nWithout a doubt, the ﬁrst cause is the gap between the research community and business sectors,\nimpeding the full penetration of the newest ML models in sectors that have traditionally lagged behind\nin the digital transformation of their processes, such as banking, ﬁnances, security and health, among\nmany others. In general this issue occurs in strictly regulated sectors with some reluctance to implement\ntechniques that may put at risk their assets.\nThe second axis is that of knowledge. AI has helped research across the world with the task of\ninferring relations that were far beyond the human cognitive reach. Every ﬁeld dealing with huge amounts\nof reliable data has largely beneﬁted from the adoption of AI and ML techniques. However, we are\nentering an era in which results and performance metrics are the only interest shown up in research\nstudies. Although for certain disciplines this might be the fair case, science and society are far from being\nconcerned just by performance. The search for understanding is what opens the door for further model\nimprovement and its practical utility.\nTarget audience\nin XAI\nWho? Domain experts/users of the model (e.g. medical doctors, insurance agents)\nWhy? Trust the model itself, gain scientiﬁc knowledge\nWho? Regulatory entities/agencies\nWhy? Certify model compliance with the\nWho? Users aﬀected by model decisions\nWhy? Understand their situation, verify\nWho? Managers and executive board members\nWhy? Assess regulatory compliance, understand\nWho? Data scientists, developers, product owners...\nWhy? Ensure/improve product eﬃciency, research,\ncorporate AI applications...\nnew functionalities...\nfair decisions...\nlegislation in force, audits, ...\n?\n?\n< / >\n?\n$ $ $\n?\n?\n?\nFigure 2: Diagram showing the different purposes of explainability in ML models sought by different audience proﬁles. Two goals\noccur to prevail across them: need for model understanding, and regulatory compliance. Image partly inspired by the one presented\nin [29], used with permission from IBM.\nThe following section develops these ideas further by analyzing the goals motivating the search for\nexplainable AI models.\n2.4. What for?\nThe research activity around XAI has so far exposed different goals to draw from the achievement\nof an explainable model. Almost none of the papers reviewed completely agrees in the goals required\nto describe what an explainable model should compel. However, all these different goals might help\ndiscriminate the purpose for which a given exercise of ML explainability is performed. Unfortunately,\nscarce contributions have attempted to deﬁne such goals from a conceptual perspective [5, 13, 24, 30].\nWe now synthesize and enumerate deﬁnitions for these XAI goals, so as to settle a ﬁrst classiﬁcation\ncriteria for the full suit of papers covered in this review:\n7\n\nXAI Goal\nMain target audience (Fig. 2)\nReferences\nTrustworthiness\nDomain experts, users of the model\naffected by decisions\n[5, 10, 24, 32, 33, 34, 35, 36, 37]\nCausality\nDomain experts, managers and\nexecutive board members,\nregulatory entities/agencies\n[35, 38, 39, 40, 41, 42, 43]\nTransferability\nDomain experts, data scientists\n[5, 44, 21, 26, 45, 30, 32, 37, 38, 39, 46, 47, 48, 49,\n50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77,\n78, 79, 80, 81, 82, 83, 84, 85]\nInformativeness\nAll\n[5, 44, 21, 25, 26, 45, 30, 32, 34, 35, 37, 38, 41, 46, 49,\n50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 63, 64, 65,\n66, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 86,\n87, 88, 89, 59, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99,\n100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110,\n111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121,\n122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132,\n133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,\n144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154]\nConﬁdence\nDomain experts, developers,\nmanagers, regulatory\nentities/agencies\n[5, 45, 35, 46, 48, 54, 61, 72, 88, 89, 96, 108, 117,\n119, 155]\nFairness\nUsers affected by model decisions,\nregulatory entities/agencies\n[5, 24, 45, 35, 47, 99, 100, 101, 120, 121, 128, 156,\n157, 158]\nAccessibility\nProduct owners, managers, users\naffected by model decisions\n[21, 26, 30, 32, 37, 50, 53, 55, 62, 67, 68, 69, 70, 71,\n74, 75, 76, 86, 93, 94, 103, 105, 107, 108, 111, 112,\n113, 114, 115, 124, 129]\nInteractivity\nDomain experts, users affected by\nmodel decisions\n[37, 50, 59, 65, 67, 74, 86, 124]\nPrivacy awareness\nUsers affected by model decisions,\nregulatory entities/agencies\n[89]\nTable 1: Goals pursued in the reviewed literature toward reaching explainability, and their main target audience.\n• Trustworthiness: several authors agree upon the search for trustworthiness as the primary aim of an\nexplainable AI model [31, 32]. However, declaring a model as explainable as per its capabilities of\ninducing trust might not be fully compliant with the requirement of model explainability. Trustworthiness might be considered as the conﬁdence of whether a model will act as intended when facing a\ngiven problem. Although it should most certainly be a property of any explainable model, it does not\nimply that every trustworthy model can be considered explainable on its own, nor is trustworthiness\na property easy to quantify. Trust might be far from being the only purpose of an explainable model\nsince the relation among the two, if agreed upon, is not reciprocal. Part of the reviewed papers mention\nthe concept of trust when stating their purpose for achieving explainability. However, as seen in Table\n1, they do not amount to a large share of the recent contributions related to XAI.\n• Causality: another common goal for explainability is that of ﬁnding causality among data variables.\nSeveral authors argue that explainable models might ease the task of ﬁnding relationships that, should\nthey occur, could be tested further for a stronger causal link between the involved variables [159, 160].\nThe inference of causal relationships from observational data is a ﬁeld that has been broadly studied\nover time [161]. As widely acknowledged by the community working on this topic, causality requires a\nwide frame of prior knowledge to prove that observed effects are causal. A ML model only discovers\ncorrelations among the data it learns from, and therefore might not sufﬁce for unveiling a cause-effect\nrelationship. However, causation involves correlation, so an explainable ML model could validate\nthe results provided by causality inference techniques, or provide a ﬁrst intuition of possible causal\n8\n\nrelationships within the available data. Again, Table 1 reveals that causality is not among the most\nimportant goals if we attend to the amount of papers that state it explicitly as their goal.\n• Transferability: models are always bounded by constraints that should allow for their seamless\ntransferability. This is the main reason why a training-testing approach is used when dealing with\nML problems [162, 163]. Explainability is also an advocate for transferability, since it may ease the\ntask of elucidating the boundaries that might affect a model, allowing for a better understanding and\nimplementation. Similarly, the mere understanding of the inner relations taking place within a model\nfacilitates the ability of a user to reuse this knowledge in another problem. There are cases in which the\nlack of a proper understanding of the model might drive the user toward incorrect assumptions and\nfatal consequences [44, 164]. Transferability should also fall between the resulting properties of an\nexplainable model, but again, not every transferable model should be considered as explainable. As\nobserved in Table 1, the amount of papers stating that the ability of rendering a model explainable is to\nbetter understand the concepts needed to reuse it or to improve its performance is the second most used\nreason for pursuing model explainability.\n• Informativeness: ML models are used with the ultimate intention of supporting decision making [92].\nHowever, it should not be forgotten that the problem being solved by the model is not equal to that\nbeing faced by its human counterpart. Hence, a great deal of information is needed in order to be able\nto relate the user’s decision to the solution given by the model, and to avoid falling in misconception\npitfalls. For this purpose, explainable ML models should give information about the problem being\ntackled. Most of the reasons found among the papers reviewed is that of extracting information about\nthe inner relations of a model. Almost all rule extraction techniques substantiate their approach on\nthe search for a simpler understanding of what the model internally does, stating that the knowledge\n(information) can be expressed in these simpler proxies that they consider explaining the antecedent.\nThis is the most used argument found among the reviewed papers to back up what they expect from\nreaching explainable models.\n• Conﬁdence: as a generalization of robustness and stability, conﬁdence should always be assessed\non a model in which reliability is expected. The methods to maintain conﬁdence under control are\ndifferent depending on the model. As stated in [165, 166, 167], stability is a must-have when drawing\ninterpretations from a certain model. Trustworthy interpretations should not be produced by models\nthat are not stable. Hence, an explainable model should contain information about the conﬁdence of its\nworking regime.\n• Fairness: from a social standpoint, explainability can be considered as the capacity to reach and\nguarantee fairness in ML models. In a certain literature strand, an explainable ML model suggests a\nclear visualization of the relations affecting a result, allowing for a fairness or ethical analysis of the\nmodel at hand [3, 100]. Likewise, a related objective of XAI is highlighting bias in the data a model\nwas exposed to [168, 169]. The support of algorithms and models is growing fast in ﬁelds that involve\nhuman lives, hence explainability should be considered as a bridge to avoid the unfair or unethical use\nof algorithm’s outputs.\n• Accessibility: a minor subset of the reviewed contributions argues for explainability as the property\nthat allows end users to get more involved in the process of improving and developing a certain ML\nmodel [37, 86] . It seems clear that explainable models will ease the burden felt by non-technical or\nnon-expert users when having to deal with algorithms that seem incomprehensible at ﬁrst sight. This\nconcept is expressed as the third most considered goal among the surveyed literature.\n• Interactivity: some contributions [50, 59] include the ability of a model to be interactive with the user\nas one of the goals targeted by an explainable ML model. Once again, this goal is related to ﬁelds in\n9\n\nwhich the end users are of great importance, and their ability to tweak and interact with the models is\nwhat ensures success.\n• Privacy awareness: almost forgotten in the reviewed literature, one of the byproducts enabled by explainability in ML models is its ability to assess privacy. ML models may have complex representations\nof their learned patterns. Not being able to understand what has been captured by the model [4] and\nstored in its internal representation may entail a privacy breach. Contrarily, the ability to explain the\ninner relations of a trained model by non-authorized third parties may also compromise the differential\nprivacy of the data origin. Due to its criticality in sectors where XAI is foreseen to play a crucial role,\nconﬁdentiality and privacy issues will be covered further in Subsections 5.4 and 6.3, respectively.\nThis subsection has reviewed the goals encountered among the broad scope of the reviewed papers.\nAll these goals are clearly under the surface of the concept of explainability introduced before in this\nsection. To round up this prior analysis on the concept of explainability, the last subsection deals with\ndifferent strategies followed by the community to address explainability in ML models.\n2.5. How?\nThe literature makes a clear distinction among models that are interpretable by design, and those\nthat can be explained by means of external XAI techniques. This duality could also be regarded as the\ndifference between interpretable models and model interpretability techniques; a more widely accepted\nclassiﬁcation is that of transparent models and post-hoc explainability. This same duality also appears\nin the paper presented in [17] in which the distinction its authors make refers to the methods to solve\nthe transparent box design problem against the problem of explaining the black-box problem. This\nwork, further extends the distinction made among transparent models including the different levels of\ntransparency considered.\nWithin transparency, three levels are contemplated: algorithmic transparency, decomposability and\nsimulatability1. Among post-hoc techniques we may distinguish among text explanations, visualizations,\nlocal explanations, explanations by example, explanations by simpliﬁcation and feature relevance. In this\ncontext, there is a broader distinction proposed by [24] discerning between 1) opaque systems, where\nthe mappings from input to output are invisible to the user; 2) interpretable systems, in which users can\nmathematically analyze the mappings; and 3) comprehensible systems, in which the models should output\nsymbols or rules along with their speciﬁc output to aid in the understanding process of the rationale\nbehind the mappings being made. This last classiﬁcation criterion could be considered included within\nthe one proposed earlier, hence this paper will attempt at following the more speciﬁc one.\n2.5.1. Levels of Transparency in Machine Learning Models\nTransparent models convey some degree of interpretability by themselves. Models belonging to\nthis category can be also approached in terms of the domain in which they are interpretable, namely,\nalgorithmic transparency, decomposability and simulatability. As we elaborate next in connection to\nFigure 3, each of these classes contains its predecessors, e.g. a simulatable model is at the same time a\nmodel that is decomposable and algorithmically transparent:\n• Simulatability denotes the ability of a model of being simulated or thought about strictly by a human,\nhence complexity takes a dominant place in this class. This being said, simple but extensive (i.e., with\ntoo large amount of rules) rule based systems fall out of this characteristic, whereas a single perceptron\nneural network falls within. This aspect aligns with the claim that sparse linear models are more\ninterpretable than dense ones [170], and that an interpretable model is one that can be easily presented\n1The alternative term simulability is also used in the literature to refer to the capacity of a system or process to be simulated.\nHowever, we note that this term does not appear in current English dictionaries.\n10\n\nto a human by means of text and visualizations [32]. Again, endowing a decomposable model with\nsimulatability requires that the model has to be self-contained enough for a human to think and reason\nabout it as a whole.\n• Decomposability stands for the ability to explain each of the parts of a model (input, parameter and\ncalculation). It can be considered as intelligibility as stated in [171]. This characteristic might empower\nthe ability to understand, interpret or explain the behavior of a model. However, as occurs with\nalgorithmic transparency, not every model can fulﬁll this property. Decomposability requires every\ninput to be readily interpretable (e.g. cumbersome features will not ﬁt the premise). The added\nconstraint for an algorithmically transparent model to become decomposable is that every part of the\nmodel must be understandable by a human without the need for additional tools.\n• Algorithmic Transparency can be seen in different ways. It deals with the ability of the user to\nunderstand the process followed by the model to produce any given output from its input data. Put\nit differently, a linear model is deemed transparent because its error surface can be understood and\nreasoned about, allowing the user to understand how the model will act in every situation it may\nface [163]. Contrarily, it is not possible to understand it in deep architectures as the loss landscape\nmight be opaque [172, 173] since it cannot be fully observed and the solution has to be approximated\nthrough heuristic optimization (e.g. through stochastic gradient descent). The main constraint for\nalgorithmically transparent models is that the model has to be fully explorable by means of mathematical\nanalysis and methods.\nMϕ\nx1\nx2\nx3\ny\n(b)\nIf x2 > 180 then y = 1\nElse if x1 + x3 > 150 then y = 1\nElse y = 0\nMϕ\nx1\nx2\nx3\ny\n(a)\nIf g(fA(x1), fB(x2)) > 5\nthen y = 1, else y = 0\nfA(x1) = 1/x2\n1, fB(x2) = log x2\ng(f, g) = 1/(f + g)\nx1: weight, x2: height, x3: age\nMϕ\nx1\nx2\nx3\ny\n(c)\n(c)\n95% of the positive training samples\nhave x2 > 180 7→Rule 1\n90% of the positive training samples\nhave x1 + x3 > 150 7→Rule 2\n?\n?\n?\nFigure 3: Conceptual diagram exemplifying the different levels of transparency characterizing a ML model Mϕ, with ϕ denoting\nthe parameter set of the model at hand: (a) simulatability; (b) decomposability; (c) algorithmic transparency. Without loss of\ngenerality, the example focuses on the ML model as the explanation target. However, other targets for explainability may include a\ngiven example, the output classes or the dataset itself.\n2.5.2. Post-hoc Explainability Techniques for Machine Learning Models\nPost-hoc explainability targets models that are not readily interpretable by design by resorting to\ndiverse means to enhance their interpretability, such as text explanations, visual explanations, local\nexplanations, explanations by example, explanations by simpliﬁcation and feature relevance explanations\ntechniques. Each of these techniques covers one of the most common ways humans explain systems and\nprocesses by themselves.\nFurther along this river, actual techniques, or better put, actual group of techniques are speciﬁed\nto ease the future work of any researcher that intends to look up for an speciﬁc technique that suits its\nknowledge. Not ending there, the classiﬁcation also includes the type of data in which the techniques has\nbeen applied. Note that many techniques might be suitable for many different types of data, although\nthe categorization only considers the type used by the authors that proposed such technique. Overall,\npost-hoc explainability techniques are divided ﬁrst by the intention of the author (explanation technique\ne.g. Explanation by simpliﬁcation), then, by the method utilized (actual technique e.g. sensitivity analysis)\nand ﬁnally by the type of data in which it was applied (e.g. images).\n11\n\n• Text explanations deal with the problem of bringing explainability for a model by means of learning to\ngenerate text explanations that help explaining the results from the model [169]. Text explanations also\ninclude every method generating symbols that represent the functioning of the model. These symbols\nmay portrait the rationale of the algorithm by means of a semantic mapping from model to symbols.\n• Visual explanation techniques for post-hoc explainability aim at visualizing the model’s behavior.\nMany of the visualization methods existing in the literature come along with dimensionality reduction\ntechniques that allow for a human interpretable simple visualization. Visualizations may be coupled\nwith other techniques to improve their understanding, and are considered as the most suitable way to\nintroduce complex interactions within the variables involved in the model to users not acquainted to\nML modeling.\n• Local explanations tackle explainability by segmenting the solution space and giving explanations\nto less complex solution subspaces that are relevant for the whole model. These explanations can be\nformed by means of techniques with the differentiating property that these only explain part of the\nwhole system’s functioning.\n• Explanations by example consider the extraction of data examples that relate to the result generated by\na certain model, enabling to get a better understanding of the model itself. Similarly to how humans\nbehave when attempting to explain a given process, explanations by example are mainly centered in\nextracting representative examples that grasp the inner relationships and correlations found by the\nmodel being analyzed.\n• Explanations by simpliﬁcation collectively denote those techniques in which a whole new system is\nrebuilt based on the trained model to be explained. This new, simpliﬁed model usually attempts at\noptimizing its resemblance to its antecedent functioning, while reducing its complexity, and keeping a\nsimilar performance score. An interesting byproduct of this family of post-hoc techniques is that the\nsimpliﬁed model is, in general, easier to be implemented due to its reduced complexity with respect to\nthe model it represents.\n• Finally, feature relevance explanation methods for post-hoc explainability clarify the inner functioning\nof a model by computing a relevance score for its managed variables. These scores quantify the affection\n(sensitivity) a feature has upon the output of the model. A comparison of the scores among different\nvariables unveils the importance granted by the model to each of such variables when producing its\noutput. Feature relevance methods can be thought to be an indirect method to explain a model.\nThe above classiﬁcation (portrayed graphically in Figure 4) will be used when reviewing speciﬁc/agnostic XAI techniques for ML models in the following sections (Table 2). For each ML model, a\ndistinction of the propositions to each of these categories is presented in order to pose an overall image of\nthe ﬁeld’s trends.\n3. Transparent Machine Learning Models\nThe previous section introduced the concept of transparent models. A model is considered to be\ntransparent if by itself it is understandable. The models surveyed in this section are a suit of transparent\nmodels that can fall in one or all of the levels of model transparency described previously (namely,\nsimulatability, decomposability and algorithmic transparency). In what follows we provide reasons for\nthis statement, with graphical support given in Figure 5.\n12\n\nBlack-box\nmodel\nx\ny\nMϕ\nx = (x1, ... , xn)\nFeature\nrelevance\n...\n“Feature x2 has a\n90% importance in y”\n...\nLocal\nexplanations\nxi\nyi\nMϕ\n“What happens with the prediction yi if\nwe change slightly the features of xi?”\nxi: input instance\nVisualization\nx\ny\nMϕ\nx1 x2 x3 x4\nxn\nx1\nx3\nModel\nsimpliﬁcation\nx\ny\nMϕ\nx1\nx3\nx7\nx13\ny′\nx\nF\nG\nText\nexplanations\nxi\nyi\nMϕ\n“The output for xi is\nyi because x3 > γ ”\nby example\nExplanations\nxi\nyi\nMϕ\n”Explanatory examples\nfor the model:”\n- xA 7→yA\n- xB 7→yB\n- xC 7→yC\nFigure 4: Conceptual diagram showing the different post-hoc explainability approaches available for a ML model Mϕ.\n3.1. Linear/Logistic Regression\nLogistic Regression (LR) is a classiﬁcation model to predict a dependent variable (category) that is\ndichotomous (binary). However, when the dependent variable is continuous, linear regression would\nbe its homonym. This model takes the assumption of linear dependence between the predictors and the\npredicted variables, impeding a ﬂexible ﬁt to the data. This speciﬁc reason (stiffness of the model) is the\none that maintains the model under the umbrella of transparent methods. However, as stated in Section 2,\nexplainability is linked to a certain audience, which makes a model fall under both categories depending\nwho is to interpret it. This way, logistic and linear regression, although clearly meeting the characteristics\nof transparent models (algorithmic transparency, decomposability and simulatability), may also demand\npost-hoc explainability techniques (mainly, visualization), particularly when the model is to be explained\nto non-expert audiences.\nThe usage of this model has been largely applied within Social Sciences for quite a long time,\nwhich has pushed researchers to create ways of explaining the results of the models to non-expert\nusers. Most authors agree on the different techniques used to analyze and express the soundness of LR\n[174, 175, 176, 177], including the overall model evaluation, statistical tests of individual predictors,\ngoodness-of-ﬁt statistics and validation of the predicted probabilities. The overall model evaluation\nshows the improvement of the applied model over a baseline, showing if it is in fact improving the model\nwithout predictions. The statistical signiﬁcance of single predictors is shown by calculating the Wald\nchi-square statistic. The goodness-of-ﬁt statistics show the quality of ﬁtness of the model to the data\nand how signiﬁcant this is. This can be achieved by resorting to different techniques e.g. the so-called\nHosmer-Lemeshow (H-L) statistic. The validation of predicted probabilities involves testing whether the\noutput of the model corresponds to what is shown by the data. These techniques show mathematical ways\nof representing the ﬁtness of the model and its behavior.\nOther techniques from other disciplines besides Statistics can be adopted for explaining these re13\n\nModel\nTransparent ML Models\nPost-hoc\nanalysis\nSimulatability\nDecomposability\nAlgorithmic Transparency\nLinear/Logistic Regression\nPredictors are human readable and\ninteractions among them are kept to a\nminimum\nVariables are still readable, but the number\nof interactions and predictors involved in\nthem have grown to force decomposition\nVariables and interactions are too complex\nto be analyzed without mathematical tools\nNot needed\nDecision Trees\nA human can simulate and obtain the\nprediction of a decision tree on his/her own,\nwithout requiring any mathematical\nbackground\nThe model comprises rules that do not alter\ndata whatsoever, and preserves their\nreadability\nHuman-readable rules that explain the\nknowledge learned from data and allows\nfor a direct understanding of the prediction\nprocess\nNot needed\nK-Nearest Neighbors\nThe complexity of the model (number of\nvariables, their understandability and the\nsimilarity measure under use) matches\nhuman naive capabilities for simulation\nThe amount of variables is too high and/or\nthe similarity measure is too complex to be\nable to simulate the model completely, but\nthe similarity measure and the set of\nvariables can be decomposed and analyzed\nseparately\nThe similarity measure cannot be\ndecomposed and/or the number of\nvariables is so high that the user has to rely\non mathematical and statistical tools to\nanalyze the model\nNot needed\nRule Based Learners\nVariables included in rules are readable,\nand the size of the rule set is manageable\nby a human user without external help\nThe size of the rule set becomes too large\nto be analyzed without decomposing it into\nsmall rule chunks\nRules have become so complicated (and\nthe rule set size has grown so much) that\nmathematical tools are needed for\ninspecting the model behaviour\nNot needed\nGeneral Additive Models\nVariables and the interaction among them\nas per the smooth functions involved in the\nmodel must be constrained within human\ncapabilities for understanding\nInteractions become too complex to be\nsimulated, so decomposition techniques are\nrequired for analyzing the model\nDue to their complexity, variables and\ninteractions cannot be analyzed without the\napplication of mathematical and statistical\ntools\nNot needed\nBayesian Models\nStatistical relationships modeled among\nvariables and the variables themselves\nshould be directly understandable by the\ntarget audience\nStatistical relationships involve so many\nvariables that they must be decomposed in\nmarginals so as to ease their analysis\nStatistical relationships cannot be\ninterpreted even if already decomposed,\nand predictors are so complex that model\ncan be only analyzed with mathematical\ntools\nNot needed\nTree Ensembles\n\u0017\n\u0017\n\u0017\nNeeded: Usually Model simpliﬁcation or\nFeature relevance techniques\nSupport Vector Machines\n\u0017\n\u0017\n\u0017\nNeeded: Usually Model simpliﬁcation or\nLocal explanations techniques\nMulti–layer Neural Network\n\u0017\n\u0017\n\u0017\nNeeded: Usually Model simpliﬁcation,\nFeature relevance or Visualization\ntechniques\nConvolutional Neural Network\n\u0017\n\u0017\n\u0017\nNeeded: Usually Feature relevance or\nVisualization techniques\nRecurrent Neural Network\n\u0017\n\u0017\n\u0017\nNeeded: Usually Feature relevance\ntechniques\nTable 2: Overall picture of the classiﬁcation of ML models attending to their level of explainability.\ngression models. Visualization techniques are very powerful when presenting statistical conclusions to\nusers not well-versed in statistics. For instance, the work in [178] shows that the usage of probabilities to\ncommunicate the results, implied that the users where able to estimate the outcomes correctly in 10% of\nthe cases, as opposed to 46% of the cases when using natural frequencies. Although logistic regression is\namong the simplest classiﬁcation models in supervised learning, there are concepts that must be taken\ncare of.\nIn this line of reasoning, the authors of [179] unveil some concerns with the interpretations derived\nfrom LR. They ﬁrst mention how dangerous it might be to interpret log odds ratios and odd ratios as\nsubstantive effects, since they also represent unobserved heterogeneity. Linked to this ﬁrst concern,\n[179] also states that a comparison between these ratios across models with different variables might be\nproblematic, since the unobserved heterogeneity is likely to vary, thereby invalidating the comparison.\nFinally they also mention that the comparison of these odds across different samples, groups and time is\nalso risky, since the variation of the heterogeneity is not known across samples, groups and time points.\nThis last paper serves the purpose of visualizing the problems a model’s interpretation might entail, even\nwhen its construction is as simple as that of LR.\nAlso interesting is to note that, for a model such as logistic or linear regression to maintain decomposability and simulatability, its size must be limited, and the variables used must be understandable by their\nusers. As stated in Section 2, if inputs to the model are highly engineered features that are complex or\ndifﬁcult to understand, the model at hand will be far from being decomposable. Similarly, if the model is\nso large that a human cannot think of the model as a whole, its simulatability will be put to question.\n3.2. Decision Trees\nDecision trees are another example of a model that can easily fulﬁll every constraint for transparency.\nDecision trees are hierarchical structures for decision making used to support regression and classiﬁcation\nproblems [132, 180]. In the simplest of their ﬂavors, decision trees are simulatable models. However,\ntheir properties can render them decomposable or algorithmically transparent.\n14\n\nx2\nx1\nx1 ≥γ\nx2 ≥γ′\nx2 ≥γ′′\nYes\nNo\nx1 ≥γ′′′\nYes\nNo\nYes\nNo\nClass\nSupport: 70%\nImpurity: 0.1\nStraightforward what-if testing\nSimple univariate thresholds\nSimulatable, decomposable\nDirect support and impurity measures\nwi: increase in y if xi\nw0 (intercept): y for a test instance\nwith average normalized features\nincreases by one unit\nx1\nx2\nxtest\n2\nxtest\n1\nYes\nPrediction by majority voting\nK similar training instances\nSimulatable, decomposable\nAlgorithmic transparency (lazy training)\nLinguistic rules: easy to interpret\nTraining\ndataset\n−If x1 is high then y =\n−If x1 is low and x2 is\nhigh then y =\n−If x2 is low then y =\nSimulatable if ruleset coverage and\nspeciﬁty are kept constrained\nFuzziness improves interpretability\ny = w1x1 + w2x2 + w0\ng(E(y)) = w1f1(x1) + w2f2(x2)\nTraining\ndataset\ng(z)\nz\nfi(xi)\nxi\nE(y): expected value\nSimulatable, decomposable\nInterpretability depends on link\nfunction g(z), the selected fi(xi)\nand the sparseness of [w1, . . . , wN]\ny\nTraining\ndataset\nTraining\ndataset\nTraining\ndataset\nTraining\ndataset\nx1\nx2\ny\np(y|x1, x2) ∝p(y|x1)p(y|x2)\nxi\np(y|xi)\nto assess the contribution of each variable\nThe independence assumption permits\nSimulatable, decomposable\nAlgorithmic transparency (distribution ﬁtting)\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nClass\nClass\nClass\nClass\n?\n?\n?\n?\n?\n?\nFigure 5: Graphical illustration of the levels of transparency of different ML models considered in this overview: (a) Linear\nregression; (b) Decision trees; (c) K-Nearest Neighbors; (d) Rule-based Learners; (e) Generalized Additive Models; (f) Bayesian\nModels.\nDecision trees have always lingered in between the different categories of transparent models. Their\nutilization has been closely linked to decision making contexts, being the reason why their complexity\nand understandability have always been considered a paramount matter. A proof of this relevance can\nbe found in the upsurge of contributions to the literature dealing with decision tree simpliﬁcation and\ngeneration [132, 180, 181, 182]. As noted above, although being capable of ﬁtting every category within\ntransparent models, the individual characteristics of decision trees can push them toward the category of\nalgorithmically transparent models. A simulatable decision tree is one that is manageable by a human\nuser. This means its size is somewhat small and the amount of features and their meaning are easily\nunderstandable. An increment in size transforms the model into a decomposable one since its size impedes\nits full evaluation (simulation) by a human. Finally, further increasing its size and using complex feature\nrelations will make the model algorithmically transparent loosing the previous characteristics.\nDecision trees have long been used in decision support contexts due to their off-the-shelf transparency.\nMany applications of these models fall out of the ﬁelds of computation and AI (even information\ntechnologies), meaning that experts from other ﬁelds usually feel comfortable interpreting the outputs of\nthese models [183, 184, 185]. However, their poor generalization properties in comparison with other\nmodels make this model family less interesting for their application to scenarios where a balance between\npredictive performance is a design driver of utmost importance. Tree ensembles aim at overcoming such\na poor performance by aggregating the predictions performed by trees learned on different subsets of\ntraining data. Unfortunately, the combination of decision trees looses every transparent property, calling\nfor the adoption of post-hoc explainability techniques as the ones reviewed later in the manuscript.\n15\n\n3.3. K-Nearest Neighbors\nAnother method that falls within transparent models is that of K-Nearest Neighbors (KNN), which\ndeals with classiﬁcation problems in a methodologically simple way: it predicts the class of a test sample\nby voting the classes of its K nearest neighbors (where the neighborhood relation is induced by a measure\nof distance between samples). When used in the context of regression problems, the voting is replaced by\nan aggregation (e.g. average) of the target values associated with the nearest neighbors.\nIn terms of model explainability, it is important to observe that predictions generated by KNN models\nrely on the notion of distance and similarity between examples, which can be tailored depending on the\nspeciﬁc problem being tackled. Interestingly, this prediction approach resembles that of experience-based\nhuman decision making, which decides upon the result of past similar cases. There lies the rationale\nof why KNN has also been adopted widely in contexts in which model interpretability is a requirement\n[186, 187, 188, 189]. Furthermore, aside from being simple to explain, the ability to inspect the reasons\nby which a new sample has been classiﬁed inside a group and to examine how these predictions evolve\nwhen the number of neighbors K is increased or decreased empowers the interaction between the users\nand the model.\nOne must keep in mind that as mentioned before, KNN’s class of transparency depends on the features,\nthe number of neighbors and the distance function used to measure the similarity between data instances.\nA very high K impedes a full simulation of the model performance by a human user. Similarly, the usage\nof complex features and/or distance functions would hinder the decomposability of the model, restricting\nits interpretability solely to the transparency of its algorithmic operations.\n3.4. Rule-based Learning\nRule-based learning refers to every model that generates rules to characterize the data it is intended to\nlearn from. Rules can take the form of simple conditional if-then rules or more complex combinations of\nsimple rules to form their knowledge. Also connected to this general family of models, fuzzy rule based\nsystems are designed for a broader scope of action, allowing for the deﬁnition of verbally formulated\nrules over imprecise domains. Fuzzy systems improve two main axis relevant for this paper. First, they\nempower more understandable models since they operate in linguistic terms. Second, they perform better\nthat classic rule systems in contexts with certain degrees of uncertainty. Rule based learners are clearly\ntransparent models that have been often used to explain complex models by generating rules that explain\ntheir predictions [126, 127, 190, 191].\nRule learning approaches have been extensively used for knowledge representation in expert systems\n[192]. However, a central problem with rule generation approaches is the coverage (amount) and the\nspeciﬁcity (length) of the rules generated. This problem relates directly to the intention for their use in\nthe ﬁrst place. When building a rule database, a typical design goal sought by the user is to be able to\nanalyze and understand the model. The amount of rules in a model will clearly improve the performance\nof the model at the stake of compromising its intepretability. Similarly, the speciﬁcity of the rules plays\nalso against interpretability, since a rule with a high number of antecedents an/or consequences might\nbecome difﬁcult to interpret. In this same line of reasoning, these two features of a rule based learner\nplay along with the classes of transparent models presented in Section 2. The greater the coverage or\nthe speciﬁcity is, the closer the model will be to being just algorithmically transparent. Sometimes, the\nreason to transition from classical rules to fuzzy rules is to relax the constraints of rule sizes, since a\ngreater range can be covered with less stress on interpretability.\nRule based learners are great models in terms of interpretability across ﬁelds. Their natural and\nseamless relation to human behaviour makes them very suitable to understand and explain other models.\nIf a certain threshold of coverage is acquired, a rule wrapper can be thought to contain enough information\nabout a model to explain its behavior to a non-expert user, without forfeiting the possibility of using the\ngenerated rules as an standalone prediction model.\n16\n\n3.5. General Additive Models\nIn statistics, a Generalized Additive Model (GAM) is a linear model in which the value of the\nvariable to be predicted is given by the aggregation of a number of unknown smooth functions deﬁned\nfor the predictor variables. The purpose of such model is to infer the smooth functions whose aggregate\ncomposition approximates the predicted variable. This structure is easily interpretable, since it allows the\nuser to verify the importance of each variable, namely, how it affects (through its corresponding function)\nthe predicted output.\nSimilarly to every other transparent model, the literature is replete with case studies where GAMs\nare in use, specially in ﬁelds related to risk assessment. When compared to other models, these are\nunderstandable enough to make users feel conﬁdent on using them for practical applications in ﬁnance\n[193, 194, 195], environmental studies [196], geology [197], healthcare [44], biology [198, 199] and\nenergy [200]. Most of these contributions use visualization methods to further ease the interpretation of\nthe model. GAMs might be also considered as simulatable and decomposable models if the properties\nmentioned in its deﬁnitions are fulﬁlled, but to an extent that depends roughly on eventual modiﬁcations\nto the baseline GAM model, such as the introduction of link functions to relate the aggregation with the\npredicted output, or the consideration of interactions between predictors.\nAll in all, applications of GAMs like the ones exempliﬁed above share one common factor: understandability. The main driver for conducting these studies with GAMs is to understand the underlying\nrelationships that build up the cases for scrutiny. In those cases the research goal is not accuracy for its\nown sake, but rather the need for understanding the problem behind and the relationship underneath the\nvariables involved in data. This is why GAMs have been accepted in certain communities as their de facto\nmodeling choice, despite their acknowledged misperforming behavior when compared to more complex\ncounterparts.\n3.6. Bayesian Models\nA Bayesian model usually takes the form of a probabilistic directed acyclic graphical model whose\nlinks represent the conditional dependencies between a set of variables. For example, a Bayesian network\ncould represent the probabilistic relationships between diseases and symptoms. Given symptoms, the\nnetwork can be used to compute the probabilities of the presence of various diseases. Similar to GAMs,\nthese models also convey a clear representation of the relationships between features and the target, which\nin this case are given explicitly by the connections linking variables to each other.\nOnce again, Bayesian models fall below the ceiling of Transparent models. Its categorization leaves\nit under simulatable, decomposable and algorithmically transparent. However, it is worth noting that\nunder certain circumstances (overly complex or cumbersome variables), a model may loose these ﬁrst\ntwo properties. Bayesian models have been shown to lead to great insights in assorted applications such\nas cognitive modeling [201, 202], ﬁshery [196, 203], gaming [204], climate [205], econometrics [206] or\nrobotics [207]. Furthermore, they have also been utilized to explain other models, such as averaging tree\nensembles [208].\n4. Post-hoc Explainability Techniques for Machile Learning Models: Taxonomy, Shallow Models\nand Deep Learning\nWhen ML models do not meet any of the criteria imposed to declare them transparent, a separate\nmethod must be devised and applied to the model to explain its decisions. This is the purpose of post-hoc\nexplainability techniques (also referred to as post-modeling explainability), which aim at communicating\nunderstandable information about how an already developed model produces its predictions for any given\ninput. In this section we categorize and review different algorithmic approaches for post-hoc explainability,\ndiscriminating among 1) those that are designed for their application to ML models of any kind; and 2)\nthose that are designed for a speciﬁc ML model and thus, can not be directly extrapolated to any other\n17\n\nlearner. We now elaborate on the trends identiﬁed around post-hoc explainability for different ML models,\nwhich are illustrated in Figure 6 in the form of hierarchical bibliographic categories and summarized next:\n• Model-agnostic techniques for post-hoc explainability (Subsection 4.1), which can be applied seamlessly to any ML model disregarding its inner processing or internal representations.\n• Post-hoc explainability that are tailored or speciﬁcally designed to explain certain ML models. We\ndivide our literature analysis into two main branches: contributions dealing with post-hoc explainability\nof shallow ML models, which collectively refers to all ML models that do not hinge on layered\nstructures of neural processing units (Subsection 4.2); and techniques devised for deep learning models,\nwhich correspondingly denote the family of neural networks and related variants, such as convolutional\nneural networks, recurrent neural networks (Subsection 4.3) and hybrid schemes encompassing deep\nneural networks and transparent models. For each model we perform a thorough review of the latest\npost-hoc methods proposed by the research community, along with a identiﬁcation of trends followed\nby such contributions.\n• We end our literature analysis with Subsection 4.4, where we present a second taxonomy that complements the more general one in Figure 6 by classifying contributions dealing with the post-hoc\nexplanation of Deep Learning models. To this end we focus on particular aspects related to this family\nof black-box ML methods, and expose how they link to the classiﬁcation criteria used in the ﬁrst\ntaxonomy.\n4.1. Model-agnostic Techniques for Post-hoc Explainability\nModel-agnostic techniques for post-hoc explainability are designed to be plugged to any model\nwith the intent of extracting some information from its prediction procedure. Sometimes, simpliﬁcation\ntechniques are used to generate proxies that mimic their antecedents with the purpose of having something\ntractable and of reduced complexity. Other times, the intent focuses on extracting knowledge directly\nfrom the models or simply visualizing them to ease the interpretation of their behavior. Following the\ntaxonomy introduced in Section 2, model-agnostic techniques may rely on model simpliﬁcation, feature\nrelevance estimation and visualization techniques:\n• Explanation by simpliﬁcation. They are arguably the broadest technique under the category of model\nagnostic post-hoc methods. Local explanations are also present within this category, since sometimes,\nsimpliﬁed models are only representative of certain sections of a model. Almost all techniques taking\nthis path for model simpliﬁcation are based on rule extraction techniques. Among the most known\ncontributions to this approach we encounter the technique of Local Interpretable Model-Agnostic\nExplanations (LIME) [32] and all its variations [214, 216]. LIME builds locally linear models around\nthe predictions of an opaque model to explain it. These contributions fall under explanations by\nsimpliﬁcation as well as under local explanations. Besides LIME and related ﬂavors, another approach\nto rule extraction is G-REX [212]. Although it was not originally intended for extracting rules from\nopaque models, the generic proposition of G-REX has been extended to also account for model\nexplainability purposes [190, 211]. In line with rule extraction methods, the work in [215] presents a\nnovel approach to learn rules in CNF (Conjunctive Normal Form) or DNF (Disjunctive Normal Form)\nto bridge from a complex model to a human-interpretable model. Another contribution that falls off the\nsame branch is that in [218], where the authors formulate model simpliﬁcation as a model extraction\nprocess by approximating a transparent model to the complex one. Simpliﬁcation is approached from a\ndifferent perspective in [120], where an approach to distill and audit black box models is presented. In\nit, two main ideas are exposed: a method for model distillation and comparison to audit black-box risk\nscoring models; and an statistical test to check if the auditing data is missing key features it was trained\nwith. The popularity of model simpliﬁcation is evident, given it temporally coincides with the most\n18\n\nXAI in ML\nTransparent Models\nLogistic / Linear Regression\nDecision Trees\nK-Nearest Neighbors\nRule-base Learners\nGeneral Additive Models:\n[44]\nBayesian Models:\n[31, 49, 209, 210]\nPost-Hoc Explainability\nModel-Agnostic\nExplanation by simpliﬁcation\nRule-based learner:\n[32, 51, 120, 190, 211, 212, 213, 214, 215, 216]\nDecision Tree:\n[21, 119, 133, 135, 149, 217, 218]\nOthers:\n[56, 219]\nFeature relevance explanation\nInﬂuence functions:\n[173, 220, 221]\nSensitivity:\n[222, 223]\nGame theory inspired:\n[224, 225] [226]\nSaliency:\n[85, 227]\nInteraction based:\n[123, 228]\nOthers:\n[140, 141, 229, 230, 231]\nLocal Explanations\nRule-based learner:\n[32, 216]\nDecision Tree:\n[232, 233]\nOthers:\n[67, 224, 230, 234, 235, 236, 237]\nVisual explanation\nConditional / Dependence / Shapley plots:\n[56, 224, 238, 239]\nSensitivity / Saliency:\n[85, 227] [222, 223]\nOthers:\n[117, 123, 140, 178, 234]\nModel-Speciﬁc\nEnsembles and Multiple Classiﬁer Systems\nExplanation by simpliﬁcation\nDecision Tree/Prototype:\n[84, 118, 122]\nFeature relevance explanation\nFeature importance / contribution:\n[103, 104, 240, 241]\nVisual explanation\nVariable importance / attribution:\n[104, 241] [242, 243]\nSupport Vector Machines\nExplanation by simpliﬁcation\nRule-based learner:\n[57, 93, 94, 98, 106, 134, 244, 245, 246]\nProbabilistic:\n[247, 248]\nOthers:\n[102]\nFeature relevance explanation\nFeature Contribution / Statistics:\n[249] [116, 249]\nVisual explanation\nInternal visualization:\n[68, 77, 250]\nMulti-Layer Neural Networks\nExplanation by simpliﬁcation\nRule-based learner:\n[82, 83, 147, 148, 251, 252, 253, 254, 255, 256]\nDecision Tree:\n[21, 56, 79, 81, 97, 135, 257, 258, 259]\nOthers:\n[80]\nFeature relevance explanation\nImportance/Contribution:\n[60, 61, 110, 260, 261]\nSensitivity / Saliency:\n[260]\n[262]\nLocal explanation\nDecision Tree / Sensitivity:\n[233] [263]\nExplanation by Example\nActivation clusters:\n[264, 144]\nText explanation\nCaption generation:\n[111] [150]\nVisual explanation\nSaliency / Weights:\n[265]\nArchitecture modiﬁcation\nOthers:\n[264] [266]\n[267]\nConvolutional Neural Networks\nExplanation by simpliﬁcation\nDecision Tree:\n[78]\nFeature relevance explanation\nActivations:\n[72, 268] [46]\nFeature Extraction:\n[72, 268]\nVisual explanation\nFilter / Activation:\n[63, 136, 137, 142, 152, 269, 270, 271]\nSensitivity / Saliency:\n[131, 272] [46]\nOthers:\n[273]\nArchitecture modiﬁcation\nLayer modiﬁcation:\n[143, 274, 275]\nModel combination:\n[91, 274, 276]\nAttention networks:\n[107, 114, 277, 278] [91]\nLoss modiﬁcation:\n[276] [113]\nOthers:\n[279]\nRecurrent Neural Networks\nExplanation by simpliﬁcation\nRule-based learner:\n[146]\nFeature relevance explanation\nActivation propagation:\n[280]\nVisual explanation\nActivations:\n[281]\nArquitecture modiﬁcation\nLoss / Layer modiﬁcation:\n[276, 282] [274]\nOthers:\n[151, 283, 284] [285]\nFigure 6: Taxonomy of the reviewed literature and trends identiﬁed for explainability techniques related to different ML models.\nReferences boxed in blue, green and red correspond to XAI techniques using image, text or tabular data, respectively. In order\nto build this taxonomy, the literature has been analyzed in depth to discriminate whether a post-hoc technique can be seamlessly\napplied to any ML model, even if, e.g., explicitly mentions Deep Learning in its title and/or abstract.\nrecent literature on XAI, including techniques such as LIME or G-REX. This symptomatically reveals\nthat this post-hoc explainability approach is envisaged to continue playing a central role on XAI.\n• Feature relevance explanation techniques aim to describe the functioning of an opaque model by\n19\n\nranking or measuring the inﬂuence, relevance or importance each feature has in the prediction output by\nthe model to be explained. An amalgam of propositions are found within this category, each resorting\nto different algorithmic approaches with the same targeted goal. One fruitful contribution to this path\nis that of [224] called SHAP (SHapley Additive exPlanations). Its authors presented a method to\ncalculate an additive feature importance score for each particular prediction with a set of desirable\nproperties (local accuracy, missingness and consistency) that its antecedents lacked. Another approach\nto tackle the contribution of each feature to predictions has been coalitional Game Theory [225] and\nlocal gradients [234]. Similarly, by means of local gradients [230] test the changes needed in each\nfeature to produce a change in the output of the model. In [228] the authors analyze the relations and\ndependencies found in the model by grouping features, that combined, bring insights about the data.\nThe work in [173] presents a broad variety of measures to tackle the quantiﬁcation of the degree of\ninﬂuence of inputs on outputs of systems. Their QII (Quantitative Input Inﬂuence) measures account\nfor correlated inputs while measuring inﬂuence. In contrast, in [222] the authors build upon the existing\nSA (Sensitivity Analysis) to construct a Global SA which extends the applicability of the existing\nmethods. In [227] a real-time image saliency method is proposed, which is applicable to differentiable\nimage classiﬁers. The study in [123] presents the so-called Automatic STRucture IDentiﬁcation method\n(ASTRID) to inspect which attributes are exploited by a classiﬁer to generate a prediction. This method\nﬁnds the largest subset of features such that the accuracy of a classiﬁer trained with this subset of\nfeatures cannot be distinguished in terms of accuracy from a classiﬁer built on the original feature set.\nIn [221] the authors use inﬂuence functions to trace a model’s prediction back to the training data, by\nonly requiring an oracle version of the model with access to gradients and Hessian-vector products.\nHeuristics for creating counterfactual examples by modifying the input of the model have been also\nfound to contribute to its explainability [236, 237]. Compared to those attempting explanations by\nsimpliﬁcation, a similar amount of publications were found tackling explainability by means of feature\nrelevance techniques. Many of the contributions date from 2017 and some from 2018, implying that as\nwith model simpliﬁcation techniques, feature relevance has also become a vibrant subject study in the\ncurrent XAI landscape.\n• Visual explanation techniques are a vehicle to achieve model-agnostic explanations. Representative\nworks in this area can be found in [222], which present a portfolio of visualization techniques to help in\nthe explanation of a black-box ML model built upon the set of extended techniques mentioned earlier\n(Global SA). Another set of visualization techniques is presented in [223]. The authors present three\nnovel SA methods (data based SA, Monte-Carlo SA, cluster-based SA) and one novel input importance\nmeasure (Average Absolute Deviation). Finally, [238] presents ICE (Individual Conditional Expectation) plots as a tool for visualizing the model estimated by any supervised learning algorithm. Visual\nexplanations are less common in the ﬁeld of model-agnostic techniques for post-hoc explainability.\nSince the design of these methods must ensure that they can be seamlessly applied to any ML model\ndisregarding its inner structure, creating visualizations from just inputs and outputs from an opaque\nmodel is a complex task. This is why almost all visualization methods falling in this category work\nalong with feature relevance techniques, which provide the information that is eventually displayed to\nthe end user.\nSeveral trends emerge from our literature analysis. To begin with, rule extraction techniques prevail\nin model-agnostic contributions under the umbrella of post-hoc explainability. This could have been\nintuitively expected if we bear in mind the wide use of rule based learning as explainability wrappers\nanticipated in Section 3.4, and the complexity imposed by not being able to get into the model itself.\nSimilarly, another large group of contributions deals with feature relevance. Lately these techniques are\ngathering much attention by the community when dealing with DL models, with hybrid approaches that\nutilize particular aspects of this class of models and therefore, compromise the independence of the feature\nrelevance method on the model being explained. Finally, visualization techniques propose interesting\n20\n\nways for visualizing the output of feature relevance techniques to ease the task of model’s interpretation.\nBy contrast, visualization techniques for other aspects of the trained model (e.g. its structure, operations,\netc) are tightly linked to the speciﬁc model to be explained.\n4.2. Post-hoc Explainability in Shallow ML Models\nShallow ML covers a diversity of supervised learning models. Within these models, there are strictly\ninterpretable (transparent) approaches (e.g. KNN and Decision Trees, already discussed in Section 3).\nHowever, other shallow ML models rely on more sophisticated learning algorithms that require additional\nlayers of explanation. Given their prominence and notable performance in predictive tasks, this section\nconcentrates on two popular shallow ML models (tree ensembles and Support Vector Machines, SVMs)\nthat require the adoption of post-hoc explainability techniques for explaining their decisions.\n4.2.1. Tree Ensembles, Random Forests and Multiple Classiﬁer Systems\nTree ensembles are arguably among the most accurate ML models in use nowadays. Their advent\ncame as an efﬁcient means to improve the generalization capability of single decision trees, which are\nusually prone to overﬁtting. To circumvent this issue, tree ensembles combine different trees to obtain an\naggregated prediction/regression. While it results to be effective against overﬁtting, the combination of\nmodels makes the interpretation of the overall ensemble more complex than each of its compounding tree\nlearners, forcing the user to draw from post-hoc explainability techniques. For tree ensembles, techniques\nfound in the literature are explanation by simpliﬁcation and feature relevance techniques; we next examine\nrecent advances in these techniques.\nTo begin with, many contributions have been presented to simplify tree ensembles while maintaining\npart of the accuracy accounted for the added complexity. The author from [119] poses the idea of training\na single albeit less complex model from a set of random samples from the data (ideally following the real\ndata distribution) labeled by the ensemble model. Another approach for simpliﬁcation is that in [118], in\nwhich authors create a Simpliﬁed Tree Ensemble Learner (STEL). Likewise, [122] presents the usage\nof two models (simple and complex) being the former the one in charge of interpretation and the latter\nof prediction by means of Expectation-Maximization and Kullback-Leibler divergence. As opposed to\nwhat was seen in model-agnostic techniques, not that many techniques to board explainability in tree\nensembles by means of model simpliﬁcation. It derives from this that either the proposed techniques are\ngood enough, or model-agnostic techniques do cover the scope of simpliﬁcation already.\nFollowing simpliﬁcation procedures, feature relevance techniques are also used in the ﬁeld of tree\nensembles. Breiman [286] was the ﬁrst to analyze the variable importance within Random Forests. His\nmethod is based on measuring MDA (Mean Decrease Accuracy) or MIE (Mean Increase Error) of the\nforest when a certain variable is randomly permuted in the out-of-bag samples. Following this contribution\n[241] shows, in an real setting, how the usage of variable importance reﬂects the underlying relationships\nof a complex system modeled by a Random Forest. Finally, a crosswise technique among post-hoc\nexplainability, [240] proposes a framework that poses recommendations that, if taken, would convert\nan example from one class to another. This idea attempts to disentangle the variables importance in a\nway that is further descriptive. In the article, the authors show how these methods can be used to elevate\nrecommendations to improve malicious online ads to make them rank higher in paying rates.\nSimilar to the trend shown in model-agnostic techniques, for tree ensembles again, simpliﬁcation and\nfeature relevance techniques seem to be the most used schemes. However, contrarily to what was observed\nbefore, most papers date back from 2017 and place their focus mostly on bagging ensembles. When\nshifting the focus towards other ensemble strategies, scarce activity has been recently noted around the\nexplainability of boosting and stacking classiﬁers. Among the latter, it is worth highlighting the connection\nbetween the reason why a compounding learner of the ensemble produces an speciﬁc prediction on a given\ndata, and its contribution to the output of the ensemble. The so-called Stacking With Auxiliary Features\n(SWAF) approach proposed in [242] points in this direction by harnessing and integrating explanations in\n21\n\nstacking ensembles to improve their generalization. This strategy allows not only relying on the output\nof the compounding learners, but also on the origin of that output and its consensus across the entire\nensemble. Other interesting studies on the explainability of ensemble techniques include model-agnostic\nschemes such as DeepSHAP [226], put into practice with stacking ensembles and multiple classiﬁer\nsystems in addition to Deep Learning models; the combination of explanation maps of multiple classiﬁers\nto produce improved explanations of the ensemble to which they belong [243]; and recent insights dealing\nwith traditional and gradient boosting ensembles [287, 288].\n4.2.2. Support Vector Machines\nAnother shallow ML model with historical presence in the literature is the SVM. SVM models are\nmore complex than tree ensembles, with a much opaquer structure. Many implementations of post-hoc\nexplainability techniques have been proposed to relate what is mathematically described internally in\nthese models, to what different authors considered explanations about the problem at hand. Technically,\nan SVM constructs a hyper-plane or set of hyper-planes in a high or inﬁnite-dimensional space, which\ncan be used for classiﬁcation, regression, or other tasks such as outlier detection. Intuitively, a good\nseparation is achieved by the hyperplane that has the largest distance (so-called functional margin) to the\nnearest training-data point of any class, since in general, the larger the margin, the lower the generalization\nerror of the classiﬁer. SVMs are among the most used ML models due to their excellent prediction\nand generalization capabilities. From the techniques stated in Section 2, post-hoc explainability applied\nto SVMs covers explanation by simpliﬁcation, local explanations, visualizations and explanations by\nexample.\nAmong explanation by simpliﬁcation, four classes of simpliﬁcations are made. Each of them differentiates from the other by how deep they go into the algorithm inner structure. First, some authors\npropose techniques to build rule based models only from the support vectors of a trained model. This is\nthe approach of [93], which proposes a method that extracts rules directly from the support vectors of a\ntrained SVM using a modiﬁed sequential covering algorithm. In [57] the same authors propose eclectic\nrule extraction, still considering only the support vectors of a trained model. The work in [94] generates\nfuzzy rules instead of classical propositional rules. Here, the authors argue that long antecedents reduce\ncomprehensibility, hence, a fuzzy approach allows for a more linguistically understandable result. The\nsecond class of simpliﬁcations can be exempliﬁed by [98], which proposed the addition of the SVM’s\nhyperplane, along with the support vectors, to the components in charge of creating the rules. His method\nrelies on the creation of hyper-rectangles from the intersections between the support vectors and the\nhyper-plane. In a third approach to model simpliﬁcation, another group of authors considered adding\nthe actual training data as a component for building the rules. In [126, 244, 246] the authors proposed a\nclustering method to group prototype vectors for each class. By combining them with the support vectors,\nit allowed deﬁning ellipsoids and hyper-rectangles in the input space. Similarly in [106], the authors\nproposed the so-called Hyper-rectangle Rule Extraction, an algorithm based on SVC (Support Vector\nClustering) to ﬁnd prototype vectors for each class and then deﬁne small hyper-rectangles around. In\n[105], the authors formulate the rule extraction problem as a multi-constrained optimization to create a\nset of non-overlapping rules. Each rule conveys a non-empty hyper-cube with a shared edge with the\nhyper-plane. In a similar study conducted in [245], extracting rules for gene expression data, the authors\npresented a novel technique as a component of a multi-kernel SVM. This multi-kernel method consists\nof feature selection, prediction modeling and rule extraction. Finally, the study in [134] makes use of a\ngrowing SVC to give an interpretation to SVM decisions in terms of linear rules that deﬁne the space in\nVoronoi sections from the extracted prototypes.\nLeaving aside rule extraction, the literature has also contemplated some other techniques to contribute\nto the interpretation of SVMs. Three of them (visualization techniques) are clearly used toward explaining\nSVM models when used for concrete applications. For instance, [77] presents an innovative approach to\nvisualize trained SVM to extract the information content from the kernel matrix. They center the study\n22\n\non Support Vector Regression models. They show the ability of the algorithm to visualize which of the\ninput variables are actually related with the associated output data. In [68] a visual way combines the\noutput of the SVM with heatmaps to guide the modiﬁcation of compounds in late stages of drug discovery.\nThey assign colors to atoms based on the weights of a trained linear SVM that allows for a much more\ncomprehensive way of debugging the process. In [116] the authors argue that many of the presented\nstudies for interpreting SVMs only account for the weight vectors, leaving the margin aside. In their study\nthey show how this margin is important, and they create an statistic that explicitly accounts for the SVM\nmargin. The authors show how this statistic is speciﬁc enough to explain the multivariate patterns shown\nin neuroimaging.\nNoteworthy is also the intersection between SVMs and Bayesian systems, the latter being adopted\nas a post-hoc technique to explain decisions made by the SVM model. This is the case of [248] and\n[247], which are studies where SVMs are interpreted as MAP (Maximum A Posteriori) solutions to\ninference problems with Gaussian Process priors. This framework makes tuning the hyper-parameters\ncomprehensible and gives the capability of predicting class probabilities instead of the classical binary\nclassiﬁcation of SVMs. Interpretability of SVM models becomes even more involved when dealing\nwith non-CPD (Conditional Positive Deﬁnite) kernels that are usually harder to interpret due to missing\ngeometrical and theoretical understanding. The work in [102] revolves around this issue with a geometrical\ninterpretation of indeﬁnite kernel SVMs, showing that these do not classify by hyper-plane margin\noptimization. Instead, they minimize the distance between convex hulls in pseudo-Euclidean spaces.\nA difference might be appreciated between the post-hoc techniques applied to other models and those\nnoted for SVMs. In previous models, model simpliﬁcation in a broad sense was the prominent method\nfor post-hoc explainability. In SVMs, local explanations have started to take some weight among the\npropositions. However, simpliﬁcation based methods are, on average, much older than local explanations.\nAs a ﬁnal remark, none of the reviewed methods treating SVM explainability are dated beyond 2017,\nwhich might be due to the progressive proliferation of DL models in almost all disciplines. Another\nplausible reason is that these models are already understood, so it is hard to improve upon what has\nalready been done.\n4.3. Explainability in Deep Learning\nPost-hoc local explanations and feature relevance techniques are increasingly the most adopted\nmethods for explaining DNNs. This section reviews explainability studies proposed for the most used\nDL models, namely multi-layer neural networks, Convolutional Neural Networks (CNN) and Recurrent\nNeural Networks (RNN).\n4.3.1. Multi-layer Neural Networks\nFrom their inception, multi-layer neural networks (also known as multi-layer perceptrons) have been\nwarmly welcomed by the academic community due to their huge ability to infer complex relations among\nvariables. However, as stated in the introduction, developers and engineers in charge of deploying these\nmodels in real-life production ﬁnd in their questionable explainability a common reason for reluctance.\nThat is why neural networks have been always considered as black-box models. The fact that explainability\nis often a must for the model to be of practical value, forced the community to generate multiple\nexplainability techniques for multi-layer neural networks, including model simpliﬁcation approaches,\nfeature relevance estimators, text explanations, local explanations and model visualizations.\nSeveral model simpliﬁcation techniques have been proposed for neural networks with one single\nhidden layer, however very few works have been presented for neural networks with multiple hidden\nlayers. One of these few works is DeepRED algorithm [257], which extends the decompositional approach\nto rule extraction (splitting at neuron level) presented in [259] for multi-layer neural network by adding\nmore decision trees and rules.\nSome other works use model simpliﬁcation as a post-hoc explainability approach. For instance, [56]\npresents a simple distillation method called Interpretable Mimic Learning to extract an interpretable model\n23\n\nby means of gradient boosting trees. In the same direction, the authors in [135] propose a hierarchical\npartitioning of the feature space that reveals the iterative rejection of unlikely class labels, until association\nis predicted. In addition, several works addressed the distillation of knowledge from an ensemble of\nmodels into a single model [80, 289, 290] .\nGiven the fact that the simpliﬁcation of multi-layer neural networks is more complex as the number of\nlayers increases, explaining these models by feature relevance methods has become progressively more\npopular. One of the representative works in this area is [60], which presents a method to decompose the\nnetwork classiﬁcation decision into contributions of its input elements. They consider each neuron as an\nobject that can be decomposed and expanded then aggregate and back-propagate these decompositions\nthrough the network, resulting in a deep Taylor decomposition. In the same direction, the authors in [110]\nproposed DeepLIFT, an approach for computing importance scores in a multi-layer neural network. Their\nmethod compares the activation of a neuron to the reference activation and assigns the score according to\nthe difference.\nOn the other hand, some works try to verify the theoretical soundness of current explainability methods.\nFor example, the authors in [262], bring up a fundamental problem of most feature relevance techniques,\ndesigned for multi-layer networks. They showed that two axioms that such techniques ought to fulﬁll\nnamely, sensitivity and implementation invariance, are violated in practice by most approaches. Following\nthese axioms, the authors of [262] created integrated gradients, a new feature relevance method proven\nto meet the aforementioned axioms. Similarly, the authors in [61] analyzed the correctness of current\nfeature relevance explanation approaches designed for Deep Neural Networks, e,g., DeConvNet, Guided\nBackProp and LRP, on simple linear neural networks. Their analysis showed that these methods do not\nproduce the theoretically correct explanation and presented two new explanation methods PatternNet and\nPatternAttribution that are more theoretically sound for both, simple and deep neural networks.\n4.3.2. Convolutional Neural Networks\nCurrently, CNNs constitute the state-of-art models in all fundamental computer vision tasks, from\nimage classiﬁcation and object detection to instance segmentation. Typically, these models are built as\na sequence of convolutional layers and pooling layers to automatically learn increasingly higher level\nfeatures. At the end of the sequence, one or multiple fully connected layers are used to map the output\nfeatures map into scores. This structure entails extremely complex internal relations that are very difﬁcult\nto explain. Fortunately, the road to explainability for CNNs is easier than for other types of models, as the\nhuman cognitive skills favors the understanding of visual data.\nExisting works that aim at understanding what CNNs learn can be divided into two broad categories:\n1) those that try to understand the decision process by mapping back the output in the input space to\nsee which parts of the input were discriminative for the output; and 2) those that try to delve inside the\nnetwork and interpret how the intermediate layers see the external world, not necessarily related to any\nspeciﬁc input, but in general.\nOne of the seminal works in the ﬁrst category was [291]. When an input image runs feed-forward\nthrough a CNN, each layer outputs a number of feature maps with strong and soft activations. The authors\nin [291] used Deconvnet, a network designed previously by the same authors [142] that, when fed with a\nfeature map from a selected layer, reconstructs the maximum activations. These reconstructions can give\nan idea about the parts of the image that produced that effect. To visualize these strongest activations in\nthe input image, the same authors used the occlusion sensitivity method to generate a saliency map [136],\nwhich consists of iteratively forwarding the same image through the network occluding a different region\nat a time.\nTo improve the quality of the mapping on the input space, several subsequent papers proposed\nsimplifying both the CNN architecture and the visualization method. In particular, [96] included a global\naverage pooling layer between the last convolutional layer of the CNN and the fully-connected layer that\npredicts the object class. With this simple architectural modiﬁcation of the CNN, the authors built a class\n24\n\nactivation map that helps identify the image regions that were particularly important for a speciﬁc object\nclass by projecting back the weights of the output layer on the convolutional feature maps. Later, in [143],\nthe authors showed that max-pooling layers can be used to replace convolutional layers with a large stride\nwithout loss in accuracy on several image recognition benchmarks. They obtained a cleaner visualization\nthan Deconvnet by using a guided backpropagation method.\nTo increase the interpretability of classical CNNs, the authors in [113] used a loss for each ﬁlter in\nhigh level convolutional layers to force each ﬁlter to learn very speciﬁc object components. The obtained\nactivation patterns are much more interpretable for their exclusiveness with respect to the different labels\nto be predicted. The authors in [72] proposed visualizing the contribution to the prediction of each single\npixel of the input image in the form of a heatmap. They used a Layer-wise Relevance Propagation (LRP)\ntechnique, which relies on a Taylor series close to the prediction point rather than partial derivatives at\nthe prediction point itself. To further improve the quality of the visualization, attribution methods such\nas heatmaps, saliency maps or class activation methods (GradCAM [292]) are used (see Figure 7). In\nparticular, the authors in [292] proposed a Gradient-weighted Class Activation Mapping (Grad-CAM),\nwhich uses the gradients of any target concept, ﬂowing into the ﬁnal convolutional layer to produce a\ncoarse localization map, highlighting the important regions in the image for predicting the concept.\n(a) Heatmap [168]\n(b) Attribution [293]\n(c) Grad-CAM [292]\nFigure 7: Examples of rendering for different XAI visualization techniques on images.\nIn addition to the aforementioned feature relevance and visual explanation methods, some works\nproposed generating text explanations of the visual content of the image. For example, the authors in [91]\ncombined a CNN feature extractor with an RNN attention model to automatically learn to describe the\ncontent of images. In the same line, [278] presented a three-level attention model to perform a ﬁne-grained\nclassiﬁcation task. The general model is a pipeline that integrates three types of attention: the object\nlevel attention model proposes candidate image regions or patches from the input image, the part-level\nattention model ﬁlters out non-relevant patches to a certain object, and the last attention model localizes\ndiscriminative patches. In the task of video captioning, the authors in [111] use a CNN model combined\nwith a bi-directional LSTM model as encoder to extract video features and then feed these features to an\nLSTM decoder to generate textual descriptions.\nOne of the seminal works in the second category is [137]. In order to analyse the visual information\ncontained inside the CNN, the authors proposed a general framework that reconstruct an image from the\nCNN internal representations and showed that several layers retain photographically accurate information\nabout the image, with different degrees of geometric and photometric invariance. To visualize the notion\nof a class captured by a CNN, the same authors created an image that maximizes the class score based on\ncomputing the gradient of the class score with respect to the input image [272]. In the same direction,\nthe authors in [268] introduced a Deep Generator Network (DGN) that generates the most representative\nimage for a given output neuron in a CNN.\nFor quantifying the interpretability of the latent representations of CNNs, the authors in [125] used a\ndifferent approach called network dissection. They run a large number of images through a CNN and then\nanalyze the top activated images by considering each unit as a concept detector to further evaluate each\n25\n\nunit for semantic segmentation. This paper also examines the effects of classical training techniques on\nthe interpretability of the learned model.\nAlthough many of the techniques examined above utilize local explanations to achieve an overall\nexplanation of a CNN model, others explicitly focus on building global explanations based on locally found\nprototypes. In [263, 294], the authors empirically showed how local explanations in deep networks are\nstrongly dominated by their lower level features. They demonstrated that deep architectures provide strong\npriors that prevent the altering of how these low-level representations are captured. All in all, visualization\nmixed with feature relevance methods are arguably the most adopted approach to explainability in CNNs.\nInstead of using one single interpretability technique, the framework proposed in [295] combines\nseveral methods to provide much more information about the network. For example, combining feature\nvisualization (what is a neuron looking for?) with attribution (how does it affect the output?) allows\nexploring how the network decides between labels. This visual interpretability interface displays different\nblocks such as feature visualization and attribution depending on the visualization goal. This interface\ncan be thought of as a union of individual elements that belong to layers (input, hidden, output), atoms (a\nneuron, channel, spatial or neuron group), content (activations – the amount a neuron ﬁres, attribution –\nwhich classes a spatial position most contributes to, which tends to be more meaningful in later layers), and\npresentation (information visualization, feature visualization). Figure 8 shows some examples. Attribution\nmethods normally rely on pixel association, displaying what part of an input example is responsible for\nthe network activating in a particular way [293].\n(a) Neuron\n(b) Channel\n(c) Layer\nFigure 8: Feature visualization at different levels of a certain network [293].\n(a) Original image\n(b) Explaining electric guitar\n(c) Explaining acoustic guitar\nFigure 9: Examples of explanation when using LIME on images [71].\nA much simpler approach to all the previously cited methods was proposed in LIME framework\n[71], as was described in Subsection 4.1 LIME perturbs the input and sees how the predictions change.\nIn image classiﬁcation, LIME creates a set of perturbed instances by dividing the input image into\ninterpretable components (contiguous superpixels), and runs each perturbed instance through the model\n26\n\nto get a probability. A simple linear model learns on this data set, which is locally weighted. At the end of\nthe process, LIME presents the superpixels with highest positive weights as an explanation (see Figure 9).\nA completely different explainability approach is proposed in adversarial detection. To understand\nmodel failures in detecting adversarial examples, the authors in [264] apply the k-nearest neighbors\nalgorithm on the representations of the data learned by each layer of the CNN. A test input image is\nconsidered as adversarial if its representations are far from the representations of the training images.\n4.3.3. Recurrent Neural Networks\nAs occurs with CNNs in the visual domain, RNNs have lately been used extensively for predictive\nproblems deﬁned over inherently sequential data, with a notable presence in natural language processing\nand time series analysis. These types of data exhibit long-term dependencies that are complex to be\ncaptured by a ML model. RNNs are able to retrieve such time-dependent relationships by formulating the\nretention of knowledge in the neuron as another parametric characteristic that can be learned from data.\nFew contributions have been made for explaining RNN models. These studies can be divided into two\ngroups: 1) explainability by understanding what a RNN model has learned (mainly via feature relevance\nmethods); and 2) explainability by modifying RNN architectures to provide insights about the decisions\nthey make (local explanations).\nIn the ﬁrst group, the authors in [280] extend the usage of LRP to RNNs. They propose a speciﬁc\npropagation rule that works with multiplicative connections as those in LSTMs (Long Short Term Memory)\nunits and GRUs (Gated Recurrent Units). The authors in [281] propose a visualization technique based on\nﬁnite horizon n-grams that discriminates interpretable cells within LSTM and GRU networks. Following\nthe premise of not altering the architecture, [296] extends the interpretable mimic learning distillation\nmethod used for CNN models to LSTM networks, so that interpretable features are learned by ﬁtting\nGradient Boosting Trees to the trained LSTM network under focus.\nAside from the approaches that do not change the inner workings of the RNNs, [285] presents RETAIN\n(REverse Time AttentIoN) model, which detects inﬂuential past patterns by means of a two-level neural\nattention model. To create an interpretable RNN, the authors in [283] propose an RNN based on SISTA\n(Sequential Iterative Soft-Thresholding Algorithm) that models a sequence of correlated observations\nwith a sequence of sparse latent vectors, making its weights interpretable as the parameters of a principled\nstatistical model. Finally, [284] constructs a combination of an HMM (Hidden Markov Model) and an\nRNN, so that the overall model approach harnesses the interpretability of the HMM and the accuracy of\nthe RNN model.\n4.3.4. Hybrid Transparent and Black-box Methods\nThe use of background knowledge in the form of logical statements or constraints in Knowledge\nBases (KBs) has shown to not only improve explainability but also performance with respect to purely\ndata-driven approaches [297, 298, 299]. A positive side effect shown is that this hybrid approach provides\nrobustness to the learning system when errors are present in the training data labels. Other approaches\nhave shown to be able to jointly learn and reason with both symbolic and sub-symbolic representations\nand inference. The interesting aspect is that this blend allows for expressive probabilistic-logical reasoning\nin an end-to-end fashion [300]. A successful use case is on dietary recommendations, where explanations\nare extracted from the reasoning behind (non-deep but KB-based) models [301].\nFuture data fusion approaches may thus consider endowing DL models with explainability by externalizing other domain information sources. Deep formulation of classical ML models has been done, e.g. in\nDeep Kalman ﬁlters (DKFs) [302], Deep Variational Bayes Filters (DVBFs) [303], Structural Variational\nAutoencoders (SVAE) [304], or conditional random ﬁelds as RNNs [305]. These approaches provide\ndeep models with the interpretability inherent to probabilistic graphical models. For instance, SVAE\ncombines probabilistic graphical models in the embedding space with neural networks to enhance the\ninterpretability of DKFs. A particular example of classical ML model enhanced with its DL counterpart is\n27\n\nDeep Nearest Neighbors DkNN [264], where the neighbors constitute human-interpretable explanations\nof predictions. The intuition is based on the rationalization of a DNN prediction based on evidence.\nThis evidence consists of a characterization of conﬁdence termed credibility that spans the hierarchy of\nrepresentations within a DNN, that must be supported by the training data [264].\nMϕ\nBlack-box\nML model\nx\ny\nTransparent design methods\n• Decision Tree\n• (Fuzzy) rule-based learning\n• KNN\nPrediction\nExplanation\nFigure 10: Pictorial representation of a hybrid model. A neural network considered as a black-box can be explained by associating\nit to a more interpretable model such as a Decision Tree [306], a (fuzzy) rule-based system [19] or KNN [264].\nA different perspective on hybrid XAI models consists of enriching black-box models knowledge\nwith that one of transparent ones, as proposed in [24] and further reﬁned in [169] and [307]. In particular,\nthis can be done by constraining the neural network thanks to a semantic KB and bias-prone concepts\n[169], or by stacking ensembles jointly encompassing white- and black-box models [307].\nOther examples of hybrid symbolic and sub-symbolic methods where a knowledge-base tool or\ngraph-perspective enhances the neural (e.g., language [308]) model are in [309, 310]. In reinforcement\nlearning, very few examples of symbolic (graphical [311] or relational [75, 312]) hybrid models exist,\nwhile in recommendation systems, for instance, explainable autoencoders are proposed [313]. A speciﬁc\ntransformer architecture symbolic visualization method (applied to music) pictorially shows how soft-max\nattention works [314]. By visualizing self-reference, i.e., the last layer of attention weights, arcs show\nwhich notes in the past are informing the future and how attention is skip over less relevant sections.\nTransformers can also help explain image captions visually [315].\nAnother hybrid approach consists of mapping an uninterpretable black-box system to a white-box twin\nthat is more interpretable. For example, an opaque neural network can be combined with a transparent\nCase Based Reasoning (CBR) system [316, 317]. In [318], the DNN and the CBR (in this case a kNN) are\npaired in order to improve interpretability while keeping the same accuracy. The explanation by example\nconsists of analyzing the feature weights of the DNN which are then used in the CBR, in order to retrieve\nnearest-neighbor cases to explain the DNN’s prediction.\n4.4. Alternative Taxonomy of Post-hoc Explainability Techniques for Deep Learning\nDL is the model family where most research has been concentrated in recent times and they have\nbecome central for most of the recent literature on XAI. While the division between model-agnostic\nand model-speciﬁc is the most common distinction made, the community has not only relied on this\ncriteria to classify XAI methods. For instance, some model-agnostic methods such as SHAP [224] are\nwidely used to explain DL models. That is why several XAI methods can be easily categorized in\ndifferent taxonomy branches depending on the angle the method is looked at. An example is LIME\nwhich can also be used over CNNs, despite not being exclusive to deal with images. Searching within\nthe alternative DL taxonomy shows us that LIME can explicitly be used for Explaining a Deep Network\nProcessing, as a kind of Linear Proxy Model. Another type of classiﬁcation is indeed proposed in [13]\nwith a segmentation based on 3 categories. The ﬁrst category groups methods explaining the processing of\ndata by the network, thus answering to the question “why does this particular input leads to this particular\noutput?”. The second one concerns methods explaining the representation of data inside the network, i.e.,\nanswering to the question “what information does the network contain?”. The third approach concerns\n28\n\nmodels speciﬁcally designed to simplify the interpretation of their own behavior. Such a multiplicity of\nclassiﬁcation possibilities leads to different ways of constructing XAI taxonomies.\nXAI in DL\nExplanation of Deep\nNetwork Processing\nLinear Proxy Models\n[32]\nDecision Trees\n[82, 257, 258, 259]\nAutomatic-Rule Extraction\n[217, 251, 252, 253, 254, 255, 256, 319, 320, 321]\nSalience Mapping\n[96, 136, 261, 262, 272, 280, 322, 323]\nExplanation of Deep\nNetwork Representation\nRole of Layers\n[324, 325]\nRole of IndividualUnits\n[125, 326, 327, 328, 329]\nRole of RepresentationVectors\n[144]\nExplanation Producing\nSystems\nAttention Networks\n[267, 278, 330, 331, 332, 333, 334]\nRepresentation Disentanglement\n[113, 279, 335, 336, 337, 338, 339, 340, 341, 342]\nExplanation Generation\n[276, 343, 344, 345]\nHybrid Transparent\nand Black-box Methods\nNeural-symbolic Systems\n[297, 298, 299, 300]\nKB-enhanced Systems\n[24, 169, 301, 308, 309, 310]\nDeep Formulation\n[264, 302, 303, 304, 305]\nRelational Reasoning\n[75, 312, 313, 314]\nCase-base Reasoning\n[316, 317, 318]\nExplanation of Deep\nExplanation Producing\nLearning Representation\nExplanation of Deep\nNetwork Processing\nSystems\nSimpliﬁcation\nLocal Explanation\nText Explanation\nFeature Relevance\nArchitecture Modiﬁcation\nVisual explanation\nXAI in DL\nXAI in ML\nHybrid Transparent and\nBlack-box Methods\nTransparent Models\n(Fig. 11.a)\n(Fig. 6)\nExplanation by Example\n(a)\n(b)\nFigure 11: (a) Alternative Deep Learning speciﬁc taxonomy extended from the categorization from [13]; and (b) its connection to\nthe taxonomy in Figure 6.\nFigure 11 shows the alternative Deep Learning taxonomy inferred from [13]. From the latter, it can be\ndeduced the complementarity and overlapping of this taxonomy to Figure 6 as:\n• Some methods [272, 280] classiﬁed in distinct categories (namely feature relevance for CNN and\nfeature relevance for RNN) in Figure 6 are included in a single category (Explanation of Deep Network\nProcessing with Salience Mapping) when considering the classiﬁcation from [13].\n• Some methods [82, 144] are classiﬁed on a single category (Explanation by simpliﬁcation for MultiLayer Neural Network) in Figure 6 while being in 2 different categories (namely, Explanation of Deep\nNetwork Processing with Decision Trees and Explanation of Deep Network Representation with the\nRole of Representation Vectors) in [13], as shown in Figure 11.\nA classiﬁcation based on explanations of model processing and explanations of model representation\nis relevant, as it leads to a differentiation between the execution trace of the model and its internal data\nstructure. This means that depending of the failure reasons of a complex model, it would be possible to\npick-up the right XAI method according to the information needed: the execution trace or the data structure.\nThis idea is analogous to testing and debugging methods used in regular programming paradigms [346].\n5. XAI: Opportunities, Challenges and Future Research Needs\nWe now capitalize on the performed literature review to put forward a critique of the achievements,\ntrends and challenges that are still to be addressed in the ﬁeld of explainability of ML and data fusion\nmodels. Actually our discussion on the advances taken so far in this ﬁeld has already anticipated some\nof these challenges. In this section we revisit them and explore new research opportunities for XAI,\nidentifying possible research paths that can be followed to address them effectively in years to come:\n29\n\n• When introducing the overview in Section 1 we already mentioned the existence of a tradeoff between\nmodel interpretability and performance, in the sense that making a ML model more understandable\ncould eventually degrade the quality of its produced decisions. In Subsection 5.1 we will stress on the\npotential of XAI developments to effectively achieve an optimal balance between the interpretability\nand performance of ML models.\n• In Subsection 2.2 we stressed on the imperative need for reaching a consensus on what explainability\nentails within the AI realm. Reasons for pursuing explainability are also assorted and, under our\nown assessment of the literature so far, not unambiguously mentioned throughout related works. In\nSubsection 5.2 we will further delve into this important issue.\n• Given its notable prevalence in the XAI literature, Subsections 4.3 and 4.4 revolved on the explainability\nof Deep Learning models, examining advances reported so far around a speciﬁc bibliographic taxonomy.\nWe go in this same direction with Subsection 5.3, which exposes several challenges that hold in regards\nto the explainability of this family of models.\n• Finally, we close up this prospective discussion with Subsections 5.4 to 5.8, which place on the table\nseveral research niches that despite its connection to model explainability, remain insufﬁciently studied\nby the community.\nBefore delving into these identiﬁed challenges, it is important to bear in mind that this prospective\nsection is complemented by Section 6, which enumerates research needs and open questions related to\nXAI within a broader context: the need for responsible AI.\n5.1. On the Tradeoff between Interpretability and Performance\nThe matter of interpretability versus performance is one that repeats itself through time, but as any\nother big statement, has its surroundings ﬁlled with myths and misconceptions.\nAs perfectly stated in [347], it is not necessarily true that models that are more complex are inherently\nmore accurate. This statement is false in cases in which the data is well structured and features at our\ndisposal are of great quality and value. This case is somewhat common in some industry environments,\nsince features being analyzed are constrained within very controlled physical problems, in which all of\nthe features are highly correlated, and not much of the possible landscape of values can be explored in\nthe data [348]. What can be hold as true, is that more complex models enjoy much more ﬂexibility than\ntheir simpler counterparts, allowing for more complex functions to be approximated. Now, returning to\nthe statement “models that are more complex are more accurate”, given the premise that the function\nto be approximated entails certain complexity, that the data available for study is greatly widespread\namong the world of suitable values for each variable and that there is enough data to harness a complex\nmodel, the statement presents itself as a true statement. It is in this situation that the trade-off between\nperformance and interpretability can be observed. It should be noted that the attempt at solving problems\nthat do not respect the aforementioned premises will fall on the trap of attempting to solve a problem that\ndoes not provide enough data diversity (variance). Hence, the added complexity of the model will only\nﬁght against the task of accurately solving the problem.\nIn this path toward performance, when the performance comes hand in hand with complexity, interpretability encounters itself on a downwards slope that until now appeared unavoidable. However,\nthe apparition of more sophisticated methods for explainability could invert or at least cancel that slope.\nFigure 12 shows a tentative representation inspired by previous works [7], in which XAI shows its\npower to improve the common trade-off between model interpretability and performance. Another aspect\nworth mentioning at this point due to its close link to model interpretability and performance is the\napproximation dilemma: explanations made for a ML model must be made drastic and approximate\nenough to match the requirements of the audience for which they are sought, ensuring that explanations\nare representative of the studied model and do not oversimplify its essential features.\n30\n\nModel interpretability\nModel\nSVM\nEnsembles\nBayesian Models\nDecision Trees\nGeneralized\nModels\nLinear/Logistic\nkNN\nAdditive\nRegression\nRule-based\nlearning\nLearning\nDeep\nModel accuracy\nPost-hoc explainability techniques\nInterpretability-driven model designs\nHybrid modelling approaches\nNew explainability-preserving modelling approaches\nInterpretable feature engineering\nXAI’s future\nresearch arena\nLow\nHigh\nLow\nHigh\nFigure 12: Trade-off between model interpretability and performance, and a representation of the area of improvement where the\npotential of XAI techniques and tools resides.\n5.2. On the Concept and Metrics\nThe literature clearly asks for an uniﬁed concept of explainability. In order for the ﬁeld to thrive,\nit is imperative to place a common ground upon which the community is enabled to contribute new\ntechniques and methods. A common concept must convey the needs expressed in the ﬁeld. It should\npropose a common structure for every XAI system. This paper attempted a new proposition of a concept\nof explainability that is built upon that from Gunning [7]. In that proposition and the following strokes to\ncomplete it (Subsection 2.2), explainability is deﬁned as the ability a model has to make its functioning\nclearer to an audience. To address it, post-hoc type methods exist. The concept portrayed in this survey\nmight not be complete but as it stands, allows for a ﬁrst common ground and reference point to sustain\na proﬁtable discussion in this matter. It is paramount that the ﬁeld of XAI reaches an agreement in this\nrespect combining the shattered efforts of a widespread ﬁeld behind the same banner.\nAnother key feature needed to relate a certain model to this concrete concept is the existence of a\nmetric. A metric, or group of them should allow for a meaningful comparison of how well a model ﬁts\nthe deﬁnition of explainable. Without such tool, any claim in this respect dilutes among the literature, not\nproviding a solid ground on which to stand. These metrics, as the classic ones (accuracy, F1, sensitivity...),\nshould express how well the model performs in a certain aspect of explainability. Some attempts have been\ndone recently around the measurement of XAI, as reviewed thoroughly in [349, 350]. In general, XAI\nmeasurements should evaluate the goodness, usefulness and satisfaction of explanations, the improvement\nof the mental model of the audience induced by model explanations, and the impact of explanations on the\nperformance of the model and on the trust and reliance of the audience. Measurement techniques surveyed\nin [349] and [350] (e.g., goodness checklist, explanation satisfaction scale, elicitation methods for mental\nmodels, computational measures for explainer ﬁdelity, explanation trustworthiness and model reliability)\nseem to be a good push in the direction of evaluating XAI techniques. Unfortunately, conclusions drawn\nfrom these overviews are aligned with our prospects on the ﬁeld: more quantiﬁable, general XAI metrics\nare really needed to support the existing measurement procedures and tools proposed by the community.\nThis survey does not tackle the problem of designing such a suite of metrics, since such a task should\nbe approached by the community as a whole prior acceptance of the broader concept of explainability, which on the other hand, is one of the aims of the current work. Nevertheless, we advocate for\nfurther efforts towards new proposals to evaluate the performance of XAI techniques, as well as comparison methodologies among XAI approaches that allow contrasting them quantitatively under different\n31\n\napplication context, models and purposes.\n5.3. Challenges to achieve Explainable Deep Learning\nWhile many efforts are currently being made in the area of XAI, there are still many challenges to be\nfaced before being able to obtain explainability in DL models. First, as explained in Subsection 2.2, there\nis a lack of agreement on the vocabulary and the different deﬁnitions surrounding XAI. As an example,\nwe often see the terms feature importance and feature relevance referring to the same concept. This is\neven more obvious for visualization methods, where there is absolutely no consistency behind what is\nknown as saliency maps, salient masks, heatmaps, neuron activations, attribution, and other approaches\nalike. As XAI is a relatively young ﬁeld, the community does not have a standardized terminology yet.\nAs it has been commented in Subsection 5.1, there is a trade-off between interpretability and accuracy\n[13], i.e., between the simplicity of the information given by the system on its internal functioning, and\nthe exhaustiveness of this description. Whether the observer is an expert in the ﬁeld, a policy-maker or a\nuser without machine learning knowledge, intelligibility does not have to be at the same level in order\nto provide the audience an understanding [6]. This is one of the reasons why, as mentioned above, a\nchallenge in XAI is establishing objective metrics on what constitutes a good explanation. A possibility\nto reduce this subjectivity is taking inspiration from experiments on human psychology, sociology or\ncognitive sciences to create objectively convincing explanations. Relevant ﬁndings to be considered when\ncreating an explainable AI model are highlighted in [12]: First, explanations are better when constrictive,\nmeaning that a prerequisite for a good explanation is that it does not only indicate why the model made a\ndecision X, but also why it made decision X rather than decision Y. It is also explained that probabilities\nare not as important as causal links in order to provide a satisfying explanation. Considering that black box\nmodels tend to process data in a quantitative manner, it would be necessary to translate the probabilistic\nresults into qualitative notions containing causal links. In addition, they state that explanations are\nselective, meaning that focusing solely on the main causes of a decision-making process is sufﬁcient. It\nwas also shown that the use of counterfactual explanations can help the user to understand the decision of\na model [40, 42, 351].\nCombining connectionist and symbolic paradigms seems a favourable way to address this challenge\n[169, 299, 312, 352, 353]. On one hand, connectionist methods are more precise but opaque. On the other\nhand, symbolic methods are popularly considered less efﬁcient, while they offer a greater explainability\nthus respecting the conditions mentioned above:\n• The ability to refer to established reasoning rules allows symbolic methods to be constrictive.\n• The use of a KB formalized e.g. by an ontology can allow data to be processed directly in a qualitative\nway.\n• Being selective is less straightforward for connectionist models than for symbolic ones.\nRecalling that a good explanation needs to inﬂuence the mental model of the user, i.e. the representation of the external reality using, among other things, symbols, it seems obvious that the use of\nthe symbolic learning paradigm is appropriate to produce an explanation. Therefore, neural-symbolic\ninterpretability could provide convincing explanations while keeping or improving generic performance\n[297].\nAs stated in [24], a truly explainable model should not leave explanation generation to the users as\ndifferent explanations may be deduced depending on their background knowledge. Having a semantic\nrepresentation of the knowledge can help a model to have the ability to produce explanations (e.g., in\nnatural language [169]) combining common sense reasoning and human-understandable features.\nFurthermore, until an objective metric has been adopted, it appears necessary to make an effort to\nrigorously formalize evaluation methods. One way may be drawing inspiration from the social sciences,\ne.g., by being consistent when choosing the evaluation questions and the population sample used [354].\n32\n\nA ﬁnal challenge XAI methods for DL need to address is providing explanations that are accessible\nfor society, policy makers and the law as a whole. In particular, conveying explanations that require\nnon-technical expertise will be paramount to both handle ambiguities, and to develop the social right to\nthe (not-yet available) right for explanation in the EU General Data Protection Regulation (GDPR) [355].\n5.4. Explanations for AI Security: XAI and Adversarial Machine Learning\nNothing has been said about conﬁdentiality concerns linked to XAI. One of the last surveys very\nbrieﬂy introduced the idea of algorithm property and trade secrets [14]. However, not much attention\nhas been payed to these concepts. If conﬁdential is the property that makes something secret, in the\nAI context many aspects involved in a model may hold this property. For example, imagine a model\nthat some company has developed through many years of research in a speciﬁc ﬁeld. The knowledge\nsynthesized in the model built might be considered to be conﬁdential, and it may be compromised even\nby providing only input and output access [356]. The latter shows that, under minimal assumptions,\ndata model functionality stealing is possible. An approach that has served to make DL models more\nrobust against intellectual property exposure based on a sequence of non accessible queries is in [357].\nThis recent work exposes the need for further research toward the development of XAI tools capable of\nexplaining ML models while keeping the model’s conﬁdentiality in mind.\nIdeally, XAI should be able to explain the knowledge within an AI model and it should be able to\nreason about what the model acts upon. However, the information revealed by XAI techniques can be used\nboth to generate more effective attacks in adversarial contexts aimed at confusing the model, at the same\ntime as to develop techniques to better protect against private content exposure by using such information.\nAdversarial attacks [358] try to manipulate a ML algorithm after learning what is the speciﬁc information\nthat should be fed to the system so as to lead it to a speciﬁc output. For instance, regarding a supervised\nML classiﬁcation model, adversarial attacks try to discover the minimum changes that should be applied\nto the input data in order to cause a different classiﬁcation. This has happened regarding computer vision\nsystems of autonomous vehicles; a minimal change in a stop signal, imperceptible to the human eye, led\nvehicles to detect it as a 45 mph signal [359]. For the particular case of DL models, available solutions\nsuch as Cleverhans [360] seek to detect adversarial vulnerabilities, and provide different approaches\nto harden the model against them. Other examples include AlfaSVMLib [361] for SVM models, and\nAdversarialLib [362] for evasion attacks. There are even available solutions for unsupervised ML, like\nclustering algorithms [363].\nWhile XAI techniques can be used to furnish more effective adversarial attacks or to reveal conﬁdential\naspects of the model itself, some recent contributions have capitalized on the possibilities of Generative\nAdversarial Networks (GANs [364]), Variational Autoencoders [365] and other generative models towards\nexplaining data-based decisions. Once trained, generative models can generate instances of what they\nhave learned based on a noise input vector that can be interpreted as a latent representation of the data at\nhand. By manipulating this latent representation and examining its impact on the output of the generative\nmodel, it is possible to draw insights and discover speciﬁc patterns related to the class to be predicted.\nThis generative framework has been adopted by several recent studies [366, 367] mainly as an attribution\nmethod to relate a particular output of a Deep Learning model to their input variables. Another interesting\nresearch direction is the use of generative models for the creation of counterfactuals, i.e., modiﬁcations\nto the input data that could eventually alter the original prediction of the model [368]. Counterfactual\nprototypes help the user understand the performance boundaries of the model under consideration for\nhis/her improved trust and informed criticism. In light of this recent trend, we deﬁnitely believe that there\nis road ahead for generative ML models to take their part in scenarios demanding understandable machine\ndecisions.\n5.5. XAI and Output Conﬁdence\nSafety issues have also been studied in regards to processes that depend on the output of AI models,\nsuch as vehicular perception and self-driving in autonomous vehicles, automated surgery, data-based\n33\n\nsupport for medical diagnosis, insurance risk assessment and cyber-physical systems in manufacturing,\namong others [369]. In all these scenarios erroneous model outputs can lead to harmful consequences,\nwhich has yielded comprehensive regulatory efforts aimed at ensuring that no decision is made solely on\nthe basis of data processing [3].\nIn parallel, research has been conducted towards minimizing both risk and uncertainty of harms\nderived from decisions made on the output of a ML model. As a result, many techniques have been\nreported to reduce such a risk, among which we pause at the evaluation of the model’s output conﬁdence\nto decide upon. In this case, the inspection of the share of epistemic uncertainty (namely, the uncertainty\ndue to lack of knowledge) of the input data and its correspondence with the model’s output conﬁdence\ncan inform the user and eventually trigger his/her rejection of the model’s output [370, 371]. To this end,\nexplaining via XAI techniques which region of the input data the model is focused on when producing a\ngiven output can discriminate possible sources of epistemic uncertainty within the input domain.\n5.6. XAI, Rationale Explanation, and Critical Data Studies\nWhen shifting the focus to the research practices seen in Data Science, it has been noted that\nreproducibility is stringently subject not only to the mere sharing of data, models and results to the\ncommunity, but also to the availability of information about the full discourse around data collection,\nunderstanding, assumptions held and insights drawn from model construction and results’ analyses [372].\nIn other words, in order to transform data into a valuable actionable asset, individuals must engage in\ncollaborative sense-making by sharing the context producing their ﬁndings, wherein context refers to sets\nof narrative stories around how data were processed, cleaned, modeled and analyzed. In this discourse\nwe ﬁnd also an interesting space for the adoption of XAI techniques due to their powerful ability to\ndescribe black-box models in an understandable, hence conveyable fashion towards colleagues from\nSocial Science, Politics, Humanities and Legal ﬁelds.\nXAI can effectively ease the process of explaining the reasons why a model reached a decision in an\naccessible way to non-expert users, i.e. the rationale explanation. This conﬂuence of multi-disciplinary\nteams in projects related to Data Science and the search for methodologies to make them appraise the\nethical implications of their data-based choices has been lately coined as Critical Data studies [373]. It\nis in this ﬁeld where XAI can signiﬁcantly boost the exchange of information among heterogeneous\naudiences about the knowledge learned by models.\n5.7. XAI and Theory-guided Data Science\nWe envision an exciting synergy between the XAI realm and Theory-guided Data Science, a paradigm\nexposed in [374] that merges both Data Science and the classic theoretical principles underlying the\napplication/context where data are produced. The rationale behind this rising paradigm is the need for databased models to generate knowledge that is the prior knowledge brought by the ﬁeld in which it operates.\nThis means that the model type should be chosen according to the type of relations we intend to encounter.\nThe structure should also follow what is previously known. Similarly, the training approach should not\nallow for the optimization process to enter regions that are not plausible. Accordingly, regularization\nterms should stand the prior premises of the ﬁeld, avoiding the elimination of badly represented true\nrelations for spurious and deceptive false relations. Finally, the output of the model should inform about\neverything the model has come to learn, allowing to reason and merge the new knowledge with what was\nalready known in the ﬁeld.\nMany examples of the implementation of this approach are currently available with promising results.\nThe studies in [375]-[382] were carried out in diverse ﬁelds, showcasing the potential of this new paradigm\nfor data science. Above all, it is relevant to notice the resemblance that all concepts and requirements of\nTheory-guided Data Science share with XAI. All the additions presented in [374] push toward techniques\nthat would eventually render a model explainable, and furthermore, knowledge consistent. The concept\nof knowledge from the beginning, central to Theory-guided Data Science, must also consider how\n34\n\nthe knowledge captured by a model should be explained for assessing its compliance with theoretical\nprinciples known beforehand. This, again, opens a magniﬁcent window of opportunity for XAI.\n5.8. Guidelines for ensuring Interpretable AI Models\nRecent surveys have emphasized on the multidisciplinary, inclusive nature of the process of making\nan AI-based model interpretable. Along this process, it is of utmost importance to scrutinize and take into\nproper account the interests, demands and requirements of all stakeholders interacting with the system to\nbe explained, from the designers of the system to the decision makers consuming its produced outputs\nand users undergoing the consequences of decisions made therefrom.\nGiven the conﬂuence of multiple criteria and the need for having the human in the loop, some\nattempts at establishing the procedural guidelines to implement and explain AI systems have been recently\ncontributed. Among them, we pause at the thorough study in [383], which suggests that the incorporation\nand consideration of explainability in practical AI design and deployment workﬂows should comprise\nfour major methodological steps:\n1. Contextual factors, potential impacts and domain-speciﬁc needs must be taken into account when\ndevising an approach to interpretability: These include a thorough understanding of the purpose for\nwhich the AI model is built, the complexity of explanations that are required by the audience, and the\nperformance and interpretability levels of existing technology, models and methods. The latter pose a\nreference point for the AI system to be deployed in lieu thereof.\n2. Interpretable techniques should be preferred when possible: when considering explainability in the\ndevelopment of an AI system, the decision of which XAI approach should be chosen should gauge\ndomain-speciﬁc risks and needs, the available data resources and existing domain knowledge, and the\nsuitability of the ML model to meet the requirements of the computational task to be addressed. It is in\nthe conﬂuence of these three design drivers where the guidelines postulated in [383] (and other studies\nin this same line of thinking [384]) recommend ﬁrst the consideration of standard interpretable models\nrather than sophisticated yet opaque modeling methods. In practice, the aforementioned aspects\n(contextual factors, impacts and domain-speciﬁc needs) can make transparent models preferable\nover complex modeling alternatives whose interpretability require the application of post-hoc XAI\ntechniques. By contrast, black-box models such as those reviewed in this work (namely, support\nvector machines, ensemble methods and neural networks) should be selected only when their superior\nmodeling capabilities ﬁt best the characteristics of the problem at hand.\n3. If a black-box model has been chosen, the third guideline establishes that ethics-, fairness- and safetyrelated impacts should be weighed. Speciﬁcally, responsibility in the design and implementation of\nthe AI system should be ensured by checking whether such identiﬁed impacts can be mitigated and\ncounteracted by supplementing the system with XAI tools that provide the level of explainability\nrequired by the domain in which it is deployed. To this end, the third guideline suggests 1) a detailed\narticulation, examination and evaluation of the applicable explanatory strategies, 2) the analysis of\nwhether the coverage and scope of the available explanatory approaches match the requirements of\nthe domain and application context where the model is to be deployed; and 3) the formulation of\nan interpretability action plan that sets forth the explanation delivery strategy, including a detailed\ntime frame for the execution of the plan, and a clearance of the roles and responsibilities of the team\ninvolved in the workﬂow.\n4. Finally, the fourth guideline encourages to rethink interpretability in terms of the cognitive skills,\ncapacities and limitations of the individual human. This is an important question on which studies\non measures of explainability are intensively revolving by considering human mental models, the\naccessibility of the audience to vocabularies of explanatory outcomes, and other means to involve the\nexpertise of the audience into the decision of what explanations should provide.\n35\n\nWe foresee that the set of guidelines proposed in [383] and summarized above will be complemented\nand enriched further by future methodological studies, ultimately heading to a more responsible use of AI.\nMethodological principles ensure that the purpose for which explainability is pursued is met by bringing\nthe manifold of requirements of all participants into the process, along with other universal aspects of\nequal relevance such as no discrimination, sustainability, privacy or accountability. A challenge remains\nin harnessing the potential of XAI to realize a Responsible AI, as we discuss in the next section.\n6. Toward Responsible AI: Principles of Artiﬁcial Intelligence, Fairness, Privacy and Data Fusion\nOver the years many organizations, both private and public, have published guidelines to indicate\nhow AI should be developed and used. These guidelines are commonly referred to as AI principles, and\nthey tackle issues related to potential AI threats to both individuals and to the society as a whole. This\nsection presents some of the most important and widely recognized principles in order to link XAI –\nwhich normally appears inside its own principle – to all of them. Should a responsible implementation\nand use of AI models be sought in practice, it is our ﬁrm claim that XAI does not sufﬁce on its own. Other\nimportant principles of Artiﬁcial Intelligence such as privacy and fairness must be carefully addressed\nin practice. In the following sections we elaborate on the concept of Responsible AI, along with the\nimplications of XAI and data fusion in the fulﬁllment of its postulated principles.\n6.1. Principles of Artiﬁcial Intelligence\nA recent review of some of the main AI principles published since 2016 appears in [385]. In this\nwork, the authors show a visual framework where different organizations are classiﬁed according to the\nfollowing parameters:\n• Nature, which could be private sector, government, inter-governmental organization, civil society or\nmultistakeholder.\n• Content of the principles: eight possible principles such as privacy, explainability, or fairness. They\nalso consider the coverage that the document grants for each of the considered principles.\n• Target audience: to whom the principles are aimed. They are normally for the organization that\ndeveloped them, but they could also be destined for another audience (see Figure 2).\n• Whether or not they are rooted on the International Human Rights, as well as whether they explicitly\ntalk about them.\nFor instance, [386] is an illustrative example of a document of AI principles for the purpose of\nthis overview, since it accounts for some of the most common principles, and deals explicitly with\nexplainability. Here, the authors propose ﬁve principles mainly to guide the development of AI within\ntheir company, while also indicating that they could also be used within other organizations and businesses.\nThe authors of those principles aim to develop AI in a way that it directly reinforces inclusion, gives\nequal opportunities for everyone, and contributes to the common good. To this end, the following aspects\nshould be considered:\n• The outputs after using AI systems should not lead to any kind of discrimination against individuals\nor collectives in relation to race, religion, gender, sexual orientation, disability, ethnic, origin or any\nother personal condition. Thus, a fundamental criteria to consider while optimizing the results of an AI\nsystem is not only their outputs in terms of error optimization, but also how the system deals with those\ngroups. This deﬁnes the principle of Fair AI.\n36\n\n• People should always know when they are communicating with a person, and when they are communicating with an AI system. People should also be aware if their personal information is being used\nby the AI system and for what purpose. It is crucial to ensure a certain level of understanding about\nthe decisions taken by an AI system. This can be achieved through the usage of XAI techniques. It\nis important that the generated explanations consider the proﬁle of the user that will receive those\nexplanations (the so-called audience as per the deﬁnition given in Subsection 2.2) in order to adjust the\ntransparency level, as indicated in [45]. This deﬁnes the principle of Transparent and Explainable AI.\n• AI products and services should always be aligned with the United Nation’s Sustainable Development\nGoals [387] and contribute to them in a positive and tangible way. Thus, AI should always generate\na beneﬁt for humanity and the common good. This deﬁnes the principle of Human-centric AI (also\nreferred to as AI for Social Good [388]).\n• AI systems, specially when they are fed by data, should always consider privacy and security standards\nduring all of its life cycle. This principle is not exclusive of AI systems since it is shared with many\nother software products. Thus, it can be inherited from processes that already exist within a company.\nThis deﬁnes the principle of Privacy and Security by Design, which was also identiﬁed as one of\nthe core ethical and societal challenges faced by Smart Information Systems under the Responsible\nResearch and Innovation paradigm (RRI, [389]). RRI refers to a package of methodological guidelines\nand recommendations aimed at considering a wider context for scientiﬁc research, from the perspective\nof the lab to global societal challenges such as sustainability, public engagement, ethics, science\neducation, gender equality, open access, and governance. Interestingly, RRI also requires openness and\ntransparency to be ensured in projects embracing its principles, which links directly to the principle of\nTransparent and Explainable AI mentioned previously.\n• The authors emphasize that all these principles should always be extended to any third-party (providers,\nconsultants, partners...).\nGoing beyond the scope of these ﬁve AI principles, the European Commission (EC) has recently\npublished ethical guidelines for Trustworthy AI [390] through an assessment checklist that can be\ncompleted by different proﬁles related to AI systems (namely, product managers, developers and other\nroles). The assessment is based in a series of principles: 1) human agency and oversight; 2) technical\nrobustness and safety; 3) privacy and data governance; 4) transparency, diversity, non-discrimination and\nfairness; 5) societal and environmental well-being; 6) accountability. These principles are aligned with\nthe ones detailed in this section, though the scope for the EC principles is more general, including any\ntype of organization involved in the development of AI.\nIt is worth mentioning that most of these AI principles guides directly approach XAI as a key aspect\nto consider and include in AI systems. In fact, the overview for these principles introduced before [385],\nindicates that 28 out of the 32 AI principles guides covered in the analysis, explicitly include XAI as a\ncrucial component. Thus, the work and scope of this article deals directly with one of the most important\naspects regarding AI at a worldwide level.\n6.2. Fairness and Accountability\nAs mentioned in the previous section, there are many critical aspects, beyond XAI, included within\nthe different AI principles guidelines published during the last decade. However, those aspects are not\ncompletely detached from XAI; in fact, they are intertwined. This section presents two key components\nwith a huge relevance within the AI principles guides, Fairness and Accountability. It also highlights how\nthey are connected to XAI.\n37\n\n6.2.1. Fairness and Discrimination\nIt is in the identiﬁcation of implicit correlations between protected and unprotected features where\nXAI techniques ﬁnd their place within discrimination-aware data mining methods. By analyzing how\nthe output of the model behaves with respect to the input feature, the model designer may unveil hidden\ncorrelations between the input variables amenable to cause discrimination. XAI techniques such as SHAP\n[224] could be used to generate counterfactual outcomes explaining the decisions of a ML model when\nfed with protected and unprotected variables.\nRecalling the Fair AI principle introduced in the previous section, [386] reminds that fairness is a\ndiscipline that generally includes proposals for bias detection within datasets regarding sensitive data that\naffect protected groups (through variables like gender, race...). Indeed, ethical concerns with black-box\nmodels arise from their tendency to unintentionally create unfair decisions by considering sensitive factors\nsuch as the individual’s race, age or gender [391]. Unfortunately, such unfair decisions can give rise to\ndiscriminatory issues, either by explicitly considering sensitive attributes or implicitly by using factors\nthat correlate with sensitive data. In fact, an attribute may implicitly encode a protected factor, as occurs\nwith postal code in credit rating [392]. The aforementioned proposals centered on fairness aspects permit\nto discover correlations between non-sensitive variables and sensitive ones, detect imbalanced outcomes\nfrom the algorithms that penalize a speciﬁc subgroup of people (discrimination), and mitigate the effect\nof bias on the model’s decisions. These approaches can deal with:\n• Individual fairness: here, fairness is analyzed by modeling the differences between each subject and the\nrest of the population.\n• Group fairness: it deals with fairness from the perspective of all individuals.\n• Counterfactual fairness: it tries to interpret the causes of bias using, for example, causal graphs.\nThe sources for bias, as indicated in [392], can be traced to:\n• Skewed data: bias within the data acquisition process.\n• Tainted data: errors in the data modelling deﬁnition, wrong feature labelling, and other possible causes.\n• Limited features: using too few features could lead to an inference of false feature relationships that\ncan lead to bias.\n• Sample size disparities: when using sensitive features, disparities between different subgroups can\ninduce bias.\n• Proxy features: there may be correlated features with sensitive ones that can induce bias even when the\nsensitive features are not present in the dataset.\nThe next question that can be asked is what criteria could be used to deﬁne when AI is not biased. For\nsupervised ML, [393] presents a framework that uses three criteria to evaluate group fairness when there\nis a sensitive feature present within the dataset:\n• Independence: this criterion is fulﬁlled when the model predictions are independent of the sensitive\nfeature. Thus, the proportion of positive samples (namely, those ones belonging to the class of interest)\ngiven by the model is the same for all the subgroups within the sensitive feature.\n• Separation: it is met when the model predictions are independent of the sensitive feature given the\ntarget variable. For instance, in classiﬁcation models, the True Positive (TP) rate and the False Positive\n(FP) rate are the same in all the subgroups within the sensitive feature. This criteria is also known as\nEqualized Odds.\n38\n\n• Sufﬁciency: it is accomplished when the target variable is independent of the sensitive feature given\nthe model output. Thus, the Positive Predictive Value is the same for all subgroups within the sensitive\nfeature. This criteria is also known as Predictive Rate Parity.\nAlthough not all of the criteria can be fulﬁlled at the same time, they can be optimized together in\norder to minimize the bias within the ML model.\nThere are two possible actions that could be used in order to achieve those criteria. On one hand,\nevaluation includes measuring the amount of bias present within the model (regarding one of the criteria\naforementioned). There are many different metrics that can be used, depending on the criteria considered.\nRegarding independence criterion, possible metrics are statistical parity difference or disparate impact.\nIn case of the separation criterion, possible metrics are equal opportunity difference and average odds\ndifference [393]. Another possible metric is the Theil index [394], which measures inequality both in\nterms of individual and group fairness.\nOn the other hand, mitigation refers to the process of ﬁxing some aspects in the model in order to\nremove the effect of the bias in terms of one or several sensitive features. Several techniques exist within\nthe literature, classiﬁed in the following categories:\n• Pre-processing: these groups of techniques are applied before the ML model is trained, looking to\nremove the bias at the ﬁrst step of the learning process. An example is Reweighing [395], which\nmodiﬁes the weights of the features in order to remove discrimination in sensitive attributes. Another\nexample is [396], which hinges on transforming the input data in order to ﬁnd a good representation\nthat obfuscates information about membership in sensitive features.\n• In-processing: these techniques are applied during the training process of the ML model. Normally,\nthey include Fairness optimization constraints along with cost functions of the ML model. An example\nis Adversarial Debiasing, [397]. This technique optimizes jointly the ability of predicting the target\nvariable while minimizing the ability of predicting sensitive features using a GAN.\n• Post-processing: these techniques are applied after the ML model is trained. They are less intrusive\nbecause they do not modify the input data or the ML model. An example is Equalized Odds [393]. This\ntechniques allows to adjust the thresholds in the classiﬁcation model in order to reduce the differences\nbetween the TP rate and the FP rate for each sensitive subgroup.\nEven though these references apparently address an AI principle that appears to be independent of\nXAI, the literature shows that they are intertwined. For instance, the survey in [385] evinces that 26 out\nof the 28 AI principles that deal with XAI, also talk about fairness explicitly. This fact elucidates that\norganizations usually consider both aspects together when implementing Responsible AI.\nThe literature also exploses that XAI proposals can be used for bias detection. For example, [398]\nproposes a framework to visually analyze the bias present in a model (both for individual and group\nfairness). Thus, the fairness report is shown just like the visual summaries used within XAI. This\nexplainability approach eases the understanding and measurement of bias. The system must report that\nthere is bias, justify it quantitatively, indicate the degree of fairness, and explain why a user or group\nwould be treated unfairly with the available data. Similarly, XAI techniques such as SHAP [224] could be\nused to generate counterfactual outcomes explaining the decisions of a ML model when fed with protected\nand unprotected variables. By identifying implicit correlations between protected and unprotected features\nthrough XAI techniques, the model designer may unveil hidden correlations between the input variables\namenable to cause discrimination.\nAnother example is [399], where the authors propose a fair-by-design approach in order to develop\nML models that jointly have less bias and include as explanations human comprehensible rules. The\nproposal is based in self-learning locally generative models that use only a small part of the whole\ndataset available (weak supervision). It ﬁrst ﬁnds recursively relevant prototypes within the dataset, and\n39\n\nextracts the empirical distribution and density of the points around them. Then it generates rules in an\nIF/THEN format that explain that a data point is classiﬁed within a speciﬁc category because it is similar\nto some prototypes. The proposal then includes an algorithm that both generates explanations and reduces\nbias, as it is demonstrated for the use case of recidivism using the Correctional Offender Management\nProﬁling for Alternative Sanctions (COMPAS) dataset [400]. The same goal has been recently pursued in\n[401], showing that post-hoc XAI techniques can forge fairer explanations from truly unfair black-box\nmodels. Finally, CERTIFAI (Counterfactual Explanations for Robustness, Transparency, Interpretability,\nand Fairness of Artiﬁcial Intelligence models) [402] uses a customized genetic algorithm to generate\ncounterfactuals that can help to see the robustness of a ML model, generate explanations, and examine\nfairness (both at the individual level and at the group level) at the same time.\nStrongly linked to the concept of fairness, much attention has been lately devoted to the concept of\ndata diversity, which essentially refers to the capability of an algorithmic model to ensure that all different\ntypes of objects are represented in its output [403]. Therefore, diversity can be thought to be an indicator\nof the quality of a collection of items that, when taking the form of a model’s output, can quantify the\nproneness of the model to produce diverse results rather than highly accurate predictions. Diversity comes\ninto play in human-centered applications with ethical restrictions that permeate to the AI modeling phase\n[404]. Likewise, certain AI problems (such as content recommendation or information retrieval) also\naim at producing diverse recommendations rather than highly-scoring yet similar results [405, 406]. In\nthese scenarios, dissecting the internals of a black-box model via XAI techniques can help identifying the\ncapability of the model to maintain the input data diversity at its output. Learning strategies to endow a\nmodel with diversity keeping capabilities could be complemented with XAI techniques in order to shed\ntransparency over the model internals, and assess the effectiveness of such strategies with respect to the\ndiversity of the data from which the model was trained. Conversely, XAI could help to discriminate which\nparts of the model are compromising its overall ability to preserve diversity.\n6.2.2. Accountability\nRegarding accountability, the EC [390] deﬁnes the following aspects to consider:\n• Auditability: it includes the assessment of algorithms, data and design processes, but preserving the\nintellectual property related to the AI systems. Performing the assessment by both internal and external\nauditors, and making the reports available, could contribute to the trustworthiness of the technology.\nWhen the AI system affects fundamental rights, including safety-critical applications, it should always\nbe audited by an external third party.\n• Minimization and reporting of negative impacts: it consists of reporting actions or decisions that yield\na certain outcome by the system. It also comprises the assessment of those outcomes and how to\nrespond to them. To address that, the development of AI systems should also consider the identiﬁcation,\nassessment, documentation and minimization of their potential negative impacts. In order to minimize\nthe potential negative impact, impact assessments should be carried out both prior to and during the\ndevelopment, deployment and use of AI systems. It is also important to guarantee protection for anyone\nwho raises concerns about an AI system (e.g., whistle-blowers). All assessments must be proportionate\nto the risk that the AI systems pose.\n• Trade-offs: in case any tension arises due to the implementation of the above requirements, trade-offs\ncould be considered but only if they are ethically acceptable. Such trade-offs should be reasoned,\nexplicitly acknowledged and documented, and they must be evaluated in terms of their risk to ethical\nprinciples. The decision maker must be accountable for the manner in which the appropriate trade-off\nis being made, and the trade-off decided should be continually reviewed to ensure the appropriateness\nof the decision. If there is no ethically acceptable trade-off, the development, deployment and use of\nthe AI system should not proceed in that form.\n40\n\n• Redress: it includes mechanisms that ensure an adequate redress for situations when unforeseen unjust\nadverse impacts take place. Guaranteeing a redress for those non-predicted scenarios is a key to ensure\ntrust. Special attention should be paid to vulnerable persons or groups.\nThese aspects addressed by the EC highlight different connections of XAI with accountability. First,\nXAI contributes to auditability as it can help explaining AI systems for different proﬁles, including\nregulatory ones. Also, since there is a connection between fairness and XAI as stated before, XAI can\nalso contribute to the minimization and report of negative impacts.\n6.3. Privacy and Data Fusion\nThe ever-growing number of information sources that nowadays coexist in almost all domains of\nactivity calls for data fusion approaches aimed at exploiting them simultaneously toward solving a learning\ntask. By merging heterogeneous information, data fusion has been proven to improve the performance of\nML models in many applications, such as industrial prognosis [348], cyber-physical social systems [407]\nor the Internet of Things [408], among others. This section speculates with the potential of data fusion\ntechniques to enrich the explainability of ML models, and to compromise the privacy of the data from\nwhich ML models are learned. To this end, we brieﬂy overview different data fusion paradigms, and later\nanalyze them from the perspective of data privacy. As we will later, despite its relevance in the context of\nResponsible AI, the conﬂuence between XAI and data fusion is an uncharted research area in the current\nresearch mainstream.\n6.3.1. Basic Levels of Data Fusion\nWe depart from the different levels of data fusion that have been identiﬁed in comprehensive surveys\non the matter [409, 410, 411, 412]. In the context of this subsection, we will distinguish among fusion at\ndata level, fusion at model level and fusion at knowledge level. Furthermore, a parallel categorization can\nbe established depending on where such data is processed and fused, yielding centralized and distributed\nmethods for data fusion. In a centralized approach, nodes deliver their locally captured data to a centralized\nprocessing system to merge them together. In contrast, in a distributed approach, each of the nodes merges\nits locally captured information, eventually sharing the result of the local fusion with its counterparts.\nFusion through the information generation process has properties and peculiarities depending on\nthe level at which the fusion is performed. At the so-called data level, fusion deals with raw data. As\nschematically shown in Figure 13, a fusion model at this stage receives raw data from different information\nsources, and combines them to create a more coherent, compliant, robust or simply representative data\nﬂow. On the other hand, fusion at the model level aggregates models, each learned from a subset of the\ndata sets that were to be fused. Finally, at the knowledge level the fusion approach deals with knowledge in\nthe form of rules, ontologies or other knowledge representation techniques with the intention of merging\nthem to create new, better or more complete knowledge from what was originally provided. Structured\nknowledge information is extracted from each data source and for every item in the data set using multiple\nknowledge extractors (e.g. a reasoning engine operating on an open semantic database). All produced\ninformation is then fused to further ensure the quality, correctness and manageability of the produced\nknowledge about the items in the data set.\nOther data fusion approaches exist beyonds the ones represented in Figure 13. As such, data-level\nfusion can be performed either by a technique speciﬁcally devoted to this end (as depicted in Figure 13.b)\nor, instead, performed along the learning process of the ML model (as done in e.g. DL models). Similarly,\nmodel-level data fusion can be made by combining the decisions of different models (as done in tree\nensembles).\n6.3.2. Emerging Data Fusion Approaches\nIn the next subsection we examine other data fusion approaches that have recently come into scene\ndue to their implications in terms of data privacy:\n41\n\nD1\nD2\nDN\n...\nD1\nD2\nDN\n...\n(d)\n(e)\n(f)\n...\n...\nW1\nSplit\nW2\nW3\nWN\nMap\nReduce\nR\n∼ML\nML1\nML2\nMLN\n...\nAggregation\nSecure\nClient-server\ndelivery\n: encrypted model\n: model update\ninformation (e.g.\nServer side\nRemote clients\ngradients)\nWn: n-th worker node\nR: reducer node\n...\nD1\nD2\nDN\n...\nMLN\n...\nView 1\nView 2\nView 3\nView V\n...\nML1\nML2\nML3\nMLV\n...\nJoint optimization\n+ Fusion\nD1\nD2\nDN\nDF\nD1\nD2\nDN\nML\nML\nML\nML\nData Fusion technique\nModel Fusion\n...\n...\n...\nD1\nD2\nDN\n...\nKnowledge\nextractor\nKnowledge\nextractor\nKnowledge\nextractor\nKnowledge Fusion\nKB\n...\nML\nKB\nDi\nDF\n...\n(a)\n(b)\n(c)\n: Predictions\n: Knowledge Base\n: Fused data\n: i-th dataset\nFigure 13: Diagrams showing different levels at which data fusion can be performed: (a) data level; (b) model level; (c) knowledge\nlevel; (d) Big Data fusion; (e) Federated Learning and (f) Multiview Learning.\n• In Big Data fusion (Figure 13.d), local models are learned on a split of the original data sources, each\nsubmitted to a Worker node in charge of performing this learning process (Map task). Then, a Reduce\nnode (or several Reduce nodes, depending on the application) combines the outputs produced by each\nMap task. Therefore, Big Data fusion can be conceived as a means to distribute the complexity of learning a ML model over a pool of Worker nodes, wherein the strategy to design how information/models\nare fused together between the Map and the Reduce tasks is what deﬁnes the quality of the ﬁnally\ngenerated outcome [413].\n• By contrast, in Federated Learning [414, 415, 416], the computation of ML models is made on data\ncaptured locally by remote client devices (Figure 13.e). Upon local model training, clients transmit\nencrypted information about their learned knowledge to a central server, which can take the form of\nlayer-wise gradients (in the case of neural ML models) or any other model-dependent content alike. The\ncentral server aggregates (fuses) the knowledge contributions received from all clients to yield a shared\nmodel harnessing the collected information from the pool of clients. It is important to observe that no\nclient data is delivered to the central server, which elicits the privacy-preserving nature of Federated\nLearning. Furthermore, computation is set closer to the collected data, which reduces the processing\nlatency and alleviates the computational burden of the central server.\n• Finally, Multiview Learning [417] constructs different views of the object as per the information\ncontained in the different data sources (Figure 13.f). These views can be produced from multiple\nsources of information and/or different feature subsets [418]. Multiview Learning devises strategies\nto jointly optimize ML models learned from the aforementioned views to enhance the generalization\nperformance, specially in those applications with weak data supervision and hence, prone to model\noverﬁtting. This joint optimization resorts to different algorithmic means, from co-training to coregularization [419].\n6.3.3. Opportunities and Challenges in Privacy and Data Fusion under the Responsible AI Paradigm\nAI systems, specially when dealing with multiple data sources, need to explicitly include privacy\nconsiderations during the system’s life cycle. This is specially critical when working with personal data,\n42\n\nbecause respecting people’s right to privacy should always be addressed. The EC highlights that privacy\nshould also address data governance, covering the quality and integrity of the used data [390]. It should\nalso include the deﬁnition of access protocols and the capability to process data in a way that ensures\nprivacy. The EC guide breaks down the privacy principle into three aspects:\n• Privacy and data protection: they should be guaranteed in AI systems throughout its entire lifecycle. It\nincludes both information provided by users and information generated about those users derived from\ntheir interactions with the system. Since digital information about a user could be used in a negative\nway against them (discrimination due to sensitive features, unfair treatment...), it is crucial to ensure\nproper usage of all the data collected.\n• Quality and integrity of data: quality of data sets is fundamental to reach good performance with AI\nsystems that are fueled with data, like ML. However, sometimes the data collected contains socially\nconstructed biases, inaccuracies, errors and mistakes. This should be tackled before training any model\nwith the data collected. Additionally, the integrity of the data sets should be ensured.\n• Access to data: if there is individual personal data, there should always be data protocols for data\ngovernance. These protocols should indicate who may access data and under which circumstances.\nThe aforementioned examples from the EC shows how data fusion is directly intertwined with privacy\nand with fairness, regardless of the technique employed for it.\nNotwithstanding this explicit concern from regulatory bodies, loss of privacy has been compromised\nby DL methods in scenarios where no data fusion is performed. For instance, a few images are enough\nto threaten users’ privacy even in the presence of image obfuscation [420], and the model parameters of\na DNN can be exposed by simply performing input queries on the model [356, 357]. An approach to\nexplain loss of privacy is by using privacy loss and intent loss subjective scores. The former provides a\nsubjective measure of the severity of the privacy violation depending on the role of a face in the image,\nwhile the latter captures the intent of the bystanders to appear in the picture. These kind of explanations\nhave motivated, for instance, secure matching cryptographic protocols for photographer and bystanders to\npreserve privacy [356, 421, 422]. We deﬁnite advocate for more efforts invested in this direction, namely,\nin ensuring that XAI methods do not pose a threat in regards to the privacy of the data used for training\nthe ML model under target.\nWhen data fusion enters the picture, different implications arise with the context of explainability\ncovered in this survey. To begin with, classical techniques for fusion at the data level only deal with data\nand have no connection to the ML model, so they have little to do with explainability. However, the\nadvent of DL models has blurred the distinction between information fusion and predictive modeling. The\nﬁrst layers of DL architectures are in charge of learning high-level features from raw data that possess\nrelevance for the task at hand. This learning process can be thought to aim at solving a data level fusion\nproblem, yet in a directed learning fashion that makes the fusion process tightly coupled to the task to be\nsolved.\nIn this context, many techniques in the ﬁeld of XAI have been proposed to deal with the analysis of\ncorrelation between features. This paves the way to explaining how data sources are actually fused through\nthe DL model, which can yield interesting insights on how the predictive task at hand induces correlations\namong the data sources over the spatial and/or time domain. Ultimately, this gained information on the\nfusion could not only improve the usability of the model as a result of its enhanced understanding by the\nuser, but could also help identifying other data sources of potential interest that could be incorporated to\nthe model, or even contribute to a more efﬁcient data fusion in other contexts.\nUnfortunately, this previously mentioned concept of fusion at data level contemplates data under\ncertain constraints of known form and source origin. As presented in [423], the Big Data era presents\nan environment in which these premises cannot be taken for granted, and methods to board Big Data\nfusion (as that illustrated in Figure 13.d) have to be thought. Conversely, a concern with model fusion\n43\n\ncontext emerges in the possibility that XAI techniques could be explanatory enough to compromise the\nconﬁdentiality of private data. This could eventually occur if sensitive information (e.g. ownership) could\nbe inferred from the explained fusion among protected and unprotected features.\nWhen turning our prospects to data fusion at model level, we have already argued that the fusion of the\noutputs of several transparent models (as in tree ensembles) could make the overall model opaque, thereby\nmaking it necessary to resort to post-hoc explainability solutions. However, model fusion may entail other\ndrawbacks when endowed with powerful post-hoc XAI techniques. Let us imagine that relationships of\na model’s input features have been discovered by means of a post-hoc technique) and that one of those\nfeatures is hidden or unknown. Will it be possible to infer another model’s features if that previous feature\nwas known to be used in that model? Would this possibility uncover a problem as privacy breaches in\ncases in which related protected input variables are not even shared in the ﬁrst place?\nTo get the example clearer, in [424] a multiview perspective is utilized in which different single views\n(representing the sources they attend to) models are fused. These models contain among others, cell-phone\ndata, transportation data, etc. which might introduce the problem that information that is not even shared\ncan be discovered through other sources that are actually shared. In the example above, what if instead of\nfeatures, a model shares with another a layer or part of its architecture as in Federated Learning? Would\nthis sharing make possible to infer information from that exchanged part of its model, to the extent of\nallowing for the design of adversarial attacks with better success rate upon the antecedent model?\nIf focused at knowledge level fusion, a similar reasoning holds: XAI comprises techniques that extract\nknowledge from ML model(s). This ability to explain models could have an impact on the necessity of\ndiscovering new knowledge through the complex interactions formed within ML models. If so, XAI might\nenrich knowledge fusion paradigms, bringing the possibility of discovering new knowledge extractors\nof relevance for the task at hand. For this purpose, it is of paramount importance that the knowledge\nextracted from a model by means of XAI techniques can be understood and extrapolated to the domain\nin which knowledge extractors operate. The concept matches with ease with that of transfer learning\nportrayed in [425]. Although XAI is not contemplated in the surveyed processes of extracting knowledge\nfrom models trained in certain feature spaces and distributions, to then be utilized in environments where\nprevious conditions do not hold, when deployed, XAI can pose a threat if the explanations given about the\nmodel can be reversely engineered through the knowledge fusion paradigm to eventually compromise, for\ninstance, the differential privacy of the overall model.\nThe distinction between centralized and distributed data fusion also spurs further challenges in\nregards to privacy and explainability. The centralized approach does not bring any further concerns\nthat those presented above. However, distributed fusion does arise new problems. Distributed fusion\nmight be applied for different reasons, mainly due to environmental constraints or due to security or\nprivacy issues. The latter context may indulge some dangers. Among other goals (e.g. computational\nefﬁciency), model-level data fusion is performed in a distributed fashion to ensure that no actual data is\nactually shared, but rather parts of an ML model trained on local data. This rationale lies at the heart\nof Federated Learning, where models exchange locally learned information among nodes. Since data\ndo not leave the local device, only the transmission of model updates is required across distributed\ndevices. This lightens the training process for network-compromised settings and guarantees data privacy\n[416]. Upon the use of post-hoc explainability techniques, a node could disguise sensitive information\nabout the local context in which the received ML model part was trained. In fact, it was shown that a\nblack-box model based on a DNN from which an input/output query interface is given can be used to\naccurately predict every single hyperparameter value used for training, allowing for potential privacyrelated consequences [357, 420, 421]. This relates to studies showing that blurring images does not\nguarantee privacy preservation.\nData fusion, privacy and model explainability are concepts that have not been analysed together so far.\nFrom the above discussion it is clear that there are unsolved concerns and caveats that demand further\nstudy by the community in forthcoming times.\n44\n\n6.4. Implementing Responsible AI Principles in an Organization\nWhile increasingly more organizations are publishing AI principles to declare that they care about\navoiding unintended negative consequences, there is much less experience on how to actually implement\nthe principles into an organization. Looking at several examples of principles declared by different\norganizations [385], we can divide them into two groups:\n• AI-speciﬁc principles that focus on aspects that are speciﬁc to AI, such as explainability, fairness and\nhuman agency.\n• End-to-end principles that cover all aspects involved in AI, including also privacy, security and safety.\nThe EC Guidelines for Trustworthy AI are an example of end-to-end principles [390], while those of\nTelefonica (a large Spanish ICT company operating worldwide) are more AI-speciﬁc [386]. For example,\nsafety and security are relevant for any connected IT system, and therefore also for AI systems. The\nsame holds for privacy, but it is probably true that privacy in the context of AI systems is even more\nimportant than for general IT systems, due to the fact that ML models need huge amounts of data and\nmost importantly, because XAI tools and data fusion techniques pose new challenges to preserve the\nprivacy of protected records.\nWhen it comes to implement the AI Principles into an organization, it is important to operationalize\nthe AI-speciﬁc parts and, at the same time, leverage the processes already existing for the more generic\nprinciples. Indeed, in many organizations there already exist norms and procedures for privacy, security\nand safety. Implementing AI principles requires a methodology such as that presented in [386] that breaks\ndown the process into different parts. The ingredients of such a methodology should include, at least:\n• AI principles (already discussed earlier), which set the values and boundaries.\n• Awareness and training about the potential issues, both technical and non-technical.\n• A questionnaire that forces people to think about certain impacts of the AI system (impact explanation).\nThis questionnaire should give concrete guidance on what to do if certain undesired impacts are\ndetected.\n• Tools that help answering some of the questions, and help mitigating any problems identiﬁed. XAI\ntools and fairness tools fall in this category, as well as other recent proposals such as model cards [426].\n• A governance model assigning responsibilities and accountabilities (responsibility explanation). There\nare two philosophies for governance: 1) based on committees that review and approve AI developments,\nand 2) based on the self-responsibility of the employees. While both are possible, given the fact\nthat agility is key for being successful in the digital world, it seems wiser to focus on awareness and\nemployee responsibility, and only use committees when there are speciﬁc, but important issues.\nFrom the above elaborations, it is clear that the implementation of Responsible AI principles in\ncompanies should balance between two requirements: 1) major cultural and organizational changes\nneeded to enforce such principles over processes endowed with AI functionalities; and 2) the feasibility\nand compliance of the implementation of such principles with the IT assets, policies and resources already\navailable at the company. It is in the gradual process of rising corporate awareness around the principles\nand values of Responsible AI where we envision that XAI will make its place and create huge impact.\n7. Conclusions and Outlook\nThis overview has revolved around eXplainable Artiﬁcial Intelligence (XAI), which has been identiﬁed\nin recent times as an utmost need for the adoption of ML methods in real-life applications. Our study\n45\n\nhas elaborated on this topic by ﬁrst clarifying different concepts underlying model explainability, as well\nas by showing the diverse purposes that motivate the search for more interpretable ML methods. These\nconceptual remarks have served as a solid baseline for a systematic review of recent literature dealing with\nexplainability, which has been approached from two different perspectives: 1) ML models that feature\nsome degree of transparency, thereby interpretable to an extent by themselves; and 2) post-hoc XAI\ntechniques devised to make ML models more interpretable. This literature analysis has yielded a global\ntaxonomy of different proposals reported by the community, classifying them under uniform criteria.\nGiven the prevalence of contributions dealing with the explainability of Deep Learning models, we have\ninspected in depth the literature dealing with this family of models, giving rise to an alternative taxonomy\nthat connects more closely with the speciﬁc domains in which explainability can be realized for Deep\nLearning models.\nWe have moved our discussions beyond what has been made so far in the XAI realm toward the concept\nof Responsible AI, a paradigm that imposes a series of AI principles to be met when implementing AI\nmodels in practice, including fairness, transparency, and privacy. We have also discussed the implications\nof adopting XAI techniques in the context of data fusion, unveiling the potential of XAI to compromise\nthe privacy of protected data involved in the fusion process. Implications of XAI in fairness have also\nbeen discussed in detail. This vision of XAI as a core concept to ensure the aforementioned principles for\nResponsible AI is summarized graphically in Figure 14.\nXAI\nInterpretability\nversus\nPerformance\nXAI\nConcepts\nand Metrics\nAchieving\nExplainability\nin Deep\nLearning\nXAI &\nSecurity:\nAdversarial\nML\nRationale\nExplanation\n& Critical\nData\nStudies\nTheory\nguided Data\nScience\nImplementation\n& Guidelines\nXAI and\nOutput\nConfidence\nXAI & Data\nFusion\nFairness\nPrivacy\nAccountability\nEthics\nTransparency\nSecurity &\nSafety\nResponsible\nAI\nFigure 14: Summary of XAI challenges discussed in this overview and its impact on the principles for Responsible AI.\nOur reﬂections about the future of XAI, conveyed in the discussions held throughout this work,\nagree on the compelling need for a proper understanding of the potentiality and caveats opened up by\nXAI techniques. It is our vision that model interpretability must be addressed jointly with requirements\nand constraints related to data privacy, model conﬁdentiality, fairness and accountability. A responsible\nimplementation and use of AI methods in organizations and institutions worldwide will be only guaranteed\nif all these AI principles are studied jointly.\nAcknowledgments\nAlejandro Barredo-Arrieta, Javier Del Ser and Sergio Gil-Lopez would like to thank the Basque\nGovernment for the funding support received through the EMAITEK and ELKARTEK programs. Javier\nDel Ser also acknowledges funding support from the Consolidated Research Group MATHMODE\n(IT1294-19) granted by the Department of Education of the Basque Government. Siham Tabik, Salvador\nGarcia, Daniel Molina and Francisco Herrera would like to thank the Spanish Government for its funding\nsupport (SMART-DaSCI project, TIN2017-89517-P), as well as the BBVA Foundation through its Ayudas\n46\n\nFundaci´on BBVA a Equipos de Investigaci´on Cient´ıﬁca 2018 call (DeepSCOP project). This work was\nalso funded in part by the European Union’s Horizon 2020 research and innovation programme AI4EU\nunder grant agreement 825619. We also thank Chris Olah, Alexander Mordvintsev and Ludwig Schubert\nfor borrowing images for illustration purposes. Part of this overview is inspired by a preliminary work of\nthe concept of Responsible AI: R. Benjamins, A. Barbado, D. Sierra, “Responsible AI by Design”, to\nappear in the Proceedings of the Human-Centered AI: Trustworthiness of AI Models & Data (HAI) track\nat AAAI Fall Symposium, DC, November 7-9, 2019 [386].\nReferences\n[1] S. J. Russell, P. Norvig, Artiﬁcial intelligence: a modern approach, Malaysia; Pearson Education\nLimited,, 2016.\n[2] D. M. West, The future of work: robots, AI, and automation, Brookings Institution Press, 2018.\n[3] B. Goodman, S. Flaxman, European union regulations on algorithmic decision-making and a “right\nto explanation”, AI Magazine 38 (3) (2017) 50–57.\n[4] D. Castelvecchi, Can we open the black box of AI?, Nature News 538 (7623) (2016) 20.\n[5] Z. C. Lipton, The mythos of model interpretability, Queue 16 (3) (2018) 30:31–30:57.\n[6] A. Preece, D. Harborne, D. Braines, R. Tomsett, S. Chakraborty, Stakeholders in Explainable AI\n(2018). arXiv:1810.00184.\n[7] D. Gunning, Explainable artiﬁcial intelligence (xAI), Tech. rep., Defense Advanced Research\nProjects Agency (DARPA) (2017).\n[8] E. Tjoa, C. Guan, A survey on explainable artiﬁcial intelligence (XAI): Towards medical XAI\n(2019). arXiv:1907.07374.\n[9] J. Zhu, A. Liapis, S. Risi, R. Bidarra, G. M. Youngblood, Explainable AI for designers: A humancentered perspective on mixed-initiative co-creation, 2018 IEEE Conference on Computational\nIntelligence and Games (CIG) (2018) 1–8.\n[10] F. K. Do˜silovi´c, M. Br˜ci´c, N. Hlupi´c, Explainable artiﬁcial intelligence: A survey, in: 41st\nInternational Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), 2018, pp. 210–215.\n[11] P. Hall, On the Art and Science of Machine Learning Explanations (2018). arXiv:1810.02909.\n[12] T. Miller, Explanation in artiﬁcial intelligence: Insights from the social sciences, Artif. Intell. 267\n(2019) 1–38.\n[13] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, L. Kagal, Explaining Explanations: An\nOverview of Interpretability of Machine Learning (2018). arXiv:1806.00069.\n[14] A. Adadi, M. Berrada, Peeking inside the black-box: A survey on explainable artiﬁcial intelligence\n(XAI), IEEE Access 6 (2018) 52138–52160.\n[15] O. Biran, C. Cotton, Explanation and justiﬁcation in machine learning: A survey, in: IJCAI-17\nworkshop on explainable AI (XAI), Vol. 8, 2017, p. 1.\n47\n\n[16] S. T. Shane T. Mueller, R. R. Hoffman, W. Clancey, G. Klein, Explanation in Human-AI Systems: A\nLiterature Meta-Review Synopsis of Key Ideas and Publications and Bibliography for Explainable\nAI, Tech. rep., Defense Advanced Research Projects Agency (DARPA) XAI Program (2019).\n[17] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, D. Pedreschi, A survey of methods\nfor explaining black box models, ACM Computing Surveys 51 (5) (2018) 93:1–93:42.\n[18] G. Montavon, W. Samek, K.-R. M¨uller, Methods for interpreting and understanding deep neural\nnetworks, Digital Signal Processing 73 (2018) 1–15. doi:10.1016/j.dsp.2017.10.011.\n[19] A. Fernandez, F. Herrera, O. Cordon, M. Jose del Jesus, F. Marcelloni, Evolutionary fuzzy systems\nfor explainable artiﬁcial intelligence: Why, when, what for, and where to?, IEEE Computational\nIntelligence Magazine 14 (1) (2019) 69–81.\n[20] M. Gleicher, A framework for considering comprehensibility in modeling, Big data 4 (2) (2016)\n75–88.\n[21] M. W. Craven, Extracting comprehensible models from trained neural networks, Tech. rep., University of Wisconsin-Madison Department of Computer Sciences (1996).\n[22] R. S. Michalski, A theory and methodology of inductive learning, in: Machine learning, Springer,\n1983, pp. 83–134.\n[23] J. D´ıez, K. Khalifa, B. Leuridan, General theories of explanation: buyer beware, Synthese 190 (3)\n(2013) 379–396.\n[24] D. Doran, S. Schulz, T. R. Besold, What does explainable AI really mean? a new conceptualization\nof perspectives (2017). arXiv:1710.00794.\n[25] F. Doshi-Velez, B. Kim, Towards a rigorous science of interpretable machine learning (2017).\narXiv:1702.08608.\n[26] A. Vellido, J. D. Mart´ın-Guerrero, P. J. Lisboa, Making machine learning models interpretable., in:\nEuropean Symposium on Artiﬁcial Neural Networks, Computational Intelligence and Machine\nLearning (ESANN), Vol. 12, Citeseer, 2012, pp. 163–172.\n[27] E. Walter, Cambridge advanced learner’s dictionary, Cambridge University Press, 2008.\n[28] P. Besnard, A. Hunter, Elements of Argumentation, The MIT Press, 2008.\n[29] F. Rossi, AI Ethics for Enterprise AI (2019).\nURL\nhttps://economics.harvard.edu/files/economics/files/rossifrancesca_4-22-19_ai-ethics-for-enterprise-ai_ec3118-hbs.pdf\n[30] A. Holzinger, C. Biemann, C. S. Pattichis, D. B. Kell, What do we need to build explainable Ai\nsystems for the medical domain? (2017). arXiv:1712.09923.\n[31] B. Kim, E. Glassman, B. Johnson, J. Shah, iBCM: Interactive bayesian case model empowering\nhumans via intuitive interaction, Tech. rep., MIT-CSAIL-TR-2015-010 (2015).\n[32] M. T. Ribeiro, S. Singh, C. Guestrin, Why should I trust you?: Explaining the predictions of any\nclassiﬁer, in: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,\nACM, 2016, pp. 1135–1144.\n[33] M. Fox, D. Long, D. Magazzeni, Explainable planning (2017). arXiv:1709.10256.\n48\n\n[34] H. C. Lane, M. G. Core, M. Van Lent, S. Solomon, D. Gomboc, Explainable artiﬁcial intelligence\nfor training and tutoring, Tech. rep., University of Southern California (2005).\n[35] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, B. Yu, Interpretable machine learning:\ndeﬁnitions, methods, and applications (2019). arXiv:1901.04592.\n[36] J. Haspiel, N. Du, J. Meyerson, L. P. Robert Jr, D. Tilbury, X. J. Yang, A. K. Pradhan, Explanations and expectations: Trust building in automated vehicles, in: Companion of the ACM/IEEE\nInternational Conference on Human-Robot Interaction, ACM, 2018, pp. 119–120.\n[37] A. Chander, R. Srinivasan, S. Chelian, J. Wang, K. Uchino, Working with beliefs: AI transparency\nin the enterprise., in: Workshops of the ACM Conference on Intelligent User Interfaces, 2018.\n[38] A. B. Tickle, R. Andrews, M. Golea, J. Diederich, The truth will come to light: Directions and\nchallenges in extracting the knowledge embedded within trained artiﬁcial neural networks, IEEE\nTransactions on Neural Networks 9 (6) (1998) 1057–1068.\n[39] C. Louizos, U. Shalit, J. M. Mooij, D. Sontag, R. Zemel, M. Welling, Causal effect inference with\ndeep latent-variable models, in: Advances in Neural Information Processing Systems, 2017, pp.\n6446–6456.\n[40] O. Goudet, D. Kalainathan, P. Caillou, I. Guyon, D. Lopez-Paz, M. Sebag, Learning functional\ncausal models with generative neural networks, in: Explainable and Interpretable Models in\nComputer Vision and Machine Learning, Springer, 2018, pp. 39–80.\n[41] S. Athey, G. W. Imbens, Machine learning methods for estimating heterogeneous causal effects,\nstat 1050 (5) (2015).\n[42] D. Lopez-Paz, R. Nishihara, S. Chintala, B. Scholkopf, L. Bottou, Discovering causal signals in\nimages, in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n2017, pp. 6979–6987.\n[43] C. Barabas, K. Dinakar, J. Ito, M. Virza, J. Zittrain, Interventions over predictions: Reframing the\nethical debate for actuarial risk assessment (2017). arXiv:1712.08238.\n[44] R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, N. Elhadad, Intelligible models for healthcare:\nPredicting pneumonia risk and hospital 30-day readmission, in: Proceedings of the 21th ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, KDD ’15, 2015,\npp. 1721–1730.\n[45] A. Theodorou, R. H. Wortham, J. J. Bryson, Designing and implementing transparency for real\ntime inspection of autonomous robots, Connection Science 29 (3) (2017) 230–241.\n[46] W. Samek, T. Wiegand, K.-R. M¨uller, Explainable artiﬁcial intelligence: Understanding, visualizing\nand interpreting deep learning models (2017). arXiv:1708.08296.\n[47] C. Wadsworth, F. Vera, C. Piech, Achieving fairness through adversarial learning: an application to\nrecidivism prediction (2018). arXiv:1807.00199.\n[48] X. Yuan, P. He, Q. Zhu, X. Li, Adversarial examples: Attacks and defenses for deep learning, IEEE\nTransactions on Neural Networks and Learning Systems 30 (9) (2019) 2805–2824.\n[49] B. Letham, C. Rudin, T. H. McCormick, D. Madigan, et al., Interpretable classiﬁers using rules\nand bayesian analysis: Building a better stroke prediction model, The Annals of Applied Statistics\n9 (3) (2015) 1350–1371.\n49\n\n[50] M. Harbers, K. van den Bosch, J.-J. Meyer, Design and evaluation of explainable BDI agents, in:\nIEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,\nVol. 2, IEEE, 2010, pp. 125–132.\n[51] M. H. Aung, P. G. Lisboa, T. A. Etchells, A. C. Testa, B. Van Calster, S. Van Huffel, L. Valentin,\nD. Timmerman, Comparing analytical decision support models through boolean rule extraction:\nA case study of ovarian tumour malignancy, in: International Symposium on Neural Networks,\nSpringer, 2007, pp. 1177–1186.\n[52] A. Weller, Challenges for transparency (2017). arXiv:1708.01870.\n[53] A. A. Freitas, Comprehensible classiﬁcation models: a position paper, ACM SIGKDD explorations\nnewsletter 15 (1) (2014) 1–10.\n[54] V. Schetinin, J. E. Fieldsend, D. Partridge, T. J. Coats, W. J. Krzanowski, R. M. Everson, T. C.\nBailey, A. Hernandez, Conﬁdent interpretation of bayesian decision tree ensembles for clinical\napplications, IEEE Transactions on Information Technology in Biomedicine 11 (3) (2007) 312–319.\n[55] D. Martens, J. Vanthienen, W. Verbeke, B. Baesens, Performance of classiﬁcation models from a\nuser perspective, Decision Support Systems 51 (4) (2011) 782–793.\n[56] Z. Che, S. Purushotham, R. Khemani, Y. Liu, Interpretable deep models for ICU outcome prediction,\nin: AMIA Annual Symposium Proceedings, Vol. 2016, American Medical Informatics Association,\n2016, p. 371.\n[57] N. Barakat, J. Diederich, Eclectic rule-extraction from support vector machines, International\nJournal of Computer, Electrical, Automation, Control and Information Engineering 2 (5) (2008)\n1672–1675.\n[58] F. J. C. Garcia, D. A. Robb, X. Liu, A. Laskov, P. Patron, H. Hastie, Explain yourself: A natural\nlanguage interface for scrutable autonomous robots (2018). arXiv:1803.02088.\n[59] P. Langley, B. Meadows, M. Sridharan, D. Choi, Explainable agency for intelligent autonomous\nsystems, in: AAAI Conference on Artiﬁcial Intelligence, 2017, pp. 4762–4763.\n[60] G. Montavon, S. Lapuschkin, A. Binder, W. Samek, K.-R. M¨uller, Explaining nonlinear classiﬁcation decisions with deep taylor decomposition, Pattern Recognition 65 (2017) 211–222.\n[61] P.-J. Kindermans, K. T. Sch¨utt, M. Alber, K.-R. M¨uller, D. Erhan, B. Kim, S. D¨ahne, Learning how\nto explain neural networks: Patternnet and patternattribution (2017). arXiv:1705.05598.\n[62] G. Ras, M. van Gerven, P. Haselager, Explanation methods in deep learning: Users, values,\nconcerns and challenges, in: Explainable and Interpretable Models in Computer Vision and\nMachine Learning, Springer, 2018, pp. 19–36.\n[63] S. Bach, A. Binder, K.-R. M¨uller, W. Samek, Controlling explanatory heatmap resolution and\nsemantics via decomposition depth, in: IEEE International Conference on Image Processing (ICIP),\nIEEE, 2016, pp. 2271–2275.\n[64] G. J. Katuwal, R. Chen, Machine learning model interpretability for precision medicine (2016).\narXiv:1610.09045.\n[65] M. A. Neerincx, J. van der Waa, F. Kaptein, J. van Diggelen, Using perceptual and cognitive explanations for enhanced human-agent team performance, in: International Conference on Engineering\nPsychology and Cognitive Ergonomics, Springer, 2018, pp. 204–214.\n50\n\n[66] J. D. Olden, D. A. Jackson, Illuminating the “black box”: a randomization approach for understanding variable contributions in artiﬁcial neural networks, Ecological modelling 154 (1-2) (2002)\n135–150.\n[67] J. Krause, A. Perer, K. Ng, Interacting with predictions: Visual inspection of black-box machine\nlearning models, in: CHI Conference on Human Factors in Computing Systems, ACM, 2016, pp.\n5686–5697.\n[68] L. Rosenbaum, G. Hinselmann, A. Jahn, A. Zell, Interpreting linear support vector machine models\nwith heat map molecule coloring, Journal of Cheminformatics 3 (1) (2011) 11.\n[69] J. Tan, M. Ung, C. Cheng, C. S. Greene, Unsupervised feature construction and knowledge\nextraction from genome-wide assays of breast cancer with denoising autoencoders, in: Paciﬁc\nSymposium on Biocomputing Co-Chairs, World Scientiﬁc, 2014, pp. 132–143.\n[70] S. Krening, B. Harrison, K. M. Feigh, C. L. Isabell, M. Riedl, A. Thomaz, Learning from explanations using sentiment and advice in RL, IEEE Transactions on Cognitive and Developmental\nSystems 9 (1) (2017) 44–55.\n[71] M. T. Ribeiro, S. Singh, C. Guestrin, Model-agnostic interpretability of machine learning (2016).\narXiv:1606.05386.\n[72] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M¨uller, W. Samek, On pixel-wise explanations for non-linear classiﬁer decisions by layer-wise relevance propagation, PloS one 10 (7)\n(2015) e0130140.\n[73] T. A. Etchells, P. J. Lisboa, Orthogonal search-based rule extraction (OSRE) for trained neural\nnetworks: a practical and efﬁcient approach, IEEE Transactions on Neural Networks 17 (2) (2006)\n374–384.\n[74] Y. Zhang, S. Sreedharan, A. Kulkarni, T. Chakraborti, H. H. Zhuo, S. Kambhampati, Plan explicability and predictability for robot task planning, in: 2017 IEEE International Conference on\nRobotics and Automation (ICRA), IEEE, 2017, pp. 1313–1320.\n[75] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia, T. Lillicrap,\nA simple neural network module for relational reasoning, in: Advances in Neural Information\nProcessing Systems, 2017, pp. 4967–4976.\n[76] C.-Y. J. Peng, T.-S. H. So, F. K. Stage, E. P. S. John, The use and interpretation of logistic regression\nin higher education journals: 1988–1999, Research in Higher Education 43 (3) (2002) 259–293.\n[77] B. ¨Ust¨un, W. Melssen, L. Buydens, Visualisation and interpretation of support vector regression\nmodels, Analytica Chimica Acta 595 (1-2) (2007) 299–309.\n[78] Q. Zhang, Y. Yang, H. Ma, Y. N. Wu, Interpreting CNNs via decision trees, in: IEEE Conference\non Computer Vision and Pattern Recognition, 2019, pp. 6261–6270.\n[79] M. Wu, M. C. Hughes, S. Parbhoo, M. Zazzi, V. Roth, F. Doshi-Velez, Beyond sparsity: Tree\nregularization of deep models for interpretability, in: AAAI Conference on Artiﬁcial Intelligence,\n2018, pp. 1670–1678.\n[80] G. Hinton, O. Vinyals, J. Dean, Distilling the knowledge in a neural network (2015).\narXiv:1503.02531.\n[81] N. Frosst, G. Hinton, Distilling a neural network into a soft decision tree (2017). arXiv:1711.09784.\n51\n\n[82] M. G. Augasta, T. Kathirvalavakumar, Reverse engineering the neural networks for rule extraction\nin classiﬁcation problems, Neural Processing Letters 35 (2) (2012) 131–150.\n[83] Z.-H. Zhou, Y. Jiang, S.-F. Chen, Extracting symbolic rules from trained neural network ensembles,\nAI Communications 16 (1) (2003) 3–15.\n[84] H. F. Tan, G. Hooker, M. T. Wells, Tree space prototypes: Another look at making tree ensembles\ninterpretable (2016). arXiv:1611.07115.\n[85] R. C. Fong, A. Vedaldi, Interpretable explanations of black boxes by meaningful perturbation, in:\nIEEE International Conference on Computer Vision, 2017, pp. 3429–3437.\n[86] T. Miller, P. Howe, L. Sonenberg, Explainable AI: Beware of inmates running the asylum, in:\nInternational Joint Conference on Artiﬁcial Intelligence, Workshop on Explainable AI (XAI),\nVol. 36, 2017, pp. 36–40.\n[87] R. Goebel, A. Chander, K. Holzinger, F. Lecue, Z. Akata, S. Stumpf, P. Kieseberg, A. Holzinger,\nExplainable AI: the new 42?, in: International Cross-Domain Conference for Machine Learning\nand Knowledge Extraction, Springer, 2018, pp. 295–303.\n[88] V. Belle, Logic meets probability: Towards explainable AI systems for uncertain worlds, in:\nInternational Joint Conference on Artiﬁcial Intelligence, 2017, pp. 5116–5120.\n[89] L. Edwards, M. Veale, Slave to the algorithm: Why a right to an explanation is probably not the\nremedy you are looking for, Duke L. & Tech. Rev. 16 (2017) 18.\n[90] Y. Lou, R. Caruana, J. Gehrke, G. Hooker, Accurate intelligible models with pairwise interactions,\nin: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM,\n2013, pp. 623–631.\n[91] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, Y. Bengio, Show, attend\nand tell: Neural image caption generation with visual attention, in: International Conference on\nMachine Learning, 2015, pp. 2048–2057.\n[92] J. Huysmans, K. Dejaeger, C. Mues, J. Vanthienen, B. Baesens, An empirical evaluation of the\ncomprehensibility of decision table, tree and rule based predictive models, Decision Support\nSystems 51 (1) (2011) 141–154.\n[93] N. H. Barakat, A. P. Bradley, Rule extraction from support vector machines: A sequential covering\napproach, IEEE Transactions on Knowledge and Data Engineering 19 (6) (2007) 729–741.\n[94] F. C. Adriana da Costa, M. M. B. Vellasco, R. Tanscheit, Fuzzy rule extraction from support vector\nmachines, in: International Conference on Hybrid Intelligent Systems, IEEE, 2005, pp. 335–340.\n[95] D. Martens, B. Baesens, T. Van Gestel, J. Vanthienen, Comprehensible credit scoring models using\nrule extraction from support vector machines, European Journal of Operational Research 183 (3)\n(2007) 1466–1476.\n[96] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Learning deep features for discriminative\nlocalization, in: IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2921–\n2929.\n[97] R. Krishnan, G. Sivakumar, P. Bhattacharya, Extracting decision trees from trained neural networks,\nPattern Recognition 32 (12) (1999) 1999–2009.\n52\n\n[98] X. Fu, C. Ong, S. Keerthi, G. G. Hung, L. Goh, Extracting the knowledge embedded in support\nvector machines, in: IEEE International Joint Conference on Neural Networks, Vol. 1, IEEE, 2004,\npp. 291–296.\n[99] B. Green, “Fair” risk assessments: A precarious approach for criminal justice reform, in: 5th\nWorkshop on Fairness, Accountability, and Transparency in Machine Learning, 2018.\n[100] A. Chouldechova, Fair prediction with disparate impact: A study of bias in recidivism prediction\ninstruments, Big Data 5 (2) (2017) 153–163.\n[101] M. Kim, O. Reingold, G. Rothblum, Fairness through computationally-bounded awareness, in:\nAdvances in Neural Information Processing Systems, 2018, pp. 4842–4852.\n[102] B. Haasdonk, Feature space interpretation of SVMs with indeﬁnite kernels, IEEE Transactions on\nPattern Analysis and Machine Intelligence 27 (4) (2005) 482–492.\n[103] A. Palczewska, J. Palczewski, R. M. Robinson, D. Neagu, Interpreting random forest classiﬁcation\nmodels using a feature contribution method, in: Integration of Reusable Systems, Springer, 2014,\npp. 193–218.\n[104] S. H. Welling, H. H. Refsgaard, P. B. Brockhoff, L. H. Clemmensen, Forest ﬂoor visualizations of\nrandom forests (2016). arXiv:1605.09196.\n[105] G. Fung, S. Sandilya, R. B. Rao, Rule extraction from linear support vector machines, in: ACM\nSIGKDD International Conference on Knowledge Discovery in Data Mining, ACM, 2005, pp.\n32–40.\n[106] Y. Zhang, H. Su, T. Jia, J. Chu, Rule extraction from trained support vector machines, in: PaciﬁcAsia Conference on Knowledge Discovery and Data Mining, Springer, 2005, pp. 61–70.\n[107] D. Linsley, D. Shiebler, S. Eberhardt, T. Serre, Global-and-local attention networks for visual\nrecognition (2018). arXiv:1805.08819.\n[108] S.-M. Zhou, J. Q. Gan, Low-level interpretability and high-level interpretability: a uniﬁed view\nof data-driven interpretable fuzzy system modelling, Fuzzy Sets and Systems 159 (23) (2008)\n3091–3131.\n[109] J. Burrell, How the machine ‘thinks’: Understanding opacity in machine learning algorithms, Big\nData & Society 3 (1) (2016) 1–12.\n[110] A. Shrikumar, P. Greenside, A. Shcherbina, A. Kundaje, Not just a black box: Learning important\nfeatures through propagating activation differences (2016). arXiv:1605.01713.\n[111] Y. Dong, H. Su, J. Zhu, B. Zhang, Improving interpretability of deep neural networks with\nsemantic information, in: IEEE Conference on Computer Vision and Pattern Recognition, 2017,\npp. 4306–4314.\n[112] G. Ridgeway, D. Madigan, T. Richardson, J. O’Kane, Interpretable boosted na¨ıve bayes classiﬁcation., in: ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 1998, pp.\n101–104.\n[113] Q. Zhang, Y. Nian Wu, S.-C. Zhu, Interpretable convolutional neural networks, in: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 8827–8836.\n53\n\n[114] S. Seo, J. Huang, H. Yang, Y. Liu, Interpretable convolutional neural networks with dual local and\nglobal attention for review rating prediction, in: Proceedings of the Eleventh ACM Conference on\nRecommender Systems, ACM, 2017, pp. 297–305.\n[115] K. Larsen, J. H. Petersen, E. Budtz-Jørgensen, L. Endahl, Interpreting parameters in the logistic\nregression model with random effects, Biometrics 56 (3) (2000) 909–914.\n[116] B. Gaonkar, R. T. Shinohara, C. Davatzikos, A. D. N. Initiative, et al., Interpreting support vector\nmachine models for multivariate group wise analysis in neuroimaging, Medical image analysis\n24 (1) (2015) 190–204.\n[117] K. Xu, D. H. Park, C. Yi, C. Sutton, Interpreting deep classiﬁer by visual distillation of dark\nknowledge (2018). arXiv:1803.04042.\n[118] H. Deng, Interpreting tree ensembles with intrees (2014). arXiv:1408.5456.\n[119] P. Domingos, Knowledge discovery via multiple models, Intelligent Data Analysis 2 (1-4) (1998)\n187–202.\n[120] S. Tan, R. Caruana, G. Hooker, Y. Lou, Distill-and-compare: Auditing black-box models using\ntransparent model distillation, in: AAAI/ACM Conference on AI, Ethics, and Society, ACM, 2018,\npp. 303–310.\n[121] R. A. Berk, J. Bleich, Statistical procedures for forecasting criminal behavior: A comparative\nassessment, Criminology & Public Policy 12 (3) (2013) 513–544.\n[122] S. Hara, K. Hayashi, Making tree ensembles interpretable (2016). arXiv:1606.05390.\n[123] A. Henelius, K. Puolam¨aki, A. Ukkonen, Interpreting classiﬁers through attribute interactions in\ndatasets (2017). arXiv:1707.07576.\n[124] H. Hastie, F. J. C. Garcia, D. A. Robb, P. Patron, A. Laskov, MIRIAM: a multimodal chat-based\ninterface for autonomous systems, in: ACM International Conference on Multimodal Interaction,\nACM, 2017, pp. 495–496.\n[125] D. Bau, B. Zhou, A. Khosla, A. Oliva, A. Torralba, Network dissection: Quantifying interpretability\nof deep visual representations, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017, pp. 6541–6549.\n[126] H. N´u˜nez, C. Angulo, A. Catal`a, Rule extraction from support vector machines., in: European\nSymposium on Artiﬁcial Neural Networks, Computational Intelligence and Machine Learning\n(ESANN), 2002, pp. 107–112.\n[127] H. N´u˜nez, C. Angulo, A. Catal`a, Rule-based learning systems for support vector machines, Neural\nProcessing Letters 24 (1) (2006) 1–18.\n[128] M. Kearns, S. Neel, A. Roth, Z. S. Wu, Preventing fairness gerrymandering: Auditing and learning\nfor subgroup fairness (2017). arXiv:1711.05144.\n[129] E. Akyol, C. Langbort, T. Basar, Price of transparency in strategic machine learning (2016).\narXiv:1610.08210.\n[130] D. Erhan, A. Courville, Y. Bengio, Understanding representations learned in deep architectures,\nDepartment dInformatique et Recherche Operationnelle, University of Montreal, QC, Canada,\nTech. Rep 1355 (2010) 1.\n54\n\n[131] Y. Zhang, B. Wallace, A sensitivity analysis of (and practitioners’ guide to) convolutional neural\nnetworks for sentence classiﬁcation (2015). arXiv:1510.03820.\n[132] J. R. Quinlan, Simplifying decision trees, International journal of man-machine studies 27 (3)\n(1987) 221–234.\n[133] Y. Zhou, G. Hooker, Interpreting models via single tree approximation (2016). arXiv:1610.09036.\n[134] A. Navia-V´azquez, E. Parrado-Hern´andez, Support vector machine interpretation, Neurocomputing\n69 (13-15) (2006) 1754–1759.\n[135] J. J. Thiagarajan, B. Kailkhura, P. Sattigeri, K. N. Ramamurthy, Treeview: Peeking into deep neural\nnetworks via feature-space partitioning (2016). arXiv:1611.07429.\n[136] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional networks, in: European\nconference on computer vision, Springer, 2014, pp. 818–833.\n[137] A. Mahendran, A. Vedaldi, Understanding deep image representations by inverting them, in:\nProceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 5188–\n5196.\n[138] J. Wagner, J. M. Kohler, T. Gindele, L. Hetzel, J. T. Wiedemer, S. Behnke, Interpretable and\nﬁne-grained visual explanations for convolutional neural networks, in: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2019, pp. 9097–9107.\n[139] A. Kanehira, T. Harada, Learning to explain with complemental examples, in: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2019, pp. 8603–8611.\n[140] D. W. Apley, Visualizing the effects of predictor variables in black box supervised learning models\n(2016). arXiv:1612.08468.\n[141] M. Staniak, P. Biecek, Explanations of Model Predictions with live and breakDown Packages, The\nR Journal 10 (2) (2018) 395–409.\n[142] M. D. Zeiler, D. Krishnan, G. W. Taylor, R. Fergus, Deconvolutional networks., in: CVPR, Vol. 10,\n2010, p. 7.\n[143] J. T. Springenberg, A. Dosovitskiy, T. Brox, M. Riedmiller, Striving for simplicity: The all\nconvolutional net (2014). arXiv:1412.6806.\n[144] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, R. Sayres, Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV) (2017).\narXiv:1711.11279.\n[145] A. Polino, R. Pascanu, D. Alistarh, Model compression via distillation and quantization (2018).\narXiv:1802.05668.\n[146] W. J. Murdoch, A. Szlam, Automatic rule extraction from long short term memory networks (2017).\narXiv:1702.02540.\n[147] M. W. Craven, J. W. Shavlik, Using sampling and queries to extract rules from trained neural\nnetworks, in: Machine learning proceedings 1994, Elsevier, 1994, pp. 37–45.\n[148] A. D. Arbatli, H. L. Akin, Rule extraction from trained neural networks using genetic algorithms,\nNonlinear Analysis: Theory, Methods & Applications 30 (3) (1997) 1639–1648.\n55\n\n[149] U. Johansson, L. Niklasson, Evolving decision trees using oracle guides, in: 2009 IEEE Symposium\non Computational Intelligence and Data Mining, IEEE, 2009, pp. 238–244.\n[150] T. Lei, R. Barzilay, T. Jaakkola, Rationalizing neural predictions (2016). arXiv:1606.04155.\n[151] A. Radford, R. Jozefowicz, I. Sutskever, Learning to generate reviews and discovering sentiment\n(2017). arXiv:1704.01444.\n[152] R. R. Selvaraju, A. Das, R. Vedantam, M. Cogswell, D. Parikh, D. Batra, Grad-CAM: Why did\nyou say that? (2016).\n[153] R. Shwartz-Ziv, N. Tishby, Opening the black box of deep neural networks via information (2017).\narXiv:1703.00810.\n[154] J. Yosinski, J. Clune, A. Nguyen, T. Fuchs, H. Lipson, Understanding neural networks through\ndeep visualization (2015). arXiv:1506.06579.\n[155] P. E. Pope, S. Kolouri, M. Rostami, C. E. Martin, H. Hoffmann, Explainability methods for graph\nconvolutional neural networks, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2019, pp. 10772–10781.\n[156] P. Gajane, M. Pechenizkiy, On formalizing fairness in prediction with machine learning (2017).\narXiv:1710.03184.\n[157] C. Dwork, C. Ilvento, Composition of fairsystems (2018). arXiv:1806.06122.\n[158] S. Barocas, M. Hardt, A. Narayanan, Fairness and Machine Learning, fairmlbook.org, 2019,\nhttp://www.fairmlbook.org.\n[159] H.-X. Wang, L. Fratiglioni, G. B. Frisoni, M. Viitanen, B. Winblad, Smoking and the occurence of\nalzheimer’s disease: Cross-sectional and longitudinal data in a population-based study, American\njournal of epidemiology 149 (7) (1999) 640–644.\n[160] P. Rani, C. Liu, N. Sarkar, E. Vanman, An empirical study of machine learning techniques for affect\nrecognition in human–robot interaction, Pattern Analysis and Applications 9 (1) (2006) 58–69.\n[161] J. Pearl, Causality, Cambridge university press, 2009.\n[162] M. Kuhn, K. Johnson, Applied predictive modeling, Vol. 26, Springer, 2013.\n[163] G. James, D. Witten, T. Hastie, R. Tibshirani, An introduction to statistical learning, Vol. 112,\nSpringer, 2013.\n[164] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, R. Fergus, Intriguing\nproperties of neural networks (2013). arXiv:1312.6199.\n[165] D. Ruppert, Robust statistics: The approach based on inﬂuence functions, Taylor & Francis, 1987.\n[166] S. Basu, K. Kumbier, J. B. Brown, B. Yu, Iterative random forests to discover predictive and\nstable high-order interactions, Proceedings of the National Academy of Sciences 115 (8) (2018)\n1943–1948.\n[167] B. Yu, et al., Stability, Bernoulli 19 (4) (2013) 1484–1500.\n[168] K. Burns, L. A. Hendricks, K. Saenko, T. Darrell, A. Rohrbach, Women also Snowboard: Overcoming Bias in Captioning Models (2018). arXiv:1803.09797.\n56\n\n[169] A. Bennetot, J.-L. Laurent, R. Chatila, N. D´ıaz-Rodr´ıguez, Towards explainable neural-symbolic\nvisual reasoning, in: NeSy Workshop IJCAI 2019, Macau, China, 2019.\n[170] R. Tibshirani, Regression shrinkage and selection via the lasso, Journal of the Royal Statistical\nSociety: Series B (Methodological) 58 (1) (1996) 267–288.\n[171] Y. Lou, R. Caruana, J. Gehrke, Intelligible models for classiﬁcation and regression, in: ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, ACM, 2012, pp.\n150–158.\n[172] K. Kawaguchi, Deep learning without poor local minima, in: Advances in neural information\nprocessing systems, 2016, pp. 586–594.\n[173] A. Datta, S. Sen, Y. Zick, Algorithmic transparency via quantitative input inﬂuence: Theory and\nexperiments with learning systems, in: 2016 IEEE symposium on security and privacy (SP), IEEE,\n2016, pp. 598–617.\n[174] Z. Bursac, C. H. Gauss, D. K. Williams, D. W. Hosmer, Purposeful selection of variables in logistic\nregression, Source code for biology and medicine 3 (1) (2008) 17.\n[175] J. Jaccard, Interaction effects in logistic regression: Quantitative applications in the social sciences,\nSage Thousand Oaks, CA, 2001.\n[176] D. W. Hosmer Jr, S. Lemeshow, R. X. Sturdivant, Applied logistic regression, Vol. 398, John Wiley\n& Sons, 2013.\n[177] C.-Y. J. Peng, K. L. Lee, G. M. Ingersoll, An introduction to logistic regression analysis and\nreporting, The journal of educational research 96 (1) (2002) 3–14.\n[178] U. Hoffrage, G. Gigerenzer, Using natural frequencies to improve diagnostic inferences, Academic\nmedicine 73 (5) (1998) 538–540.\n[179] C. Mood, Logistic regression: Why we cannot do what we think we can do, and what we can do\nabout it, European sociological review 26 (1) (2010) 67–82.\n[180] H. Laurent, R. L. Rivest, Constructing optimal binary decision trees is Np-complete, Information\nprocessing letters 5 (1) (1976) 15–17.\n[181] P. E. Utgoff, Incremental induction of decision trees, Machine learning 4 (2) (1989) 161–186.\n[182] J. R. Quinlan, Induction of decision trees, Machine learning 1 (1) (1986) 81–106.\n[183] L. Rokach, O. Z. Maimon, Data mining with decision trees: theory and applications, Vol. 69, World\nscientiﬁc, 2014.\n[184] S. Rovnyak, S. Kretsinger, J. Thorp, D. Brown, Decision trees for real-time transient stability\nprediction, IEEE Transactions on Power Systems 9 (3) (1994) 1417–1426.\n[185] H. Nefeslioglu, E. Sezer, C. Gokceoglu, A. Bozkir, T. Duman, Assessment of landslide susceptibility by decision trees in the metropolitan area of istanbul, turkey, Mathematical Problems in\nEngineering 2010 (2010) Article ID 901095.\n[186] S. B. Imandoust, M. Bolandraftar, Application of k-nearest neighbor (knn) approach for predicting\neconomic events: Theoretical background, International Journal of Engineering Research and\nApplications 3 (5) (2013) 605–610.\n57\n\n[187] L. Li, D. M. Umbach, P. Terry, J. A. Taylor, Application of the GA/KNN method to SELDI\nproteomics data, Bioinformatics 20 (10) (2004) 1638–1640.\n[188] G. Guo, H. Wang, D. Bell, Y. Bi, K. Greer, An KNN model-based approach and its application in\ntext categorization, in: International Conference on Intelligent Text Processing and Computational\nLinguistics, Springer, 2004, pp. 559–570.\n[189] S. Jiang, G. Pang, M. Wu, L. Kuang, An improved k-nearest-neighbor algorithm for text categorization, Expert Systems with Applications 39 (1) (2012) 1503–1509.\n[190] U. Johansson, R. K¨onig, L. Niklasson, The truth is in there-rule extraction from opaque models\nusing genetic programming., in: FLAIRS Conference, Miami Beach, FL, 2004, pp. 658–663.\n[191] J. R. Quinlan, Generating production rules from decision trees., in: ijcai, Vol. 87, Citeseer, 1987,\npp. 304–307.\n[192] P. Langley, H. A. Simon, Applications of machine learning and rule induction, Communications of\nthe ACM 38 (11) (1995) 54–64.\n[193] D. Berg, Bankruptcy prediction by generalized additive models, Applied Stochastic Models in\nBusiness and Industry 23 (2) (2007) 129–143.\n[194] R. Calabrese, et al., Estimating bank loans loss given default by generalized additive models, UCD\nGeary Institute Discussion Paper Series, WP2012/24 (2012).\n[195] P. Taylan, G.-W. Weber, A. Beck, New approaches to regression by generalized additive models and\ncontinuous optimization for modern applications in ﬁnance, science and technology, Optimization\n56 (5-6) (2007) 675–698.\n[196] H. Murase, H. Nagashima, S. Yonezaki, R. Matsukura, T. Kitakado, Application of a generalized\nadditive model (GAM) to reveal relationships between environmental factors and distributions of\npelagic ﬁsh and krill: a case study in sendai bay, Japan, ICES Journal of Marine Science 66 (6)\n(2009) 1417–1424.\n[197] N. Tomi´c, S. Boˇzi´c, A modiﬁed geosite assessment model (M-GAM) and its application on the\nlazar canyon area (serbia), International journal of environmental research 8 (4) (2014) 1041–1052.\n[198] A. Guisan, T. C. Edwards Jr, T. Hastie, Generalized linear and generalized additive models in\nstudies of species distributions: setting the scene, Ecological Modelling 157 (2-3) (2002) 89–100.\n[199] P. Rothery, D. B. Roy, Application of generalized additive models to butterﬂy transect count data,\nJournal of Applied Statistics 28 (7) (2001) 897–909.\n[200] A. Pierrot, Y. Goude, Short-term electricity load forecasting with generalized additive models, in:\n16th Intelligent System Applications to Power Systems Conference, ISAP 2011, IEEE, 2011, pp.\n410–415.\n[201] T. L. Grifﬁths, C. Kemp, J. B. Tenenbaum, Bayesian models of cognition.\n(4 2008).\ndoi:10.1184/R1/6613682.v1.\nURL\nhttps://kilthub.cmu.edu/articles/Bayesian_models_of_\ncognition/6613682\n[202] B. H. Neelon, A. J. O’Malley, S.-L. T. Normand, A bayesian model for repeated measures zeroinﬂated count data with application to outpatient psychiatric service use, Statistical modelling\n10 (4) (2010) 421–439.\n58\n\n[203] M. McAllister, G. Kirkwood, Bayesian stock assessment: a review and example application using\nthe logistic model, ICES Journal of Marine Science 55 (6) (1998) 1031–1060.\n[204] G. Synnaeve, P. Bessiere, A bayesian model for opening prediction in RTS games with application\nto starcraft, in: Computational Intelligence and Games (CIG), 2011 IEEE Conference on, IEEE,\n2011, pp. 281–288.\n[205] S.-K. Min, D. Simonis, A. Hense, Probabilistic climate change predictions applying bayesian model\naveraging, Philosophical transactions of the royal society of london a: mathematical, physical and\nengineering sciences 365 (1857) (2007) 2103–2116.\n[206] G. Koop, D. J. Poirier, J. L. Tobias, Bayesian econometric methods, Cambridge University Press,\n2007.\n[207] A. R. Cassandra, L. P. Kaelbling, J. A. Kurien, Acting under uncertainty: Discrete bayesian models\nfor mobile-robot navigation, in: Proceedings of IEEE/RSJ International Conference on Intelligent\nRobots and Systems. IROS’96, Vol. 2, IEEE, 1996, pp. 963–972.\n[208] H. A. Chipman, E. I. George, R. E. McCulloch, Bayesian cart model search, Journal of the\nAmerican Statistical Association 93 (443) (1998) 935–948.\n[209] B. Kim, C. Rudin, J. A. Shah, The bayesian case model: A generative approach for case-based\nreasoning and prototype classiﬁcation, in: Advances in Neural Information Processing Systems,\n2014, pp. 1952–1960.\n[210] B. Kim, R. Khanna, O. O. Koyejo, Examples are not enough, learn to criticize! criticism for\ninterpretability, in: Advances in Neural Information Processing Systems, 2016, pp. 2280–2288.\n[211] U. Johansson, L. Niklasson, R. K¨onig, Accuracy vs. comprehensibility in data mining models,\nin: Proceedings of the seventh international conference on information fusion, Vol. 1, 2004, pp.\n295–300.\n[212] R. Konig, U. Johansson, L. Niklasson, G-rex: A versatile framework for evolutionary data mining,\nin: 2008 IEEE International Conference on Data Mining Workshops, IEEE, 2008, pp. 971–974.\n[213] H. Lakkaraju, E. Kamar, R. Caruana, J. Leskovec, Interpretable & explorable approximations of\nblack box models (2017). arXiv:1707.01154.\n[214] S. Mishra, B. L. Sturm, S. Dixon, Local interpretable model-agnostic explanations for music\ncontent analysis., in: ISMIR, 2017, pp. 537–543.\n[215] G. Su, D. Wei, K. R. Varshney, D. M. Malioutov, Interpretable two-level boolean rule learning for\nclassiﬁcation (2015). arXiv:1511.07361.\n[216] M. T. Ribeiro, S. Singh, C. Guestrin, Nothing else matters: Model-agnostic explanations by\nidentifying prediction invariance (2016). arXiv:1611.05817.\n[217] M. W. Craven, Extracting comprehensible models from trained neural networks, Ph.D. thesis,\naAI9700774 (1996).\n[218] O. Bastani, C. Kim, H. Bastani, Interpretability via model extraction (2017). arXiv:1706.09773.\n[219] G. Hooker, Discovering additive structure in black box functions, in: Proceedings of the tenth\nACM SIGKDD international conference on Knowledge discovery and data mining, ACM, 2004,\npp. 575–580.\n59\n\n[220] P. Adler, C. Falk, S. A. Friedler, T. Nix, G. Rybeck, C. Scheidegger, B. Smith, S. Venkatasubramanian, Auditing black-box models for indirect inﬂuence, Knowledge and Information Systems\n54 (1) (2018) 95–122.\n[221] P. W. Koh, P. Liang, Understanding black-box predictions via inﬂuence functions, in: Proceedings\nof the 34th International Conference on Machine Learning-Volume 70, JMLR. org, 2017, pp.\n1885–1894.\n[222] P. Cortez, M. J. Embrechts, Opening black box data mining models using sensitivity analysis, in:\n2011 IEEE Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2011, pp.\n341–348.\n[223] P. Cortez, M. J. Embrechts, Using sensitivity analysis and visualization techniques to open black\nbox data mining models, Information Sciences 225 (2013) 1–17.\n[224] S. M. Lundberg, S.-I. Lee, A uniﬁed approach to interpreting model predictions, in: Advances in\nNeural Information Processing Systems, 2017, pp. 4765–4774.\n[225] I. Kononenko, et al., An efﬁcient explanation of individual classiﬁcations using game theory,\nJournal of Machine Learning Research 11 (Jan) (2010) 1–18.\n[226] H. Chen, S. Lundberg, S.-I. Lee, Explaining models by propagating shapley values of local\ncomponents (2019). arXiv:arXiv:1911.11888.\n[227] P. Dabkowski, Y. Gal, Real time image saliency for black box classiﬁers, in: Advances in Neural\nInformation Processing Systems, 2017, pp. 6967–6976.\n[228] A. Henelius, K. Puolam¨aki, H. Bostr¨om, L. Asker, P. Papapetrou, A peek into the black box:\nexploring classiﬁers by randomization, Data mining and knowledge discovery 28 (5-6) (2014)\n1503–1529.\n[229] J. Moeyersoms, B. d’Alessandro, F. Provost, D. Martens, Explaining classiﬁcation models built on\nhigh-dimensional sparse data (2016). arXiv:1607.06280.\n[230] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, K.-R. M ˜Aˇzller, How to\nexplain individual classiﬁcation decisions, Journal of Machine Learning Research 11 (Jun) (2010)\n1803–1831.\n[231] J. Adebayo, L. Kagal, Iterative orthogonal feature projection for diagnosing bias in black-box\nmodels (2016). arXiv:1611.04967.\n[232] R. Guidotti, A. Monreale, S. Ruggieri, D. Pedreschi, F. Turini, F. Giannotti, Local rule-based\nexplanations of black box decision systems (2018). arXiv:1805.10820.\n[233] S. Krishnan, E. Wu, Palm: Machine learning explanations for iterative debugging, in: Proceedings\nof the 2nd Workshop on Human-In-the-Loop Data Analytics, ACM, 2017, p. 4.\n[234] M. Robnik-ˇSikonja, I. Kononenko, Explaining classiﬁcations for individual instances, IEEE\nTransactions on Knowledge and Data Engineering 20 (5) (2008) 589–600.\n[235] M. T. Ribeiro, S. Singh, C. Guestrin, Anchors: High-precision model-agnostic explanations, in:\nAAAI Conference on Artiﬁcial Intelligence, 2018, pp. 1527–1535.\n[236] D. Martens, F. Provost, Explaining data-driven document classiﬁcations, MIS Quarterly 38 (1)\n(2014) 73–100.\n60\n\n[237] D. Chen, S. P. Fraiberger, R. Moakler, F. Provost, Enhancing transparency and control when\ndrawing data-driven inferences about individuals, Big data 5 (3) (2017) 197–212.\n[238] A. Goldstein, A. Kapelner, J. Bleich, E. Pitkin, Peeking inside the black box: Visualizing statistical\nlearning with plots of individual conditional expectation, Journal of Computational and Graphical\nStatistics 24 (1) (2015) 44–65.\n[239] G. Casalicchio, C. Molnar, B. Bischl, Visualizing the feature importance for black box models,\nin: Joint European Conference on Machine Learning and Knowledge Discovery in Databases,\nSpringer, 2018, pp. 655–670.\n[240] G. Tolomei, F. Silvestri, A. Haines, M. Lalmas, Interpretable predictions of tree-based ensembles via\nactionable feature tweaking, in: Proceedings of the 23rd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, ACM, 2017, pp. 465–474.\n[241] L. Auret, C. Aldrich, Interpretation of nonlinear relationships between process variables by use of\nrandom forests, Minerals Engineering 35 (2012) 27–42.\n[242] N. F. Rajani, R. Mooney, Stacking with auxiliary features for visual question answering, in: Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long Papers), 2018, pp. 2217–2226.\n[243] N. F. Rajani, R. J. Mooney, Ensembling visual explanations, in: Explainable and Interpretable\nModels in Computer Vision and Machine Learning, Springer, 2018, pp. 155–172.\n[244] H. N´u˜nez, C. Angulo, A. Catal`a, Rule-based learning systems for support vector machines, Neural\nProcessing Letters 24 (1) (2006) 1–18.\n[245] Z. Chen, J. Li, L. Wei, A multiple kernel support vector machine scheme for feature selection and\nrule extraction from gene expression data of cancer tissue, Artiﬁcial Intelligence in Medicine 41 (2)\n(2007) 161–175.\n[246] H. N´u˜nez, C. Angulo, A. Catal`a, Support vector machines with symbolic interpretation, in: VII\nBrazilian Symposium on Neural Networks, 2002. SBRN 2002. Proceedings., IEEE, 2002, pp.\n142–147.\n[247] P. Sollich, Bayesian methods for support vector machines: Evidence and predictive class probabilities, Machine learning 46 (1-3) (2002) 21–52.\n[248] P. Sollich, Probabilistic methods for support vector machines, in: Advances in neural information\nprocessing systems, 2000, pp. 349–355.\n[249] W. Landecker, M. D. Thomure, L. M. Bettencourt, M. Mitchell, G. T. Kenyon, S. P. Brumby,\nInterpreting individual classiﬁcations of hierarchical networks, in: 2013 IEEE Symposium on\nComputational Intelligence and Data Mining (CIDM), IEEE, 2013, pp. 32–38.\n[250] A. Jakulin, M. Moˇzina, J. Demˇsar, I. Bratko, B. Zupan, Nomograms for visualizing support vector\nmachines, in: Proceedings of the eleventh ACM SIGKDD international conference on Knowledge\ndiscovery in data mining, ACM, 2005, pp. 108–117.\n[251] L. Fu, Rule generation from neural networks, IEEE Transactions on Systems, Man, and Cybernetics\n24 (8) (1994) 1114–1124.\n[252] G. G. Towell, J. W. Shavlik, Extracting reﬁned rules from knowledge-based neural networks,\nMachine Learning 13 (1) (1993) 71–101.\n61\n\n[253] S. Thrun, Extracting rules from artiﬁcial neural networks with distributed representations, in:\nProceedings of the 7th International Conference on Neural Information Processing Systems,\nNIPS’94, 1994, pp. 505–512.\n[254] R. Setiono, W. K. Leow, FERNN: An algorithm for fast extraction of rules from neural networks,\nApplied Intelligence 12 (1) (2000) 15–25.\n[255] I. A. Taha, J. Ghosh, Symbolic interpretation of artiﬁcial neural networks, IEEE Transactions on\nKnowledge and Data Engineering 11 (3) (1999) 448–463.\n[256] H. Tsukimoto, Extracting rules from trained neural networks, IEEE Transactions on Neural\nNetworks 11 (2) (2000) 377–389.\n[257] J. R. Zilke, E. L. Menc´ıa, F. Janssen, Deepred–rule extraction from deep neural networks, in:\nInternational Conference on Discovery Science, Springer, 2016, pp. 457–473.\n[258] G. P. J. Schmitz, C. Aldrich, F. S. Gouws, ANN-DT: an algorithm for extraction of decision trees\nfrom artiﬁcial neural networks, IEEE Transactions on Neural Networks 10 (6) (1999) 1392–1401.\n[259] M. Sato, H. Tsukimoto, Rule extraction from neural networks via decision tree induction, in:\nIJCNN’01. International Joint Conference on Neural Networks. Proceedings (Cat. No. 01CH37222),\nVol. 3, IEEE, 2001, pp. 1870–1875.\n[260] R. F´eraud, F. Cl´erot, A methodology to explain neural network classiﬁcation, Neural networks\n15 (2) (2002) 237–246.\n[261] A. Shrikumar, P. Greenside, A. Kundaje, Learning Important Features Through Propagating\nActivation Differences (2017). arXiv:1704.02685.\n[262] M. Sundararajan, A. Taly, Q. Yan, Axiomatic attribution for deep networks, in: International\nConference on Machine Learning, Vol. 70, JMLR. org, 2017, pp. 3319–3328.\n[263] J. Adebayo, J. Gilmer, I. Goodfellow, B. Kim, Local explanation methods for deep neural networks\nlack sensitivity to parameter values (2018). arXiv:1810.03307.\n[264] N. Papernot, P. McDaniel, Deep k-nearest neighbors: Towards conﬁdent, interpretable and robust\ndeep learning (2018). arXiv:1803.04765.\n[265] J. Li, X. Chen, E. Hovy, D. Jurafsky, Visualizing and understanding neural models in NLP (2015).\narXiv:1506.01066.\n[266] S. Tan, K. C. Sim, M. Gales, Improving the interpretability of deep neural networks with stimulated\nlearning, in: 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),\nIEEE, 2015, pp. 617–623.\n[267] L. Rieger, C. Singh, W. J. Murdoch, B. Yu, Interpretations are useful: penalizing explanations to\nalign neural networks with prior knowledge (2019). arXiv:arXiv:1909.13584.\n[268] A. Nguyen, A. Dosovitskiy, J. Yosinski, T. Brox, J. Clune, Synthesizing the preferred inputs for\nneurons in neural networks via deep generator networks, in: Advances in Neural Information\nProcessing Systems, 2016, pp. 3387–3395.\n[269] Y. Li, J. Yosinski, J. Clune, H. Lipson, J. E. Hopcroft, Convergent learning: Do different neural\nnetworks learn the same representations?, in: ICLR, 2016.\n62\n\n[270] M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, S. Liu, Towards better analysis of deep convolutional neural\nnetworks, IEEE transactions on visualization and computer graphics 23 (1) (2016) 91–100.\n[271] Y. Goyal, A. Mohapatra, D. Parikh, D. Batra, Towards transparent AI systems: Interpreting visual\nquestion answering models (2016). arXiv:1608.08974.\n[272] K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional networks: Visualising image\nclassiﬁcation models and saliency maps (2013). arXiv:1312.6034.\n[273] A. Nguyen, J. Yosinski, J. Clune, Deep neural networks are easily fooled: High conﬁdence\npredictions for unrecognizable images, in: Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2015, pp. 427–436.\n[274] J. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko,\nT. Darrell, Long-term recurrent convolutional networks for visual recognition and description,\nin: Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp.\n2625–2634.\n[275] M. Lin, Q. Chen, S. Yan, Network in network (2013). arXiv:1312.4400.\n[276] L. A. Hendricks, Z. Akata, M. Rohrbach, J. Donahue, B. Schiele, T. Darrell, Generating Visual\nExplanations (2016). arXiv:1603.08507.\n[277] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang, X. Tang, Residual attention\nnetwork for image classiﬁcation, in: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017, pp. 3156–3164.\n[278] T. Xiao, Y. Xu, K. Yang, J. Zhang, Y. Peng, Z. Zhang, The application of two-level attention models\nin deep convolutional neural network for ﬁne-grained image classiﬁcation, in: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 842–850.\n[279] Q. Zhang, R. Cao, Y. Nian Wu, S.-C. Zhu, Growing Interpretable Part Graphs on ConvNets via\nMulti-Shot Learning (2016). arXiv:1611.04246.\n[280] L. Arras, G. Montavon, K.-R. M¨uller, W. Samek, Explaining recurrent neural network predictions\nin sentiment analysis (2017). arXiv:1706.07206.\n[281] A. Karpathy, J. Johnson, L. Fei-Fei, Visualizing and understanding recurrent networks (2015).\narXiv:1506.02078.\n[282] J. Clos, N. Wiratunga, S. Massie, Towards explainable text classiﬁcation by jointly learning lexicon\nand modiﬁer terms, in: IJCAI-17 Workshop on Explainable AI (XAI), 2017, p. 19.\n[283] S. Wisdom, T. Powers, J. Pitton, L. Atlas, Interpretable recurrent neural networks using sequential\nsparse recovery (2016). arXiv:1611.07252.\n[284] V. Krakovna, F. Doshi-Velez, Increasing the interpretability of recurrent neural networks using\nhidden markov models (2016). arXiv:1606.05320.\n[285] E. Choi, M. T. Bahadori, J. Sun, J. Kulas, A. Schuetz, W. Stewart, Retain: An interpretable\npredictive model for healthcare using reverse time attention mechanism, in: Advances in Neural\nInformation Processing Systems, 2016, pp. 3504–3512.\n[286] L. Breiman, Classiﬁcation and regression trees, Routledge, 2017.\n63\n\n[287] A. Lucic, H. Haned, M. de Rijke, Explaining predictions from tree-based boosting ensembles\n(2019). arXiv:arXiv:1907.02582.\n[288] S. M. Lundberg, G. G. Erion, S.-I. Lee, Consistent individualized feature attribution for tree\nensembles (2018). arXiv:arXiv:1802.03888.\n[289] C. Buciluˇa, R. Caruana, A. Niculescu-Mizil, Model compression, in: ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, ACM, 2006, pp. 535–541.\n[290] R. Traor´e, H. Caselles-Dupr´e, T. Lesort, T. Sun, G. Cai, N. D. Rodr´ıguez, D. Filliat, DisCoRL:\nContinual reinforcement learning via policy distillation (2019). arXiv:1907.05855.\n[291] M. D. Zeiler, G. W. Taylor, R. Fergus, et al., Adaptive deconvolutional networks for mid and high\nlevel feature learning., in: ICCV, Vol. 1, 2011, p. 6.\n[292] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra, Grad-cam: Visual\nexplanations from deep networks via gradient-based localization, in: Proceedings of the IEEE\nInternational Conference on Computer Vision, 2017, pp. 618–626.\n[293] C. Olah, A. Mordvintsev, L. Schubert, Feature visualization., DistillHttps://distill.pub/2017/featurevisualization (2017). doi:10.23915/distill.00007.\n[294] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, B. Kim, Sanity checks for saliency\nmaps, in: Advances in Neural Information Processing Systems, 2018, pp. 9505–9515.\n[295] C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, A. Mordvintsev, The building\nblocks of interpretability, Distill (2018).\nURL https://distill.pub/2018/building-blocks/\n[296] Z. Che, S. Purushotham, R. Khemani, Y. Liu, Distilling knowledge from deep networks with\napplications to healthcare domain (2015). arXiv:1512.03542.\n[297] I. Donadello, L. Seraﬁni, A. D. Garcez, Logic tensor networks for semantic image interpretation,\nProceedings of the Twenty-Sixth International Joint Conference on Artiﬁcial Intelligence, IJCAI\n(2017) 1596–1602.\n[298] I. Donadello, Semantic image interpretation-integration of numerical data and logical knowledge\nfor cognitive vision, Ph.D. thesis, University of Trento (2018).\n[299] A. S. d’Avila Garcez, M. Gori, L. C. Lamb, L. Seraﬁni, M. Spranger, S. N. Tran, Neural-symbolic\ncomputing: An effective methodology for principled integration of machine learning and reasoning\n(2019). arXiv:1905.06088.\n[300] R. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester, L. De Raedt, DeepProbLog: Neural\nprobabilistic logic programming, in: Advances in Neural Information Processing Systems 31, 2018,\npp. 3749–3759.\n[301] I. Donadello, M. Dragoni, C. Eccher, Persuasive explanation of reasoning inferences on dietary\ndata, in: First Workshop on Semantic Explainability @ ISWC 2019, 2019.\n[302] R. G. Krishnan, U. Shalit, D. Sontag, Deep Kalman Filters (2015). arXiv:1511.05121.\n[303] M. Karl, M. Soelch, J. Bayer, P. van der Smagt, Deep Variational Bayes Filters: Unsupervised\nLearning of State Space Models from Raw Data (2016). arXiv:1605.06432.\n64\n\n[304] M. J. Johnson, D. K. Duvenaud, A. Wiltschko, R. P. Adams, S. R. Datta, Composing graphical\nmodels with neural networks for structured representations and fast inference, in: Advances in\nNeural Information Processing Systems 29, 2016, pp. 2946–2954.\n[305] S. Zheng, S. Jayasumana, B. Romera-Paredes, V. Vineet, Z. Su, D. Du, C. Huang, P. H. Torr,\nConditional random ﬁelds as recurrent neural networks, in: Proceedings of the IEEE international\nconference on computer vision, 2015, pp. 1529–1537.\n[306] N. Narodytska, A. Ignatiev, F. Pereira, J. Marques-Silva, Learning optimal decision trees with SAT,\nin: Proceedings of the Twenty-Seventh International Joint Conference on Artiﬁcial Intelligence,\nIJCAI-18, 2018, pp. 1362–1368.\n[307] O. Loyola-Gonz´alez, Black-box vs. white-box: Understanding their advantages and weaknesses\nfrom a practical point of view, IEEE Access 7 (2019) 154096–154113.\n[308] F. Petroni, T. Rockt¨aschel, P. Lewis, A. Bakhtin, Y. Wu, A. H. Miller, S. Riedel, Language models\nas knowledge bases? (2019). arXiv:1909.01066.\n[309] K. Bollacker, N. D´ıaz-Rodr´ıguez, X. Li, Extending knowledge graphs with subjective inﬂuence\nnetworks for personalized fashion, in: E. Portmann, M. E. Tabacchi, R. Seising, A. Habenstein\n(Eds.), Designing Cognitive Cities, Springer International Publishing, 2019, pp. 203–233.\n[310] W. Shang, A. Trott, S. Zheng, C. Xiong, R. Socher, Learning world graphs to accelerate hierarchical\nreinforcement learning (2019). arXiv:1907.00664.\n[311] M. Zolotas, Y. Demiris, Towards explainable shared control using augmented reality, 2019.\n[312] M. Garnelo, K. Arulkumaran, M. Shanahan, Towards deep symbolic reinforcement learning (2016).\narXiv:1609.05518.\n[313] V. Bellini, A. Schiavone, T. Di Noia, A. Ragone, E. Di Sciascio, Knowledge-aware autoencoders\nfor explainable recommender systems, in: Proceedings of the 3rd Workshop on Deep Learning for\nRecommender Systems, DLRS 2018, 2018, pp. 24–31.\n[314] C.-Z. A. Huang, A. Vaswani, J. Uszkoreit, N. Shazeer, C. Hawthorne, A. M. Dai, M. D. Hoffman,\nD. Eck, Music transformer: Generating music with long-term structure (2018). arXiv:1809.04281.\n[315] M. Cornia, L. Baraldi, R. Cucchiara, Smart: Training shallow memory-aware transformers for\nrobotic explainability (2019). arXiv:1910.02974.\n[316] A. Aamodt, E. Plaza, Case-based reasoning: Foundational issues, Methodological Variations, and\nSystem Approaches 7 (1) (1994) 39–59.\n[317] R. Caruana, Case-based explanation for artiﬁcial neural nets, in: Artiﬁcial Neural Networks in\nMedicine and Biology, Proceedings of the ANNIMAB-1 Conference, 2000, pp. 303–308.\n[318] M. T. Keane, E. M. Kenny, The Twin-System Approach as One Generic Solution for XAI: An\nOverview of ANN-CBR Twins for Explaining Deep Learning (2019). arXiv:1905.08069.\n[319] T. Hailesilassie, Rule extraction algorithm for deep neural networks:\nA review (2016).\narXiv:1610.05267.\n[320] J. M. Benitez, J. L. Castro, I. Requena, Are artiﬁcial neural networks black boxes?, IEEE Trans.\nNeural Networks 8 (5) (1997) 1156–1164.\n65\n\n[321] U. Johansson, R. K¨onig, L. Niklasson, Automatically balancing accuracy and comprehensibility in\npredictive modeling, in: Proceedings of the 8th International Conference on Information Fusion,\nVol. 2, 2005, p. 7 pp.\n[322] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, M. Wattenberg, SmoothGrad: removing noise by adding\nnoise (2017). arXiv:1706.03825.\n[323] M. Ancona, E. Ceolini, C. ¨Oztireli, M. Gross, Towards better understanding of gradient-based\nattribution methods for Deep Neural Networks (2017). arXiv:1711.06104.\n[324] J. Yosinski, J. Clune, Y. Bengio, H. Lipson, How transferable are features in deep neural networks?\n(2014). arXiv:1411.1792.\n[325] A. Sharif Razavian, H. Azizpour, J. Sullivan, S. Carlsson, CNN Features off-the-shelf: an Astounding Baseline for Recognition (2014). arXiv:1403.6382.\n[326] S. Du, H. Guo, A. Simpson, Self-driving car steering angle prediction based on image recognition,\nTech. rep., Technical Report, Stanford University (2017).\n[327] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, A. Torralba, Object Detectors Emerge in Deep Scene\nCNNs (2014). arXiv:1412.6856.\n[328] Y. Zhang, X. Chen, Explainable Recommendation: A Survey and New Perspectives (2018).\narXiv:1804.11192.\n[329] J. Frankle, M. Carbin, The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n(2018). arXiv:1803.03635.\n[330] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin,\nAttention Is All You Need (2017). arXiv:1706.03762.\n[331] J. Lu, J. Yang, D. Batra, D. Parikh, Hierarchical question-image co-attention for visual question\nanswering, in: Proceedings of the 30th International Conference on Neural Information Processing\nSystems, NIPS’16, 2016, pp. 289–297.\n[332] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, D. Batra, Human Attention in Visual Question\nAnswering: Do Humans and Deep Networks Look at the Same Regions? (2016). arXiv:1606.03556.\n[333] D. Huk Park, L. A. Hendricks, Z. Akata, A. Rohrbach, B. Schiele, T. Darrell, M. Rohrbach, Multimodal Explanations: Justifying Decisions and Pointing to the Evidence (2018). arXiv:1802.08129.\n[334] A. Slavin Ross, M. C. Hughes, F. Doshi-Velez, Right for the Right Reasons: Training Differentiable\nModels by Constraining their Explanations (2017). arXiv:1703.03717.\n[335] I. T. Jolliffe, Principal Component Analysis and Factor Analysis, Springer New York, 1986, pp.\n115–128.\n[336] A. Hyv¨arinen, E. Oja, Oja, e.: Independent component analysis: Algorithms and applications.\nneural networks 13(4-5), 411-430, Neural networks 13 (2000) 411–430.\n[337] M. W. Berry, M. Browne, A. N. Langville, V. P. Pauca, R. J. Plemmons, Algorithms and applications\nfor approximate nonnegative matrix factorization, Computational Statistics & Data Analysis 52\n(2007) 155–173.\n[338] D. P. Kingma, M. Welling, Auto-Encoding Variational Bayes (2013). arXiv:1312.6114.\n66\n\n[339] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. M. Botvinick, S. Mohamed, A. Lerchner,\nbeta-vae: Learning basic visual concepts with a constrained variational framework, in: ICLR, 2017.\n[340] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, P. Abbeel, InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets (2016).\narXiv:1606.03657.\n[341] Q. Zhang, Y. Yang, Y. Liu, Y. Nian Wu, S.-C. Zhu, Unsupervised Learning of Neural Networks to\nExplain Neural Networks (2018). arXiv:1805.07468.\n[342] S. Sabour, N. Frosst, G. E Hinton, Dynamic Routing Between Capsules (2017). arXiv:1710.09829.\n[343] A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Batra, D. Parikh, VQA: Visual Question\nAnswering (2015). arXiv:1505.00468.\n[344] A. Fukui, D. Huk Park, D. Yang, A. Rohrbach, T. Darrell, M. Rohrbach, Multimodal Compact\nBilinear Pooling for Visual Question Answering and Visual Grounding (2016). arXiv:1606.01847.\n[345] D. Bouchacourt, L. Denoyer, EDUCE: explaining model decisions through unsupervised concepts\nextraction (2019). arXiv:1905.11852.\n[346] C. Hofer, M. Denker, S. Ducasse, Design and Implementation of a Backward-In-Time Debugger,\nin: NODe 2006, Vol. P-88 of Lecture Notes in Informatics, 2006, pp. 17–32.\n[347] C. Rudin, Please stop explaining black box models for high stakes decisions (2018).\narXiv:1811.10154.\n[348] A. Diez-Olivan, J. Del Ser, D. Galar, B. Sierra, Data fusion and machine learning for industrial\nprognosis: Trends and perspectives towards Industry 4.0, Information Fusion 50 (2019) 92–111.\n[349] R. R. Hoffman, S. T. Mueller, G. Klein, J. Litman, Metrics for explainable ai: Challenges and\nprospects (2018). arXiv:arXiv:1812.04608.\n[350] S. Mohseni, N. Zarei, E. D. Ragan, A multidisciplinary survey and framework for design and\nevaluation of explainable ai systems (2018). arXiv:arXiv:1811.11839.\n[351] R. M. J. Byrne, Counterfactuals in explainable artiﬁcial intelligence (XAI): Evidence from human\nreasoning, in: Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-19, 2019, pp. 6276–6282.\n[352] M. Garnelo, M. Shanahan, Reconciling deep learning with symbolic artiﬁcial intelligence: representing objects and relations, Current Opinion in Behavioral Sciences 29 (2019) 17–23.\n[353] G. Marra, F. Giannini, M. Diligenti, M. Gori, Integrating learning and reasoning with deep logic\nmodels (2019). arXiv:1901.04195.\n[354] K. Kelley, B. Clark, V. Brown, J. Sitzia, Good practice in the conduct and reporting of survey\nresearch, International Journal for Quality in Health Care 15 (3) (2003) 261–266.\n[355] S. Wachter, B. Mittelstadt, L. Floridi, Why a right to explanation of automated decision-making\ndoes not exist in the general data protection regulation, International Data Privacy Law 7 (2) (2017)\n76–99.\n[356] T. Orekondy, B. Schiele, M. Fritz, Knockoff nets: Stealing functionality of black-box models\n(2018). arXiv:1812.02766.\n67\n\n[357] S. J. Oh, B. Schiele, M. Fritz, Towards reverse-engineering black-box neural networks, in: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, Springer, 2019, pp. 121–144.\n[358] I. J. Goodfellow, J. Shlens, C. Szegedy, Explaining and harnessing adversarial examples (2014).\narXiv:1412.6572.\n[359] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, D. Song,\nRobust physical-world attacks on deep learning models (2017). arXiv:1707.08945.\n[360] I. J. Goodfellow, N. Papernot, P. D. McDaniel, cleverhans v0.1: an adversarial machine learning\nlibrary (2016). arXiv:1610.00768.\n[361] H. Xiao, B. Biggio, B. Nelson, H. Xiao, C. Eckert, F. Roli, Support vector machines under\nadversarial label contamination, Neurocomputing 160 (C) (2015) 53–62.\n[362] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c, P. Laskov, G. Giacinto, F. Roli, Evasion\nattacks against machine learning at test time, in: Proceedings of the 2013th European Conference\non Machine Learning and Knowledge Discovery in Databases - Volume Part III, ECMLPKDD’13,\n2013, pp. 387–402.\n[363] B. Biggio, I. Pillai, S. R. Bul`o, D. Ariu, M. Pelillo, F. Roli, Is data clustering in adversarial settings\nsecure? (2018). arXiv:1811.09982.\n[364] Z. Pan, W. Yu, X. Yi, A. Khan, F. Yuan, Y. Zheng, Recent progress on generative adversarial\nnetworks (gans): A survey, IEEE Access 7 (2019) 36322–36333.\n[365] D. Charte, F. Charte, S. Garc´ıa, M. J. del Jesus, F. Herrera, A practical tutorial on autoencoders\nfor nonlinear feature fusion: Taxonomy, models, software and guidelines, Information Fusion 44\n(2018) 78–96.\n[366] C. F. Baumgartner, L. M. Koch, K. Can Tezcan, J. Xi Ang, E. Konukoglu, Visual feature attribution\nusing wasserstein gans, in: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 8309–8319.\n[367] C. Bifﬁ, O. Oktay, G. Tarroni, W. Bai, A. De Marvao, G. Doumou, M. Rajchl, R. Bedair, S. Prasad,\nS. Cook, et al., Learning interpretable anatomical features through deep generative models: Application to cardiac remodeling, in: International Conference on Medical Image Computing and\nComputer-Assisted Intervention, Springer, 2018, pp. 464–471.\n[368] S. Liu, B. Kailkhura, D. Loveland, Y. Han, Generative counterfactual introspection for explainable\ndeep learning (2019). arXiv:arXiv:1907.03077.\n[369] K. R. Varshney, H. Alemzadeh, On the safety of machine learning: Cyber-physical systems,\ndecision sciences, and data products, Big data 5 (3) (2017) 246–255.\n[370] G. M. Weiss, Mining with rarity: a unifying framework, ACM Sigkdd Explorations Newsletter\n6 (1) (2004) 7–19.\n[371] J. Attenberg, P. Ipeirotis, F. Provost, Beat the machine: Challenging humans to ﬁnd a predictive\nmodel’s “unknown unknowns”, Journal of Data and Information Quality (JDIQ) 6 (1) (2015) 1.\n[372] G. Neff, A. Tanweer, B. Fiore-Gartland, L. Osburn, Critique and contribute: A practice-based\nframework for improving critical data studies and data science, Big data 5 (2) (2017) 85–97.\n[373] A. Iliadis, F. Russo, Critical data studies: An introduction, Big Data & Society 3 (2) (2016)\n2053951716674238.\n68\n\n[374] A. Karpatne, G. Atluri, J. H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar,\nN. Samatova, V. Kumar, Theory-guided data science: A new paradigm for scientiﬁc discovery\nfrom data, IEEE Transactions on Knowledge and Data Engineering 29 (10) (2017) 2318–2331.\n[375] G. Hautier, C. C. Fischer, A. Jain, T. Mueller, G. Ceder, Finding nature’s missing ternary oxide\ncompounds using machine learning and density functional theory, Chemistry of Materials 22 (12)\n(2010) 3762–3767.\n[376] C. C. Fischer, K. J. Tibbetts, D. Morgan, G. Ceder, Predicting crystal structure by merging data\nmining with quantum mechanics, Nature materials 5 (8) (2006) 641.\n[377] S. Curtarolo, G. L. Hart, M. B. Nardelli, N. Mingo, S. Sanvito, O. Levy, The high-throughput\nhighway to computational materials design, Nature materials 12 (3) (2013) 191.\n[378] K. C. Wong, L. Wang, P. Shi, Active model with orthotropic hyperelastic material for cardiac image\nanalysis, in: International Conference on Functional Imaging and Modeling of the Heart, Springer,\n2009, pp. 229–238.\n[379] J. Xu, J. L. Sapp, A. R. Dehaghani, F. Gao, M. Horacek, L. Wang, Robust transmural electrophysiological imaging: Integrating sparse and dynamic physiological models into ecg-based inference,\nin: International Conference on Medical Image Computing and Computer-Assisted Intervention,\nSpringer, 2015, pp. 519–527.\n[380] T. Lesort, M. Seurin, X. Li, N. D´ıaz-Rodr´ıguez, D. Filliat, Unsupervised state representation\nlearning with robotic priors: a robustness benchmark (2017). arXiv:arXiv:1709.05185.\n[381] J. Z. Leibo, Q. Liao, F. Anselmi, W. A. Freiwald, T. Poggio, View-tolerant face recognition and\nhebbian learning imply mirror-symmetric neural tuning to head orientation, Current Biology 27 (1)\n(2017) 62–67.\n[382] F. Schrodt, J. Kattge, H. Shan, F. Fazayeli, J. Joswig, A. Banerjee, M. Reichstein, G. B¨onisch,\nS. D´ıaz, J. Dickie, et al., Bhpmf–a hierarchical bayesian approach to gap-ﬁlling and trait prediction\nfor macroecology and functional biogeography, Global Ecology and Biogeography 24 (12) (2015)\n1510–1521.\n[383] D. Leslie, Understanding artiﬁcial intelligence ethics and safety (2019). arXiv:arXiv:1906.05684,\ndoi:10.5281/zenodo.3240529.\n[384] C. Rudin, Stop explaining black box machine learning models for high stakes decisions and use\ninterpretable models instead (2018). arXiv:arXiv:1811.10154.\n[385] J. Fjeld, H. Hilligoss, N. Achten, M. L. Daniel, J. Feldman, S. Kagay, Principled artiﬁcial intelligence: A map of ethical and rights-based approaches (2019).\nURL https://ai-hr.cyber.harvard.edu/images/primp-viz.pdf\n[386] R. Benjamins, A. Barbado, D. Sierra, Responsible AI by design (2019). arXiv:1909.12838.\n[387] United-Nations, Transforming our world: the 2030 agenda for sustainable development, Tech. rep.,\neSocialSciences (2015).\nURL https://EconPapers.repec.org/RePEc:ess:wpaper:id:7559\n[388] G. D. Hager, A. Drobnis, F. Fang, R. Ghani, A. Greenwald, T. Lyons, D. C. Parkes,\nJ. Schultz, S. Saria, S. F. Smith, M. Tambe, Artiﬁcial intelligence for social good (2019).\narXiv:arXiv:1901.05406.\n69\n\n[389] B. C. Stahl, D. Wright, Ethics and privacy in ai and big data: Implementing responsible research\nand innovation, IEEE Security & Privacy 16 (3) (2018) 26–33.\n[390] High Level Expert Group on Artiﬁcial Intelligence, Ethics guidelines for trustworthy ai, Tech. rep.,\nEuropean Commission (2019).\n[391] B. d’Alessandro, C. O’Neil, T. LaGatta, Conscientious classiﬁcation: A data scientist’s guide to\ndiscrimination-aware classiﬁcation, Big data 5 (2) (2017) 120–134.\n[392] S. Barocas, A. D. Selbst, Big data’s disparate impact, Calif. L. Rev. 104 (2016) 671.\n[393] M. Hardt, E. Price, N. Srebro, et al., Equality of opportunity in supervised learning, in: Advances\nin neural information processing systems, 2016, pp. 3315–3323.\n[394] T. Speicher, H. Heidari, N. Grgic-Hlaca, K. P. Gummadi, A. Singla, A. Weller, M. B. Zafar, A\nuniﬁed approach to quantifying algorithmic unfairness: Measuring individual group unfairness\nvia inequality indices, in: Proceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery Data Mining, ACM, 2018, pp. 2239–2248.\n[395] F. Kamiran, T. Calders, Data preprocessing techniques for classiﬁcation without discrimination,\nKnowledge and Information Systems 33 (1) (2012) 1–33.\n[396] R. Zemel, Y. Wu, K. Swersky, T. Pitassi, C. Dwork, Learning fair representations, in: International\nConference on Machine Learning, 2013, pp. 325–333.\n[397] B. H. Zhang, B. Lemoine, M. Mitchell, Mitigating unwanted biases with adversarial learning, in:\nProceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, ACM, 2018, pp.\n335–340.\n[398] Y. Ahn, Y.-R. Lin, Fairsight: Visual analytics for fairness in decision making, IEEE transactions on\nvisualization and computer graphics (2019).\n[399] E. Soares, P. Angelov, Fair-by-design explainable models for prediction of recidivism, arXiv\npreprint arXiv:1910.02043 (2019).\n[400] J. Dressel, H. Farid, The accuracy, fairness, and limits of predicting recidivism, Science advances\n4 (1) (2018) eaao5580.\n[401] U. Aivodji, H. Arai, O. Fortineau, S. Gambs, S. Hara, A. Tapp, Fairwashing: the risk of rationalization, in: International Conference on Machine Learning, 2019, pp. 161–170.\n[402] S. Sharma, J. Henderson, J. Ghosh, Certifai:\nCounterfactual explanations for robustness,\ntransparency, interpretability, and fairness of artiﬁcial intelligence models, arXiv preprint\narXiv:1905.07857 (2019).\n[403] M. Drosou, H. Jagadish, E. Pitoura, J. Stoyanovich, Diversity in big data: A review, Big data 5 (2)\n(2017) 73–84.\n[404] J. Lerman, Big data and its exclusions, Stan. L. Rev. Online 66 (2013) 55.\n[405] R. Agrawal, S. Gollapudi, A. Halverson, S. Ieong, Diversifying search results, in: Proceedings of\nthe second ACM international conference on web search and data mining, ACM, 2009, pp. 5–14.\n[406] B. Smyth, P. McClave, Similarity vs. diversity, in: International conference on case-based reasoning,\nSpringer, 2001, pp. 347–361.\n70\n\n[407] P. Wang, L. T. Yang, J. Li, J. Chen, S. Hu, Data fusion in cyber-physical-social systems: State-ofthe-art and perspectives, Information Fusion 51 (2019) 42–57.\n[408] W. Ding, X. Jing, Z. Yan, L. T. Yang, A survey on data fusion in internet of things: Towards secure\nand privacy-preserving fusion, Information Fusion 51 (2019) 129–144.\n[409] A. Smirnov, T. Levashova, Knowledge fusion patterns: A survey, Information Fusion 52 (2019)\n31–40.\n[410] W. Ding, X. Jing, Z. Yan, L. T. Yang, A survey on data fusion in internet of things: Towards secure\nand privacy-preserving fusion, Information Fusion 51 (2019) 129–144.\n[411] P. Wang, L. T. Yang, J. Li, J. Chen, S. Hu, Data fusion in cyber-physical-social systems: State-ofthe-art and perspectives, Information Fusion 51 (2019) 42–57.\n[412] B. P. L. Lau, S. H. Marakkalage, Y. Zhou, N. U. Hassan, C. Yuen, M. Zhang, U.-X. Tan, A survey\nof data fusion in smart city applications, Information Fusion 52 (2019) 357–374.\n[413] S. Ram´ırez-Gallego, A. Fern´andez, S. Garc´ıa, M. Chen, F. Herrera, Big data: Tutorial and guidelines\non information and process fusion for analytics algorithms with mapreduce, Information Fusion 42\n(2018) 51–61.\n[414] J. Koneˇcn´y, H. B. McMahan, D. Ramage, P. Richt´arik, Federated optimization: Distributed machine\nlearning for on-device intelligence (2016). arXiv:1610.02527.\n[415] B. McMahan, E. Moore, D. Ramage, S. Hampson, B. A. y Arcas, Communication-efﬁcient\nlearning of deep networks from decentralized data, in: Artiﬁcial Intelligence and Statistics, 2017,\npp. 1273–1282.\n[416] J. Koneˇcn`y, H. B. McMahan, F. X. Yu, P. Richt´arik, A. T. Suresh, D. Bacon, Federated learning:\nStrategies for improving communication efﬁciency (2016). arXiv:1610.05492.\n[417] S. Sun, A survey of multi-view machine learning, Neural computing and applications 23 (7-8)\n(2013) 2031–2038.\n[418] R. Zhang, F. Nie, X. Li, X. Wei, Feature selection with multi-view data: A survey, Information\nFusion 50 (2019) 158–167.\n[419] J. Zhao, X. Xie, X. Xu, S. Sun, Multi-view learning overview: Recent progress and new challenges,\nInformation Fusion 38 (2017) 43–54.\n[420] S. J. Oh, R. Benenson, M. Fritz, B. Schiele, Faceless person recognition: Privacy implications\nin social media, in: Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam,\nProceedings, Part III, 2016, pp. 19–35.\n[421] P. Aditya, R. Sen, P. Druschel, S. Joon Oh, R. Benenson, M. Fritz, B. Schiele, B. Bhattacharjee,\nT. T. Wu, I-pic: A platform for privacy-compliant image capture, in: Proceedings of the 14th annual\ninternational conference on mobile systems, applications, and services, ACM, 2016, pp. 235–248.\n[422] Q. Sun, A. Tewari, W. Xu, M. Fritz, C. Theobalt, B. Schiele, A hybrid model for identity obfuscation\nby face replacement, in: Proceedings of the European Conference on Computer Vision (ECCV),\n2018, pp. 553–569.\n[423] X. L. Dong, D. Srivastava, Big data integration, in: 2013 IEEE 29th international conference on\ndata engineering (ICDE), IEEE, 2013, pp. 1245–1248.\n71\n\n[424] D. Zhang, J. Zhao, F. Zhang, T. He, comobile: Real-time human mobility modeling at urban scale\nusing multi-view learning, in: Proceedings of the 23rd SIGSPATIAL International Conference on\nAdvances in Geographic Information Systems, ACM, 2015, p. 40.\n[425] S. J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on knowledge and data\nengineering 22 (10) (2009) 1345–1359.\n[426] M. Mitchell, S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji,\nT. Gebru, Model cards for model reporting, in: Proceedings of the Conference on Fairness,\nAccountability, and Transparency, ACM, 2019, pp. 220–229.\n72",
  "stats": {
    "raw_length": 260631,
    "normalized_length": 260431,
    "raw_lines": 3811,
    "normalized_lines": 3682
  }
}