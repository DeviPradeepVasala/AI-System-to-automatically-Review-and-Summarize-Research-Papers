{
  "pdf_path": "C:\\Users\\hp\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\Review of deep learning_ concepts, CNN architectures, challenges, applications, future directions.pdf",
  "pdf_name": "Review of deep learning_ concepts, CNN architectures, challenges, applications, future directions.pdf",
  "file_hash": "5db85404be4227131e3125da4353cd219acc22b88c9c592b156fb3e0c640967b",
  "metadata": {
    "title": "Review of deep learning: concepts, CNN architectures, challenges, applications, future directions",
    "author": "Laith Alzubaidi ",
    "subject": "Journal of Big Data, https://doi.org/10.1186/s40537-021-00444-8",
    "keywords": "Deep learning,Machine learning,Convolution neural network (CNN),Deep neural network architectures,Deep learning applications,Image classification,Transfer learning,Medical image analysis,Supervised learning,FPGA,GPU",
    "creator": "Springer",
    "producer": "Acrobat Distiller 10.1.8 (Windows); modified using iText® 5.3.5 ©2000-2012 1T3XT BVBA (SPRINGER SBM; licensed version)",
    "creation_date": "D:20210327160518+05'30'",
    "modification_date": "D:20210331115429+02'00'"
  },
  "raw_text": "Review of deep learning: concepts, CNN \narchitectures, challenges, applications, future \ndirections\nLaith Alzubaidi1,5*  , Jinglan Zhang1, Amjad J. Humaidi2, Ayad Al‑Dujaili3, Ye Duan4, Omran Al‑Shamma5, \nJ. Santamaría6, Mohammed A. Fadhel7, Muthana Al‑Amidie4 and Laith Farhan8 \nAbstract \nIn the last few years, the deep learning (DL) computing paradigm has been deemed \nthe Gold Standard in the machine learning (ML) community. Moreover, it has gradually \nbecome the most widely used computational approach in the field of ML, thus achiev‑\ning outstanding results on several complex cognitive tasks, matching or even beating \nthose provided by human performance. One of the benefits of DL is the ability to learn \nmassive amounts of data. The DL field has grown fast in the last few years and it has \nbeen extensively used to successfully address a wide range of traditional applications. \nMore importantly, DL has outperformed well-known ML techniques in many domains, \ne.g., cybersecurity, natural language processing, bioinformatics, robotics and control, \nand medical information processing, among many others. Despite it has been contrib‑\nuted several works reviewing the State-of-the-Art on DL, all of them only tackled one \naspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this \ncontribution, we propose using a more holistic approach in order to provide a more \nsuitable starting point from which to develop a full understanding of DL. Specifically, \nthis review attempts to provide a more comprehensive survey of the most impor‑\ntant aspects of DL and including those enhancements recently added to the field. In \nparticular, this paper outlines the importance of DL, presents the types of DL tech‑\nniques and networks. It then presents convolutional neural networks (CNNs) which the \nmost utilized DL network type and describes the development of CNNs architectures \ntogether with their main features, e.g., starting with the AlexNet network and closing \nwith the High-Resolution network (HR.Net). Finally, we further present the challenges \nand suggested solutions to help researchers understand the existing research gaps. \nIt is followed by a list of the major DL applications. Computational tools including \nFPGA, GPU, and CPU are summarized along with a description of their influence on \nDL. The paper ends with the evolution matrix, benchmark datasets, and summary and \nconclusion.\nKeywords:  Deep learning, Machine learning, Convolution neural network (CNN), Deep \nneural network architectures, Deep learning applications, Image classification, Transfer \nlearning, Medical image analysis, Supervised learning, FPGA, GPU\nOpen Access\n© The Author(s) 2021. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permit‑\nted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nSURVEY PAPER\nAlzubaidi et al. J Big Data            (2021) 8:53  \nhttps://doi.org/10.1186/s40537-021-00444-8\n*Correspondence:   \nlaith.alzubaidi@hdr.qut.\nedu.au \n1 School of Computer \nScience, Queensland \nUniversity of Technology, \nBrisbane, QLD 4000, Australia\nFull list of author information \nis available at the end of the \narticle\n\n\nPage 2 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nIntroduction\nRecently, machine learning (ML) has become very widespread in research and has been \nincorporated in a variety of applications, including text mining, spam detection, video \nrecommendation, image classification, and multimedia concept retrieval [1–6]. Among \nthe different ML algorithms, deep learning (DL) is very commonly employed in these \napplications [7–9]. Another name for DL is representation learning (RL). The continuing \nappearance of novel studies in the fields of deep and distributed learning is due to both \nthe unpredictable growth in the ability to obtain data and the amazing progress made in \nthe hardware technologies, e.g. High Performance Computing (HPC) [10].\nDL is derived from the conventional neural network but considerably outperforms its \npredecessors. Moreover, DL employs transformations and graph technologies simulta-\nneously in order to build up multi-layer learning models. The most recently developed \nDL techniques have obtained good outstanding performance across a variety of applica-\ntions, including audio and speech processing, visual data processing, natural language \nprocessing (NLP), among others [11–14].\nUsually, the effectiveness of an ML algorithm is highly dependent on the integrity of \nthe input-data representation. It has been shown that a suitable data representation pro-\nvides an improved performance when compared to a poor data representation. Thus, \na significant research trend in ML for many years has been feature engineering, which \nhas informed numerous research studies. This approach aims at constructing features \nfrom raw data. In addition, it is extremely field-specific and frequently requires sizable \nhuman effort. For instance, several types of features were introduced and compared in \nthe computer vision context, such as, histogram of oriented gradients (HOG) [15], scale-\ninvariant feature transform (SIFT) [16], and bag of words (BoW) [17]. As soon as a novel \nfeature is introduced and is found to perform well, it becomes a new research direction \nthat is pursued over multiple decades.\nRelatively speaking, feature extraction is achieved in an automatic way throughout the \nDL algorithms. This encourages researchers to extract discriminative features using the \nsmallest possible amount of human effort and field knowledge [18]. These algorithms \nhave a multi-layer data representation architecture, in which the first layers extract the \nlow-level features while the last layers extract the high-level features. Note that artificial \nintelligence (AI) originally inspired this type of architecture, which simulates the process \nthat occurs in core sensorial regions within the human brain. Using different scenes, the \nhuman brain can automatically extract data representation. More specifically, the output \nof this process is the classified objects, while the received scene information represents \nthe input. This process simulates the working methodology of the human brain. Thus, it \nemphasizes the main benefit of DL.\nIn the field of ML, DL, due to its considerable success, is currently one of the most \nprominent research trends. In this paper, an overview of DL is presented that adopts \nvarious perspectives such as the main concepts, architectures, challenges, applications, \ncomputational tools and evolution matrix. Convolutional neural network (CNN) is one \nof the most popular and used of DL networks [19, 20]. Because of CNN, DL is very pop-\nular nowadays. The main advantage of CNN compared to its predecessors is that it auto-\nmatically detects the significant features without any human supervision which made \nit the most used. Therefore, we have dug in deep with CNN by presenting the main \n\n\nPage 3 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\ncomponents of it. Furthermore, we have elaborated in detail the most common CNN \narchitectures, starting with the AlexNet network and ending with the High-Resolution \nnetwork (HR.Net).\nSeveral published DL review papers have been presented in the last few years. How-\never, all of them have only been addressed one side focusing on one application or topic \nsuch as the review of CNN architectures [21], DL for classification of plant diseases [22], \nDL for object detection [23], DL applications in medical image analysis [24], and etc. \nAlthough these reviews present good topics, they do not provide a full understanding of \nDL topics such as concepts, detailed research gaps, computational tools, and DL applica-\ntions. First, It is required to understand DL aspects including concepts, challenges, and \napplications then going deep in the applications. To achieve that, it requires extensive \ntime and a large number of research papers to learn about DL including research gaps \nand applications. Therefore, we propose a deep review of DL to provide a more suit-\nable starting point from which to develop a full understanding of DL from one review \npaper. The motivation behinds our review was to cover the most important aspect of DL \nincluding open challenges, applications, and computational tools perspective. Further-\nmore, our review can be the first step towards other DL topics.\nThe main aim of this review is to present the most important aspects of DL to make it \neasy for researchers and students to have a clear image of DL from single review paper. \nThis review will further advance DL research by helping people discover more about \nrecent developments in the field. Researchers would be allowed to decide the more suit-\nable direction of work to be taken in order to provide more accurate alternatives to the \nfield. Our contributions are outlined as follows:\n•\t This is the first review that almost provides a deep survey of the most important \naspects of deep learning. This review helps researchers and students to have a good \nunderstanding from one paper.\n•\t We explain CNN in deep which the most popular deep learning algorithm by \ndescribing the concepts, theory, and state-of-the-art architectures.\n•\t We review current challenges (limitations) of Deep Learning including lack of train-\ning data, Imbalanced Data, Interpretability of data, Uncertainty scaling, Catastrophic \nforgetting, Model compression, Overfitting, Vanishing gradient problem, Exploding \nGradient Problem, and Underspecification. We additionally discuss the proposed \nsolutions tackling these issues.\n•\t We provide an exhaustive list of medical imaging applications with deep learning by \ncategorizing them based on the tasks by starting with classification and ending with \nregistration.\n•\t We discuss the computational approaches (CPU, GPU, FPGA) by comparing the \ninfluence of each tool on deep learning algorithms.\nThe rest of the paper is organized as follows: “Survey methodology” section describes \nThe survey methodology. “Background” section presents the background. “Classification \nof DL approaches” section defines the classification of DL approaches. “Types of DL net-\nworks” section displays types of DL networks. “CNN architectures” section shows CNN \nArchitectures. “Challenges (limitations) of deep learning and alternate solutions” section \n\n\nPage 4 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \ndetails the challenges of DL and alternate solutions. “Applications of deep learning” sec-\ntion outlines the applications of DL. “Computational approaches” section explains the \ninfluence of computational approaches (CPU, GPU, FPGA) on DL. “Evaluation met-\nrics” section presents the evaluation metrics. “Frameworks and datasets” section lists \nframeworks and datasets. “Summary and conclusion” section presents the summary and \nconclusion.\nSurvey methodology\nWe have reviewed the significant research papers in the field published during 2010–\n2020, mainly from the years of 2020 and 2019 with some papers from 2021. The main \nfocus was papers from the most reputed publishers such as IEEE, Elsevier, MDPI, \nNature, ACM, and Springer. Some papers have been selected from ArXiv. We have \nreviewed more than 300 papers on various DL topics. There are 108 papers from the \nyear 2020, 76 papers from the year 2019, and 48 papers from the year 2018. This indi-\ncates that this review focused on the latest publications in the field of DL. The selected \npapers were analyzed and reviewed to (1) list and define the DL approaches and network \ntypes, (2) list and explain CNN architectures, (3) present the challenges of DL and sug-\ngest the alternate solutions, (4) assess the applications of DL, (5) assess computational \napproaches. The most keywords used for search criteria for this review paper are (“Deep \nLearning”), (“Machine Learning”), (“Convolution Neural Network”), (“Deep Learning” \nAND “Architectures”), ((“Deep Learning”) AND (“Image”) AND (“detection” OR “classi-\nfication” OR “segmentation” OR “Localization”)), (“Deep Learning” AND “detection” OR \n“classification” OR “segmentation” OR “Localization”), (“Deep Learning” AND “CPU” \nOR “GPU” OR “FPGA”), (“Deep Learning” AND “Transfer Learning”), (“Deep Learn-\ning” AND “Imbalanced Data”), (“Deep Learning” AND “Interpretability of data”), (“Deep \nLearning” AND “Overfitting”), (“Deep Learning” AND “Underspecification”). Figure 1 \nshows our search structure of the survey paper. Table 1 presents the details of some of \nthe journals that have been cited in this review paper.\nBackground\nThis section will present a background of DL. We begin with a quick introduction to \nDL, followed by the difference between DL and ML. We then show the situations that \nrequire DL. Finally, we present the reasons for applying DL.\nDL, a subset of ML (Fig. 2), is inspired by the information processing patterns found \nin the human brain. DL does not require any human-designed rules to operate; rather, \nit uses a large amount of data to map the given input to specific labels. DL is designed \nusing numerous layers of algorithms (artificial neural networks, or ANNs), each of which \nprovides a different interpretation of the data that has been fed to them [18, 25].\nAchieving the classification task using conventional ML techniques requires several \nsequential steps, specifically pre-processing, feature extraction, wise feature selection, \nlearning, and classification. Furthermore, feature selection has a great impact on the \nperformance of ML techniques. Biased feature selection may lead to incorrect discrimi-\nnation between classes. Conversely, DL has the ability to automate the learning of fea-\nture sets for several tasks, unlike conventional ML methods [18, 26]. DL enables learning \nand classification to be achieved in a single shot (Fig. 3). DL has become an incredibly \n\n\nPage 5 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\npopular type of ML algorithm in recent years due to the huge growth and evolution of \nthe field of big data [27, 28]. It is still in continuous development regarding novel per-\nformance for several ML tasks [22, 29–31] and has simplified the improvement of many \nlearning fields [32, 33], such as image super-resolution [34], object detection [35, 36], \nand image recognition [30, 37]. Recently, DL performance has come to exceed human \nperformance on tasks such as image classification (Fig. 4).\nNearly all scientific fields have felt the impact of this technology. Most industries and \nbusinesses have already been disrupted and transformed through the use of DL. The \nleading technology and economy-focused companies around the world are in a race to \nimprove DL. Even now, human-level performance and capability cannot exceed that the \nperformance of DL in many areas, such as predicting the time taken to make car deliv-\neries, decisions to certify loan requests, and predicting movie ratings [38]. The winners \nof the 2019 “Nobel Prize” in computing, also known as the Turing Award, were three \npioneers in the field of DL (Yann LeCun, Geoffrey Hinton, and Yoshua Bengio) [39]. \nAlthough a large number of goals have been achieved, there is further progress to be \nmade in the DL context. In fact, DL has the ability to enhance human lives by provid-\ning additional accuracy in diagnosis, including estimating natural disasters [40], the dis-\ncovery of new drugs [41], and cancer diagnosis [42–44]. Esteva et al. [45] found that a \nDL network has the same ability to diagnose the disease as twenty-one board-certified \ndermatologists using 129,450 images of 2032 diseases. Furthermore, in grading prostate \nFig. 1  Search framework\n\n\nPage 6 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \ncancer, US board-certified general pathologists achieved an average accuracy of 61%, \nwhile the Google AI [44] outperformed these specialists by achieving an average accu-\nracy of 70%. In 2020, DL is playing an increasingly vital role in early diagnosis of the \nnovel coronavirus (COVID-19) [29, 46–48]. DL has become the main tool in many hos-\npitals around the world for automatic COVID-19 classification and detection using chest \nTable 1  Some of the journals have been cited in this review paper\nJournal\nIF 2019 CiteScore 2019 Publisher Journal homepage\nPattern Recognition\n7.196\n13.1\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\npatte​rn-​recog​nition\nPattern Recognition Letter\n3.255\n6.3\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\npatte​rn-​recog​nition-​lette​rs\nArtificial Intelligence Review\n5.747\n9.1\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n10462?​refer​er=​www.​sprin​geron​\nline.​com\nExpert Systems with Applications\n5.452\n11\nElsevier\nhttps://​www.​scien​cedir​ect.​com/​journ​\nal/​expert-​syste​ms-​with-​appli​catio​ns\nNeurocomputing\n4.438\n9.5\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\nneuro​compu​ting\nNature Medicine\n36.130\n45.9\nNature\nhttps://​www.​nature.​com/​nm/\nNature\n42.779\n51\nNature\nhttps://​www.​nature.​com/\nJournal of Big Data\n–\n6.1\nSpringer\nhttps://​journ​alofb​igdata.​sprin​gerop​\nen.​com/\nMultimedia Tools and Applications\n2.313\n3.7\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n11042\nComputer Methods and Programs \nin Biomedicine\n3.632\n7.5\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\ncompu​ter-​metho​ds-​and-​progr​ams-​\nin-​biome​dicine\nMachine Learning\n2.672\n5.0\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n10994\nMachine Vision and Applications\n1.605\n4.2\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n138\nMedical Image Analysis\n11.148\n17.2\nElsevier\nhttps://​www.​scien​cedir​ect.​com/​journ​\nal/​medic​al-​image-​analy​sis\nIEEE Access\n3.745\n3.9\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​62876​39\nIEEE Transactions on Knowledge \nand Data Engineering\n4.935\n12.0\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​69\nNature Communications\n12.121\n18.1\nNature\nhttps://​www.​nature.​com/​ncomms/\nIEEE Transactions on Intelligent \nTransportation Systems\n6.319\n12.7\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​6979\nMethods\n3.812\n8.0\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\nmetho​ds\nACM Journal on Emerging Tech‑\nnologies in Computing Systems\n1.652\n4.3\nACM\nhttps://​dl.​acm.​org/​journ​al/​jetc\nACM Computing Surveys\n6.319\n12.7\nACM\nhttps://​dl.​acm.​org/​journ​al/​csur\nApplied Soft Computing\n5.472\n10.2\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\nappli​ed-​soft-​compu​ting\nElectronics\n2.412\n1.9\nMDPI\nhttps://​www.​mdpi.​com/​journ​al/​elect​\nronics\nApplied Sciences\n2.474\n2.4\nMDPI\nhttps://​www.​mdpi.​com/​journ​al/​\nappls​ci\nIEEE Transactions on Industrial \nInformatics\n9.112\n13.9\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​9424\n\n\nPage 7 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nFig. 2  Deep learning family\nFig. 3  The difference between deep learning and traditional machine learning\n\n\nPage 8 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nX-ray images or other types of images. We end this section by the saying of AI pioneer \nGeoffrey Hinton “Deep learning is going to be able to do everything”.\nWhen to apply deep learning\nMachine intelligence is useful in many situations which is equal or better than human \nexperts in some cases [49–52], meaning that DL can be a solution to the following \nproblems:\n•\t Cases where human experts are not available.\n•\t Cases where humans are unable to explain decisions made using their expertise (lan-\nguage understanding, medical decisions, and speech recognition).\n•\t Cases where the problem solution updates over time (price prediction, stock prefer-\nence, weather prediction, and tracking).\n•\t Cases where solutions require adaptation based on specific cases (personalization, \nbiometrics).\n•\t Cases where size of the problem is extremely large and exceeds our inadequate rea-\nsoning abilities (sentiment analysis, matching ads to Facebook, calculation webpage \nranks).\nWhy deep learning?\nSeveral performance features may answer this question, e.g \n1.\t Universal Learning Approach: Because DL has the ability to perform in approxi-\nmately all application domains, it is sometimes referred to as universal learning.\n2.\t Robustness: In general, precisely designed features are not required in DL tech-\nniques. Instead, the optimized features are learned in an automated fashion related \nFig. 4  Deep learning performance compared to human\n\n\nPage 9 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nto the task under consideration. Thus, robustness to the usual changes of the input \ndata is attained.\n3.\t Generalization: Different data types or different applications can use the same DL \ntechnique, an approach frequently referred to as transfer learning (TL) which \nexplained in the latter section. Furthermore, it is a useful approach in problems \nwhere data is insufficient.\n4.\t Scalability: DL is highly scalable. ResNet [37], which was invented by Microsoft, \ncomprises 1202 layers and is frequently applied at a supercomputing scale. Law-\nrence Livermore National Laboratory (LLNL), a large enterprise working on evolving \nframeworks for networks, adopted a similar approach, where thousands of nodes can \nbe implemented [53].\nClassification of DL approaches\nDL techniques are classified into three major categories: unsupervised, partially super-\nvised (semi-supervised) and supervised. Furthermore, deep reinforcement learning \n(DRL), also known as RL, is another type of learning technique, which is mostly con-\nsidered to fall into the category of partially supervised (and occasionally unsupervised) \nlearning techniques.\nDeep supervised learning\nThis technique deals with labeled data. When considering such a technique, the environs \nhave a collection of inputs and resultant outputs (xt, yt) ∼ρ . For instance, the smart \nagent guesses \n if the input is xt and will obtain \n as a loss value. Next, \nthe network parameters are repeatedly updated by the agent to obtain an improved esti-\nmate for the preferred outputs. Following a positive training outcome, the agent acquires \nthe ability to obtain the right solutions to the queries from the environs. For DL, there \nare several supervised learning techniques, such as recurrent neural networks (RNNs), \nconvolutional neural networks (CNNs), and deep neural networks (DNNs). In addition, \nthe RNN category includes gated recurrent units (GRUs) and long short-term memory \n(LSTM) approaches. The main advantage of this technique is the ability to collect data \nor generate a data output from the prior knowledge. However, the disadvantage of this \ntechnique is that decision boundary might be overstrained when training set doesn’t \nown samples that should be in a class. Overall, this technique is simpler than other tech-\nniques in the way of learning with high performance.\nDeep semi‑supervised learning\nIn this technique, the learning process is based on semi-labeled datasets. Occasionally, \ngenerative adversarial networks (GANs) and DRL are employed in the same way as this \ntechnique. In addition, RNNs, which include GRUs and LSTMs, are also employed for \npartially supervised learning. One of the advantages of this technique is to minimize \nthe amount of labeled data needed. On other the hand, One of the disadvantages of this \ntechnique is irrelevant input feature present training data could furnish incorrect deci-\nsions. Text document classifier is one of the most popular example of an application of \n\n\nPage 10 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nsemi-supervised learning. Due to difficulty of obtaining a large amount of labeled text \ndocuments, semi-supervised learning is ideal for text document classification task.\nDeep unsupervised learning\nThis technique makes it possible to implement the learning process in the absence \nof available labeled data (i.e. no labels are required). Here, the agent learns the sig-\nnificant features or interior representation required to discover the unidentified \nstructure or relationships in the input data. Techniques of generative networks, \ndimensionality reduction and clustering are frequently counted within the category \nof unsupervised learning. Several members of the DL family have performed well on \nnon-linear dimensionality reduction and clustering tasks; these include restricted \nBoltzmann machines, auto-encoders and GANs as the most recently developed tech-\nniques. Moreover, RNNs, which include GRUs and LSTM approaches, have also \nbeen employed for unsupervised learning in a wide range of applications. The main \ndisadvantages of unsupervised learning are unable to provide accurate information \nconcerning data sorting and computationally complex. One of the most popular \nunsupervised learning approaches is clustering [54].\nDeep reinforcement learning\nReinforcement Learning operates on interacting with the environment, while super-\nvised learning operates on provided sample data. This technique was developed in \n2013 with Google Deep Mind [55]. Subsequently, many enhanced techniques depend-\nent on reinforcement learning were constructed. For example, if the input environ-\nment samples: xt ∼ρ , agent predict: \n and the received cost of the agent is \n, P here is the unknown probability distribution, then the environ-\nment asks a question to the agent. The answer it gives is a noisy score. This method \nis sometimes referred to as semi-supervised learning. Based on this concept, several \nsupervised and unsupervised techniques were developed. In comparison with tradi-\ntional supervised techniques, performing this learning is much more difficult, as no \nstraightforward loss function is available in the reinforcement learning technique. In \naddition, there are two essential differences between supervised learning and rein-\nforcement learning: first, there is no complete access to the function, which requires \noptimization, meaning that it should be queried via interaction; second, the state \nbeing interacted with is founded on an environment, where the input xt is based on \nthe preceding actions [9, 56].\nFor solving a task, the selection of the type of reinforcement learning that needs to \nbe performed is based on the space or the scope of the problem. For example, DRL is \nthe best way for problems involving many parameters to be optimized. By contrast, \nderivative-free reinforcement learning is a technique that performs well for problems \nwith limited parameters. Some of the applications of reinforcement learning are busi-\nness strategy planning and robotics for industrial automation. The main drawback of \nReinforcement Learning is that parameters may influence the speed of learning. Here \nare the main motivations for utilizing Reinforcement Learning:\n\n\nPage 11 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n•\t It assists you to identify which action produces the highest reward over a longer \nperiod.\n•\t It assists you to discover which situation requires action.\n•\t It also enables it to figure out the best approach for reaching large rewards.\n•\t Reinforcement Learning also gives the learning agent a reward function.\nReinforcement Learning can’t utilize in all the situation such as:\n•\t In case there is sufficient data to resolve the issue with supervised learning tech-\nniques.\n•\t Reinforcement Learning is computing-heavy and time-consuming. Specially when \nthe workspace is large.\nTypes of DL networks\nThe most famous types of deep learning networks are discussed in this section: these \ninclude recursive neural networks (RvNNs), RNNs, and CNNs. RvNNs and RNNs were \nbriefly explained in this section while CNNs were explained in deep due to the impor-\ntance of this type. Furthermore, it is the most used in several applications among other \nnetworks.\nRecursive neural networks\nRvNN can achieve predictions in a hierarchical structure also classify the outputs uti-\nlizing compositional vectors [57]. Recursive auto-associative memory (RAAM) [58] is \nthe primary inspiration for the RvNN development. The RvNN architecture is gener-\nated for processing objects, which have randomly shaped structures like graphs or trees. \nThis approach generates a fixed-width distributed representation from a variable-size \nrecursive-data structure. The network is trained using an introduced back-propagation \nthrough structure (BTS) learning system [58]. The BTS system tracks the same tech-\nnique as the general-back propagation algorithm and has the ability to support a treelike \nstructure. Auto-association trains the network to regenerate the input-layer pattern at \nthe output layer. RvNN is highly effective in the NLP context. Socher et al.  [59] intro-\nduced RvNN architecture designed to process inputs from a variety of modalities. These \nauthors demonstrate two applications for classifying natural language sentences: cases \nwhere each sentence is split into words and nature images, and cases where each image \nis separated into various segments of interest. RvNN computes a likely pair of scores for \nmerging and constructs a syntactic tree. Furthermore, RvNN calculates a score related \nto the merge plausibility for every pair of units. Next, the pair with the largest score is \nmerged within a composition vector. Following every merge, RvNN generates (a) a larger \narea of numerous units, (b) a compositional vector of the area, and (c) a label for the \nclass (for instance, a noun phrase will become the class label for the new area if two units \nare noun words). The compositional vector for the entire area is the root of the RvNN \ntree structure. An example RvNN tree is shown in Fig. 5. RvNN has been employed in \nseveral applications [60–62].\n\n\nPage 12 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nRecurrent neural networks\nRNNs are a commonly employed and familiar algorithm in the discipline of DL [63–\n65]. RNN is mainly applied in the area of speech processing and NLP contexts [66, \n67]. Unlike conventional networks, RNN uses sequential data in the network. Since \nthe embedded structure in the sequence of the data delivers valuable information, this \nfeature is fundamental to a range of different applications. For instance, it is impor-\ntant to understand the context of the sentence in order to determine the meaning of \na specific word in it. Thus, it is possible to consider the RNN as a unit of short-term \nmemory, where x represents the input layer, y is the output layer, and s represents the \nstate (hidden) layer. For a given input sequence, a typical unfolded RNN diagram is \nillustrated in Fig. 6. Pascanu et al. [68] introduced three different types of deep RNN \ntechniques, namely “Hidden-to-Hidden”, “Hidden-to-Output”, and “Input-to-Hidden”. \nA deep RNN is introduced that lessens the learning difficulty in the deep network and \nbrings the benefits of a deeper RNN based on these three techniques.\nHowever, RNN’s sensitivity to the exploding gradient and vanishing problems rep-\nresent one of the main issues with this approach [69]. More specifically, during the \ntraining process, the reduplications of several large or small derivatives may cause the \ngradients to exponentially explode or decay. With the entrance of new inputs, the net-\nwork stops thinking about the initial ones; therefore, this sensitivity decays over time. \nFurthermore, this issue can be handled using LSTM [70]. This approach offers recur-\nrent connections to memory blocks in the network. Every memory block contains a \nnumber of memory cells, which have the ability to store the temporal states of the \nnetwork. In addition, it contains gated units for controlling the flow of information. \nIn very deep networks [37], residual connections also have the ability to considerably \nreduce the impact of the vanishing gradient issue which explained in later sections. \nFig. 5  An example of RvNN tree\n\n\nPage 13 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nCNN is considered to be more powerful than RNN. RNN includes less feature com-\npatibility when compared to CNN.\nConvolutional neural networks\nIn the field of DL, the CNN is the most famous and commonly employed algorithm [30, \n71–75]. The main benefit of CNN compared to its predecessors is that it automatically \nidentifies the relevant features without any human supervision [76]. CNNs have been \nextensively applied in a range of different fields, including computer vision [77], speech \nprocessing [78], Face Recognition [79], etc. The structure of CNNs was inspired by neu-\nrons in human and animal brains, similar to a conventional neural network. More specif-\nically, in a cat’s brain, a complex sequence of cells forms the visual cortex; this sequence \nis simulated by the CNN [80]. Goodfellow et al. [28] identified three key benefits of the \nCNN: equivalent representations, sparse interactions, and parameter sharing. Unlike \nconventional fully connected (FC) networks, shared weights and local connections in \nthe CNN are employed to make full use of 2D input-data structures like image signals. \nThis operation utilizes an extremely small number of parameters, which both simplifies \nthe training process and speeds up the network. This is the same as in the visual cor-\ntex cells. Notably, only small regions of a scene are sensed by these cells rather than the \nwhole scene (i.e., these cells spatially extract the local correlation available in the input, \nlike local filters over the input).\nA commonly used type of CNN, which is similar to the multi-layer perceptron (MLP), \nconsists of numerous convolution layers preceding sub-sampling (pooling) layers, while \nthe ending layers are FC layers. An example of CNN architecture for image classification \nis illustrated in Fig. 7.\nThe input x of each layer in a CNN model is organized in three dimensions: height, \nwidth, and depth, or m × m × r , where the height (m) is equal to the width. The depth \nis also referred to as the channel number. For example, in an RGB image, the depth (r) is \nequal to three. Several kernels (filters) available in each convolutional layer are denoted \nby k and also have three dimensions ( n × n × q ), similar to the input image; here, \nFig. 6  Typical unfolded RNN diagram\n\n\nPage 14 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nhowever, n must be smaller than m, while q is either equal to or smaller than r. In addi-\ntion, the kernels are the basis of the local connections, which share similar parameters \n(bias bk and weight W k ) for generating k feature maps hk with a size of ( m −n −1 ) each \nand are convolved with input, as mentioned above. The convolution layer calculates a \ndot product between its input and the weights as in Eq. 1, similar to NLP, but the inputs \nare undersized areas of the initial image size. Next, by applying the nonlinearity or an \nactivation function to the convolution-layer output, we obtain the following:\nThe next step is down-sampling every feature map in the sub-sampling layers. This leads \nto a reduction in the network parameters, which accelerates the training process and \nin turn enables handling of the overfitting issue. For all feature maps, the pooling func-\ntion (e.g. max or average) is applied to an adjacent area of size p × p , where p is the \nkernel size. Finally, the FC layers receive the mid- and low-level features and create the \nhigh-level abstraction, which represents the last-stage layers as in a typical neural net-\nwork. The classification scores are generated using the ending layer [e.g. support vector \nmachines (SVMs) or softmax]. For a given instance, every score represents the probabil-\nity of a specific class.\nBenefits of employing CNNs\nThe benefits of using CNNs over other traditional neural networks in the computer \nvision environment are listed as follows: \n1.\t The main reason to consider CNN is the weight sharing feature, which reduces the \nnumber of trainable network parameters and in turn helps the network to enhance \ngeneralization and to avoid overfitting.\n2.\t Concurrently learning the feature extraction layers and the classification layer causes \nthe model output to be both highly organized and highly reliant on the extracted fea-\ntures.\n(1)\nhk = f (W k ∗x + bk)\nFig. 7  An example of CNN architecture for image classification\n\n\nPage 15 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n3.\t Large-scale network implementation is much easier with CNN than with other neu-\nral networks.\nCNN layers\nThe CNN architecture consists of a number of layers (or so-called multi-building blocks). \nEach layer in the CNN architecture, including its function, is described in detail below. \n1.\t Convolutional Layer: In CNN architecture, the most significant component is the \nconvolutional layer. It consists of a collection of convolutional filters (so-called ker-\nnels). The input image, expressed as N-dimensional metrics, is convolved with these \nfilters to generate the output feature map.\n•\t Kernel definition: A grid of discrete numbers or values describes the kernel. \nEach value is called the kernel weight. Random numbers are assigned to act as \nthe weights of the kernel at the beginning of the CNN training process. In addi-\ntion, there are several different methods used to initialize the weights. Next, these \nweights are adjusted at each training era; thus, the kernel learns to extract signifi-\ncant features.\n•\t Convolutional Operation: Initially, the CNN input format is described. The \nvector format is the input of the traditional neural network, while the multi-\nchanneled image is the input of the CNN. For instance, single-channel is the \nformat of the gray-scale image, while the RGB image format is three-channeled. \nTo understand the convolutional operation, let us take an example of a 4 × 4 \ngray-scale image with a 2 × 2 random weight-initialized kernel. First, the ker-\nnel slides over the whole image horizontally and vertically. In addition, the dot \nproduct between the input image and the kernel is determined, where their \ncorresponding values are multiplied and then summed up to create a single sca-\nlar value, calculated concurrently. The whole process is then repeated until no \nfurther sliding is possible. Note that the calculated dot product values represent \nthe feature map of the output. Figure 8 graphically illustrates the primary cal-\nculations executed at each step. In this figure, the light green color represents \nthe 2 × 2 kernel, while the light blue color represents the similar size area of the \ninput image. Both are multiplied; the end result after summing up the resulting \nproduct values (marked in a light orange color) represents an entry value to the \noutput feature map.\n\t\nHowever, padding to the input image is not applied in the previous example, \nwhile a stride of one (denoted for the selected step-size over all vertical or hori-\nzontal locations) is applied to the kernel. Note that it is also possible to use \nanother stride value. In addition, a feature map of lower dimensions is obtained \nas a result of increasing the stride value.\n\t\nOn the other hand, padding is highly significant to determining border size \ninformation related to the input image. By contrast, the border side-features \nmoves carried away very fast. By applying padding, the size of the input image \n\n\nPage 16 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nwill increase, and in turn, the size of the output feature map will also increase. \nCore Benefits of Convolutional Layers.\n•\t Sparse Connectivity: Each neuron of a layer in FC neural networks links with \nall neurons in the following layer. By contrast, in CNNs, only a few weights are \navailable between two adjacent layers. Thus, the number of required weights or \nconnections is small, while the memory required to store these weights is also \nFig. 8  The primary calculations executed at each step of convolutional layer\n\n\nPage 17 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nsmall; hence, this approach is memory-effective. In addition, matrix operation \nis computationally much more costly than the dot (.) operation in CNN.\n•\t Weight Sharing: There are no allocated weights between any two neurons of \nneighboring layers in CNN, as the whole weights operate with one and all \npixels of the input matrix. Learning a single group of weights for the whole \ninput will significantly decrease the required training time and various costs, \nas it is not necessary to learn additional weights for each neuron.\n2.\t Pooling Layer: The main task of the pooling layer is the sub-sampling of the feature \nmaps. These maps are generated by following the convolutional operations. In other \nwords, this approach shrinks large-size feature maps to create smaller feature maps. \nConcurrently, it maintains the majority of the dominant information (or features) in \nevery step of the pooling stage. In a similar manner to the convolutional operation, \nboth the stride and the kernel are initially size-assigned before the pooling operation \nis executed. Several types of pooling methods are available for utilization in various \npooling layers. These methods include tree pooling, gated pooling, average pooling, \nmin pooling, max pooling, global average pooling (GAP), and global max pooling. \nThe most familiar and frequently utilized pooling methods are the max, min, and \nGAP pooling. Figure 9 illustrates these three pooling operations.\n\t\nSometimes, the overall CNN performance is decreased as a result; this represents \nthe main shortfall of the pooling layer, as this layer helps the CNN to determine \nwhether or not a certain feature is available in the particular input image, but focuses \nexclusively on ascertaining the correct location of that feature. Thus, the CNN model \nmisses the relevant information.\n3.\t Activation Function (non-linearity) Mapping the input to the output is the core func-\ntion of all types of activation function in all types of neural network. The input value \nis determined by computing the weighted summation of the neuron input along with \nits bias (if present). This means that the activation function makes the decision as to \nwhether or not to fire a neuron with reference to a particular input by creating the \ncorresponding output.\nFig. 9  Three types of pooling operations\n\n\nPage 18 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nNon-linear activation layers are employed after all layers with weights (so-called \nlearnable layers, such as FC layers and convolutional layers) in CNN architecture. \nThis non-linear performance of the activation layers means that the mapping of input \nto output will be non-linear; moreover, these layers give the CNN the ability to learn \nextra-complicated things. The activation function must also have the ability to differ-\nentiate, which is an extremely significant feature, as it allows error back-propagation \nto be used to train the network. The following types of activation functions are most \ncommonly used in CNN and other deep neural networks.\n•\t Sigmoid: The input of this activation function is real numbers, while the output is \nrestricted to between zero and one. The sigmoid function curve is S-shaped and \ncan be represented mathematically by Eq. 2. \n•\t Tanh: It is similar to the sigmoid function, as its input is real numbers, but the out-\nput is restricted to between − 1 and 1. Its mathematical representation is in Eq. 3. \n•\t ReLU: The mostly commonly used function in the CNN context. It converts the \nwhole values of the input to positive numbers. Lower computational load is the \nmain benefit of ReLU over the others. Its mathematical representation is in Eq. 4. \n Occasionally, a few significant issues may occur during the use of ReLU. For \ninstance, consider an error back-propagation algorithm with a larger gradient \nflowing through it. Passing this gradient within the ReLU function will update the \nweights in a way that makes the neuron certainly not activated once more. This \nissue is referred to as “Dying ReLU”. Some ReLU alternatives exist to solve such \nissues. The following discusses some of them.\n•\t Leaky ReLU: Instead of ReLU down-scaling the negative inputs, this activation \nfunction ensures these inputs are never ignored. It is employed to solve the Dying \nReLU problem. Leaky ReLU can be represented mathematically as in Eq. 5. \n Note that the leak factor is denoted by m. It is commonly set to a very small value, \nsuch as 0.001.\n•\t Noisy ReLU: This function employs a Gaussian distribution to make ReLU noisy. \nIt can be represented mathematically as in Eq. 6. \n(2)\nf (x)sigm =\n1\n1 + e−x\n(3)\nf (x)tanh = ex −e−x\nex + e−x\n(4)\nf (x)ReLU = max(0, x)\n(5)\nf (x)LeakyReLU =\n\u001f\nx,\nif\nx > 0\nmx, x ≤0\n\u001e\n(6)\nf (x)NoisyReLU = max(x + Y ), with Y ∼N(0, σ(x))\n\n\nPage 19 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n•\t Parametric Linear Units: This is mostly the same as Leaky ReLU. The main differ-\nence is that the leak factor in this function is updated through the model training \nprocess. The parametric linear unit can be represented mathematically as in Eq. 7. \n Note that the learnable weight is denoted as a.\n4.\t Fully Connected Layer: Commonly, this layer is located at the end of each CNN \narchitecture. Inside this layer, each neuron is connected to all neurons of the pre-\nvious layer, the so-called Fully Connected (FC) approach. It is utilized as the CNN \nclassifier. It follows the basic method of the conventional multiple-layer perceptron \nneural network, as it is a type of feed-forward ANN. The input of the FC layer comes \nfrom the last pooling or convolutional layer. This input is in the form of a vector, \nwhich is created from the feature maps after flattening. The output of the FC layer \nrepresents the final CNN output, as illustrated in Fig. 10.\n5.\t Loss Functions: The previous section has presented various layer-types of CNN \narchitecture. In addition, the final classification is achieved from the output layer, \nwhich represents the last layer of the CNN architecture. Some loss functions are uti-\nlized in the output layer to calculate the predicted error created across the training \nsamples in the CNN model. This error reveals the difference between the actual out-\nput and the predicted one. Next, it will be optimized through the CNN learning pro-\ncess.\n\t\nHowever, two parameters are used by the loss function to calculate the error. The \nCNN estimated output (referred to as the prediction) is the first parameter. The \nactual output (referred to as the label) is the second parameter. Several types of loss \nfunction are employed in various problem types. The following concisely explains \nsome of the loss function types. \n(7)\nf (x)ParametricLinear =\n\u001f\nx, if x > 0\nax,\nx ≤0\n\u001e\nFig. 10  Fully connected layer\n\n\nPage 20 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n(a)\t Cross-Entropy or Softmax Loss Function: This function is commonly employed \nfor measuring the CNN model performance. It is also referred to as the log \nloss function. Its output is the probability p ∈{0, 1} . In addition, it is usually \nemployed as a substitution of the square error loss function in multi-class clas-\nsification problems. In the output layer, it employs the softmax activations to \ngenerate the output within a probability distribution. The mathematical repre-\nsentation of the output class probability is Eq. 8. \n Here, eai represents the non-normalized output from the preceding layer, while \nN represents the number of neurons in the output layer. Finally, the mathemat-\nical representation of cross-entropy loss function is Eq. 9. \n(b)\t Euclidean Loss Function: This function is widely used in regression problems. \nIn addition, it is also the so-called mean square error. The mathematical expres-\nsion of the estimated Euclidean loss is Eq. 10. \n(c)\t Hinge Loss Function: This function is commonly employed in problems related \nto binary classification. This problem relates to maximum-margin-based clas-\nsification; this is mostly important for SVMs, which use the hinge loss function, \nwherein the optimizer attempts to maximize the margin around dual objective \nclasses. Its mathematical formula is Eq. 11. \n The margin m is commonly set to 1. Moreover, the predicted output is denoted \nas pi , while the desired output is denoted as yi.\nRegularization to CNN\nFor CNN models, over-fitting represents the central issue associated with obtaining \nwell-behaved generalization. The model is entitled over-fitted in cases where the model \nexecutes especially well on training data and does not succeed on test data (unseen data) \nwhich is more explained in the latter section. An under-fitted model is the opposite; this \ncase occurs when the model does not learn a sufficient amount from the training data. \nThe model is referred to as “just-fitted” if it executes well on both training and testing \ndata. These three types are illustrated in Fig. 11. Various intuitive concepts are used to \nhelp the regularization to avoid over-fitting; more details about over-fitting and under-\nfitting are discussed in latter sections. \n(8)\npi =\neai\n\u001fN\nk=1 ea\nk\n(9)\nH(p, y) = −\n\u001f\ni\nyi log(pi)\nwhere\ni ∈[1, N]\n(10)\nH(p, y) =\n1\n2N\nN\n\u001f\ni=1\n(pi −yi)2\n(11)\nH(p, y) =\nN\n\u001f\ni=1\nmax(0, m −(2yi −1)pi)\n\n\nPage 21 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n1.\t Dropout: This is a widely utilized technique for generalization. During each training \nepoch, neurons are randomly dropped. In doing this, the feature selection power is \ndistributed equally across the whole group of neurons, as well as forcing the model to \nlearn different independent features. During the training process, the dropped neu-\nron will not be a part of back-propagation or forward-propagation. By contrast, the \nfull-scale network is utilized to perform prediction during the testing process.\n2.\t Drop-Weights: This method is highly similar to dropout. In each training epoch, the \nconnections between neurons (weights) are dropped rather than dropping the neu-\nrons; this represents the only difference between drop-weights and dropout.\n3.\t Data Augmentation: Training the model on a sizeable amount of data is the easiest \nway to avoid over-fitting. To achieve this, data augmentation is used. Several tech-\nniques are utilized to artificially expand the size of the training dataset. More details \ncan be found in the latter section, which describes the data augmentation techniques.\n4.\t Batch Normalization: This method ensures the performance of the output activations \n[81]. This performance follows a unit Gaussian distribution. Subtracting the mean \nand dividing by the standard deviation will normalize the output at each layer. While \nit is possible to consider this as a pre-processing task at each layer in the network, it \nis also possible to differentiate and to integrate it with other networks. In addition, it \nis employed to reduce the “internal covariance shift” of the activation layers. In each \nlayer, the variation in the activation distribution defines the internal covariance shift. \nThis shift becomes very high due to the continuous weight updating through train-\ning, which may occur if the samples of the training data are gathered from numer-\nous dissimilar sources (for example, day and night images). Thus, the model will con-\nsume extra time for convergence, and in turn, the time required for training will also \nincrease. To resolve this issue, a layer representing the operation of batch normaliza-\ntion is applied in the CNN architecture.\n\t\nThe advantages of utilizing batch normalization are as follows:\n•\t It prevents the problem of vanishing gradient from arising.\n•\t It can effectively control the poor weight initialization.\n•\t It significantly reduces the time required for network convergence (for large-scale \ndatasets, this will be extremely useful).\n•\t It struggles to decrease training dependency across hyper-parameters.\nFig. 11  Over-fitting and under-fitting issues\n\n\nPage 22 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n•\t Chances of over-fitting are reduced, since it has a minor influence on regulariza-\ntion.\nOptimizer selection\nThis section discusses the CNN learning process. Two major issues are included in the \nlearning process: the first issue is the learning algorithm selection (optimizer), while the \nsecond issue is the use of many enhancements (such as AdaDelta, Adagrad, and momen-\ntum) along with the learning algorithm to enhance the output.\nLoss functions, which are founded on numerous learnable parameters (e.g. biases, \nweights, etc.) or minimizing the error (variation between actual and predicted output), \nare the core purpose of all supervised learning algorithms. The techniques of gradient-\nbased learning for a CNN network appear as the usual selection. The network parame-\nters should always update though all training epochs, while the network should also look \nfor the locally optimized answer in all training epochs in order to minimize the error.\nThe learning rate is defined as the step size of the parameter updating. The train-\ning epoch represents a complete repetition of the parameter update that involves the \ncomplete training dataset at one time. Note that it needs to select the learning rate \nwisely so that it does not influence the learning process imperfectly, although it is a \nhyper-parameter.\nGradient Descent or Gradient-based learning algorithm: To minimize the train-\ning error, this algorithm repetitively updates the network parameters through every \ntraining epoch. More specifically, to update the parameters correctly, it needs to com-\npute the objective function gradient (slope) by applying a first-order derivative with \nrespect to the network parameters. Next, the parameter is updated in the reverse \ndirection of the gradient to reduce the error. The parameter updating process is per-\nformed though network back-propagation, in which the gradient at every neuron is \nback-propagated to all neurons in the preceding layer. The mathematical representa-\ntion of this operation is as Eq. 12.\nThe final weight in the current training epoch is denoted by wijt , while the weight in the \npreceding (t −1) training epoch is denoted wijt−1 . The learning rate is η and the predic-\ntion error is E. Different alternatives of the gradient-based learning algorithm are avail-\nable and commonly employed; these include the following: \n1.\t Batch Gradient Descent: During the execution of this technique [82], the network \nparameters are updated merely one time behind considering all training datasets via \nthe network. In more depth, it calculates the gradient of the whole training set and \nsubsequently uses this gradient to update the parameters. For a small-sized dataset, \nthe CNN model converges faster and creates an extra-stable gradient using BGD. \nSince the parameters are changed only once for every training epoch, it requires a \nsubstantial amount of resources. By contrast, for a large training dataset, additional \n(12)\nwijt = wijt−1 −\u001fwijt,\n\u001fwijt = η ∗∂E\n∂wij\n\n\nPage 23 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\ntime is required for converging, and it could converge to a local optimum (for non-\nconvex instances).\n2.\t Stochastic Gradient Descent: The parameters are updated at each training sample in \nthis technique [83]. It is preferred to arbitrarily sample the training samples in every \nepoch in advance of training. For a large-sized training dataset, this technique is both \nmore memory-effective and much faster than BGD. However, because it is frequently \nupdated, it takes extremely noisy steps in the direction of the answer, which in turn \ncauses the convergence behavior to become highly unstable.\n3.\t Mini-batch Gradient Descent: In this approach, the training samples are partitioned \ninto several mini-batches, in which every mini-batch can be considered an under-\nsized collection of samples with no overlap between them [84]. Next, parameter \nupdating is performed following gradient computation on every mini-batch. The \nadvantage of this method comes from combining the advantages of both BGD and \nSGD techniques. Thus, it has a steady convergence, more computational efficiency \nand extra memory effectiveness. The following describes several enhancement tech-\nniques in gradient-based learning algorithms (usually in SGD), which further power-\nfully enhance the CNN training process.\n4.\t Momentum: For neural networks, this technique is employed in the objective func-\ntion. It enhances both the accuracy and the training speed by summing the com-\nputed gradient at the preceding training step, which is weighted via a factor \u001f (known \nas the momentum factor). However, it therefore simply becomes stuck in a local \nminimum rather than a global minimum. This represents the main disadvantage of \ngradient-based learning algorithms. Issues of this kind frequently occur if the issue \nhas no convex surface (or solution space).\n\t\nTogether with the learning algorithm, momentum is used to solve this issue, which \ncan be expressed mathematically as in Eq. 13. \n The weight increment in the current t′th training epoch is denoted as \u001fwijt , while \nη is the learning rate, and the weight increment in the preceding (t −1)′th training \nepoch. The momentum factor value is maintained within the range 0 to 1; in turn, \nthe step size of the weight updating increases in the direction of the bare minimum \nto minimize the error. As the value of the momentum factor becomes very low, the \nmodel loses its ability to avoid the local bare minimum. By contrast, as the momen-\ntum factor value becomes high, the model develops the ability to converge much \nmore rapidly. If a high value of momentum factor is used together with LR, then the \nmodel could miss the global bare minimum by crossing over it.\n\t\nHowever, when the gradient varies its direction continually throughout the training \nprocess, then the suitable value of the momentum factor (which is a hyper-parame-\nter) causes a smoothening of the weight updating variations.\n5.\t Adaptive Moment Estimation (Adam): It is another optimization technique or learn-\ning algorithm that is widely used. Adam [85] represents the latest trends in deep \n(13)\n\u001fwijt =\n\u001f\nη ∗∂E\n∂wij\n\u001e\n+ (\u001f ∗\u001fwijt−1)\n\n\nPage 24 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nlearning optimization. This is represented by the Hessian matrix, which employs a \nsecond-order derivative. Adam is a learning strategy that has been designed specifi-\ncally for training deep neural networks. More memory efficient and less computa-\ntional power are two advantages of Adam. The mechanism of Adam is to calculate \nadaptive LR for each parameter in the model. It integrates the pros of both Momen-\ntum and RMSprop. It utilizes the squared gradients to scale the learning rate as \nRMSprop and it is similar to the momentum by using the moving average of the gra-\ndient. The equation of Adam is represented in Eq. 14. \nDesign of algorithms (backpropagation)\nLet’s start with a notation that refers to weights in the network unambiguously. We \ndenote wh\nij to be the weight for the connection from ith input or (neuron at (h −1)th) \nto the jt neuron in the hth layer. So, Fig. 12 shows the weight on a connection from the \nneuron in the first layer to another neuron in the next layer in the network.\nWhere w2\n11 has represented the weight from the first neuron in the first layer to the \nfirst neuron in the second layer, based on that the second weight for the same neuron \nwill be w2\n21 which means is the weight comes from the second neuron in the previous \nlayer to the first layer in the next layer which is the second in this net. Regarding the \nbias, since the bias is not the connection between the neurons for the layers, so it is \neasily handled each neuron must have its own bias, some network each layer has a \ncertain bias. It can be seen from the above net that each layer has its own bias. Each \nnetwork has the parameters such as the no of the layer in the net, the number of the \nneurons in each layer, no of the weight (connection) between the layers, the no of \nconnection can be easily determined based on the no of neurons in each layer, for \nexample, if there are ten input fully connect with two neurons in the next layer then \n(14)\nwijt = wijt−1 −\nη\n\u001f\n\u001f\nE[δ2]t+ ∈\n∗\u001f\nE[δ2]t\nFig. 12  MLP structure\n\n\nPage 25 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nthe number of connection between them is (10 ∗2 = 20 connection, weights), how \nthe error is defined, and the weight is updated, we will imagine there is there are two \nlayers in our neural network,\nwhere d is the label of induvial input ith and y is the output of the same individual input. \nBackpropagation is about understanding how to change the weights and biases in a net-\nwork based on the changes of the cost function (Error). Ultimately, this means comput-\ning the partial derivatives ∂E/∂wh\nij and ∂E/∂bh\nj . But to compute those, a local variable is \nintroduced, δ1\nj  which is called the local error in the jth neuron in the hth layer. Based on \nthat local error Backpropagation will give the procedure to compute ∂E/∂wh\nij and ∂E/∂bh\nj  \nhow the error is defined, and the weight is updated, we will imagine there is there are \ntwo layers in our neural network that is shown in Fig. 13.\nOutput error for δ1\nj  each 1 = 1 : L where L is no. of neuron in output\nwhere e(k) is the error of the epoch k as shown in Eq. (2) and ϑ′\u001f\nvj(k)\n\u001e\n is the derivate of \nthe activation function for vj at the output.\nBackpropagate the error at all the rest layer except the output\nwhere δ1\nj (k) is the output error and wh+1\njl\n(k) is represented the weight after the layer \nwhere the error need to obtain.\nAfter finding the error at each neuron in each layer, now we can update the weight \nin each layer based on Eqs. (16) and (17).\nImproving performance of CNN\nBased on our experiments in different DL applications [86–88]. We can conclude the \nmost active solutions that may improve the performance of CNN are:\n(15)\nerror = 1/2\n\u001f\ndi −yi\n\u001e2\n(16)\nδ1\nj (k) = (−1)e(k)ϑ′\u001f\nvj(k)\n\u001e\n(17)\nδh\nj (k) = ϑ′\u001f\nvj(k)\n\u001e\nL\n\u001d\nl=1\nδ1\nj\nwh+1\njl\n(k)\nFig. 13  Neuron activation functions\n\n\nPage 26 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n•\t Expand the dataset with data augmentation or use transfer learning (explained in lat-\nter sections).\n•\t Increase the training time.\n•\t Increase the depth (or width) of the model.\n•\t Add regularization.\n•\t Increase hyperparameters tuning.\nCNN architectures\nOver the last 10 years, several CNN architectures have been presented [21, 26]. Model \narchitecture is a critical factor in improving the performance of different applications. \nVarious modifications have been achieved in CNN architecture from 1989 until today. \nSuch modifications include structural reformulation, regularization, parameter optimi-\nzations, etc. Conversely, it should be noted that the key upgrade in CNN performance \noccurred largely due to the processing-unit reorganization, as well as the development \nof novel blocks. In particular, the most novel developments in CNN architectures were \nperformed on the use of network depth. In this section, we review the most popular \nCNN architectures, beginning from the AlexNet model in 2012 and ending at the High-\nResolution (HR) model in 2020. Studying these architectures features (such as input size, \ndepth, and robustness) is the key to help researchers to choose the suitable architecture \nfor the their target task. Table 2 presents the brief overview of CNN architectures.\nAlexNet\nThe history of deep CNNs began with the appearance of LeNet [89] (Fig. 14). At that \ntime, the CNNs were restricted to handwritten digit recognition tasks, which cannot \nbe scaled to all image classes. In deep CNN architecture, AlexNet is highly respected \n[30], as it achieved innovative results in the fields of image recognition and classification. \nKrizhevesky et  al. [30] first proposed AlexNet and consequently improved the CNN \nlearning ability by increasing its depth and implementing several parameter optimiza-\ntion strategies. Figure 15 illustrates the basic design of the AlexNet architecture.\nThe learning ability of the deep CNN was limited at this time due to hardware \nrestrictions. To overcome these hardware limitations, two GPUs (NVIDIA GTX 580) \nwere used in parallel to train AlexNet. Moreover, in order to enhance the applicabil-\nity of the CNN to different image categories, the number of feature extraction stages \nwas increased from five in LeNet to seven in AlexNet. Regardless of the fact that depth \nenhances generalization for several image resolutions, it was in fact overfitting that rep-\nresented the main drawback related to the depth. Krizhevesky et al. used Hinton’s idea to \naddress this problem [90, 91]. To ensure that the features learned by the algorithm were \nextra robust, Krizhevesky et  al.’s algorithm randomly passes over several transforma-\ntional units throughout the training stage. Moreover, by reducing the vanishing gradient \nproblem, ReLU [92] could be utilized as a non-saturating activation function to enhance \nthe rate of convergence [93]. Local response normalization and overlapping subsampling \nwere also performed to enhance the generalization by decreasing the overfitting. To \nimprove on the performance of previous networks, other modifications were made by \nusing large-size filters (5 × 5 and 11 × 11) in the earlier layers. AlexNet has considerable \n\n\nPage 27 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nTable 2  Brief overview of CNN architectures\nModel\nMain finding\nDepth\nDataset\nError rate\nInput size\nYear\nAlexNet\nUtilizes Dropout \nand ReLU\n8\nImageNet\n16.4\n227 × 227 × 3\n2012\nNIN\nNew layer, called \n‘mlpconv’, utilizes \nGAP\n3\nCIFAR-10, CIFAR-\n100, MNIST\n10.41, 35.68, 0.45\n32 × 32 × 3\n2013\nZfNet\nVisualization idea \nof middle layers\n8\nImageNet\n11.7\n224 × 224 × 3\n2014\nVGG\nIncreased depth, \nsmall filter size\n16, 19\nImageNet\n7.3\n224 × 224 × 3\n2014\nGoogLeNet\nIncreased \ndepth,block \nconcept, differ‑\nent filter size, \nconcatenation \nconcept\n22\nImageNet\n6.7\n224 × 224 × 3\n2015\nInception-V3\nUtilizes small \nfiltersize, better \nfeature represen‑\ntation\n48\nImageNet\n3.5\n229 × 229 × 3\n2015\nHighway\nPresented the mul‑\ntipath concept\n19, 32\nCIFAR-10\n7.76\n32 × 32 × 3\n2015\nInception-V4\nDivided transform \nand integration \nconcepts\n70\nImageNet\n3.08\n229 × 229 × 3\n2016\nResNet\nRobust against \noverfitting due \nto symmetry \nmapping-based \nskip links\n152\nImageNet\n3.57\n224 × 224 × 3\n2016\nInception-ResNet-\nv2\nIntroduced the \nconcept of \nresidual links\n164\nImageNet\n3.52\n229 × 229 × 3\n2016\nFractalNet\nIntroduced the \nconcept of \nDrop-Path as \nregularization\n40,80\nCIFAR-10\n4.60\n32 × 32 × 3\n2016\nCIFAR-100\n18.85\nWideResNet\nDecreased the \ndepth and \nincreased the \nwidth\n28\nCIFAR-10\n3.89\n32 × 32 × 3\n2016\nCIFAR-100\n18.85\nXception\nA depthwise con‑\nvolutionfollowed \nby a pointwise \nconvolution\n71\nImageNet\n0.055\n229 × 229 × 3\n2017\nResidual attention \nneural network\nPresented the \nattention tech‑\nnique\n452\nCIFAR-10, CIFAR-\n100\n3.90, 20.4\n40 × 40 × 3\n2017\nSqueeze-and-exci‑\ntation networks\nModeled inter‑\ndependencies \nbetween chan‑\nnels\n152\nImageNet\n2.25\n229 × 229 × 3\n2017\n224 × 224 × 3\n320 × 320 × 3\nDenseNet\nBlocks of layers; \nlayers connected \nto each other\n201\nCIFAR-10, CIFAR-\n100,ImageNet\n3.46, 17.18, 5.54\n224 × 224 × 3\n2017\nCompetitive \nsqueeze and \nexcitation net‑\nwork\nBoth residual and \nidentity map‑\npings utilized \nto rescale the \nchannel\n152\nCIFAR-10\n3.58\n32 × 32 × 3\n2018\nCIFAR-100\n18.47\n\n\nPage 28 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nsignificance in the recent CNN generations, as well as beginning an innovative research \nera in CNN applications.\nNetwork‑in‑network\nThis network model, which has some slight differences from the preceding models, \nintroduced two innovative concepts [94]. The first was employing multiple layers of \nTable 2  (continued)\nModel\nMain finding\nDepth\nDataset\nError rate\nInput size\nYear\nMobileNet-v2\nInverted residual \nstructure\n53\nImageNet\n–\n224 × 224 × 3\n2018\nCapsuleNet\nPays attention to \nspecial relation‑\nships between \nfeatures\n3\nMNIST\n0.00855\n28 × 28 × 1\n2018\nHRNetV2\nHigh-resolution \nrepresentations\n–\nImageNet\n5.4\n224 × 224 × 3\n2020\nFig. 14  The architecture of LeNet\nFig. 15  The architecture of AlexNet\n\n\nPage 29 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nperception convolution. These convolutions are executed using a 1×1 filter, which sup-\nports the addition of extra nonlinearity in the networks. Moreover, this supports enlarg-\ning the network depth, which may later be regularized using dropout. For DL models, \nthis idea is frequently employed in the bottleneck layer. As a substitution for a FC layer, \nthe GAP is also employed, which represents the second novel concept and enables a sig-\nnificant reduction in the number of model parameters. In addition, GAP considerably \nupdates the network architecture. Generating a final low-dimensional feature vector \nwith no reduction in the feature maps dimension is possible when GAP is used on a \nlarge feature map [95, 96]. Figure 16 shows the structure of the network.\nZefNet\nBefore 2013, the CNN learning mechanism was basically constructed on a trial-and-\nerror basis, which precluded an understanding of the precise purpose following the \nenhancement. This issue restricted the deep CNN performance on convoluted images. In \nresponse, Zeiler and Fergus introduced DeconvNet (a multilayer de-convolutional neu-\nral network) in 2013 [97]. This method later became known as ZefNet, which was devel-\noped in order to quantitively visualize the network. Monitoring the CNN performance \nvia understanding the neuron activation was the purpose of the network activity visuali-\nzation. However, Erhan et al. utilized this exact concept to optimize deep belief network \n(DBN) performance by visualizing the features of the hidden layers [98]. Moreover, in \naddition to this issue, Le et al. assessed the deep unsupervised auto-encoder (AE) per-\nformance by visualizing the created classes of the image using the output neurons [99]. \nBy reversing the operation order of the convolutional and pooling layers, DenconvNet \noperates like a forward-pass CNN. Reverse mapping of this kind launches the convolu-\ntional layer output backward to create visually observable image shapes that accordingly \ngive the neural interpretation of the internal feature representation learned at each layer \n[100]. Monitoring the learning schematic through the training stage was the key con-\ncept underlying ZefNet. In addition, it utilized the outcomes to recognize an ability issue \ncoupled with the model. This concept was experimentally proven on AlexNet by apply-\ning DeconvNet. This indicated that only certain neurons were working, while the others \nwere out of action in the first two layers of the network. Furthermore, it indicated that \nthe features extracted via the second layer contained aliasing objects. Thus, Zeiler and \nFergus changed the CNN topology due to the existence of these outcomes. In addition, \nthey executed parameter optimization, and also exploited the CNN learning by decreas-\ning the stride and the filter sizes in order to retain all features of the initial two convo-\nlutional layers. An improvement in performance was accordingly achieved due to this \nFig. 16  The architecture of network-in-network\n\n\nPage 30 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nrearrangement in CNN topology. This rearrangement proposed that the visualization of \nthe features could be employed to identify design weaknesses and conduct appropriate \nparameter alteration. Figure 17 shows the structure of the network.\nVisual geometry group (VGG)\nAfter CNN was determined to be effective in the field of image recognition, an easy and \nefficient design principle for CNN was proposed by Simonyan and Zisserman. This inno-\nvative design was called Visual Geometry Group (VGG). A multilayer model [101], it fea-\ntured nineteen more layers than ZefNet [97] and AlexNet [30] to simulate the relations \nof the network representational capacity in depth. Conversely, in the 2013-ILSVRC com-\npetition, ZefNet was the frontier network, which proposed that filters with small sizes \ncould enhance the CNN performance. With reference to these results, VGG inserted a \nlayer of the heap of 3 × 3 filters rather than the 5 × 5 and 11 × 11 filters in ZefNet. This \nshowed experimentally that the parallel assignment of these small-size filters could pro-\nduce the same influence as the large-size filters. In other words, these small-size filters \nmade the receptive field similarly efficient to the large-size filters (7 × 7 and 5 × 5) . By \ndecreasing the number of parameters, an extra advantage of reducing computational \ncomplication was achieved by using small-size filters. These outcomes established a \nnovel research trend for working with small-size filters in CNN. In addition, by inserting \n1 × 1 convolutions in the middle of the convolutional layers, VGG regulates the network \ncomplexity. It learns a linear grouping of the subsequent feature maps. With respect \nto network tuning, a max pooling layer [102] is inserted following the convolutional \nlayer, while padding is implemented to maintain the spatial resolution. In general, VGG \nobtained significant results for localization problems and image classification. While it \ndid not achieve first place in the 2014-ILSVRC competition, it acquired a reputation due \nto its enlarged depth, homogenous topology, and simplicity. However, VGG’s computa-\ntional cost was excessive due to its utilization of around 140 million parameters, which \nrepresented its main shortcoming. Figure 18 shows the structure of the network.\nGoogLeNet\nIn the 2014-ILSVRC competition, GoogleNet (also called Inception-V1) emerged as the \nwinner [103]. Achieving high-level accuracy with decreased computational cost is the \ncore aim of the GoogleNet architecture. It proposed a novel inception block (module) \nconcept in the CNN context, since it combines multiple-scale convolutional transfor-\nmations by employing merge, transform, and split functions for feature extraction. Fig-\nure 19 illustrates the inception block architecture. This architecture incorporates filters \nFig. 17  The architecture of ZefNet\n\n\nPage 31 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nof different sizes ( 5 × 5, 3 × 3, and 1 × 1 ) to capture channel information together with \nspatial information at diverse ranges of spatial resolution. The common convolutional \nlayer of GoogLeNet is substituted by small blocks using the same concept of network-\nin-network (NIN) architecture [94], which replaced each layer with a micro-neural net-\nwork. The GoogLeNet concepts of merge, transform, and split were utilized, supported \nby attending to an issue correlated with different learning types of variants existing in a \nsimilar class of several images. The motivation of GoogLeNet was to improve the effi-\nciency of CNN parameters, as well as to enhance the learning capacity. In addition, it \nregulates the computation by inserting a 1 × 1 convolutional filter, as a bottleneck layer, \nahead of using large-size kernels. GoogleNet employed sparse connections to overcome \nthe redundant information problem. It decreased cost by neglecting the irrelevant chan-\nnels. It should be noted here that only some of the input channels are connected to some \nof the output channels. By employing a GAP layer as the end layer, rather than utilizing a \nFC layer, the density of connections was decreased. The number of parameters was also \nsignificantly decreased from 40 to 5 million parameters due to these parameter tunings. \nThe additional regularity factors used included the employment of RmsProp as opti-\nmizer and batch normalization [104]. Furthermore, GoogleNet proposed the idea of aux-\niliary learners to speed up the rate of convergence. Conversely, the main shortcoming of \nGoogleNet was its heterogeneous topology; this shortcoming requires adaptation from \none module to another. Other shortcomings of GoogleNet include the representation \nFig. 18  The architecture of VGG\nFig. 19  The basic structure of Google Block\n\n\nPage 32 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \njam, which substantially decreased the feature space in the following layer, and in turn \noccasionally leads to valuable information loss.\nHighway network\nIncreasing the network depth enhances its performance, mainly for complicated tasks. \nBy contrast, the network training becomes difficult. The presence of several layers in \ndeeper networks may result in small gradient values of the back-propagation of error at \nlower layers. In 2015, Srivastava et al. [105] suggested a novel CNN architecture, called \nHighway Network, to overcome this issue. This approach is based on the cross-connec-\ntivity concept. The unhindered information flow in Highway Network is empowered by \ninstructing two gating units inside the layer. The gate mechanism concept was motivated \nby LSTM-based RNN [106, 107]. The information aggregation was conducted by merg-\ning the information of the ıth −k layers with the next ıth layer to generate a regulariza-\ntion impact, which makes the gradient-based training of the deeper network very simple. \nThis empowers the training of networks with more than 100 layers, such as a deeper \nnetwork of 900 layers with the SGD algorithm. A Highway Network with a depth of fifty \nlayers presented an improved rate of convergence, which is better than thin and deep \narchitectures at the same time [108]. By contrast, [69] empirically demonstrated that \nplain Net performance declines when more than ten hidden layers are inserted. It should \nbe noted that even a Highway Network 900 layers in depth converges much more rapidly \nthan the plain network.\nResNet\nHe et al. [37] developed ResNet (Residual Network), which was the winner of ILS-\nVRC 2015. Their objective was to design an ultra-deep network free of the vanishing \ngradient issue, as compared to the previous networks. Several types of ResNet were \ndeveloped based on the number of layers (starting with 34 layers and going up to 1202 \nlayers). The most common type was ResNet50, which comprised 49 convolutional lay-\ners plus a single FC layer. The overall number of network weights was 25.5 M, while \nthe overall number of MACs was 3.9 M. The novel idea of ResNet is its use of the \nbypass pathway concept, as shown in Fig. 20, which was employed in Highway Nets to \naddress the problem of training a deeper network in 2015. This is illustrated in Fig. 20, \nwhich contains the fundamental ResNet block diagram. This is a conventional feed-\nforward network plus a residual connection. The residual layer output can be identi-\nfied as the (l −1)th outputs, which are delivered from the preceding layer (xl −1) . \nAfter executing different operations [such as convolution using variable-size filters, \nor batch normalization, before applying an activation function like ReLU on (xl −1) ], \nthe output is F(xl −1) . The ending residual output is xl , which can be mathematically \nrepresented as in Eq. 18.\nThere are numerous basic residual blocks included in the residual network. Based on \nthe type of the residual network architecture, operations in the residual block are also \nchanged [37].\n(18)\nxl = F(xl −1) + xl −1\n\n\nPage 33 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nIn comparison to the highway network, ResNet presented shortcut connections \ninside layers to enable cross-layer connectivity, which are parameter-free and data-\nindependent. Note that the layers characterize non-residual functions when a gated \nshortcut is closed in the highway network. By contrast, the individuality shortcuts are \nnever closed, while the residual information is permanently passed in ResNet. Fur-\nthermore, ResNet has the potential to prevent the problems of gradient diminishing, \nas the shortcut connections (residual links) accelerate the deep network convergence. \nResNet was the winner of the 2015-ILSVRC championship with 152 layers of depth; \nthis represents 8 times the depth of VGG and 20 times the depth of AlexNet. In com-\nparison with VGG, it has lower computational complexity, even with enlarged depth.\nInception: ResNet and Inception‑V3/4\nSzegedy et  al. [103, 109, 110] proposed Inception-ResNet and Inception-V3/4 as \nupgraded types of Inception-V1/2. The concept behind Inception-V3 was to minimize \nthe computational cost with no effect on the deeper network generalization. Thus, \nSzegedy et  al. used asymmetric small-size filters ( 1 × 5 and 1 × 7 ) rather than large-\nsize filters ( 7 × 7 and 5 × 5 ); moreover, they utilized a bottleneck of 1 × 1 convolution \nprior to the large-size filters [110]. These changes make the operation of the traditional \nFig. 20  The block diagram for ResNet\n\n\nPage 34 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nconvolution very similar to cross-channel correlation. Previously, Lin et al. utilized the \n1 × 1 filter potential in NIN architecture [94]. Subsequently, [110] utilized the same \nidea in an intelligent manner. By using 1 × 1 convolutional operation in Inception-V3, \nthe input data are mapped into three or four isolated spaces, which are smaller than the \ninitial input spaces. Next, all of these correlations are mapped in these smaller spaces \nthrough common 5 × 5 or 3 × 3 convolutions. By contrast, in Inception-ResNet, Szegedy \net al. bring together the inception block and the residual learning power by replacing the \nfilter concatenation with the residual connection [111]. Szegedy et al. empirically dem-\nonstrated that Inception-ResNet (Inception-4 with residual connections) can achieve a \nsimilar generalization power to Inception-V4 with enlarged width and depth and with-\nout residual connections. Thus, it is clearly illustrated that using residual connections in \ntraining will significantly accelerate the Inception network training. Figure 21 shows The \nbasic block diagram for Inception Residual unit.\nDenseNet\nTo solve the problem of the vanishing gradient, DenseNet was presented, following the \nsame direction as ResNet and the Highway network [105, 111, 112]. One of the draw-\nbacks of ResNet is that it clearly conserves information by means of preservative indi-\nviduality transformations, as several layers contribute extremely little or no information. \nIn addition, ResNet has a large number of weights, since each layer has an isolated group \nof weights. DenseNet employed cross-layer connectivity in an improved approach to \naddress this problem [112–114]. It connected each layer to all layers in the network \nusing a feed-forward approach. Therefore, the feature maps of each previous layer were \nemployed to input into all of the following layers. In traditional CNNs, there are l con-\nnections between the previous layer and the current layer, while in DenseNet, there are \nl(l+1)\n2\n direct connections. DenseNet demonstrates the influence of cross-layer depth \nFig. 21  The basic block diagram for Inception Residual unit\n\n\nPage 35 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nwise-convolutions. Thus, the network gains the ability to discriminate clearly between \nthe added and the preserved information, since DenseNet concatenates the features of \nthe preceding layers rather than adding them. However, due to its narrow layer struc-\nture, DenseNet becomes parametrically high-priced in addition to the increased number \nof feature maps. The direct admission of all layers to the gradients via the loss function \nenhances the information flow all across the network. In addition, this includes a regu-\nlarizing impact, which minimizes overfitting on tasks alongside minor training sets. Fig-\nure 22 shows the architecture of DenseNet Network.\nResNext\nResNext is an enhanced version of the Inception Network [115]. It is also known as the \nAggregated Residual Transform Network. Cardinality, which is a new term presented by \n[115], utilized the split, transform, and merge topology in an easy and effective way. It \ndenotes the size of the transformation set as an extra dimension [116–118]. However, the \nInception network manages network resources more efficiently, as well as enhancing the \nlearning ability of the conventional CNN. In the transformation branch, different spatial \nembeddings (employing e.g. 5 × 5 , 3 × 3 , and 1 × 1 ) are used. Thus, customizing each \nlayer is required separately. By contrast, ResNext derives its characteristic features from \nResNet, VGG, and Inception. It employed the VGG deep homogenous topology with \nthe basic architecture of GoogleNet by setting 3 × 3 filters as spatial resolution inside \nthe blocks of split, transform, and merge. Figure 23 shows the ResNext building blocks. \nResNext utilized multi-transformations inside the blocks of split, transform, and merge, \nas well as outlining such transformations in cardinality terms. The performance is sig-\nnificantly improved by increasing the cardinality, as Xie et al. showed. The complexity \nFig. 22  The architecture of DenseNet Network (adopted from [112])\n\n\nPage 36 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nof ResNext was regulated by employing 1 × 1 filters (low embeddings) ahead of a 3 × 3 \nconvolution. By contrast, skipping connections are used for optimized training [115].\nWideResNet\nThe feature reuse problem is the core shortcoming related to deep residual networks, \nsince certain feature blocks or transformations contribute a very small amount to learn-\ning. Zagoruyko and Komodakis [119] accordingly proposed WideResNet to address this \nproblem. These authors advised that the depth has a supplemental influence, while the \nresidual units convey the core learning ability of deep residual networks. WideResNet \nutilized the residual block power via making the ResNet wider instead of deeper [37]. It \nenlarged the width by presenting an extra factor, k, which handles the network width. \nIn other words, it indicated that layer widening is a highly successful method of per-\nformance enhancement compared to deepening the residual network. While enhanced \nrepresentational capacity is achieved by deep residual networks, these networks also \nhave certain drawbacks, such as the exploding and vanishing gradient problems, feature \nreuse problem (inactivation of several feature maps), and the time-intensive nature of \nthe training. He et al. [37] tackled the feature reuse problem by including a dropout in \neach residual block to regularize the network in an efficient manner. In a similar manner, \nutilizing dropouts, Huang et al. [120] presented the stochastic depth concept to solve the \nslow learning and gradient vanishing problems. Earlier research was focused on increas-\ning the depth; thus, any small enhancement in performance required the addition of \nseveral new layers. When comparing the number of parameters, WideResNet has twice \nthat of ResNet, as an experimental study showed. By contrast, WideResNet presents an \nimproved method for training relative to deep networks [119]. Note that most architec-\ntures prior to residual networks (including the highly effective VGG and Inception) were \nwider than ResNet. Thus, wider residual networks were established once this was deter-\nmined. However, inserting a dropout between the convolutional layers (as opposed to \nwithin the residual block) made the learning more effective in WideResNet [121, 122].\nPyramidal Net\nThe depth of the feature map increases in the succeeding layer due to the deep stack-\ning of multi-convolutional layers, as shown in previous deep CNN architectures such as \nFig. 23  The basic block diagram for the ResNext building blocks\n\n\nPage 37 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nResNet, VGG, and AlexNet. By contrast, the spatial dimension reduces, since a sub-sam-\npling follows each convolutional layer. Thus, augmented feature representation is recom-\npensed by decreasing the size of the feature map. The extreme expansion in the depth \nof the feature map, alongside the spatial information loss, interferes with the learning \nability in the deep CNNs. ResNet obtained notable outcomes for the issue of image clas-\nsification. Conversely, deleting a convolutional block—in which both the number of \nchannel and spatial dimensions vary (channel depth enlarges, while spatial dimension \nreduces)—commonly results in decreased classifier performance. Accordingly, the sto-\nchastic ResNet enhanced the performance by decreasing the information loss accom-\npanying the residual unit drop. Han et al. [123] proposed Pyramidal Net to address the \nResNet learning interference problem. To address the depth enlargement and extreme \nreduction in spatial width via ResNet, Pyramidal Net slowly enlarges the residual unit \nwidth to cover the most feasible places rather than saving the same spatial dimension \ninside all residual blocks up to the appearance of the down-sampling. It was referred to \nas Pyramidal Net due to the slow enlargement in the feature map depth based on the \nup-down method. Factor l, which was determined by Eq. 19, regulates the depth of the \nfeature map.\nHere, the dimension of the lth residual unit is indicated by dl ; moreover, n indicates the \noverall number of residual units, the step factor is indicated by \u001f , and the depth increase \nis regulated by the factor \u001f\nn , which uniformly distributes the weight increase across the \ndimension of the feature map. Zero-padded identity mapping is used to insert the resid-\nual connections among the layers. In comparison to the projection-based shortcut con-\nnections, zero-padded identity mapping requires fewer parameters, which in turn leads \nto enhanced generalization [124]. Multiplication- and addition-based widening are two \ndifferent approaches used in Pyramidal Nets for network widening. More specifically, \nthe first approach (multiplication) enlarges geometrically, while the second one (addi-\ntion) enlarges linearly [92]. The main problem associated with the width enlargement is \nthe growth in time and space required related to the quadratic time.\nXception\nExtreme inception architecture is the main characteristic of Xception. The main idea \nbehind Xception is its depthwise separable convolution [125]. The Xception model \nadjusted the original inception block by making it wider and exchanging a single \ndimension ( 3 × 3 ) followed by a 1 × 1 convolution to reduce computational complex-\nity. Figure 24 shows the Xception block architecture. The Xception network becomes \nextra computationally effective through the use of the decoupling channel and spatial \ncorrespondence. Moreover, it first performs mapping of the convolved output to the \nembedding short dimension by applying 1 × 1 convolutions. It then performs k spatial \ntransformations. Note that k here represents the width-defining cardinality, which is \nobtained via the transformations number in Xception. However, the computations were \nmade simpler in Xception by distinctly convolving each channel around the spatial axes. \n(19)\ndl =\n\u001f\n16\nif l = 1\n\u001e\ndl−1 + \u001f\nn\n\u001d\nif 2 ≤l ≤n + 1\n\u001c\n\n\nPage 38 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nThese axes are subsequently used as the 1 × 1 convolutions (pointwise convolution) for \nperforming cross-channel correspondence. The 1 × 1 convolution is utilized in Xception \nto regularize the depth of the channel. The traditional convolutional operation in Xcep-\ntion utilizes a number of transformation segments equivalent to the number of channels; \nInception, moreover, utilizes three transformation segments, while traditional CNN \narchitecture utilizes only a single transformation segment. Conversely, the suggested \nXception transformation approach achieves extra learning efficiency and better perfor-\nmance but does not minimize the number of parameters [126, 127].\nResidual attention neural network\nTo improve the network feature representation, Wang et al. [128] proposed the Residual \nAttention Network (RAN). Enabling the network to learn aware features of the object is \nthe main purpose of incorporating attention into the CNN. The RAN consists of stacked \nresidual blocks in addition to the attention module; hence, it is a feed-forward CNN. \nHowever, the attention module is divided into two branches, namely the mask branch \nand trunk branch. These branches adopt a top-down and bottom-up learning strategy \nrespectively. Encapsulating two different strategies in the attention model supports top-\ndown attention feedback and fast feed-forward processing in only one particular feed-\nforward process. More specifically, the top-down architecture generates dense features \nto make inferences about every aspect. Moreover, the bottom-up feedforward architec-\nture generates low-resolution feature maps in addition to robust semantic information. \nRestricted Boltzmann machines employed a top-down bottom-up strategy as in previ-\nously proposed studies [129]. During the training reconstruction phase, Goh et al. [130] \nused the mechanism of top-down attention in deep Boltzmann machines (DBMs) as a \nregularizing factor. Note that the network can be globally optimized using a top-down \nlearning strategy in a similar manner, where the maps progressively output to the input \nthroughout the learning process [129–132].\nIncorporating the attention concept with convolutional blocks in an easy way was used \nby the transformation network, as obtained in a previous study [133]. Unfortunately, \nthese are inflexible, which represents the main problem, along with their inability to be \nFig. 24  The basic block diagram for the Xception block architecture\n\n\nPage 39 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nused for varying surroundings. By contrast, stacking multi-attention modules has made \nRAN very effective at recognizing noisy, complex, and cluttered images. RAN’s hierar-\nchical organization gives it the capability to adaptively allocate a weight for every feature \nmap depending on its importance within the layers. Furthermore, incorporating three \ndistinct levels of attention (spatial, channel, and mixed) enables the model to use this \nability to capture the object-aware features at these distinct levels.\nConvolutional block attention module\nThe importance of the feature map utilization and the attention mechanism is certified \nvia SE-Network and RAN [128, 134, 135]. The convolutional block attention (CBAM) \nmodule, which is a novel attention-based CNN, was first developed by Woo et al. [136]. \nThis module is similar to SE-Network and simple in design. SE-Network disregards the \nobject’s spatial locality in the image and considers only the channels’ contribution during \nthe image classification. Regarding object detection, object spatial location plays a sig-\nnificant role. The convolutional block attention module sequentially infers the attention \nmaps. More specifically, it applies channel attention preceding the spatial attention to \nobtain the refined feature maps. Spatial attention is performed using 1 × 1 convolution \nand pooling functions, as in the literature. Generating an effective feature descriptor can \nbe achieved by using a spatial axis along with the pooling of features. In addition, gen-\nerating a robust spatial attention map is possible, as CBAM concatenates the max pool-\ning and average pooling operations. In a similar manner, a collection of GAP and max \npooling operations is used to model the feature map statistics. Woo et al. [136] demon-\nstrated that utilizing GAP will return a sub-optimized inference of channel attention, \nwhereas max pooling provides an indication of the distinguishing object features. Thus, \nthe utilization of max pooling and average pooling enhances the network’s representa-\ntional power. The feature maps improve the representational power, as well as facilitating \na focus on the significant portion of the chosen features. The expression of 3D attention \nmaps through a serial learning procedure assists in decreasing the computational cost \nand the number of parameters, as Woo et al. [136] experimentally proved. Note that any \nCNN architecture can be simply integrated with CBAM.\nConcurrent spatial and channel excitation mechanism\nTo make the work valid for segmentation tasks, Roy et al. [137, 138] expanded Hu et al. \n[134] effort by adding the influence of spatial information to the channel information. \nRoy et al. [137, 138] presented three types of modules: (1) channel squeeze and excita-\ntion with concurrent channels (scSE); (2) exciting spatially and squeezing channel-wise \n(sSE); (3) exciting channel-wise and squeezing spatially (cSE). For segmentation pur-\nposes, they employed auto-encoder-based CNNs. In addition, they suggested inserting \nmodules following the encoder and decoder layers. To specifically highlight the object-\nspecific feature maps, they further allocated attention to every channel by expressing a \nscaling factor from the channel and spatial information in the first module (scSE). In the \nsecond module (sSE), the feature map information has lower importance than the spatial \nlocality, as the spatial information plays a significant role during the segmentation pro-\ncess. Therefore, several channel collections are spatially divided and developed so that \n\n\nPage 40 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nthey can be employed in segmentation. In the final module (cSE), a similar SE-block con-\ncept is used. Furthermore, the scaling factor is derived founded on the contribution of \nthe feature maps within the object detection [137, 138].\nCapsuleNet\nCNN is an efficient technique for detecting object features and achieving well-behaved \nrecognition performance in comparison with innovative handcrafted feature detectors. \nA number of restrictions related to CNN are present, meaning that the CNN does not \nconsider certain relations, orientation, size, and perspectives of features. For instance, \nwhen considering a face image, the CNN does not count the various face components \n(such as mouth, eyes, nose, etc.) positions, and will incorrectly activate the CNN neu-\nrons and recognize the face without taking specific relations (such as size, orientation \netc.) into account. At this point, consider a neuron that has probability in addition to \nfeature properties such as size, orientation, perspective, etc. A specific neuron/capsule of \nthis type has the ability to effectively detect the face along with different types of infor-\nmation. Thus, many layers of capsule nodes are used to construct the capsule network. \nAn encoding unit, which contains three layers of capsule nodes, forms the CapsuleNet \nor CapsNet (the initial version of the capsule networks).\nFor example, the MNIST architecture comprises 28 × 28 images, applying 256 filters \nof size 9 × 9 and with stride 1. The 28 −9 + 1 = 20 is the output plus 256 feature maps. \nNext, these outputs are input to the first capsule layer, while producing an 8D vector \nrather than a scalar; in fact, this is a modified convolution layer. Note that a stride 2 \nwith 9 × 9 filters is employed in the first convolution layer. Thus, the dimension of the \noutput is (20 −9)/2 + 1 = 6 . The initial capsules employ 8 × 32 filters, which generate \n32 × 8 × 6 × 6 (32 for groups, 8 for neurons, while 6 × 6 is the neuron size).\nFigure 25 represents the complete CapsNet encoding and decoding processes. In the \nCNN context, a max-pooling layer is frequently employed to handle the translation \nchange. It can detect the feature moves in the event that the feature is still within the \nmax-pooling window. This approach has the ability to detect the overlapped features; \nFig. 25  The complete CapsNet encoding and decoding processes\n\n\nPage 41 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nthis is highly significant in detection and segmentation operations, since the capsule \ninvolves the weighted features sum from the preceding layer.\nIn conventional CNNs, a particular cost function is employed to evaluate the global \nerror that grows toward the back throughout the training process. Conversely, in such \ncases, the activation of a neuron will not grow further once the weight between two \nneurons turns out to be zero. Instead of a single size being provided with the com-\nplete cost function in repetitive dynamic routing alongside the agreement, the signal \nis directed based on the feature parameters. Sabour et al. [139] provides more details \nabout this architecture. When using MNIST to recognize handwritten digits, this \ninnovative CNN architecture gives superior accuracy. From the application perspec-\ntive, this architecture has extra suitability for segmentation and detection approaches \nwhen compared with classification approaches [140–142].\nHigh‑resolution network (HRNet)\nHigh-resolution representations are necessary for position-sensitive vision tasks, \nsuch as semantic segmentation, object detection, and human pose estimation. In the \npresent up-to-date frameworks, the input image is encoded as a low-resolution repre-\nsentation using a subnetwork that is constructed as a connected series of high-to-low \nresolution convolutions such as VGGNet and ResNet. The low-resolution representa-\ntion is then recovered to become a high-resolution one. Alternatively, high-resolu-\ntion representations are maintained during the entire process using a novel network, \nreferred to as a High-Resolution Network (HRNet) [143, 144]. This network has two \nprincipal features. First, the convolution series of high-to-low resolutions are con-\nnected in parallel. Second, the information across the resolutions are repeatedly \nexchanged. The advantage achieved includes getting a representation that is more \naccurate in the spatial domain and extra-rich in the semantic domain. Moreover, \nHRNet has several applications in the fields of object detection, semantic segmenta-\ntion, and human pose prediction. For computer vision problems, the HRNet repre-\nsents a more robust backbone. Figure 26 illustrates the general architecture of HRNet.\nChallenges (limitations) of deep learning and alternate solutions\nWhen employing DL, several difficulties are often taken into consideration. Those \nmore challenging are listed next and several possible alternatives are accordingly \nprovided.\nFig. 26  The general architecture of HRNet\n\n\nPage 42 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nTraining data\nDL is extremely data-hungry considering it also involves representation learning [145, \n146]. DL demands an extensively large amount of data to achieve a well-behaved per-\nformance model, i.e. as the data increases, an extra well-behaved performance model \ncan be achieved (Fig. 27). In most cases, the available data are sufficient to obtain a \ngood performance model. However, sometimes there is a shortage of data for using \nDL directly [87]. To properly address this issue, three suggested methods are avail-\nable. The first involves the employment of the transfer-learning concept after data is \ncollected from similar tasks. Note that while the transferred data will not directly aug-\nment the actual data, it will help in terms of both enhancing the original input repre-\nsentation of data and its mapping function [147]. In this way, the model performance \nis boosted. Another technique involves employing a well-trained model from a similar \ntask and fine-tuning the ending of two layers or even one layer based on the limited \noriginal data. Refer to [148, 149] for a review of different transfer-learning techniques \napplied in the DL approach. In the second method, data augmentation is performed \n[150]. This task is very helpful for use in augmenting the image data, since the image \ntranslation, mirroring, and rotation commonly do not change the image label. Con-\nversely, it is important to take care when applying this technique in some cases such \nas with bioinformatics data. For instance, when mirroring an enzyme sequence, the \noutput data may not represent the actual enzyme sequence. In the third method, the \nsimulated data can be considered for increasing the volume of the training set. It is \noccasionally possible to create simulators based on the physical process if the issue is \nwell understood. Therefore, the result will involve the simulation of as much data as \nneeded. Processing the data requirement for DL-based simulation is obtained as an \nexample in Ref. [151].\nTransfer learning\nRecent research has revealed a widespread use of deep CNNs, which offer ground-\nbreaking support for answering many classification problems. Generally speaking, \ndeep CNN models require a sizable volume of data to obtain good performance. The \nFig. 27  The performance of DL regarding the amount of data\n\n\nPage 43 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\ncommon challenge associated with using such models concerns the lack of training \ndata. Indeed, gathering a large volume of data is an exhausting job, and no successful \nsolution is available at this time. The undersized dataset problem is therefore cur-\nrently solved using the TL technique [148, 149], which is highly efficient in address-\ning the lack of training data issue. The mechanism of TL involves training the CNN \nmodel with large volumes of data. In the next step, the model is fine-tuned for train-\ning on a small request dataset.\nThe student-teacher relationship is a suitable approach to clarifying TL. Gathering \ndetailed knowledge of the subject is the first step [152]. Next, the teacher provides a \n“course” by conveying the information within a “lecture series” over time. Put simply, \nthe teacher transfers the information to the student. In more detail, the expert (teacher) \ntransfers the knowledge (information) to the learner (student). Similarly, the DL network \nis trained using a vast volume of data, and also learns the bias and the weights during the \ntraining process. These weights are then transferred to different networks for retraining \nor testing a similar novel model. Thus, the novel model is enabled to pre-train weights \nrather than requiring training from scratch. Figure 28 illustrates the conceptual diagram \nof the TL technique. \n1.\t Pre-trained models: Many CNN models, e.g. AlexNet [30], GoogleNet [103], and \nResNet [37], have been trained on large datasets such as ImageNet for image rec-\nognition purposes. These models can then be employed to recognize a different task \nwithout the need to train from scratch. Furthermore, the weights remain the same \napart from a few learned features. In cases where data samples are lacking, these \nmodels are very useful. There are many reasons for employing a pre-trained model. \nFirst, training large models on sizeable datasets requires high-priced computational \npower. Second, training large models can be time-consuming, taking up to multiple \nweeks. Finally, a pre-trained model can assist with network generalization and speed \nup the convergence.\nFig. 28  The conceptual diagram of the TL technique\n\n\nPage 44 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n2.\t A research problem using pre-trained models: Training a DL approach requires a \nmassive number of images. Thus, obtaining good performance is a challenge under \nthese circumstances. Achieving excellent outcomes in image classification or rec-\nognition applications, with performance occasionally superior to that of a human, \nbecomes possible through the use of deep convolutional neural networks (DCNNs) \nincluding several layers if a huge amount of data is available [37, 148, 153]. How-\never, avoiding overfitting problems in such applications requires sizable datasets and \nproperly generalizing DCNN models. When training a DCNN model, the dataset \nsize has no lower limit. However, the accuracy of the model becomes insufficient \nin the case of the utilized model has fewer layers, or if a small dataset is used for \ntraining due to over- or under-fitting problems. Due to they have no ability to uti-\nlize the hierarchical features of sizable datasets, models with fewer layers have poor \naccuracy. It is difficult to acquire sufficient training data for DL models. For exam-\nple, in medical imaging and environmental science, gathering labelled datasets is \nvery costly [148]. Moreover, the majority of the crowdsourcing workers are unable to \nmake accurate notes on medical or biological images due to their lack of medical or \nbiological knowledge. Thus, ML researchers often rely on field experts to label such \nimages; however, this process is costly and time consuming. Therefore, producing \nthe large volume of labels required to develop flourishing deep networks turns out \nto be unfeasible. Recently, TL has been widely employed to address the later issue. \nNevertheless, although TL enhances the accuracy of several tasks in the fields of pat-\ntern recognition and computer vision [154, 155], there is an essential issue related to \nthe source data type used by the TL as compared to the target dataset. For instance, \nenhancing the medical image classification performance of CNN models is achieved \nby training the models using the ImageNet dataset, which contains natural images \n[153]. However, such natural images are completely dissimilar from the raw medical \nimages, meaning that the model performance is not enhanced. It has further been \nproven that TL from different domains does not significantly affect performance on \nmedical imaging tasks, as lightweight models trained from scratch perform nearly \nas well as standard ImageNet-transferred models [156]. Therefore, there exists sce-\nnarios in which using pre-trained models do not become an affordable solution. In \n2020, some researchers have utilized same-domain TL and achieved excellent results \n[86–88, 157]. Same-domain TL is an approach of using images that look similar to \nthe target dataset for training. For example, using X-ray images of different chest dis-\neases to train the model, then fine-tuning and training it on chest X-ray images for \nCOVID-19 diagnosis. More details about same-domain TL and how to implement \nthe fine-tuning process can be found in [87].\nData augmentation techniques\nIf the goal is to increase the amount of available data and avoid the overfitting issue, data \naugmentation techniques are one possible solution [150, 158, 159]. These techniques are \ndata-space solutions for any limited-data problem. Data augmentation incorporates a \ncollection of methods that improve the attributes and size of training datasets. Thus, DL \n\n\nPage 45 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nnetworks can perform better when these techniques are employed. Next, we list some \ndata augmentation alternate solutions. \n1.\t Flipping: Flipping the vertical axis is a less common practice than flipping the hori-\nzontal one. Flipping has been verified as valuable on datasets like ImageNet and \nCIFAR-10. Moreover, it is highly simple to implement. In addition, it is not a label-\nconserving transformation on datasets that involve text recognition (such as SVHN \nand MNIST).\n2.\t Color space: Encoding digital image data is commonly used as a dimension tensor \n( height × width × colorchannels ). Accomplishing augmentations in the color space \nof the channels is an alternative technique, which is extremely workable for imple-\nmentation. A very easy color augmentation involves separating a channel of a par-\nticular color, such as Red, Green, or Blue. A simple way to rapidly convert an image \nusing a single-color channel is achieved by separating that matrix and inserting addi-\ntional double zeros from the remaining two color channels. Furthermore, increas-\ning or decreasing the image brightness is achieved by using straightforward matrix \noperations to easily manipulate the RGB values. By deriving a color histogram that \ndescribes the image, additional improved color augmentations can be obtained. \nLighting alterations are also made possible by adjusting the intensity values in histo-\ngrams similar to those employed in photo-editing applications.\n3.\t Cropping: Cropping a dominant patch of every single image is a technique employed \nwith combined dimensions of height and width as a specific processing step for \nimage data. Furthermore, random cropping may be employed to produce an impact \nsimilar to translations. The difference between translations and random cropping is \nthat translations conserve the spatial dimensions of this image, while random crop-\nping reduces the input size [for example from (256, 256) to (224, 224)]. According to \nthe selected reduction threshold for cropping, the label-preserving transformation \nmay not be addressed.\n4.\t Rotation: When rotating an image left or right from within 0 to 360 degrees around \nthe axis, rotation augmentations are obtained. The rotation degree parameter greatly \ndetermines the suitability of the rotation augmentations. In digit recognition tasks, \nsmall rotations (from 0 to 20 degrees) are very helpful. By contrast, the data label \ncannot be preserved post-transformation when the rotation degree increases.\n5.\t Translation: To avoid positional bias within the image data, a very useful transforma-\ntion is to shift the image up, down, left, or right. For instance, it is common that the \nwhole dataset images are centered; moreover, the tested dataset should be entirely \nmade up of centered images to test the model. Note that when translating the initial \nimages in a particular direction, the residual space should be filled with Gaussian or \nrandom noise, or a constant value such as 255 s or 0 s. The spatial dimensions of the \nimage post-augmentation are preserved using this padding.\n6.\t Noise injection This approach involves injecting a matrix of arbitrary values. Such \na matrix is commonly obtained from a Gaussian distribution. Moreno-Barea et al. \n[160] employed nine datasets to test the noise injection. These datasets were taken \nfrom the UCI repository [161]. Injecting noise within images enables the CNN to \nlearn additional robust features.\n\n\nPage 46 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nHowever, highly well-behaved solutions for positional biases available within the \ntraining data are achieved by means of geometric transformations. To separate the \ndistribution of the testing data from the training data, several prospective sources \nof bias exist. For instance, when all faces should be completely centered within the \nframes (as in facial recognition datasets), the problem of positional biases emerges. \nThus, geometric translations are the best solution. Geometric translations are helpful \ndue to their simplicity of implementation, as well as their effective capability to dis-\nable the positional biases. Several libraries of image processing are available, which \nenables beginning with simple operations such as rotation or horizontal flipping. \nAdditional training time, higher computational costs, and additional memory are \nsome shortcomings of geometric transformations. Furthermore, a number of geo-\nmetric transformations (such as arbitrary cropping or translation) should be manu-\nally observed to ensure that they do not change the image label. Finally, the biases \nthat separate the test data from the training data are more complicated than transi-\ntional and positional changes. Hence, it is not trivial answering to when and where \ngeometric transformations are suitable to be applied.\nImbalanced data\nCommonly, biological data tend to be imbalanced, as negative samples are much more \nnumerous than positive ones [162–164]. For example, compared to COVID-19-positive \nX-ray images, the volume of normal X-ray images is very large. It should be noted that \nundesirable results may be produced when training a DL model using imbalanced data. \nThe following techniques are used to solve this issue. First, it is necessary to employ the \ncorrect criteria for evaluating the loss, as well as the prediction result. In considering \nthe imbalanced data, the model should perform well on small classes as well as larger \nones. Thus, the model should employ area under curve (AUC) as the resultant loss as \nwell as the criteria [165]. Second, it should employ the weighted cross-entropy loss, \nwhich ensures the model will perform well with small classes if it still prefers to employ \nthe cross-entropy loss. Simultaneously, during model training, it is possible either to \ndown-sample the large classes or up-sample the small classes. Finally, to make the data \nbalanced as in Ref. [166], it is possible to construct models for every hierarchical level, \nas a biological system frequently has hierarchical label space. However, the effect of the \nimbalanced data on the performance of the DL model has been comprehensively inves-\ntigated. In addition, to lessen the problem, the most frequently used techniques were \nalso compared. Nevertheless, note that these techniques are not specified for biological \nproblems.\nInterpretability of data\nOccasionally, DL techniques are analyzed to act as a black box. In fact, they are inter-\npretable. The need for a method of interpreting DL, which is used to obtain the valu-\nable motifs and patterns recognized by the network, is common in many fields, such as \nbioinformatics [167]. In the task of disease diagnosis, it is not only required to know the \ndisease diagnosis or prediction results of a trained DL model, but also how to enhance \nthe surety of the prediction outcomes, as the model makes its decisions based on these \nverifications [168]. To achieve this, it is possible to give a score of importance for every \n\n\nPage 47 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nportion of the particular example. Within this solution, back-propagation-based tech-\nniques or perturbation-based approaches are used [169]. In the perturbation-based \napproaches, a portion of the input is changed and the effect of this change on the model \noutput is observed [170–173]. This concept has high computational complexity, but it is \nsimple to understand. On the other hand, to check the score of the importance of vari-\nous input portions, the signal from the output propagates back to the input layer in the \nback-propagation-based techniques. These techniques have been proven valuable in \n[174]. In different scenarios, various meanings can represent the model interpretability.\nUncertainty scaling\nCommonly, the final prediction label is not the only label required when employing DL \ntechniques to achieve the prediction; the score of confidence for every inquiry from the \nmodel is also desired. The score of confidence is defined as how confident the model is \nin its prediction [175]. Since the score of confidence prevents belief in unreliable and \nmisleading predictions, it is a significant attribute, regardless of the application scenario. \nIn biology, the confidence score reduces the resources and time expended in proving \nthe outcomes of the misleading prediction. Generally speaking, in healthcare or similar \napplications, the uncertainty scaling is frequently very significant; it helps in evaluating \nautomated clinical decisions and the reliability of machine learning-based disease-diag-\nnosis [176, 177]. Because overconfident prediction can be the output of different DL \nmodels, the score of probability (achieved from the softmax output of the direct-DL) is \noften not in the correct scale [178]. Note that the softmax output requires post-scaling \nto achieve a reliable probability score. For outputting the probability score in the cor-\nrect scale, several techniques have been introduced, including Bayesian Binning into \nQuantiles (BBQ) [179], isotonic regression [180], histogram binning [181], and the leg-\nendary Platt scaling [182]. More specifically, for DL techniques, temperature scaling \nwas recently introduced, which achieves superior performance compared to the other \ntechniques.\nCatastrophic forgetting\nThis is defined as incorporating new information into a plain DL model, made possible \nby interfering with the learned information. For instance, consider a case where there \nare 1000 types of flowers and a model is trained to classify these flowers, after which \na new type of flower is introduced; if the model is fine-tuned only with this new class, \nits performance will become unsuccessful with the older classes [183, 184]. The logical \ndata are continually collected and renewed, which is in fact a highly typical scenario in \nmany fields, e.g. Biology. To address this issue, there is a direct solution that involves \nemploying old and new data to train an entirely new model from scratch. This solution \nis time-consuming and computationally intensive; furthermore, it leads to an unstable \nstate for the learned representation of the initial data. At this time, three different types \nof ML techniques, which have not catastrophic forgetting, are made available to solve \nthe human brain problem founded on the neurophysiological theories [185, 186]. Tech-\nniques of the first type are founded on regularizations such as EWC [183] Techniques \nof the second type employ rehearsal training techniques and dynamic neural network \n\n\nPage 48 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \narchitecture like iCaRL [187, 188]. Finally, techniques of the third type are founded on \ndual-memory learning systems [189]. Refer to [190–192] in order to gain more details.\nModel compression\nTo obtain well-trained models that can still be employed productively, DL models have \nintensive memory and computational requirements due to their huge complexity and \nlarge numbers of parameters [193, 194]. One of the fields that is characterized as data-\nintensive is the field of healthcare and environmental science. These needs reduce the \ndeployment of DL in limited computational-power machines, mainly in the healthcare \nfield. The numerous methods of assessing human health and the data heterogeneity have \nbecome far more complicated and vastly larger in size [195]; thus, the issue requires \nadditional computation [196]. Furthermore, novel hardware-based parallel processing \nsolutions such as FPGAs and GPUs [197–199] have been developed to solve the com-\nputation issues associated with DL. Recently, numerous techniques for compressing \nthe DL models, designed to decrease the computational issues of the models from the \nstarting point, have also been introduced. These techniques can be classified into four \nclasses. In the first class, the redundant parameters (which have no significant impact on \nmodel performance) are reduced. This class, which includes the famous deep compres-\nsion method, is called parameter pruning [200]. In the second class, the larger model \nuses its distilled knowledge to train a more compact model; thus, it is called knowledge \ndistillation [201, 202]. In the third class, compact convolution filters are used to reduce \nthe number of parameters [203]. In the final class, the information parameters are esti-\nmated for preservation using low-rank factorization [204]. For model compression, these \nclasses represent the most representative techniques. In [193], it has been provided a \nmore comprehensive discussion about the topic.\nOverfitting\nDL models have excessively high possibilities of resulting in data overfitting at the train-\ning stage due to the vast number of parameters involved, which are correlated in a com-\nplex manner. Such situations reduce the model’s ability to achieve good performance on \nthe tested data [90, 205]. This problem is not only limited to a specific field, but involves \ndifferent tasks. Therefore, when proposing DL techniques, this problem should be fully \nconsidered and accurately handled. In DL, the implied bias of the training process \nenables the model to overcome crucial overfitting problems, as recent studies suggest \n[205–208]. Even so, it is still necessary to develop techniques that handle the overfit-\nting problem. An investigation of the available DL algorithms that ease the overfitting \nproblem can categorize them into three classes. The first class acts on both the model \narchitecture and model parameters and includes the most familiar approaches, such as \nweight decay [209], batch normalization [210], and dropout [90]. In DL, the default tech-\nnique is weight decay [209], which is used extensively in almost all ML algorithms as a \nuniversal regularizer. The second class works on model inputs such as data corruption \nand data augmentation [150, 211]. One reason for the overfitting problem is the lack \nof training data, which makes the learned distribution not mirror the real distribution. \nData augmentation enlarges the training data. By contrast, marginalized data corrup-\ntion improves the solution exclusive to augmenting the data. The final class works on the \n\n\nPage 49 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nmodel output. A recently proposed technique penalizes the over-confident outputs for \nregularizing the model [178]. This technique has demonstrated the ability to regularize \nRNNs and CNNs.\nVanishing gradient problem\nIn general, when using backpropagation and gradient-based learning techniques along \nwith ANNs, largely in the training stage, a problem called the vanishing gradient prob-\nlem arises [212–214]. More specifically, in each training iteration, every weight of the \nneural network is updated based on the current weight and is proportionally relative to \nthe partial derivative of the error function. However, this weight updating may not occur \nin some cases due to a vanishingly small gradient, which in the worst case means that no \nextra training is possible and the neural network will stop completely. Conversely, simi-\nlarly to other activation functions, the sigmoid function shrinks a large input space to a \ntiny input space. Thus, the derivative of the sigmoid function will be small due to large \nvariation at the input that produces a small variation at the output. In a shallow network, \nonly some layers use these activations, which is not a significant issue. While using more \nlayers will lead the gradient to become very small in the training stage, in this case, the \nnetwork works efficiently. The back-propagation technique is used to determine the gra-\ndients of the neural networks. Initially, this technique determines the network deriva-\ntives of each layer in the reverse direction, starting from the last layer and progressing \nback to the first layer. The next step involves multiplying the derivatives of each layer \ndown the network in a similar manner to the first step. For instance, multiplying N small \nderivatives together when there are N hidden layers employs an activation function such \nas the sigmoid function. Hence, the gradient declines exponentially while propagating \nback to the first layer. More specifically, the biases and weights of the first layers cannot \nbe updated efficiently during the training stage because the gradient is small. Moreover, \nthis condition decreases the overall network accuracy, as these first layers are frequently \ncritical to recognizing the essential elements of the input data. However, such a problem \ncan be avoided through employing activation functions. These functions lack the squish-\ning property, i.e., the ability to squish the input space to within a small space. By mapping \nX to max, the ReLU [91] is the most popular selection, as it does not yield a small deriva-\ntive that is employed in the field. Another solution involves employing the batch nor-\nmalization layer [81]. As mentioned earlier, the problem occurs once a large input space \nis squashed into a small space, leading to vanishing the derivative. Employing batch nor-\nmalization degrades this issue by simply normalizing the input, i.e., the expression |x| \ndoes not accomplish the exterior boundaries of the sigmoid function. The normalization \nprocess makes the largest part of it come down in the green area, which ensures that the \nderivative is large enough for further actions. Furthermore, faster hardware can tackle \nthe previous issue, e.g. that provided by GPUs. This makes standard back-propagation \npossible for many deeper layers of the network compared to the time required to recog-\nnize the vanishing gradient problem [215].\n\n\nPage 50 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nExploding gradient problem\nOpposite to the vanishing problem is the one related to gradient. Specifically, large error \ngradients are accumulated during back-propagation [216–218]. The latter will lead to \nextremely significant updates to the weights of the network, meaning that the system \nbecomes unsteady. Thus, the model will lose its ability to learn effectively. Grosso modo, \nmoving backward in the network during back-propagation, the gradient grows exponen-\ntially by repetitively multiplying gradients. The weight values could thus become incred-\nibly large and may overflow to become a not-a-number (NaN) value. Some potential \nsolutions include: \n1.\t Using different weight regularization techniques.\n2.\t Redesigning the architecture of the network model.\nUnderspecification\nIn 2020, a team of computer scientists at Google has identified a new challenge called \nunderspecification [219]. ML models including DL models often show surprisingly poor \nbehavior when they are tested in real-world applications such as computer vision, medi-\ncal imaging, natural language processing, and medical genomics. The reason behind the \nweak performance is due to underspecification. It has been shown that small modifica-\ntions can force a model towards a completely different solution as well as lead to dif-\nferent predictions in deployment domains. There are different techniques of addressing \nunderspecification issue. One of them is to design “stress tests” to examine how good \na model works on real-world data and to find out the possible issues. Nevertheless, \nthis demands a reliable understanding of the process the model can work inaccurately. \nThe team stated that “Designing stress tests that are well-matched to applied require-\nments, and that provide good “coverage” of potential failure modes is a major challenge”. \nUnderspecification puts major constraints on the credibility of ML predictions and may \nrequire some reconsidering over certain applications. Since ML is linked to human by \nserving several applications such as medical imaging and self-driving cars, it will require \nproper attention to this issue.\nApplications of deep learning\nPresently, various DL applications are widespread around the world. These applica-\ntions include healthcare, social network analysis, audio and speech processing (like rec-\nognition and enhancement), visual data processing methods (such as multimedia data \nanalysis and computer vision), and NLP (translation and sentence classification), among \nothers (Fig. 29) [220–224]. These applications have been classified into five categories: \nclassification, localization, detection, segmentation, and registration. Although each of \nthese tasks has its own target, there is fundamental overlap in the pipeline implementa-\ntion of these applications as shown in Fig. 30. Classification is a concept that categorizes \na set of data into classes. Detection is used to locate interesting objects in an image with \nconsideration given to the background. In detection, multiple objects, which could be \nfrom dissimilar classes, are surrounded by bounding boxes. Localization is the concept \n\n\nPage 51 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nused to locate the object, which is surrounded by a single bounding box. In segmenta-\ntion (semantic segmentation), the target object edges are surrounded by outlines, which \nalso label them; moreover, fitting a single image (which could be 2D or 3D) onto another \nrefers to registration. One of the most important and wide-ranging DL applications are \nin healthcare [225–230]. This area of research is critical due to its relation to human \nlives. Moreover, DL has shown tremendous performance in healthcare. Therefore, we \ntake DL applications in the medical image analysis field as an example to describe the DL \napplications.\nFig. 29  Examples of DL applications\nFig. 30  Workflow of deep learning tasks\n\n\nPage 52 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nClassification\nComputer-Aided Diagnosis (CADx) is another title sometimes used for classifica-\ntion. Bharati et al. [231] used a chest X-ray dataset for detecting lung diseases based \non a CNN. Another study attempted to read X-ray images by employing CNN [232]. \nIn this modality, the comparative accessibility of these images has likely enhanced the \nprogress of DL. [233] used an improved pre-trained GoogLeNet CNN containing more \nthan 150,000 images for training and testing processes. This dataset was augmented \nfrom 1850 chest X-rays. The creators reorganized the image orientation into lateral and \nfrontal views and achieved approximately 100% accuracy. This work of orientation clas-\nsification has clinically limited use. As a part of an ultimately fully automated diagno-\nsis workflow, it obtained the data augmentation and pre-trained efficiency in learning \nthe metadata of relevant images. Chest infection, commonly referred to as pneumo-\nnia, is extremely treatable, as it is a commonly occurring health problem worldwide. \nConversely, Rajpurkar et al. [234] utilized CheXNet, which is an improved version of \nDenseNet [112] with 121 convolution layers, for classifying fourteen types of disease. \nThese authors used the CheXNet14 dataset [235], which comprises 112,000 images. This \nnetwork achieved an excellent performance in recognizing fourteen different diseases. \nIn particular, pneumonia classification accomplished a 0.7632 AUC score using receiver \noperating characteristics (ROC) analysis. In addition, the network obtained better than \nor equal to the performance of both a three-radiologist panel and four individual radi-\nologists. Zuo et al. [236] have adopted CNN for candidate classification in lung nodule. \nShen et al. [237] employed both Random Forest (RF) and SVM classifiers with CNNs to \nclassify lung nodules. They employed two convolutional layers with each of the three \nparallel CNNs. The LIDC-IDRI (Lung Image Database Consortium) dataset, which con-\ntained 1010-labeled CT lung scans, was used to classify the two types of lung nodules \n(malignant and benign). Different scales of the image patches were used by every CNN \nto extract features, while the output feature vector was constructed using the learned \nfeatures. Next, these vectors were classified into malignant or benign using either the \nRF classifier or SVM with radial basis function (RBF) filter. The model was robust to \nvarious noisy input levels and achieved an accuracy of 86% in nodule classification. \nConversely, the model of [238] interpolates the image data missing between PET and \nMRI images using 3D CNNs. The Alzheimer Disease Neuroimaging Initiative (ADNI) \ndatabase, containing 830 PET and MRI patient scans, was utilized in their work. The \nPET and MRI images are used to train the 3D CNNs, first as input and then as output. \nFurthermore, for patients who have no PET images, the 3D CNNs utilized the trained \nimages to rebuild the PET images. These rebuilt images approximately fitted the actual \ndisease recognition outcomes. However, this approach did not address the overfitting \nissues, which in turn restricted their technique in terms of its possible capacity for gen-\neralization. Diagnosing normal versus Alzheimer’s disease patients has been achieved \nby several CNN models [239, 240]. Hosseini-Asl et al. [241] attained 99% accuracy for \nup-to-date outcomes in diagnosing normal versus Alzheimer’s disease patients. These \nauthors applied an auto-encoder architecture using 3D CNNs. The generic brain fea-\ntures were pre-trained on the CADDementia dataset. Subsequently, the outcomes of \nthese learned features became inputs to higher layers to differentiate between patient \nscans of Alzheimer’s disease, mild cognitive impairment, or normal brains based on the \n\n\nPage 53 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nADNI dataset and using fine-tuned deep supervision techniques. The architectures of \nVGGNet and RNNs, in that order, were the basis of both VOXCNN and ResNet models \ndeveloped by Korolev et al. [242]. They also discriminated between Alzheimer’s disease \nand normal patients using the ADNI database. Accuracy was 79% for Voxnet and 80% \nfor ResNet. Compared to Hosseini-Asl’s work, both models achieved lower accuracies. \nConversely, the implementation of the algorithms was simpler and did not require fea-\nture hand-crafting, as Korolev declared. In 2020, Mehmood et al. [240] trained a devel-\noped CNN-based network called “SCNN” with MRI images for the tasks of classification \nof Alzheimer’s disease. They achieved state-of-the-art results by obtaining an accuracy \nof 99.05%.\nRecently, CNN has taken some medical imaging classification tasks to different level \nfrom traditional diagnosis to automated diagnosis with tremendous performance. Exam-\nples of these tasks are diabetic foot ulcer (DFU) (as normal and abnormal (DFU) classes) \n[87, 243–246], sickle cells anemia (SCA) (as normal, abnormal (SCA), and other blood \ncomponents) [86, 247], breast cancer by classify hematoxylin–eosin-stained breast \nbiopsy images into four classes: invasive carcinoma, in-situ carcinoma, benign tumor \nand normal tissue [42, 88, 248–252], and multi-class skin cancer classification [253–255].\nIn 2020, CNNs are playing a vital role in early diagnosis of the novel coronavirus \n(COVID-2019). CNN has become the primary tool for automatic COVID-19 diagnosis \nin many hospitals around the world using chest X-ray images [256–260]. More details \nabout the classification of medical imaging applications can be found in [226, 261–265].\nLocalization\nAlthough applications in anatomy education could increase, the practicing clinician is \nmore likely to be interested in the localization of normal anatomy. Radiological images \nare independently examined and described outside of human intervention, while locali-\nzation could be applied in completely automatic end-to-end applications [266–268]. \nZhao et al. [269] introduced a new deep learning-based approach to localize pancreatic \ntumor in projection X-ray images for image-guided radiation therapy without the need \nfor fiducials. Roth et al. [270] constructed and trained a CNN using five convolutional \nlayers to classify around 4000 transverse-axial CT images. These authors used five cat-\negories for classification: legs, pelvis, liver, lung, and neck. After data augmentation tech-\nniques were applied, they achieved an AUC score of 0.998 and the classification error \nrate of the model was 5.9%. For detecting the positions of the spleen, kidney, heart, and \nliver, Shin et al. [271] employed stacked auto-encoders on 78 contrast-improved MRI \nscans of the stomach area containing the kidneys or liver. Temporal and spatial domains \nwere used to learn the hierarchal features. Based on the organs, these approaches \nachieved detection accuracies of 62–79%. Sirazitdinov et al. [268] presented an aggre-\ngate of two convolutional neural networks, namely RetinaNet and Mask R-CNN for \npneumonia detection and localization.\nDetection\nComputer-Aided Detection (CADe) is another method used for detection. For both the \nclinician and the patient, overlooking a lesion on a scan may have dire consequences. \n\n\nPage 54 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nThus, detection is a field of study requiring both accuracy and sensitivity [272–274]. \nChouhan et al. [275] introduced an innovative deep learning framework for the detec-\ntion of pneumonia by adopting the idea of transfer learning. Their approach obtained \nan accuracy of 96.4% with a recall of 99.62% on unseen data. In the area of COVID-\n19 and pulmonary disease, several convolutional neural network approaches have been \nproposed for automatic detection from X-ray images which showed an excellent perfor-\nmance [46, 276–279].\nIn the area of skin cancer, there several applications were introduced for the detection \ntask [280–282]. Thurnhofer-Hemsi et al. [283] introduced a deep learning approach for \nskin cancer detection by fine-tuning five state-of-art convolutional neural network mod-\nels. They addressed the issue of a lack of training data by adopting the ideas of transfer \nlearning and data augmentation techniques. DenseNet201 network has shown superior \nresults compared to other models.\nAnother interesting area is that of histopathological images, which are progressively \ndigitized. Several papers have been published in this field [284–290]. Human patholo-\ngists read these images laboriously; they search for malignancy markers, such as a high \nindex of cell proliferation, using molecular markers (e.g. Ki-67), cellular necrosis signs, \nabnormal cellular architecture, enlarged numbers of mitotic figures denoting augmented \ncell replication, and enlarged nucleus-to-cytoplasm ratios. Note that the histopathologi-\ncal slide may contain a huge number of cells (up to the thousands). Thus, the risk of \ndisregarding abnormal neoplastic regions is high when wading through these cells at \nexcessive levels of magnification. Ciresan et al. [291] employed CNNs of 11–13 layers \nfor identifying mitotic figures. Fifty breast histology images from the MITOS dataset \nwere used. Their technique attained recall and precision scores of 0.7 and 0.88 respec-\ntively. Sirinukunwattana et al. [292] utilized 100 histology images of colorectal adenocar-\ncinoma to detect cell nuclei using CNNs. Roughly 30,000 nuclei were hand-labeled for \ntraining purposes. The novelty of this approach was in the use of Spatially Constrained \nCNN. This CNN detects the center of nuclei using the surrounding spatial context and \nspatial regression. Instead of this CNN, Xu et al. [293] employed a stacked sparse auto-\nencoder (SSAE) to identify nuclei in histological slides of breast cancer, achieving 0.83 \nand 0.89 recall and precision scores respectively. In this field, they showed that unsu-\npervised learning techniques are also effectively utilized. In medical images, Albarquoni \net  al. [294] investigated the problem of insufficient labeling. They crowd-sourced the \nactual mitoses labeling in the histology images of breast cancer (from amateurs online). \nSolving the recurrent issue of inadequate labeling during the analysis of medical images \ncan be achieved by feeding the crowd-sourced input labels into the CNN. This method \nsignifies a remarkable proof-of-concept effort. In 2020, Lei et al. [285] introduced the \nemployment of deep convolutional neural networks for automatic identification of \nmitotic candidates from histological sections for mitosis screening. They obtained the \nstate-of-the-art detection results on the dataset of the International Pattern Recognition \nConference (ICPR) 2012 Mitosis Detection Competition.\nSegmentation\nAlthough MRI and CT image segmentation research includes different organs such \nas knee cartilage, prostate, and liver, most research work has concentrated on brain \n\n\nPage 55 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nsegmentation, particularly tumors [295–300]. This issue is highly significant in surgical \npreparation to obtain the precise tumor limits for the shortest surgical resection. Dur-\ning surgery, excessive sacrificing of key brain regions may lead to neurological shortfalls \nincluding cognitive damage, emotionlessness, and limb difficulty. Conventionally, medi-\ncal anatomical segmentation was done by hand; more specifically, the clinician draws \nout lines within the complete stack of the CT or MRI volume slice by slice. Thus, it is \nperfect for implementing a solution that computerizes this painstaking work. Wadhwa \net  al. [301] presented a brief overview on brain tumor segmentation of MRI images. \nAkkus et al. [302] wrote a brilliant review of brain MRI segmentation that addressed \nthe different metrics and CNN architectures employed. Moreover, they explain several \ncompetitions in detail, as well as their datasets, which included Ischemic Stroke Lesion \nSegmentation (ISLES), Mild Traumatic brain injury Outcome Prediction (MTOP), and \nBrain Tumor Segmentation (BRATS).\nChen et  al. [299] proposed convolutional neural networks for precise brain tumor \nsegmentation. The approach that they employed involves several approaches for better \nfeatures learning including the DeepMedic model, a novel dual-force training scheme, \na label distribution-based loss function, and Multi-Layer Perceptron-based post-pro-\ncessing. They conducted their method on the two most modern brain tumor segmenta-\ntion datasets, i.e., BRATS 2017 and BRATS 2015 datasets. Hu et al. [300] introduced the \nbrain tumor segmentation method by adopting a multi-cascaded convolutional neural \nnetwork (MCCNN) and fully connected conditional random fields (CRFs). The achieved \nresults were excellent compared with the state-of-the-art methods.\nMoeskops et al. [303] employed three parallel-running CNNs, each of which had a 2D \ninput patch of dissimilar size, for segmenting and classifying MRI brain images. These \nimages, which include 35 adults and 22 pre-term infants, were classified into various tis-\nsue categories such as cerebrospinal fluid, grey matter, and white matter. Every patch \nconcentrates on capturing various image aspects with the benefit of employing three \ndissimilar sizes of input patch; here, the bigger sizes incorporated the spatial features, \nwhile the lowest patch sizes concentrated on the local textures. In general, the algorithm \nhas Dice coefficients in the range of 0.82–0.87 and achieved a satisfactory accuracy. \nAlthough 2D image slices are employed in the majority of segmentation research, Mil-\nletrate et al. [304] implemented 3D CNN for segmenting MRI prostate images. Further-\nmore, they used the PROMISE2012 challenge dataset, from which fifty MRI scans were \nused for training and thirty for testing. The U-Net architecture of Ronnerberger et al. \n[305] inspired their V-net. This model attained a 0.869 Dice coefficient score, the same \nas the winning teams in the competition. To reduce overfitting and create the model of \na deeper 11-convolutional layer CNN, Pereira et al. [306] applied intentionally small-\nsized filters of 3x3. Their model used MRI scans of 274 gliomas (a type of brain tumor) \nfor training. They achieved first place in the 2013 BRATS challenge, as well as second \nplace in the BRATS challenge 2015. Havaei et al. [307] also considered gliomas using \nthe 2013 BRATS dataset. They investigated different 2D CNN architectures. Compared \nto the winner of BRATS 2013, their algorithm worked better, as it required only 3 min \nto execute rather than 100 min. The concept of cascaded architecture formed the basis \nof their model. Thus, it is referred to as an InputCascadeCNN. Employing FC Condi-\ntional Random Fields (CRFs), atrous spatial pyramid pooling, and up-sampled filters \n\n\nPage 56 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nwere techniques introduced by Chen et al. [308]. These authors aimed to enhance the \naccuracy of localization and enlarge the field of view of every filter at a multi-scale. Their \nmodel, DeepLab, attained 79.7% mIOU (mean Intersection Over Union). In the PAS-\nCAL VOC-2012 image segmentation, their model obtained an excellent performance.\nRecently, the Automatic segmentation of COVID-19 Lung Infection from CT Images \nhelps to detect the development of COVID-19 infection by employing several deep \nlearning techniques [309–312].\nRegistration\nUsually, given two input images, the four main stages of the canonical procedure of the \nimage registration task are [313, 314]:\n•\t Target Selection: it illustrates the determined input image that the second counter-\npart input image needs to remain accurately superimposed to.\n•\t Feature Extraction: it computes the set of features extracted from each input image.\n•\t Feature Matching: it allows finding similarities between the previously obtained fea-\ntures.\n•\t Pose Optimization: it is aimed to minimize the distance between both input images.\nThen, the result of the registration procedure is the suitable geometric transformation \n(e.g. translation, rotation, scaling, etc.) that provides both input images within the same \ncoordinate system in a way the distance between them is minimal, i.e. their level of \nsuperimposition/overlapping is optimal. It is out of the scope of this work to provide an \nextensive review of this topic. Nevertheless, a short summary is accordingly introduced \nnext.\nCommonly, the input images for the DL-based registration approach could be in vari-\nous forms, e.g. point clouds, voxel grids, and meshes. Additionally, some techniques \nallow as inputs the result of the Feature Extraction or Matching steps in the canonical \nscheme. Specifically, the outcome could be some data in a particular form as well as the \nresult of the steps from the classical pipeline (feature vector, matching vector, and trans-\nformation). Nevertheless, with the newest DL-based methods, a novel conceptual type \nof ecosystem issues. It contains acquired characteristics about the target, materials, and \ntheir behavior that can be registered with the input data. Such a conceptual ecosystem is \nformed by a neural network and its training manner, and it could be counted as an input \nto the registration approach. Nevertheless, it is not an input that one might adopt in \nevery registration situation since it corresponds to an interior data representation.\nFrom a DL view-point, the interpretation of the conceptual design enables differen-\ntiating the input data of a registration approach into defined or non-defined models. In \nparticular, the illustrated phases are models that depict particular spatial data (e.g. 2D or \n3D) while a non-defined one is a generalization of a data set created by a learning system. \nYumer et al. [315] developed a framework in which the model acquires characteristics of \nobjects, meaning ready to identify what a more sporty car seems like or a more comfy \nchair is, also adjusting a 3D model to fit those characteristics while maintaining the main \ncharacteristics of the primary data. Likewise, a fundamental perspective of the unsuper-\nvised learning method introduced by Ding et al. [316] is that there is no target for the \n\n\nPage 57 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nregistration approach. In this instance, the network is able of placing each input point \ncloud in a global space, solving SLAM issues in which many point clouds have to be reg-\nistered rigidly. On the other hand, Mahadevan [317] proposed the combination of two \nconceptual models utilizing the growth of Imagination Machines to give flexible artifi-\ncial intelligence systems and relationships between the learned phases through training \nschemes that are not inspired on labels and classifications. Another practical application \nof DL, especially CNNs, to image registration is the 3D reconstruction of objects. Wang \net al. [318] applied an adversarial way using CNNs to rebuild a 3D model of an object \nfrom its 2D image. The network learns many objects and orally accomplishes the regis-\ntration between the image and the conceptual model. Similarly, Hermoza et al. [319] also \nutilize the GAN network for prognosticating the absent geometry of damaged archaeo-\nlogical objects, providing the reconstructed object based on a voxel grid format and a \nlabel selecting its class.\nDL for medical image registration has numerous applications, which were listed by \nsome review papers [320–322]. Yang et al. [323] implemented stacked convolutional lay-\ners as an encoder-decoder approach to predict the morphing of the input pixel into its \nlast formation using MRI brain scans from the OASIS dataset. They employed a regis-\ntration model known as Large Deformation Diffeomorphic Metric Mapping (LDDMM) \nand attained remarkable enhancements in computation time. Miao et al. [324] used syn-\nthetic X-ray images to train a five-layer CNN to register 3D models of a trans-esophageal \nprobe, a hand implant, and a knee implant onto 2D X-ray images for pose estimation. \nThey determined that their model achieved an execution time of 0.1 s, representing \nan important enhancement against the conventional registration techniques based on \nintensity; moreover, it achieved effective registrations 79–99% of the time. Li et al. [325] \nintroduced a neural network-based approach for the non-rigid 2D–3D registration of \nthe lateral cephalogram and the volumetric cone-beam CT (CBCT) images.\nComputational approaches\nFor computationally exhaustive applications, complex ML and DL approaches have rap-\nidly been identified as the most significant techniques and are widely used in different \nfields. The development and enhancement of algorithms aggregated with capabilities of \nwell-behaved computational performance and large datasets make it possible to effec-\ntively execute several applications, as earlier applications were either not possible or dif-\nficult to take into consideration.\nCurrently, several standard DNN configurations are available. The interconnection \npatterns between layers and the total number of layers represent the main differences \nbetween these configurations. The Table 2 illustrates the growth rate of the overall num-\nber of layers over time, which seems to be far faster than the “Moore’s Law growth rate”. \nIn normal DNN, the number of layers grew by around 2.3× each year in the period from \n2012 to 2016. Recent investigations of future ResNet versions reveal that the number of \nlayers can be extended up to 1000. However, an SGD technique is employed to fit the \nweights (or parameters), while different optimization techniques are employed to obtain \nparameter updating during the DNN training process. Repetitive updates are required to \nenhance network accuracy in addition to a minorly augmented rate of enhancement. For \nexample, the training process using ImageNet as a large dataset, which contains more \n\n\nPage 58 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nthan 14 million images, along with ResNet as a network model, take around 30K to 40K \nrepetitions to converge to a steady solution. In addition, the overall computational load, \nas an upper-level prediction, may exceed 1020 FLOPS when both the training set size \nand the DNN complexity increase.\nPrior to 2008, boosting the training to a satisfactory extent was achieved by using \nGPUs. Usually, days or weeks are needed for a training session, even with GPU sup-\nport. By contrast, several optimization strategies were developed to reduce the extensive \nlearning time. The computational requirements are believed to increase as the DNNs \ncontinuously enlarge in both complexity and size.\nIn addition to the computational load cost, the memory bandwidth and capacity have \na significant effect on the entire training performance, and to a lesser extent, deduction. \nMore specifically, the parameters are distributed through every layer of the input data, \nthere is a sizeable amount of reused data, and the computation of several network layers \nexhibits an excessive computation-to-bandwidth ratio. By contrast, there are no distrib-\nuted parameters, the amount of reused data is extremely small, and the additional FC \nlayers have an extremely small computation-to-bandwidth ratio. Table 3 presents a com-\nparison between different aspects related to the devices. In addition, the table is estab-\nlished to facilitate familiarity with the tradeoffs by obtaining the optimal approach for \nconfiguring a system based on either FPGA, GPU, or CPU devices. It should be noted \nthat each has corresponding weaknesses and strengths; accordingly, there are no clear \none-size-fits-all solutions.\nAlthough GPU processing has enhanced the ability to address the computational \nchallenges related to such networks, the maximum GPU (or CPU) performance is not \nachieved, and several techniques or models have turned out to be strongly linked to \nbandwidth. In the worst cases, the GPU efficiency is between 15 and 20% of the maxi-\nmum theoretical performance. This issue is required to enlarge the memory bandwidth \nusing high-bandwidth stacked memory. Next, different approaches based on FPGA, \nGPU, and CPU are accordingly detailed.\nTable 3  A comparison between different aspects related to the devices\nFeature\nAssessment\nLeader\nDevelopment\nCPU is the easiest to program, then GPU, then FPGA\nCPU\nSize\nBoth FPGA and CPU have smaller volume solutions due to their lower \npower consumption\nFPGA-CPU\nCustomization\nBroader flexibility is provided by FPGA\nFPGA\nEase of change\nEasier way to vary application functionality is provided by GPU and CPU\nGPU-CPU\nBackward compatibility Transferring RTL to novel FPGA requires additional work. Furthermore, GPU \nhas less stable architecture than CPU\nCPU\nInterfaces\nSeveral varieties of interfaces can be implemented using FPGA\nFPGA\nProcessing/$\nFPGA configurability assists utilization in wider acceleration space. Due to \nthe considerable processing abilities, GPU wins\nFPGA-GPU\nProcessing/watt\nCustomized designs can be optimized\nFPGA\nTiming latency\nImplemented FPGA algorithm offers deterministic timing, which is in turn \nmuch faster than GPU\nFPGA\nLarge data analysis\nFPGA performs well for inline processing, while CPU supports storage \ncapabilities and largest memory\nFPGA-CPU\nDCNN inference\nFPGA has lower latency and can be customized\nFPGA\nDCNN training\nGreater float-point capabilities provided by GPU\nGPU\n\n\nPage 59 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\nCPU‑based approach\nThe well-behaved performance of the CPU nodes usually assists robust network con-\nnectivity, storage abilities, and large memory. Although CPU nodes are more common-\npurpose than those of FPGA or GPU, they lack the ability to match them in unprocessed \ncomputation facilities, since this requires increased network ability and a larger memory \ncapacity.\nGPU‑based approach\nGPUs are extremely effective for several basic DL primitives, which include greatly \nparallel-computing operations such as activation functions, matrix multiplication, and \nconvolutions [326–330]. Incorporating HBM-stacked memory into the up-to-date \nGPU models significantly enhances the bandwidth. This enhancement allows numerous \nprimitives to efficiently utilize all computational resources of the available GPUs. The \nimprovement in GPU performance over CPU performance is usually 10-20:1 related to \ndense linear algebra operations.\nMaximizing parallel processing is the base of the initial GPU programming model. For \nexample, a GPU model may involve up to sixty-four computational units. There are four \nSIMD engines per each computational layer, and each SIMD has sixteen floating-point \ncomputation lanes. The peak performance is 25 TFLOPS (fp16) and 10 TFLOPS (fp32) \nas the percentage of the employment approaches 100%. Additional GPU performance \nmay be achieved if the addition and multiply functions for vectors combine the inner \nproduction instructions for matching primitives related to matrix operations.\nFor DNN training, the GPU is usually considered to be an optimized design, while for \ninference operations, it may also offer considerable performance improvements.\nFPGA‑based approach\nFPGA is wildly utilized in various tasks including deep learning [199, 247, 331–334]. \nInference accelerators are commonly implemented utilizing FPGA. The FPGA can be \neffectively configured to reduce the unnecessary or overhead functions involved in GPU \nsystems. Compared to GPU, the FPGA is restricted to both weak-behaved floating-point \nperformance and integer inference. The main FPGA aspect is the capability to dynami-\ncally reconfigure the array characteristics (at run-time), as well as the capability to con-\nfigure the array by means of effective design with little or no overhead.\nAs mentioned earlier, the FPGA offers both performance and latency for every watt it \ngains over GPU and CPU in DL inference operations. Implementation of custom high-\nperformance hardware, pruned networks, and reduced arithmetic precision are three \nfactors that enable the FPGA to implement DL algorithms and to achieve FPGA with \nthis level of efficiency. In addition, FPGA may be employed to implement CNN over-\nlay engines with over 80% efficiency, eight-bit accuracy, and over 15 TOPs peak perfor-\nmance; this is used for a few conventional CNNs, as Xillinx and partners demonstrated \nrecently. By contrast, pruning techniques are mostly employed in the LSTM context. The \nsizes of the models can be efficiently minimized by up to 20×, which provides an impor-\ntant benefit during the implementation of the optimal solution, as MLP neural process-\ning demonstrated. A recent study in the field of implementing fixed-point precision and \n\n\nPage 60 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \ncustom floating-point has revealed that lowering the 8-bit is extremely promising; more-\nover, it aids in supplying additional advancements to implementing peak performance \nFPGA related to the DNN models.\nEvaluation metrics\nEvaluation metrics adopted within DL tasks play a crucial role in achieving the opti-\nmized classifier [335]. They are utilized within a usual data classification procedure \nthrough two main stages: training and testing. It is utilized to optimize the classification \nalgorithm during the training stage. This means that the evaluation metric is utilized to \ndiscriminate and select the optimized solution, e.g., as a discriminator, which can gen-\nerate an extra-accurate forecast of upcoming evaluations related to a specific classifier. \nFor the time being, the evaluation metric is utilized to measure the efficiency of the cre-\nated classifier, e.g. as an evaluator, within the model testing stage using hidden data. As \ngiven in Eq. 20, TN and TP are defined as the number of negative and positive instances, \nrespectively, which are successfully classified. In addition, FN and FP are defined as the \nnumber of misclassified positive and negative instances respectively. Next, some of the \nmost well-known evaluation metrics are listed below. \n1.\t Accuracy: Calculates the ratio of correct predicted classes to the total number of \nsamples evaluated (Eq. 20). \n2.\t Sensitivity or Recall: Utilized to calculate the fraction of positive patterns that are \ncorrectly classified (Eq. 21). \n3.\t Specificity: Utilized to calculate the fraction of negative patterns that are correctly \nclassified (Eq. 22). \n4.\t Precision: Utilized to calculate the positive patterns that are correctly predicted by all \npredicted patterns in a positive class (Eq. 23). \n5.\t F1-Score: Calculates the harmonic average between recall and precision rates \n(Eq. 24). \n6.\t J Score: This metric is also called Youdens J statistic. Eq. 25 represents the metric. \n(20)\nAccuracy =\nTP + TN\nTP + TN + FP + FN\n(21)\nSensitivity =\nTP\nTP + FN\n(22)\nSpeciﬁcity =\nTN\nFP + TN\n(23)\nPrecision =\nTP\nTP + FP\n(24)\nF1score = 2 × Precision × Recall\nPrecision + Recall\n\n\nPage 61 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n7.\t False Positive Rate (FPR): This metric refers to the possibility of a false alarm ratio as \ncalculated in Eq. 26\n8.\t Area Under the ROC Curve: AUC is a common ranking type metric. It is utilized to \nconduct comparisons between learning algorithms [336–338], as well as to construct \nan optimal learning model [339, 340]. In contrast to probability and threshold met-\nrics, the AUC value exposes the entire classifier ranking performance. The following \nformula is used to calculate the AUC value for two-class problem [341] (Eq. 27) \n Here, Sp represents the sum of all positive ranked samples. The number of negative \nand positive samples is denoted as nn and np , respectively. Compared to the accu-\nracy metrics, the AUC value was verified empirically and theoretically, making it \nvery helpful for identifying an optimized solution and evaluating the classifier perfor-\nmance through classification training.\n\t\nWhen considering the discrimination and evaluation processes, the AUC perfor-\nmance was brilliant. However, for multiclass issues, the AUC computation is primar-\nily cost-effective when discriminating a large number of created solutions. In addi-\ntion, the time complexity for computing the AUC is O\n\u001f\n|C|2 n log n\n\u001e\n with respect to \nthe Hand and Till AUC model [341] and O\n\u001f\n|C| n log n\n\u001e\n according to Provost and \nDomingo’s AUC model [336].\nFrameworks and datasets\nSeveral DL frameworks and datasets have been developed in the last few years. vari-\nous frameworks and libraries have also been used in order to expedite the work with \ngood results. Through their use, the training process has become easier. Table 4 lists \nthe most utilized frameworks and libraries.\nBased on the star ratings on Github, as well as our own background in the field, \nTensorFlow is deemed the most effective and easy to use. It has the ability to work on \nseveral platforms. (Github is one of the biggest software hosting sites, while Github \nstars refer to how well-regarded a project is on the site). Moreover, there are several \nother benchmark datasets employed for different DL tasks. Some of these are listed in \nTable 5.\nSummary and conclusion\nFinally, it is mandatory the inclusion of a brief discussion by gathering all the relevant \ndata provided along this extensive research. Next, an itemized analysis is presented in \norder to conclude our review and exhibit the future directions.\n(25)\nJscore = Sensitivity + Speciﬁcity −1\n(26)\nFPR = 1 −Speciﬁcity\n(27)\nAUC = Sp −np(nn + 1)/2\nnpnn\n\n\nPage 62 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n•\t DL already experiences difficulties in simultaneously modeling multi-complex \nmodalities of data. In recent DL developments, another common approach is that of \nmultimodal DL.\n•\t DL requires sizeable datasets (labeled data preferred) to predict unseen data and to \ntrain the models. This challenge turns out to be particularly difficult when real-time \ndata processing is required or when the provided datasets are limited (such as in the \nTable 5  Benchmark datasets\nDataset\nNum. of classes Applications\nLink to dataset\nImageNet\n1000\nImage classification, object \nlocalization, object detection, \netc.\nhttp://​www.​image-​net.​org/\nCIFAR10/100\n10/100\nImage classification\nhttps://​www.​cs.​toron​to.​edu/​~kriz/​\ncifar.​html\nMNIST\n10\nClassification of handwritten \ndigits\nhttp://​yann.​lecun.​com/​exdb/​\nmnist/\nPascal VOC\n20\nImage classification, segmenta‑\ntion, object detection\nhttp://​host.​robots.​ox.​ac.​uk/​pascal/​\nVOC/​voc20​12/\nMicrosoft COCO\n80\nObject detection, semantic \nsegmentation\nhttps://​cocod​ataset.​org/#​home\nYFCC100M\n8M\nVideo and image understanding\nhttp://​proje​cts.​dfki.​unikl.​de/​yfcc1​\n00m/\nYouTube-8M\n4716\nVideo classification\nhttps://​resea​rch.​google.​com/​\nyoutu​be8m/\nUCF-101\n101\nHuman action detection\nhttps://​www.​crcv.​ucf.​edu/​data/​\nUCF101.​php\nKinetics\n400\nHuman action detection\nhttps://​deepm​ind.​com/​resea​rch/​\nopen-​source/​kinet​ics\nGoogle Open Images\n350\nImage classification, segmenta‑\ntion, object detection\nhttps://​stora​ge.​googl​eapis.​com/​\nopeni​mages/​web/​index.​html\nCalTech101\n101\nClassification\nhttp://​www.​vision.​calte​ch.​edu/​\nImage_​Datas​ets/​Calte​ch101/\nLabeled Faces in the Wild –\nFace recognition\nhttp://​vis-​www.​cs.​umass.​edu/​lfw/\nMIT-67 scene dataset\n67\nIndoor scene recognition\nhttp://​web.​mit.​edu/​torra​lba/​\nwww/​indoor.​htm\nTable 4  List of the most common frameworks and libraries\nFramework\nLicense\nCore language\nYear of release\nHomepages\nTensorFlow\nApache 2.0\nC++ & Python\n2015\nhttps://​www.​tenso​rflow.​org/\nKeras\nMIT\nPython\n2015\nhttps://​keras.​io/\nCaffe\nBSD\nC++\n2015\nhttp://​caffe.​berke​leyvi​sion.​org/\nMatConvNet\nOxford\nMATLAB\n2014\nhttp://​www.​vlfeat.​org/​matco​nvnet/\nMXNet\nApache 2.0\nC++\n2015\nhttps://​github.​com/​dmlc/​mxnet\nCNTK\nMIT\nC++\n2016\nhttps://​github.​com/​Micro​soft/​CNTK\nTheano\nBSD\nPython\n2008\nhttp://​deepl​earni​ng.​net/​softw​are/​theano/\nTorch\nBSD\nC & Lua\n2002\nhttp://​torch.​ch/\nDL4j\nApache 2.0\nJava\n2014\nhttps://​deepl​earni​ng4j.​org/\nGluon\nAWS Microsoft\nC++\n2017\nhttps://​github.​com/​gluon-​api/​gluon-​api/\nOpenDeep\nMIT\nPython\n2017\nhttp://​www.​opend​eep.​org/\n\n\nPage 63 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\ncase of healthcare data). To alleviate this issue, TL and data augmentation have been \nresearched over the last few years.\n•\t Although ML slowly transitions to semi-supervised and unsupervised learning to \nmanage practical data without the need for manual human labeling, many of the cur-\nrent deep-learning models utilize supervised learning.\n•\t The CNN performance is greatly influenced by hyper-parameter selection. Any small \nchange in the hyper-parameter values will affect the general CNN performance. \nTherefore, careful parameter selection is an extremely significant issue that should be \nconsidered during optimization scheme development.\n•\t Impressive and robust hardware resources like GPUs are required for effective CNN \ntraining. Moreover, they are also required for exploring the efficiency of using CNN \nin smart and embedded systems.\n•\t In the CNN context, ensemble learning [342, 343] represents a prospective research \narea. The collection of different and multiple architectures will support the model \nin improving its generalizability across different image categories through extracting \nseveral levels of semantic image representation. Similarly, ideas such as new activa-\ntion functions, dropout, and batch normalization also merit further investigation.\n•\t The exploitation of depth and different structural adaptations is significantly \nimproved in the CNN learning capacity. Substituting the traditional layer configura-\ntion with blocks results in significant advances in CNN performance, as has been \nshown in the recent literature. Currently, developing novel and efficient block archi-\ntectures is the main trend in new research models of CNN architectures. HRNet is \nonly one example that shows there are always ways to improve the architecture.\n•\t It is expected that cloud-based platforms will play an essential role in the future \ndevelopment of computational DL applications. Utilizing cloud computing offers \na solution to handling the enormous amount of data. It also helps to increase effi-\nciency and reduce costs. Furthermore, it offers the flexibility to train DL architec-\ntures.\n•\t With the recent development in computational tools including a chip for neural net-\nworks and a mobile GPU, we will see more DL applications on mobile devices. It will \nbe easier for users to use DL.\n•\t Regarding the issue of lack of training data, It is expected that various techniques of \ntransfer learning will be considered such as training the DL model on large unlabeled \nimage datasets and next transferring the knowledge to train the DL model on a small \nnumber of labeled images for the same task.\n•\t Last, this overview provides a starting point for the community of DL being inter-\nested in the field of DL. Furthermore, researchers would be allowed to decide the \nmore suitable direction of work to be taken in order to provide more accurate alter-\nnatives to the field.\nAcknowledgements\nWe would like to thank the professors from the Queensland University of Technology and the University of Information \nTechnology and Communications who gave their feedback on the paper.\nAuthors’ contributions\nConceptualization: LA, and JZ; methodology: LA, JZ, and JS; software: LA, and MAF; validation: LA, JZ, MA, and LF; formal \nanalysis: LA, JZ, YD, and JS; investigation: LA, and JZ; resources: LA, JZ, and MAF; data curation: LA, and OA.; writing–origi‑\nnal draft preparation: LA, and OA; writing—review and editing: LA, JZ, AJH, AA, YD, OA, JS, MAF, MA, and LF; visualization: \n\n\nPage 64 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \nLA, and MAF; supervision: JZ, and YD; project administration: JZ, YD, and JS; funding acquisition: LA, AJH, AA, and YD. All \nauthors read and approved the final manuscript.\nFunding\nThis research received no external funding.\nAvailability of data and materials\nNot applicable.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 School of Computer Science, Queensland University of Technology, Brisbane, QLD 4000, Australia. 2 Control and Sys‑\ntems Engineering Department, University of Technology, Baghdad 10001, Iraq. 3 Electrical Engineering Technical College, \nMiddle Technical University, Baghdad 10001, Iraq. 4 Faculty of Electrical Engineering & Computer Science, University \nof Missouri, Columbia, MO 65211, USA. 5 AlNidhal Campus, University of Information Technology & Communications, \nBaghdad 10001, Iraq. 6 Department of Computer Science, University of Jaén, 23071 Jaén, Spain. 7 College of Computer \nScience and Information Technology, University of Sumer, Thi Qar 64005, Iraq. 8 School of Engineering, Manchester \nMetropolitan University, Manchester M1 5GD, UK. \nReceived: 21 January 2021   Accepted: 22 March 2021\nReferences\n\t\n1.\t Rozenwald MB, Galitsyna AA, Sapunov GV, Khrameeva EE, Gelfand MS. A machine learning framework for the \nprediction of chromatin folding in Drosophila using epigenetic features. PeerJ Comput Sci. 2020;6:307.\n\t\n2.\t Amrit C, Paauw T, Aly R, Lavric M. Identifying child abuse through text mining and machine learning. Expert Syst \nAppl. 2017;88:402–18.\n\t\n3.\t Hossain E, Khan I, Un-Noor F, Sikander SS, Sunny MSH. Application of big data and machine learning in smart grid, \nand associated security concerns: a review. IEEE Access. 2019;7:13960–88.\n\t\n4.\t Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine \nlearning techniques. J Big Data. 2015;2(1):23.\n\t\n5.\t Deldjoo Y, Elahi M, Cremonesi P, Garzotto F, Piazzolla P, Quadrana M. Content-based video recommendation \nsystem based on stylistic visual features. J Data Semant. 2016;5(2):99–113.\n\t\n6.\t Al-Dulaimi K, Chandran V, Nguyen K, Banks J, Tomeo-Reyes I. Benchmarking hep-2 specimen cells classifica‑\ntion using linear discriminant analysis on higher order spectra features of cell shape. Pattern Recogn Lett. \n2019;125:534–41.\n\t\n7.\t Liu W, Wang Z, Liu X, Zeng N, Liu Y, Alsaadi FE. A survey of deep neural network architectures and their applica‑\ntions. Neurocomputing. 2017;234:11–26.\n\t\n8.\t Pouyanfar S, Sadiq S, Yan Y, Tian H, Tao Y, Reyes MP, Shyu ML, Chen SC, Iyengar S. A survey on deep learning: algo‑\nrithms, techniques, and applications. ACM Comput Surv (CSUR). 2018;51(5):1–36.\n\t\n9.\t Alom MZ, Taha TM, Yakopcic C, Westberg S, Sidike P, Nasrin MS, Hasan M, Van Essen BC, Awwal AA, Asari VK. A \nstate-of-the-art survey on deep learning theory and architectures. Electronics. 2019;8(3):292.\n\t 10.\t Potok TE, Schuman C, Young S, Patton R, Spedalieri F, Liu J, Yao KT, Rose G, Chakma G. A study of complex deep \nlearning networks on high-performance, neuromorphic, and quantum computers. ACM J Emerg Technol Comput \nSyst (JETC). 2018;14(2):1–21.\n\t 11.\t Adeel A, Gogate M, Hussain A. Contextual deep learning-based audio-visual switching for speech enhancement \nin real-world environments. Inf Fusion. 2020;59:163–70.\n\t 12.\t Tian H, Chen SC, Shyu ML. Evolutionary programming based deep learning feature selection and network con‑\nstruction for visual data classification. Inf Syst Front. 2020;22(5):1053–66.\n\t 13.\t Young T, Hazarika D, Poria S, Cambria E. Recent trends in deep learning based natural language processing. IEEE \nComput Intell Mag. 2018;13(3):55–75.\n\t 14.\t Koppe G, Meyer-Lindenberg A, Durstewitz D. Deep learning for small and big data in psychiatry. Neuropsychop‑\nharmacology. 2021;46(1):176–90.\n\t 15.\t Dalal N, Triggs B. Histograms of oriented gradients for human detection. In: 2005 IEEE computer society confer‑\nence on computer vision and pattern recognition (CVPR’05), vol. 1. IEEE; 2005. p. 886–93.\n\t 16.\t Lowe DG. Object recognition from local scale-invariant features. In: Proceedings of the seventh IEEE international \nconference on computer vision, vol. 2. IEEE; 1999. p. 1150–7.\n\t 17.\t Wu L, Hoi SC, Yu N. Semantics-preserving bag-of-words models and applications. IEEE Trans Image Process. \n2010;19(7):1908–20.\n\n\nPage 65 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n\t 18.\t LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436–44.\n\t 19.\t Yao G, Lei T, Zhong J. A review of convolutional-neural-network-based action recognition. Pattern Recogn Lett. \n2019;118:14–22.\n\t 20.\t Dhillon A, Verma GK. Convolutional neural network: a review of models, methodologies and applications to object \ndetection. Prog Artif Intell. 2020;9(2):85–112.\n\t 21.\t Khan A, Sohail A, Zahoora U, Qureshi AS. A survey of the recent architectures of deep convolutional neural net‑\nworks. Artif Intell Rev. 2020;53(8):5455–516.\n\t 22.\t Hasan RI, Yusuf SM, Alzubaidi L. Review of the state of the art of deep learning for plant diseases: a broad analysis \nand discussion. Plants. 2020;9(10):1302.\n\t 23.\t Xiao Y, Tian Z, Yu J, Zhang Y, Liu S, Du S, Lan X. A review of object detection based on deep learning. Multimed \nTools Appl. 2020;79(33):23729–91.\n\t 24.\t Ker J, Wang L, Rao J, Lim T. Deep learning applications in medical image analysis. IEEE Access. 2017;6:9375–89.\n\t 25.\t Zhang Z, Cui P, Zhu W. Deep learning on graphs: a survey. IEEE Trans Knowl Data Eng. 2020. https://​doi.​org/​10.​\n1109/​TKDE.​2020.​29813​33.\n\t 26.\t Shrestha A, Mahmood A. Review of deep learning algorithms and architectures. IEEE Access. 2019;7:53040–65.\n\t 27.\t Najafabadi MM, Villanustre F, Khoshgoftaar TM, Seliya N, Wald R, Muharemagic E. Deep learning applications and \nchallenges in big data analytics. J Big Data. 2015;2(1):1.\n\t 28.\t Goodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning, vol. 1. Cambridge: MIT press; 2016.\n\t 29.\t Shorten C, Khoshgoftaar TM, Furht B. Deep learning applications for COVID-19. J Big Data. 2021;8(1):1–54.\n\t 30.\t Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Commun \nACM. 2017;60(6):84–90.\n\t 31.\t Bhowmick S, Nagarajaiah S, Veeraraghavan A. Vision and deep learning-based algorithms to detect and quantify \ncracks on concrete surfaces from uav videos. Sensors. 2020;20(21):6299.\n\t 32.\t Goh GB, Hodas NO, Vishnu A. Deep learning for computational chemistry. J Comput Chem. 2017;38(16):1291–307.\n\t 33.\t Li Y, Zhang T, Sun S, Gao X. Accelerating flash calculation through deep learning methods. J Comput Phys. \n2019;394:153–65.\n\t 34.\t Yang W, Zhang X, Tian Y, Wang W, Xue JH, Liao Q. Deep learning for single image super-resolution: a brief review. \nIEEE Trans Multimed. 2019;21(12):3106–21.\n\t 35.\t Tang J, Li S, Liu P. A review of lane detection methods based on deep learning. Pattern Recogn. 2020;111:107623.\n\t 36.\t Zhao ZQ, Zheng P, Xu ST, Wu X. Object detection with deep learning: a review. IEEE Trans Neural Netw Learn Syst. \n2019;30(11):3212–32.\n\t 37.\t He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference \non computer vision and pattern recognition; 2016. p. 770–8.\n\t 38.\t Ng A. Machine learning yearning: technical strategy for AI engineers in the era of deep learning. 2019. https://​\nwww.​mlyea​rning.​org.\n\t 39.\t Metz C. Turing award won by 3 pioneers in artificial intelligence. The New York Times. 2019;27.\n\t 40.\t Nevo S, Anisimov V, Elidan G, El-Yaniv R, Giencke P, Gigi Y, Hassidim A, Moshe Z, Schlesinger M, Shalev G, et al. Ml \nfor flood forecasting at scale; 2019. arXiv preprint arXiv:​1901.​09583.\n\t 41.\t Chen H, Engkvist O, Wang Y, Olivecrona M, Blaschke T. The rise of deep learning in drug discovery. Drug Discov \nToday. 2018;23(6):1241–50.\n\t 42.\t Benhammou Y, Achchab B, Herrera F, Tabik S. Breakhis based breast cancer automatic diagnosis using deep learn‑\ning: taxonomy, survey and insights. Neurocomputing. 2020;375:9–24.\n\t 43.\t Wulczyn E, Steiner DF, Xu Z, Sadhwani A, Wang H, Flament-Auvigne I, Mermel CH, Chen PHC, Liu Y, Stumpe MC. \nDeep learning-based survival prediction for multiple cancer types using histopathology images. PLoS ONE. \n2020;15(6):e0233678.\n\t 44.\t Nagpal K, Foote D, Liu Y, Chen PHC, Wulczyn E, Tan F, Olson N, Smith JL, Mohtashamian A, Wren JH, et al. Develop‑\nment and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer. NPJ Digit \nMed. 2019;2(1):1–10.\n\t 45.\t Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer \nwith deep neural networks. Nature. 2017;542(7639):115–8.\n\t 46.\t Brunese L, Mercaldo F, Reginelli A, Santone A. Explainable deep learning for pulmonary disease and coronavirus \nCOVID-19 detection from X-rays. Comput Methods Programs Biomed. 2020;196(105):608.\n\t 47.\t Jamshidi M, Lalbakhsh A, Talla J, Peroutka Z, Hadjilooei F, Lalbakhsh P, Jamshidi M, La Spada L, Mirmozafari M, \nDehghani M, et al. Artificial intelligence and COVID-19: deep learning approaches for diagnosis and treatment. \nIEEE Access. 2020;8:109581–95.\n\t 48.\t Shorfuzzaman M, Hossain MS. Metacovid: a siamese neural network framework with contrastive loss for n-shot \ndiagnosis of COVID-19 patients. Pattern Recogn. 2020;113:107700.\n\t 49.\t Carvelli L, Olesen AN, Brink-Kjær A, Leary EB, Peppard PE, Mignot E, Sørensen HB, Jennum P. Design of a deep \nlearning model for automatic scoring of periodic and non-periodic leg movements during sleep validated against \nmultiple human experts. Sleep Med. 2020;69:109–19.\n\t 50.\t De Fauw J, Ledsam JR, Romera-Paredes B, Nikolov S, Tomasev N, Blackwell S, Askham H, Glorot X, O’Donoghue \nB, Visentin D, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat Med. \n2018;24(9):1342–50.\n\t 51.\t Topol EJ. High-performance medicine: the convergence of human and artificial intelligence. Nat Med. \n2019;25(1):44–56.\n\t 52.\t Kermany DS, Goldbaum M, Cai W, Valentim CC, Liang H, Baxter SL, McKeown A, Yang G, Wu X, Yan F, et al. Identify‑\ning medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31.\n\t 53.\t Van Essen B, Kim H, Pearce R, Boakye K, Chen B. Lbann: livermore big artificial neural network HPC toolkit. In: \nProceedings of the workshop on machine learning in high-performance computing environments; 2015. p. 1–6.\n\t 54.\t Saeed MM, Al Aghbari Z, Alsharidah M. Big data clustering techniques based on spark: a literature review. PeerJ \nComput Sci. 2020;6:321.\n\n\nPage 66 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t 55.\t Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski \nG, et al. Human-level control through deep reinforcement learning. Nature. 2015;518(7540):529–33.\n\t 56.\t Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA. Deep reinforcement learning: a brief survey. IEEE Signal \nProcess Mag. 2017;34(6):26–38.\n\t 57.\t Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compo‑\nsitionality over a sentiment treebank. In: Proceedings of the 2013 conference on empirical methods in natural \nlanguage processing; 2013. p. 1631–42.\n\t 58.\t Goller C, Kuchler A. Learning task-dependent distributed representations by backpropagation through structure. \nIn: Proceedings of international conference on neural networks (ICNN’96), vol 1. IEEE; 1996. p. 347–52.\n\t 59.\t Socher R, Lin CCY, Ng AY, Manning CD. Parsing natural scenes and natural language with recursive neural net‑\nworks. In: ICML; 2011.\n\t 60.\t Louppe G, Cho K, Becot C, Cranmer K. QCD-aware recursive neural networks for jet physics. J High Energy Phys. \n2019;2019(1):57.\n\t 61.\t Sadr H, Pedram MM, Teshnehlab M. A robust sentiment analysis method based on sequential combination of \nconvolutional and recursive neural networks. Neural Process Lett. 2019;50(3):2745–61.\n\t 62.\t Urban G, Subrahmanya N, Baldi P. Inner and outer recursive neural networks for chemoinformatics applications. J \nChem Inf Model. 2018;58(2):207–11.\n\t 63.\t Hewamalage H, Bergmeir C, Bandara K. Recurrent neural networks for time series forecasting: current status and \nfuture directions. Int J Forecast. 2020;37(1):388–427.\n\t 64.\t Jiang Y, Kim H, Asnani H, Kannan S, Oh S, Viswanath P. Learn codes: inventing low-latency codes via recurrent \nneural networks. IEEE J Sel Areas Inf Theory. 2020;1(1):207–16.\n\t 65.\t John RA, Acharya J, Zhu C, Surendran A, Bose SK, Chaturvedi A, Tiwari N, Gao Y, He Y, Zhang KK, et al. Optogenetics \ninspired transition metal dichalcogenide neuristors for in-memory deep recurrent neural networks. Nat Commun. \n2020;11(1):1–9.\n\t 66.\t Batur Dinler Ö, Aydin N. An optimal feature parameter set based on gated recurrent unit recurrent neural net‑\nworks for speech segment detection. Appl Sci. 2020;10(4):1273.\n\t 67.\t Jagannatha AN, Yu H. Structured prediction models for RNN based sequence labeling in clinical text. In: Proceed‑\nings of the conference on empirical methods in natural language processing. conference on empirical methods \nin natural language processing, vol. 2016, NIH Public Access; 2016. p. 856.\n\t 68.\t Pascanu R, Gulcehre C, Cho K, Bengio Y. How to construct deep recurrent neural networks. In: Proceedings of the \nsecond international conference on learning representations (ICLR 2014); 2014.\n\t 69.\t Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of \nthe thirteenth international conference on artificial intelligence and statistics; 2010. p. 249–56.\n\t 70.\t Gao C, Yan J, Zhou S, Varshney PK, Liu H. Long short-term memory-based deep recurrent neural networks for \ntarget tracking. Inf Sci. 2019;502:279–96.\n\t 71.\t Zhou DX. Theory of deep convolutional neural networks: downsampling. Neural Netw. 2020;124:319–27.\n\t 72.\t Jhong SY, Tseng PY, Siriphockpirom N, Hsia CH, Huang MS, Hua KL, Chen YY. An automated biometric identifica‑\ntion system using CNN-based palm vein recognition. In: 2020 international conference on advanced robotics and \nintelligent systems (ARIS). IEEE; 2020. p. 1–6.\n\t 73.\t Al-Azzawi A, Ouadou A, Max H, Duan Y, Tanner JJ, Cheng J. Deepcryopicker: fully automated deep neural network \nfor single protein particle picking in cryo-EM. BMC Bioinform. 2020;21(1):1–38.\n\t 74.\t Wang T, Lu C, Yang M, Hong F, Liu C. A hybrid method for heartbeat classification via convolutional neural net‑\nworks, multilayer perceptrons and focal loss. PeerJ Comput Sci. 2020;6:324.\n\t 75.\t Li G, Zhang M, Li J, Lv F, Tong G. Efficient densely connected convolutional neural networks. Pattern Recogn. \n2021;109:107610.\n\t 76.\t Gu J, Wang Z, Kuen J, Ma L, Shahroudy A, Shuai B, Liu T, Wang X, Wang G, Cai J, et al. Recent advances in convolu‑\ntional neural networks. Pattern Recogn. 2018;77:354–77.\n\t 77.\t Fang W, Love PE, Luo H, Ding L. Computer vision for behaviour-based safety in construction: a review and future \ndirections. Adv Eng Inform. 2020;43:100980.\n\t 78.\t Palaz D, Magimai-Doss M, Collobert R. End-to-end acoustic modeling using convolutional neural networks for \nhmm-based automatic speech recognition. Speech Commun. 2019;108:15–32.\n\t 79.\t Li HC, Deng ZY, Chiang HH. Lightweight and resource-constrained learning network for face recognition with \nperformance optimization. Sensors. 2020;20(21):6114.\n\t 80.\t Hubel DH, Wiesel TN. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. J \nPhysiol. 1962;160(1):106.\n\t 81.\t Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift; \n2015. arXiv preprint arXiv:​1502.​03167.\n\t 82.\t Ruder S. An overview of gradient descent optimization algorithms; 2016. arXiv preprint arXiv:​1609.​04747.\n\t 83.\t Bottou L. Large-scale machine learning with stochastic gradient descent. In: Proceedings of COMPSTAT’2010. \nSpringer; 2010. p. 177–86.\n\t 84.\t Hinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradi‑\nent descent. Cited on. 2012;14(8).\n\t 85.\t Zhang Z. Improved Adam optimizer for deep neural networks. In: 2018 IEEE/ACM 26th international symposium \non quality of service (IWQoS). IEEE; 2018. p. 1–2.\n\t 86.\t Alzubaidi L, Fadhel MA, Al-Shamma O, Zhang J, Duan Y. Deep learning models for classification of red blood cells \nin microscopy images to aid in sickle cell anemia diagnosis. Electronics. 2020;9(3):427.\n\t 87.\t Alzubaidi L, Fadhel MA, Al-Shamma O, Zhang J, Santamaría J, Duan Y, Oleiwi SR. Towards a better understanding of \ntransfer learning for medical imaging: a case study. Appl Sci. 2020;10(13):4523.\n\t 88.\t Alzubaidi L, Al-Shamma O, Fadhel MA, Farhan L, Zhang J, Duan Y. Optimizing the performance of breast cancer \nclassification by employing the same domain transfer learning from hybrid deep convolutional neural network \nmodel. Electronics. 2020;9(3):445.\n\n\nPage 67 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n\t 89.\t LeCun Y, Jackel LD, Bottou L, Cortes C, Denker JS, Drucker H, Guyon I, Muller UA, Sackinger E, Simard P, et al. Learn‑\ning algorithms for classification: a comparison on handwritten digit recognition. Neural Netw Stat Mech Perspect. \n1995;261:276.\n\t 90.\t Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural net‑\nworks from overfitting. J Mach Learn Res. 2014;15(1):1929–58.\n\t 91.\t Dahl GE, Sainath TN, Hinton GE. Improving deep neural networks for LVCSR using rectified linear units and drop‑\nout. In: 2013 IEEE international conference on acoustics, speech and signal processing. IEEE; 2013. p. 8609–13.\n\t 92.\t Xu B, Wang N, Chen T, Li M. Empirical evaluation of rectified activations in convolutional network; 2015. arXiv \npreprint arXiv:​1505.​00853.\n\t 93.\t Hochreiter S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int J \nUncertain Fuzziness Knowl Based Syst. 1998;6(02):107–16.\n\t 94.\t Lin M, Chen Q, Yan S. Network in network; 2013. arXiv preprint arXiv:​1312.​4400.\n\t 95.\t Hsiao TY, Chang YC, Chou HH, Chiu CT. Filter-based deep-compression with global average pooling for convolu‑\ntional networks. J Syst Arch. 2019;95:9–18.\n\t 96.\t Li Z, Wang SH, Fan RR, Cao G, Zhang YD, Guo T. Teeth category classification via seven-layer deep convolutional \nneural network with max pooling and global average pooling. Int J Imaging Syst Technol. 2019;29(4):577–83.\n\t 97.\t Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. In: European conference on computer \nvision. Springer; 2014. p. 818–33.\n\t 98.\t Erhan D, Bengio Y, Courville A, Vincent P. Visualizing higher-layer features of a deep network. Univ Montreal. \n2009;1341(3):1.\n\t 99.\t Le QV. Building high-level features using large scale unsupervised learning. In: 2013 IEEE international conference \non acoustics, speech and signal processing. IEEE; 2013. p. 8595–8.\n\t100.\t Grün F, Rupprecht C, Navab N, Tombari F. A taxonomy and library for visualizing learned features in convolutional \nneural networks; 2016. arXiv preprint arXiv:​1606.​07757.\n\t101.\t Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition; 2014. arXiv pre‑\nprint arXiv:​1409.​1556.\n\t102.\t Ranzato M, Huang FJ, Boureau YL, LeCun Y. Unsupervised learning of invariant feature hierarchies with applications \nto object recognition. In: 2007 IEEE conference on computer vision and pattern recognition. IEEE; 2007. p. 1–8.\n\t103.\t Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with \nconvolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2015. p. 1–9.\n\t104.\t Bengio Y, et al. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization; 2015. arXiv:​1502.​\n04390corr abs/1502.04390\n\t105.\t Srivastava RK, Greff K, Schmidhuber J. Highway networks; 2015. arXiv preprint arXiv:​1505.​00387.\n\t106.\t Kong W, Dong ZY, Jia Y, Hill DJ, Xu Y, Zhang Y. Short-term residential load forecasting based on LSTM recurrent \nneural network. IEEE Trans Smart Grid. 2017;10(1):841–51.\n\t107.\t Ordóñez FJ, Roggen D. Deep convolutional and LSTM recurrent neural networks for multimodal wearable activity \nrecognition. Sensors. 2016;16(1):115.\n\t108.\t CireşAn D, Meier U, Masci J, Schmidhuber J. Multi-column deep neural network for traffic sign classification. Neural \nNetw. 2012;32:333–8.\n\t109.\t Szegedy C, Ioffe S, Vanhoucke V, Alemi A. Inception-v4, inception-resnet and the impact of residual connections \non learning; 2016. arXiv preprint arXiv:​1602.​07261.\n\t110.\t Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inception architecture for computer vision. In: \nProceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 2818–26.\n\t111.\t Wu S, Zhong S, Liu Y. Deep residual learning for image steganalysis. Multimed Tools Appl. 2018;77(9):10437–53.\n\t112.\t Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: Proceedings of \nthe IEEE conference on computer vision and pattern recognition; 2017. p. 4700–08.\n\t113.\t Rubin J, Parvaneh S, Rahman A, Conroy B, Babaeizadeh S. Densely connected convolutional networks for detec‑\ntion of atrial fibrillation from short single-lead ECG recordings. J Electrocardiol. 2018;51(6):S18-21.\n\t114.\t Kuang P, Ma T, Chen Z, Li F. Image super-resolution with densely connected convolutional networks. Appl Intell. \n2019;49(1):125–36.\n\t115.\t Xie S, Girshick R, Dollár P, Tu Z, He K. Aggregated residual transformations for deep neural networks. In: Proceed‑\nings of the IEEE conference on computer vision and pattern recognition; 2017. p. 1492–500.\n\t116.\t Su A, He X, Zhao X. Jpeg steganalysis based on ResNeXt with gauss partial derivative filters. Multimed Tools Appl. \n2020;80(3):3349–66.\n\t117.\t Yadav D, Jalal A, Garlapati D, Hossain K, Goyal A, Pant G. Deep learning-based ResNeXt model in phycological stud‑\nies for future. Algal Res. 2020;50:102018.\n\t118.\t Han W, Feng R, Wang L, Gao L. Adaptive spatial-scale-aware deep convolutional neural network for high-resolu‑\ntion remote sensing imagery scene classification. In: IGARSS 2018-2018 IEEE international geoscience and remote \nsensing symposium. IEEE; 2018. p. 4736–9.\n\t119.\t Zagoruyko S, Komodakis N. Wide residual networks; 2016. arXiv preprint arXiv:​1605.​07146.\n\t120.\t Huang G, Sun Y, Liu Z, Sedra D, Weinberger KQ. Deep networks with stochastic depth. In: European conference on \ncomputer vision. Springer; 2016. p. 646–61.\n\t121.\t Huynh HT, Nguyen H. Joint age estimation and gender classification of Asian faces using wide ResNet. SN Comput \nSci. 2020;1(5):1–9.\n\t122.\t Takahashi R, Matsubara T, Uehara K. Data augmentation using random image cropping and patching for deep \ncnns. IEEE Trans Circuits Syst Video Technol. 2019;30(9):2917–31.\n\t123.\t Han D, Kim J, Kim J. Deep pyramidal residual networks. In: Proceedings of the IEEE conference on computer vision \nand pattern recognition; 2017. p. 5927–35.\n\t124.\t Wang Y, Wang L, Wang H, Li P. End-to-end image super-resolution via deep and shallow convolutional networks. \nIEEE Access. 2019;7:31959–70.\n\n\nPage 68 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t125.\t Chollet F. Xception: Deep learning with depthwise separable convolutions. In: Proceedings of the IEEE conference \non computer vision and pattern recognition; 2017. p. 1251–8.\n\t126.\t Lo WW, Yang X, Wang Y. An xception convolutional neural network for malware classification with transfer learn‑\ning. In: 2019 10th IFIP international conference on new technologies, mobility and security (NTMS). IEEE; 2019. p. \n1–5.\n\t127.\t Rahimzadeh M, Attar A. A modified deep convolutional neural network for detecting COVID-19 and pneumo‑\nnia from chest X-ray images based on the concatenation of xception and resnet50v2. Inform Med Unlocked. \n2020;19:100360.\n\t128.\t Wang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X. Residual attention network for image classification. \nIn: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 3156–64.\n\t129.\t Salakhutdinov R, Larochelle H. Efficient learning of deep boltzmann machines. In: Proceedings of the thirteenth \ninternational conference on artificial intelligence and statistics; 2010. p. 693–700.\n\t130.\t Goh H, Thome N, Cord M, Lim JH. Top-down regularization of deep belief networks. Adv Neural Inf Process Syst. \n2013;26:1878–86.\n\t131.\t Guan J, Lai R, Xiong A, Liu Z, Gu L. Fixed pattern noise reduction for infrared images based on cascade residual \nattention CNN. Neurocomputing. 2020;377:301–13.\n\t132.\t Bi Q, Qin K, Zhang H, Li Z, Xu K. RADC-Net: a residual attention based convolution network for aerial scene clas‑\nsification. Neurocomputing. 2020;377:345–59.\n\t133.\t Jaderberg M, Simonyan K, Zisserman A, et al. Spatial transformer networks. In: Advances in neural information \nprocessing systems. San Mateo: Morgan Kaufmann Publishers; 2015. p. 2017–25.\n\t134.\t Hu J, Shen L, Sun G. Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision \nand pattern recognition; 2018. p. 7132–41.\n\t135.\t Mou L, Zhu XX. Learning to pay attention on spectral domain: a spectral attention module-based convolutional \nnetwork for hyperspectral image classification. IEEE Trans Geosci Remote Sens. 2019;58(1):110–22.\n\t136.\t Woo S, Park J, Lee JY, So Kweon I. CBAM: Convolutional block attention module. In: Proceedings of the European \nconference on computer vision (ECCV); 2018. p. 3–19.\n\t137.\t Roy AG, Navab N, Wachinger C. Concurrent spatial and channel ‘squeeze & excitation’ in fully convolutional net‑\nworks. In: International conference on medical image computing and computer-assisted intervention. Springer; \n2018. p. 421–9.\n\t138.\t Roy AG, Navab N, Wachinger C. Recalibrating fully convolutional networks with spatial and channel “squeeze and \nexcitation’’ blocks. IEEE Trans Med Imaging. 2018;38(2):540–9.\n\t139.\t Sabour S, Frosst N, Hinton GE. Dynamic routing between capsules. In: Advances in neural information processing \nsystems. San Mateo: Morgan Kaufmann Publishers; 2017. p. 3856–66.\n\t140.\t Arun P, Buddhiraju KM, Porwal A. Capsulenet-based spatial-spectral classifier for hyperspectral images. IEEE J Sel \nTopics Appl Earth Obs Remote Sens. 2019;12(6):1849–65.\n\t141.\t Xinwei L, Lianghao X, Yi Y. Compact video fingerprinting via an improved capsule net. Syst Sci Control Eng. \n2020;9:1–9.\n\t142.\t Ma B, Li X, Xia Y, Zhang Y. Autonomous deep learning: a genetic DCNN designer for image classification. Neuro‑\ncomputing. 2020;379:152–61.\n\t143.\t Wang J, Sun K, Cheng T, Jiang B, Deng C, Zhao Y, Liu D, Mu Y, Tan M, Wang X, et al. Deep high-resolution repre‑\nsentation learning for visual recognition. IEEE Trans Pattern Anal Mach Intell. 2020. https://​doi.​org/​10.​1109/​TPAMI.​\n2020.​29836​86.\n\t144.\t Cheng B, Xiao B, Wang J, Shi H, Huang TS, Zhang L. Higherhrnet: scale-aware representation learning for bottom-\nup human pose estimation. In: CVPR 2020; 2020. https://​www.​micro​soft.​com/​en-​us/​resea​rch/​publi​cation/​highe​\nrhrnet-​scale-​aware-​repre​senta​tion-​learn​ing-​for-​bottom-​up-​human-​pose-​estim​ation/.\n\t145.\t Karimi H, Derr T, Tang J. Characterizing the decision boundary of deep neural networks; 2019. arXiv preprint arXiv:​\n1912.​11460.\n\t146.\t Li Y, Ding L, Gao X. On the decision boundary of deep neural networks; 2018. arXiv preprint arXiv:​1808.​05385.\n\t147.\t Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? In: Advances in \nneural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2014. p. 3320–8.\n\t148.\t Tan C, Sun F, Kong T, Zhang W, Yang C, Liu C. A survey on deep transfer learning. In: International conference on \nartificial neural networks. Springer; 2018. p. 270–9.\n\t149.\t Weiss K, Khoshgoftaar TM, Wang D. A survey of transfer learning. J Big Data. 2016;3(1):9.\n\t150.\t Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learning. J Big Data. 2019;6(1):60.\n\t151.\t Wang F, Wang H, Wang H, Li G, Situ G. Learning from simulation: an end-to-end deep-learning approach for com‑\nputational ghost imaging. Opt Express. 2019;27(18):25560–72.\n\t152.\t Pan W. A survey of transfer learning for collaborative recommendation with auxiliary data. Neurocomputing. \n2016;177:447–53.\n\t153.\t Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L. Imagenet: a large-scale hierarchical image database. In: 2009 IEEE \nconference on computer vision and pattern recognition. IEEE; 2009. p. 248–55.\n\t154.\t Cook D, Feuz KD, Krishnan NC. Transfer learning for activity recognition: a survey. Knowl Inf Syst. 2013;36(3):537–56.\n\t155.\t Cao X, Wang Z, Yan P, Li X. Transfer learning for pedestrian detection. Neurocomputing. 2013;100:51–7.\n\t156.\t Raghu M, Zhang C, Kleinberg J, Bengio S. Transfusion: understanding transfer learning for medical imaging. In: \nAdvances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2019. p. 3347–57.\n\t157.\t Pham TN, Van Tran L, Dao SVT. Early disease classification of mango leaves using feed-forward neural network and \nhybrid metaheuristic feature selection. IEEE Access. 2020;8:189960–73.\n\t158.\t Saleh AM, Hamoud T. Analysis and best parameters selection for person recognition based on gait model using \nCNN algorithm and image augmentation. J Big Data. 2021;8(1):1–20.\n\t159.\t Hirahara D, Takaya E, Takahara T, Ueda T. Effects of data count and image scaling on deep learning training. PeerJ \nComput Sci. 2020;6:312.\n\n\nPage 69 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n\t160.\t Moreno-Barea FJ, Strazzera F, Jerez JM, Urda D, Franco L. Forward noise adjustment scheme for data augmenta‑\ntion. In: 2018 IEEE symposium series on computational intelligence (SSCI). IEEE; 2018. p. 728–34.\n\t161.\t Dua D, Karra Taniskidou E. Uci machine learning repository. Irvine: University of california. School of Information \nand Computer Science; 2017. http://​archi​ve.​ics.​uci.​edu/​ml\n\t162.\t Johnson JM, Khoshgoftaar TM. Survey on deep learning with class imbalance. J Big Data. 2019;6(1):27.\n\t163.\t Yang P, Zhang Z, Zhou BB, Zomaya AY. Sample subset optimization for classifying imbalanced biological data. In: \nPacific-Asia conference on knowledge discovery and data mining. Springer; 2011. p. 333–44.\n\t164.\t Yang P, Yoo PD, Fernando J, Zhou BB, Zhang Z, Zomaya AY. Sample subset optimization techniques for imbalanced \nand ensemble learning problems in bioinformatics applications. IEEE Trans Cybern. 2013;44(3):445–55.\n\t165.\t Wang S, Sun S, Xu J. Auc-maximized deep convolutional neural fields for sequence labeling 2015. arXiv preprint \narXiv:​1511.​05265.\n\t166.\t Li Y, Wang S, Umarov R, Xie B, Fan M, Li L, Gao X. Deepre: sequence-based enzyme EC number prediction by deep \nlearning. Bioinformatics. 2018;34(5):760–9.\n\t167.\t Li Y, Huang C, Ding L, Li Z, Pan Y, Gao X. Deep learning in bioinformatics: introduction, application, and perspective \nin the big data era. Methods. 2019;166:4–21.\n\t168.\t Choi E, Bahadori MT, Sun J, Kulas J, Schuetz A, Stewart W. Retain: An interpretable predictive model for healthcare \nusing reverse time attention mechanism. In: Advances in neural information processing systems. San Mateo: \nMorgan Kaufmann Publishers; 2016. p. 3504–12.\n\t169.\t Ching T, Himmelstein DS, Beaulieu-Jones BK, Kalinin AA, Do BT, Way GP, Ferrero E, Agapow PM, Zietz M, Hoff‑\nman MM, et al. Opportunities and obstacles for deep learning in biology and medicine. J R Soc Interface. \n2018;15(141):20170,387.\n\t170.\t Zhou J, Troyanskaya OG. Predicting effects of noncoding variants with deep learning-based sequence model. Nat \nMethods. 2015;12(10):931–4.\n\t171.\t Pokuri BSS, Ghosal S, Kokate A, Sarkar S, Ganapathysubramanian B. Interpretable deep learning for guided \nmicrostructure-property explorations in photovoltaics. NPJ Comput Mater. 2019;5(1):1–11.\n\t172.\t Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” explaining the predictions of any classifier. In: Proceed‑\nings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 2016. p. \n1135–44.\n\t173.\t Wang L, Nie R, Yu Z, Xin R, Zheng C, Zhang Z, Zhang J, Cai J. An interpretable deep-learning architecture of \ncapsule networks for identifying cell-type gene expression programs from single-cell RNA-sequencing data. Nat \nMach Intell. 2020;2(11):1–11.\n\t174.\t Sundararajan M, Taly A, Yan Q. Axiomatic attribution for deep networks; 2017. arXiv preprint arXiv:​1703.​01365.\n\t175.\t Platt J, et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood meth‑\nods. Adv Large Margin Classif. 1999;10(3):61–74.\n\t176.\t Nair T, Precup D, Arnold DL, Arbel T. Exploring uncertainty measures in deep networks for multiple sclerosis lesion \ndetection and segmentation. Med Image Anal. 2020;59:101557.\n\t177.\t Herzog L, Murina E, Dürr O, Wegener S, Sick B. Integrating uncertainty in deep neural networks for MRI based \nstroke analysis. Med Image Anal. 2020;65:101790.\n\t178.\t Pereyra G, Tucker G, Chorowski J, Kaiser Ł, Hinton G. Regularizing neural networks by penalizing confident output \ndistributions; 2017. arXiv preprint arXiv:​1701.​06548.\n\t179.\t Naeini MP, Cooper GF, Hauskrecht M. Obtaining well calibrated probabilities using bayesian binning. In: Proceed‑\nings of the... AAAI conference on artificial intelligence. AAAI conference on artificial intelligence, vol. 2015. NIH \nPublic Access; 2015. p. 2901.\n\t180.\t Li M, Sethi IK. Confidence-based classifier design. Pattern Recogn. 2006;39(7):1230–40.\n\t181.\t Zadrozny B, Elkan C. Obtaining calibrated probability estimates from decision trees and Naive Bayesian classifiers. \nIn: ICML, vol. 1, Citeseer; 2001. p. 609–16.\n\t182.\t Steinwart I. Consistency of support vector machines and other regularized kernel classifiers. IEEE Trans Inf Theory. \n2005;51(1):128–42.\n\t183.\t Lee K, Lee K, Shin J, Lee H. Overcoming catastrophic forgetting with unlabeled data in the wild. In: Proceedings of \nthe IEEE international conference on computer vision; 2019. p. 312–21.\n\t184.\t Shmelkov K, Schmid C, Alahari K. Incremental learning of object detectors without catastrophic forgetting. In: \nProceedings of the IEEE international conference on computer vision; 2017. p. 3400–09.\n\t185.\t Zenke F, Gerstner W, Ganguli S. The temporal paradox of Hebbian learning and homeostatic plasticity. Curr Opin \nNeurobiol. 2017;43:166–76.\n\t186.\t Andersen N, Krauth N, Nabavi S. Hebbian plasticity in vivo: relevance and induction. Curr Opin Neurobiol. \n2017;45:188–92.\n\t187.\t Zheng R, Chakraborti S. A phase ii nonparametric adaptive exponentially weighted moving average control chart. \nQual Eng. 2016;28(4):476–90.\n\t188.\t Rebuffi SA, Kolesnikov A, Sperl G, Lampert CH. ICARL: Incremental classifier and representation learning. In: Pro‑\nceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 2001–10.\n\t189.\t Hinton GE, Plaut DC. Using fast weights to deblur old memories. In: Proceedings of the ninth annual conference of \nthe cognitive science society; 1987. p. 177–86.\n\t190.\t Parisi GI, Kemker R, Part JL, Kanan C, Wermter S. Continual lifelong learning with neural networks: a review. Neural \nNetw. 2019;113:54–71.\n\t191.\t Soltoggio A, Stanley KO, Risi S. Born to learn: the inspiration, progress, and future of evolved plastic artificial neural \nnetworks. Neural Netw. 2018;108:48–67.\n\t192.\t Parisi GI, Tani J, Weber C, Wermter S. Lifelong learning of human actions with deep neural network self-organiza‑\ntion. Neural Netw. 2017;96:137–49.\n\t193.\t Cheng Y, Wang D, Zhou P, Zhang T. Model compression and acceleration for deep neural networks: the principles, \nprogress, and challenges. IEEE Signal Process Mag. 2018;35(1):126–36.\n\n\nPage 70 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t194.\t Wiedemann S, Kirchhoffer H, Matlage S, Haase P, Marban A, Marinč T, Neumann D, Nguyen T, Schwarz H, Wiegand \nT, et al. Deepcabac: a universal compression algorithm for deep neural networks. IEEE J Sel Topics Signal Process. \n2020;14(4):700–14.\n\t195.\t Mehta N, Pandit A. Concurrence of big data analytics and healthcare: a systematic review. Int J Med Inform. \n2018;114:57–65.\n\t196.\t Esteva A, Robicquet A, Ramsundar B, Kuleshov V, DePristo M, Chou K, Cui C, Corrado G, Thrun S, Dean J. A guide to \ndeep learning in healthcare. Nat Med. 2019;25(1):24–9.\n\t197.\t Shawahna A, Sait SM, El-Maleh A. Fpga-based accelerators of deep learning networks for learning and classifica‑\ntion: a review. IEEE Access. 2018;7:7823–59.\n\t198.\t Min Z. Public welfare organization management system based on FPGA and deep learning. Microprocess \nMicrosyst. 2020;80:103333.\n\t199.\t Al-Shamma O, Fadhel MA, Hameed RA, Alzubaidi L, Zhang J. Boosting convolutional neural networks performance \nbased on fpga accelerator. In: International conference on intelligent systems design and applications. Springer; \n2018. p. 509–17.\n\t200.\t Han S, Mao H, Dally WJ. Deep compression: compressing deep neural networks with pruning, trained quantization \nand huffman coding; 2015. arXiv preprint arXiv:​1510.​00149.\n\t201.\t Chen Z, Zhang L, Cao Z, Guo J. Distilling the knowledge from handcrafted features for human activity recognition. \nIEEE Trans Ind Inform. 2018;14(10):4334–42.\n\t202.\t Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network; 2015. arXiv preprint arXiv:​1503.​02531.\n\t203.\t Lenssen JE, Fey M, Libuschewski P. Group equivariant capsule networks. In: Advances in neural information pro‑\ncessing systems. San Mateo: Morgan Kaufmann Publishers; 2018. p. 8844–53.\n\t204.\t Denton EL, Zaremba W, Bruna J, LeCun Y, Fergus R. Exploiting linear structure within convolutional networks for \nefficient evaluation. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Pub‑\nlishers; 2014. p. 1269–77.\n\t205.\t Xu Q, Zhang M, Gu Z, Pan G. Overfitting remedy by sparsifying regularization on fully-connected layers of CNNs. \nNeurocomputing. 2019;328:69–74.\n\t206.\t Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization. \nCommun ACM. 2018;64(3):107–15.\n\t207.\t Xu X, Jiang X, Ma C, Du P, Li X, Lv S, Yu L, Ni Q, Chen Y, Su J, et al. A deep learning system to screen novel coronavi‑\nrus disease 2019 pneumonia. Engineering. 2020;6(10):1122–9.\n\t208.\t Sharma K, Alsadoon A, Prasad P, Al-Dala’in T, Nguyen TQV, Pham DTH. A novel solution of using deep learning for \nleft ventricle detection: enhanced feature extraction. Comput Methods Programs Biomed. 2020;197:105751.\n\t209.\t Zhang G, Wang C, Xu B, Grosse R. Three mechanisms of weight decay regularization; 2018. arXiv preprint arXiv:​\n1810.​12281.\n\t210.\t Laurent C, Pereyra G, Brakel P, Zhang Y, Bengio Y. Batch normalized recurrent neural networks. In: 2016 IEEE interna‑\ntional conference on acoustics, speech and signal processing (ICASSP), IEEE; 2016. p. 2657–61.\n\t211.\t Salamon J, Bello JP. Deep convolutional neural networks and data augmentation for environmental sound clas‑\nsification. IEEE Signal Process Lett. 2017;24(3):279–83.\n\t212.\t Wang X, Qin Y, Wang Y, Xiang S, Chen H. ReLTanh: an activation function with vanishing gradient resistance for \nSAE-based DNNs and its application to rotating machinery fault diagnosis. Neurocomputing. 2019;363:88–98.\n\t213.\t Tan HH, Lim KH. Vanishing gradient mitigation with deep learning neural network optimization. In: 2019 7th \ninternational conference on smart computing & communications (ICSCC). IEEE; 2019. p. 1–4.\n\t214.\t MacDonald G, Godbout A, Gillcash B, Cairns S. Volume-preserving neural networks: a solution to the vanishing \ngradient problem; 2019. arXiv preprint arXiv:​1911.​09576.\n\t215.\t Mittal S, Vaishay S. A survey of techniques for optimizing deep learning on GPUs. J Syst Arch. 2019;99:101635.\n\t216.\t Kanai S, Fujiwara Y, Iwamura S. Preventing gradient explosions in gated recurrent units. In: Advances in neural \ninformation processing systems. San Mateo: Morgan Kaufmann Publishers; 2017. p. 435–44.\n\t217.\t Hanin B. Which neural net architectures give rise to exploding and vanishing gradients? In: Advances in neural \ninformation processing systems. San Mateo: Morgan Kaufmann Publishers; 2018. p. 582–91.\n\t218.\t Ribeiro AH, Tiels K, Aguirre LA, Schön T. Beyond exploding and vanishing gradients: analysing RNN training using \nattractors and smoothness. In: International conference on artificial intelligence and statistics, PMLR; 2020. p. \n2370–80.\n\t219.\t D’Amour A, Heller K, Moldovan D, Adlam B, Alipanahi B, Beutel A, Chen C, Deaton J, Eisenstein J, Hoffman MD, et al. \nUnderspecification presents challenges for credibility in modern machine learning; 2020. arXiv preprint arXiv:​\n2011.​03395.\n\t220.\t Chea P, Mandell JC. Current applications and future directions of deep learning in musculoskeletal radiology. \nSkelet Radiol. 2020;49(2):1–15.\n\t221.\t Wu X, Sahoo D, Hoi SC. Recent advances in deep learning for object detection. Neurocomputing. 2020;396:39–64.\n\t222.\t Kuutti S, Bowden R, Jin Y, Barber P, Fallah S. A survey of deep learning applications to autonomous vehicle control. \nIEEE Trans Intell Transp Syst. 2020;22:712–33.\n\t223.\t Yolcu G, Oztel I, Kazan S, Oz C, Bunyak F. Deep learning-based face analysis system for monitoring customer inter‑\nest. J Ambient Intell Humaniz Comput. 2020;11(1):237–48.\n\t224.\t Jiao L, Zhang F, Liu F, Yang S, Li L, Feng Z, Qu R. A survey of deep learning-based object detection. IEEE Access. \n2019;7:128837–68.\n\t225.\t Muhammad K, Khan S, Del Ser J, de Albuquerque VHC. Deep learning for multigrade brain tumor classification in \nsmart healthcare systems: a prospective survey. IEEE Trans Neural Netw Learn Syst. 2020;32:507–22.\n\t226.\t Litjens G, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, Van Der Laak JA, Van Ginneken B, Sánchez CI. A \nsurvey on deep learning in medical image analysis. Med Image Anal. 2017;42:60–88.\n\t227.\t Mukherjee D, Mondal R, Singh PK, Sarkar R, Bhattacharjee D. Ensemconvnet: a deep learning approach for \nhuman activity recognition using smartphone sensors for healthcare applications. Multimed Tools Appl. \n2020;79(41):31663–90.\n\n\nPage 71 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n\t228.\t Zeleznik R, Foldyna B, Eslami P, Weiss J, Alexander I, Taron J, Parmar C, Alvi RM, Banerji D, Uno M, et al. Deep \nconvolutional neural networks to predict cardiovascular risk from computed tomography. Nature Commun. \n2021;12(1):1–9.\n\t229.\t Wang J, Liu Q, Xie H, Yang Z, Zhou H. Boosted efficientnet: detection of lymph node metastases in breast cancer \nusing convolutional neural networks. Cancers. 2021;13(4):661.\n\t230.\t Yu H, Yang LT, Zhang Q, Armstrong D, Deen MJ. Convolutional neural networks for medical image analysis: \nstate-of-the-art, comparisons, improvement and perspectives. Neurocomputing. 2021. https://​doi.​org/​10.​1016/j.​\nneucom.​2020.​04.​157.\n\t231.\t Bharati S, Podder P, Mondal MRH. Hybrid deep learning for detecting lung diseases from X-ray images. Inform Med \nUnlocked. 2020;20:100391.\n\t232.\t Dong Y, Pan Y, Zhang J, Xu W. Learning to read chest X-ray images from 16000+ examples using CNN. In: 2017 \nIEEE/ACM international conference on connected health: applications, systems and engineering technologies \n(CHASE). IEEE; 2017. p. 51–7.\n\t233.\t Rajkomar A, Lingam S, Taylor AG, Blum M, Mongan J. High-throughput classification of radiographs using deep \nconvolutional neural networks. J Digit Imaging. 2017;30(1):95–101.\n\t234.\t Rajpurkar P, Irvin J, Zhu K, Yang B, Mehta H, Duan T, Ding D, Bagul A, Langlotz C, Shpanskaya K, et al. Chexnet: \nradiologist-level pneumonia detection on chest X-rays with deep learning; 2017. arXiv preprint arXiv:​1711.​05225.\n\t235.\t Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale chest X-ray database and bench‑\nmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE \nconference on computer vision and pattern recognition; 2017. p. 2097–106.\n\t236.\t Zuo W, Zhou F, Li Z, Wang L. Multi-resolution CNN and knowledge transfer for candidate classification in lung \nnodule detection. IEEE Access. 2019;7:32510–21.\n\t237.\t Shen W, Zhou M, Yang F, Yang C, Tian J. Multi-scale convolutional neural networks for lung nodule classification. In: \nInternational conference on information processing in medical imaging. Springer; 2015. p. 588–99.\n\t238.\t Li R, Zhang W, Suk HI, Wang L, Li J, Shen D, Ji S. Deep learning based imaging data completion for improved brain \ndisease diagnosis. In: International conference on medical image computing and computer-assisted intervention. \nSpringer; 2014. p. 305–12.\n\t239.\t Wen J, Thibeau-Sutre E, Diaz-Melo M, Samper-González J, Routier A, Bottani S, Dormont D, Durrleman S, Burgos N, \nColliot O, et al. Convolutional neural networks for classification of Alzheimer’s disease: overview and reproducible \nevaluation. Med Image Anal. 2020;63:101694.\n\t240.\t Mehmood A, Maqsood M, Bashir M, Shuyuan Y. A deep siamese convolution neural network for multi-class clas‑\nsification of Alzheimer disease. Brain Sci. 2020;10(2):84.\n\t241.\t Hosseini-Asl E, Ghazal M, Mahmoud A, Aslantas A, Shalaby A, Casanova M, Barnes G, Gimel’farb G, Keynton R, \nEl-Baz A. Alzheimer’s disease diagnostics by a 3d deeply supervised adaptable convolutional network. Front Biosci. \n2018;23:584–96.\n\t242.\t Korolev S, Safiullin A, Belyaev M, Dodonova Y. Residual and plain convolutional neural networks for 3D brain MRI \nclassification. In: 2017 IEEE 14th international symposium on biomedical imaging (ISBI 2017). IEEE; 2017. p. 835–8.\n\t243.\t Alzubaidi L, Fadhel MA, Oleiwi SR, Al-Shamma O, Zhang J. DFU_QUTNet: diabetic foot ulcer classification using \nnovel deep convolutional neural network. Multimed Tools Appl. 2020;79(21):15655–77.\n\t244.\t Goyal M, Reeves ND, Davison AK, Rajbhandari S, Spragg J, Yap MH. Dfunet: convolutional neural networks for \ndiabetic foot ulcer classification. IEEE Trans Emerg Topics Comput Intell. 2018;4(5):728–39.\n\t245.\t Yap MH., Hachiuma R, Alavi A, Brungel R, Goyal M, Zhu H, Cassidy B, Ruckert J, Olshansky M, Huang X, et al. Deep \nlearning in diabetic foot ulcers detection: a comprehensive evaluation; 2020. arXiv preprint arXiv:​2010.​03341.\n\t246.\t Tulloch J, Zamani R, Akrami M. Machine learning in the prevention, diagnosis and management of diabetic foot \nulcers: a systematic review. IEEE Access. 2020;8:198977–9000.\n\t247.\t Fadhel MA, Al-Shamma O, Alzubaidi L, Oleiwi SR. Real-time sickle cell anemia diagnosis based hardware accelera‑\ntor. In: International conference on new trends in information and communications technology applications, \nSpringer; 2020. p. 189–99.\n\t248.\t Debelee TG, Kebede SR, Schwenker F, Shewarega ZM. Deep learning in selected cancers’ image analysis—a survey. \nJ Imaging. 2020;6(11):121.\n\t249.\t Khan S, Islam N, Jan Z, Din IU, Rodrigues JJC. A novel deep learning based framework for the detection and clas‑\nsification of breast cancer using transfer learning. Pattern Recogn Lett. 2019;125:1–6.\n\t250.\t Alzubaidi L, Hasan RI, Awad FH, Fadhel MA, Alshamma O, Zhang J. Multi-class breast cancer classification by a \nnovel two-branch deep convolutional neural network architecture. In: 2019 12th international conference on \ndevelopments in eSystems engineering (DeSE). IEEE; 2019. p. 268–73.\n\t251.\t Roy K, Banik D, Bhattacharjee D, Nasipuri M. Patch-based system for classification of breast histology images using \ndeep learning. Comput Med Imaging Gr. 2019;71:90–103.\n\t252.\t Hameed Z, Zahia S, Garcia-Zapirain B, Javier Aguirre J, María Vanegas A. Breast cancer histopathology image clas‑\nsification using an ensemble of deep learning models. Sensors. 2020;20(16):4373.\n\t253.\t Hosny KM, Kassem MA, Foaud MM. Skin cancer classification using deep learning and transfer learning. In: 2018 \n9th Cairo international biomedical engineering conference (CIBEC). IEEE; 2018. p. 90–3.\n\t254.\t Dorj UO, Lee KK, Choi JY, Lee M. The skin cancer classification using deep convolutional neural network. Multimed \nTools Appl. 2018;77(8):9909–24.\n\t255.\t Kassem MA, Hosny KM, Fouad MM. Skin lesions classification into eight classes for ISIC 2019 using deep convolu‑\ntional neural network and transfer learning. IEEE Access. 2020;8:114822–32.\n\t256.\t Heidari M, Mirniaharikandehei S, Khuzani AZ, Danala G, Qiu Y, Zheng B. Improving the performance of CNN to \npredict the likelihood of COVID-19 using chest X-ray images with preprocessing algorithms. Int J Med Inform. \n2020;144:104284.\n\t257.\t Al-Timemy AH, Khushaba RN, Mosa ZM, Escudero J. An efficient mixture of deep and machine learning models for \nCOVID-19 and tuberculosis detection using X-ray images in resource limited settings 2020. arXiv preprint arXiv:​\n2007.​08223.\n\n\nPage 72 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t258.\t Abraham B, Nair MS. Computer-aided detection of COVID-19 from X-ray images using multi-CNN and Bayesnet \nclassifier. Biocybern Biomed Eng. 2020;40(4):1436–45.\n\t259.\t Nour M, Cömert Z, Polat K. A novel medical diagnosis model for COVID-19 infection detection based on deep \nfeatures and Bayesian optimization. Appl Soft Comput. 2020;97:106580.\n\t260.\t Mallio CA, Napolitano A, Castiello G, Giordano FM, D’Alessio P, Iozzino M, Sun Y, Angeletti S, Russano M, Santini D, \net al. Deep learning algorithm trained with COVID-19 pneumonia also identifies immune checkpoint inhibitor \ntherapy-related pneumonitis. Cancers. 2021;13(4):652.\n\t261.\t Fourcade A, Khonsari R. Deep learning in medical image analysis: a third eye for doctors. J Stomatol Oral Maxillofac \nSurg. 2019;120(4):279–88.\n\t262.\t Guo Z, Li X, Huang H, Guo N, Li Q. Deep learning-based image segmentation on multimodal medical imaging. \nIEEE Trans Radiat Plasma Med Sci. 2019;3(2):162–9.\n\t263.\t Thakur N, Yoon H, Chong Y. Current trends of artificial intelligence for colorectal cancer pathology image analysis: \na systematic review. Cancers. 2020;12(7):1884.\n\t264.\t Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing on MRI. Zeitschrift für \nMedizinische Physik. 2019;29(2):102–27.\n\t265.\t Yadav SS, Jadhav SM. Deep convolutional neural network based medical image classification for disease diagnosis. \nJ Big Data. 2019;6(1):113.\n\t266.\t Nehme E, Freedman D, Gordon R, Ferdman B, Weiss LE, Alalouf O, Naor T, Orange R, Michaeli T, Shechtman Y. Deep‑\nSTORM3D: dense 3D localization microscopy and PSF design by deep learning. Nat Methods. 2020;17(7):734–40.\n\t267.\t Zulkifley MA, Abdani SR, Zulkifley NH. Pterygium-Net: a deep learning approach to pterygium detection and \nlocalization. Multimed Tools Appl. 2019;78(24):34563–84.\n\t268.\t Sirazitdinov I, Kholiavchenko M, Mustafaev T, Yixuan Y, Kuleev R, Ibragimov B. Deep neural network ensemble for \npneumonia localization from a large-scale chest X-ray database. Comput Electr Eng. 2019;78:388–99.\n\t269.\t Zhao W, Shen L, Han B, Yang Y, Cheng K, Toesca DA, Koong AC, Chang DT, Xing L. Markerless pancreatic tumor \ntarget localization enabled by deep learning. Int J Radiat Oncol Biol Phys. 2019;105(2):432–9.\n\t270.\t Roth HR, Lee CT, Shin HC, Seff A, Kim L, Yao J, Lu L, Summers RM. Anatomy-specific classification of medical images \nusing deep convolutional nets. In: 2015 IEEE 12th international symposium on biomedical imaging (ISBI). IEEE; \n2015. p. 101–4.\n\t271.\t Shin HC, Orton MR, Collins DJ, Doran SJ, Leach MO. Stacked autoencoders for unsupervised feature learn‑\ning and multiple organ detection in a pilot study using 4D patient data. IEEE Trans Pattern Anal Mach Intell. \n2012;35(8):1930–43.\n\t272.\t Li Z, Dong M, Wen S, Hu X, Zhou P, Zeng Z. CLU-CNNs: object detection for medical images. Neurocomputing. \n2019;350:53–9.\n\t273.\t Gao J, Jiang Q, Zhou B, Chen D. Convolutional neural networks for computer-aided detection or diagnosis in \nmedical image analysis: an overview. Math Biosci Eng. 2019;16(6):6536.\n\t274.\t Lumini A, Nanni L. Review fair comparison of skin detection approaches on publicly available datasets. Expert Syst \nAppl. 2020. https://​doi.​org/​10.​1016/j.​eswa.​2020.​113677.\n\t275.\t Chouhan V, Singh SK, Khamparia A, Gupta D, Tiwari P, Moreira C, Damaševičius R, De Albuquerque VHC. A novel \ntransfer learning based approach for pneumonia detection in chest X-ray images. Appl Sci. 2020;10(2):559.\n\t276.\t Apostolopoulos ID, Mpesiana TA. COVID-19: automatic detection from X-ray images utilizing transfer learning with \nconvolutional neural networks. Phys Eng Sci Med. 2020;43(2):635–40.\n\t277.\t Mahmud T, Rahman MA, Fattah SA. CovXNet: a multi-dilation convolutional neural network for automatic COVID-\n19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimiza‑\ntion. Comput Biol Med. 2020;122:103869.\n\t278.\t Tayarani-N MH. Applications of artificial intelligence in battling against COVID-19: a literature review. Chaos Soli‑\ntons Fractals. 2020;142:110338.\n\t279.\t Toraman S, Alakus TB, Turkoglu I. Convolutional capsnet: a novel artificial neural network approach to detect \nCOVID-19 disease from X-ray images using capsule networks. Chaos Solitons Fractals. 2020;140:110122.\n\t280.\t Dascalu A, David E. Skin cancer detection by deep learning and sound analysis algorithms: a prospective clinical \nstudy of an elementary dermoscope. EBioMedicine. 2019;43:107–13.\n\t281.\t Adegun A, Viriri S. Deep learning techniques for skin lesion analysis and melanoma cancer detection: a survey of \nstate-of-the-art. Artif Intell Rev. 2020;54:1–31.\n\t282.\t Zhang N, Cai YX, Wang YY, Tian YT, Wang XL, Badami B. Skin cancer diagnosis based on optimized convolutional \nneural network. Artif Intell Med. 2020;102:101756.\n\t283.\t Thurnhofer-Hemsi K, Domínguez E. A convolutional neural network framework for accurate skin cancer detection. \nNeural Process Lett. 2020. https://​doi.​org/​10.​1007/​s11063-​020-​10364-y.\n\t284.\t Jain MS, Massoud TF. Predicting tumour mutational burden from histopathological images using multiscale deep \nlearning. Nat Mach Intell. 2020;2(6):356–62.\n\t285.\t Lei H, Liu S, Elazab A, Lei B. Attention-guided multi-branch convolutional neural network for mitosis detection \nfrom histopathological images. IEEE J Biomed Health Inform. 2020;25(2):358–70.\n\t286.\t Celik Y, Talo M, Yildirim O, Karabatak M, Acharya UR. Automated invasive ductal carcinoma detection based using \ndeep transfer learning with whole-slide images. Pattern Recogn Lett. 2020;133:232–9.\n\t287.\t Sebai M, Wang X, Wang T. Maskmitosis: a deep learning framework for fully supervised, weakly supervised, and \nunsupervised mitosis detection in histopathology images. Med Biol Eng Comput. 2020;58:1603–23.\n\t288.\t Sebai M, Wang T, Al-Fadhli SA. Partmitosis: a partially supervised deep learning framework for mitosis detection in \nbreast cancer histopathology images. IEEE Access. 2020;8:45133–47.\n\t289.\t Mahmood T, Arsalan M, Owais M, Lee MB, Park KR. Artificial intelligence-based mitosis detection in breast cancer \nhistopathology images using faster R-CNN and deep CNNs. J Clin Med. 2020;9(3):749.\n\t290.\t Srinidhi CL, Ciga O, Martel AL. Deep neural network models for computational histopathology: a survey. Med \nImage Anal. 2020;67:101813.\n\n\nPage 73 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t\n\t291.\t Cireşan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with \ndeep neural networks. In: International conference on medical image computing and computer-assisted inter‑\nvention. Springer; 2013. p. 411–8.\n\t292.\t Sirinukunwattana K, Raza SEA, Tsang YW, Snead DR, Cree IA, Rajpoot NM. Locality sensitive deep learning \nfor detection and classification of nuclei in routine colon cancer histology images. IEEE Trans Med Imaging. \n2016;35(5):1196–206.\n\t293.\t Xu J, Xiang L, Liu Q, Gilmore H, Wu J, Tang J, Madabhushi A. Stacked sparse autoencoder (SSAE) for nuclei detec‑\ntion on breast cancer histopathology images. IEEE Trans Med Imaging. 2015;35(1):119–30.\n\t294.\t Albarqouni S, Baur C, Achilles F, Belagiannis V, Demirci S, Navab N. Aggnet: deep learning from crowds for mitosis \ndetection in breast cancer histology images. IEEE Trans Med Imaging. 2016;35(5):1313–21.\n\t295.\t Abd-Ellah MK, Awad AI, Khalaf AA, Hamed HF. Two-phase multi-model automatic brain tumour diagnosis \nsystem from magnetic resonance images using convolutional neural networks. EURASIP J Image Video Process. \n2018;2018(1):97.\n\t296.\t Thaha MM, Kumar KPM, Murugan B, Dhanasekeran S, Vijayakarthick P, Selvi AS. Brain tumor segmentation using \nconvolutional neural networks in MRI images. J Med Syst. 2019;43(9):294.\n\t297.\t Talo M, Yildirim O, Baloglu UB, Aydin G, Acharya UR. Convolutional neural networks for multi-class brain disease \ndetection using MRI images. Comput Med Imaging Gr. 2019;78:101673.\n\t298.\t Gabr RE, Coronado I, Robinson M, Sujit SJ, Datta S, Sun X, Allen WJ, Lublin FD, Wolinsky JS, Narayana PA. Brain and \nlesion segmentation in multiple sclerosis using fully convolutional neural networks: a large-scale study. Mult Scler \nJ. 2020;26(10):1217–26.\n\t299.\t Chen S, Ding C, Liu M. Dual-force convolutional neural networks for accurate brain tumor segmentation. Pattern \nRecogn. 2019;88:90–100.\n\t300.\t Hu K, Gan Q, Zhang Y, Deng S, Xiao F, Huang W, Cao C, Gao X. Brain tumor segmentation using multi-cascaded \nconvolutional neural networks and conditional random field. IEEE Access. 2019;7:92615–29.\n\t301.\t Wadhwa A, Bhardwaj A, Verma VS. A review on brain tumor segmentation of MRI images. Magn Reson Imaging. \n2019;61:247–59.\n\t302.\t Akkus Z, Galimzianova A, Hoogi A, Rubin DL, Erickson BJ. Deep learning for brain MRI segmentation: state of the \nart and future directions. J Digit Imaging. 2017;30(4):449–59.\n\t303.\t Moeskops P, Viergever MA, Mendrik AM, De Vries LS, Benders MJ, Išgum I. Automatic segmentation of MR brain \nimages with a convolutional neural network. IEEE Trans Med Imaging. 2016;35(5):1252–61.\n\t304.\t Milletari F, Navab N, Ahmadi SA. V-net: Fully convolutional neural networks for volumetric medical image segmen‑\ntation. In: 2016 fourth international conference on 3D vision (3DV). IEEE; 2016. p. 565–71.\n\t305.\t Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: Interna‑\ntional conference on medical image computing and computer-assisted intervention. Springer; 2015. p. 234–41.\n\t306.\t Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images. \nIEEE Trans Med Imaging. 2016;35(5):1240–51.\n\t307.\t Havaei M, Davy A, Warde-Farley D, Biard A, Courville A, Bengio Y, Pal C, Jodoin PM, Larochelle H. Brain tumor seg‑\nmentation with deep neural networks. Med Image Anal. 2017;35:18–31.\n\t308.\t Chen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL. DeepLab: semantic image segmentation with \ndeep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Trans Pattern Anal Mach Intell. \n2017;40(4):834–48.\n\t309.\t Yan Q, Wang B, Gong D, Luo C, Zhao W, Shen J, Shi Q, Jin S, Zhang L, You Z. COVID-19 chest CT image segmenta‑\ntion—a deep convolutional neural network solution; 2020. arXiv preprint arXiv:​2004.​10987.\n\t310.\t Wang G, Liu X, Li C, Xu Z, Ruan J, Zhu H, Meng T, Li K, Huang N, Zhang S. A noise-robust framework for automatic \nsegmentation of COVID-19 pneumonia lesions from CT images. IEEE Trans Med Imaging. 2020;39(8):2653–63.\n\t311.\t Khan SH, Sohail A, Khan A, Lee YS. Classification and region analysis of COVID-19 infection using lung CT images \nand deep convolutional neural networks; 2020. arXiv preprint arXiv:​2009.​08864.\n\t312.\t Shi F, Wang J, Shi J, Wu Z, Wang Q, Tang Z, He K, Shi Y, Shen D. Review of artificial intelligence techniques in imag‑\ning data acquisition, segmentation and diagnosis for COVID-19. IEEE Rev Biomed Eng. 2020;14:4–5.\n\t313.\t Santamaría J, Rivero-Cejudo M, Martos-Fernández M, Roca F. An overview on the latest nature-inspired and \nmetaheuristics-based image registration algorithms. Appl Sci. 2020;10(6):1928.\n\t314.\t Santamaría J, Cordón O, Damas S. A comparative study of state-of-the-art evolutionary image registration meth‑\nods for 3D modeling. Comput Vision Image Underst. 2011;115(9):1340–54.\n\t315.\t Yumer ME, Mitra NJ. Learning semantic deformation flows with 3D convolutional networks. In: European confer‑\nence on computer vision. Springer; 2016. p. 294–311.\n\t316.\t Ding L, Feng C. Deepmapping: unsupervised map estimation from multiple point clouds. In: Proceedings of the \nIEEE conference on computer vision and pattern recognition; 2019. p. 8650–9.\n\t317.\t Mahadevan S. Imagination machines: a new challenge for artificial intelligence. AAAI. 2018;2018:7988–93.\n\t318.\t Wang L, Fang Y. Unsupervised 3D reconstruction from a single image via adversarial learning; 2017. arXiv preprint \narXiv:​1711.​09312.\n\t319.\t Hermoza R, Sipiran I. 3D reconstruction of incomplete archaeological objects using a generative adversarial \nnetwork. In: Proceedings of computer graphics international 2018. Association for Computing Machinery; 2018. p. \n5–11.\n\t320.\t Fu Y, Lei Y, Wang T, Curran WJ, Liu T, Yang X. Deep learning in medical image registration: a review. Phys Med Biol. \n2020;65(20):20TR01.\n\t321.\t Haskins G, Kruger U, Yan P. Deep learning in medical image registration: a survey. Mach Vision Appl. 2020;31(1):8.\n\t322.\t de Vos BD, Berendsen FF, Viergever MA, Sokooti H, Staring M, Išgum I. A deep learning framework for unsupervised \naffine and deformable image registration. Med Image Anal. 2019;52:128–43.\n\t323.\t Yang X, Kwitt R, Styner M, Niethammer M. Quicksilver: fast predictive image registration—a deep learning \napproach. NeuroImage. 2017;158:378–96.\n\n\nPage 74 of 74\nAlzubaidi et al. J Big Data            (2021) 8:53 \n\t324.\t Miao S, Wang ZJ, Liao R. A CNN regression approach for real-time 2D/3D registration. IEEE Trans Med Imaging. \n2016;35(5):1352–63.\n\t325.\t Li P, Pei Y, Guo Y, Ma G, Xu T, Zha H. Non-rigid 2D–3D registration using convolutional autoencoders. In: 2020 IEEE \n17th international symposium on biomedical imaging (ISBI). IEEE; 2020. p. 700–4.\n\t326.\t Zhang J, Yeung SH, Shu Y, He B, Wang W. Efficient memory management for GPU-based deep learning systems; \n2019. arXiv preprint arXiv:​1903.​06631.\n\t327.\t Zhao H, Han Z, Yang Z, Zhang Q, Yang F, Zhou L, Yang M, Lau FC, Wang Y, Xiong Y, et al. Hived: sharing a {GPU} \ncluster for deep learning with guarantees. In: 14th {USENIX} symposium on operating systems design and imple‑\nmentation ({OSDI} 20); 2020. p. 515–32.\n\t328.\t Lin Y, Jiang Z, Gu J, Li W, Dhar S, Ren H, Khailany B, Pan DZ. DREAMPlace: deep learning toolkit-enabled GPU accel‑\neration for modern VLSI placement. IEEE Trans Comput Aided Des Integr Circuits Syst. 2020;40:748–61.\n\t329.\t Hossain S, Lee DJ. Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a \nflying robot with GPU-based embedded devices. Sensors. 2019;19(15):3371.\n\t330.\t Castro FM, Guil N, Marín-Jiménez MJ, Pérez-Serrano J, Ujaldón M. Energy-based tuning of convolutional neural \nnetworks on multi-GPUs. Concurr Comput Pract Exp. 2019;31(21):4786.\n\t331.\t Gschwend D. Zynqnet: an fpga-accelerated embedded convolutional neural network; 2020. arXiv preprint arXiv:​\n2005.​06892.\n\t332.\t Zhang N, Wei X, Chen H, Liu W. FPGA implementation for CNN-based optical remote sensing object detection. \nElectronics. 2021;10(3):282.\n\t333.\t Zhao M, Hu C, Wei F, Wang K, Wang C, Jiang Y. Real-time underwater image recognition with FPGA embedded \nsystem for convolutional neural network. Sensors. 2019;19(2):350.\n\t334.\t Liu X, Yang J, Zou C, Chen Q, Yan X, Chen Y, Cai C. Collaborative edge computing with FPGA-based CNN accelera‑\ntors for energy-efficient and time-aware face tracking system. IEEE Trans Comput Soc Syst. 2021. https://​doi.​org/​\n10.​1109/​TCSS.​2021.​30593​18.\n\t335.\t Hossin M, Sulaiman M. A review on evaluation metrics for data classification evaluations. Int J Data Min Knowl \nManag Process. 2015;5(2):1.\n\t336.\t Provost F, Domingos P. Tree induction for probability-based ranking. Mach Learn. 2003;52(3):199–215.\n\t337.\t Rakotomamonyj A. Optimizing area under roc with SVMS. In: Proceedings of the European conference on artificial \nintelligence workshop on ROC curve and artificial intelligence (ROCAI 2004), 2004. p. 71–80.\n\t338.\t Mingote V, Miguel A, Ortega A, Lleida E. Optimization of the area under the roc curve using neural network super‑\nvectors for text-dependent speaker verification. Comput Speech Lang. 2020;63:101078.\n\t339.\t Fawcett T. An introduction to roc analysis. Pattern Recogn Lett. 2006;27(8):861–74.\n\t340.\t Huang J, Ling CX. Using AUC and accuracy in evaluating learning algorithms. IEEE Trans Knowl Data Eng. \n2005;17(3):299–310.\n\t341.\t Hand DJ, Till RJ. A simple generalisation of the area under the ROC curve for multiple class classification problems. \nMach Learn. 2001;45(2):171–86.\n\t342.\t Masoudnia S, Mersa O, Araabi BN, Vahabie AH, Sadeghi MA, Ahmadabadi MN. Multi-representational learning for \noffline signature verification using multi-loss snapshot ensemble of CNNs. Expert Syst Appl. 2019;133:317–30.\n\t343.\t Coupé P, Mansencal B, Clément M, Giraud R, de Senneville BD, Ta VT, Lepetit V, Manjon JV. Assemblynet: a large \nensemble of CNNs for 3D whole brain MRI segmentation. NeuroImage. 2020;219:117026.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\n",
  "normalized_text": "Review of deep learning: concepts, CNN\narchitectures, challenges, applications, future\ndirections\nLaith Alzubaidi1,5*  , Jinglan Zhang1, Amjad J. Humaidi2, Ayad Al‑Dujaili3, Ye Duan4, Omran Al‑Shamma5,\nJ. Santamaría6, Mohammed A. Fadhel7, Muthana Al‑Amidie4 and Laith Farhan8\nAbstract\nIn the last few years, the deep learning (DL) computing paradigm has been deemed\nthe Gold Standard in the machine learning (ML) community. Moreover, it has gradually\nbecome the most widely used computational approach in the field of ML, thus achiev‑\ning outstanding results on several complex cognitive tasks, matching or even beating\nthose provided by human performance. One of the benefits of DL is the ability to learn\nmassive amounts of data. The DL field has grown fast in the last few years and it has\nbeen extensively used to successfully address a wide range of traditional applications.\nMore importantly, DL has outperformed well-known ML techniques in many domains,\ne.g., cybersecurity, natural language processing, bioinformatics, robotics and control,\nand medical information processing, among many others. Despite it has been contrib‑\nuted several works reviewing the State-of-the-Art on DL, all of them only tackled one\naspect of the DL, which leads to an overall lack of knowledge about it. Therefore, in this\ncontribution, we propose using a more holistic approach in order to provide a more\nsuitable starting point from which to develop a full understanding of DL. Specifically,\nthis review attempts to provide a more comprehensive survey of the most impor‑\ntant aspects of DL and including those enhancements recently added to the field. In\nparticular, this paper outlines the importance of DL, presents the types of DL tech‑\nniques and networks. It then presents convolutional neural networks (CNNs) which the\nmost utilized DL network type and describes the development of CNNs architectures\ntogether with their main features, e.g., starting with the AlexNet network and closing\nwith the High-Resolution network (HR.Net). Finally, we further present the challenges\nand suggested solutions to help researchers understand the existing research gaps.\nIt is followed by a list of the major DL applications. Computational tools including\nFPGA, GPU, and CPU are summarized along with a description of their influence on\nDL. The paper ends with the evolution matrix, benchmark datasets, and summary and\nconclusion.\nKeywords:  Deep learning, Machine learning, Convolution neural network (CNN), Deep\nneural network architectures, Deep learning applications, Image classification, Transfer\nlearning, Medical image analysis, Supervised learning, FPGA, GPU\nOpen Access\n© The Author(s) 2021. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permit‑\nted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\nSURVEY PAPER\nAlzubaidi et al. J Big Data      (2021) 8:53\nhttps://doi.org/10.1186/s40537-021-00444-8\n*Correspondence:\nlaith.alzubaidi@hdr.qut.\nedu.au\n1 School of Computer\nScience, Queensland\nUniversity of Technology,\nBrisbane, QLD 4000, Australia\nFull list of author information\nis available at the end of the\narticle\n\nPage 2 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nIntroduction\nRecently, machine learning (ML) has become very widespread in research and has been\nincorporated in a variety of applications, including text mining, spam detection, video\nrecommendation, image classification, and multimedia concept retrieval [1–6]. Among\nthe different ML algorithms, deep learning (DL) is very commonly employed in these\napplications [7–9]. Another name for DL is representation learning (RL). The continuing\nappearance of novel studies in the fields of deep and distributed learning is due to both\nthe unpredictable growth in the ability to obtain data and the amazing progress made in\nthe hardware technologies, e.g. High Performance Computing (HPC) [10].\nDL is derived from the conventional neural network but considerably outperforms its\npredecessors. Moreover, DL employs transformations and graph technologies simultaneously in order to build up multi-layer learning models. The most recently developed\nDL techniques have obtained good outstanding performance across a variety of applications, including audio and speech processing, visual data processing, natural language\nprocessing (NLP), among others [11–14].\nUsually, the effectiveness of an ML algorithm is highly dependent on the integrity of\nthe input-data representation. It has been shown that a suitable data representation provides an improved performance when compared to a poor data representation. Thus,\na significant research trend in ML for many years has been feature engineering, which\nhas informed numerous research studies. This approach aims at constructing features\nfrom raw data. In addition, it is extremely field-specific and frequently requires sizable\nhuman effort. For instance, several types of features were introduced and compared in\nthe computer vision context, such as, histogram of oriented gradients (HOG) [15], scaleinvariant feature transform (SIFT) [16], and bag of words (BoW) [17]. As soon as a novel\nfeature is introduced and is found to perform well, it becomes a new research direction\nthat is pursued over multiple decades.\nRelatively speaking, feature extraction is achieved in an automatic way throughout the\nDL algorithms. This encourages researchers to extract discriminative features using the\nsmallest possible amount of human effort and field knowledge [18]. These algorithms\nhave a multi-layer data representation architecture, in which the first layers extract the\nlow-level features while the last layers extract the high-level features. Note that artificial\nintelligence (AI) originally inspired this type of architecture, which simulates the process\nthat occurs in core sensorial regions within the human brain. Using different scenes, the\nhuman brain can automatically extract data representation. More specifically, the output\nof this process is the classified objects, while the received scene information represents\nthe input. This process simulates the working methodology of the human brain. Thus, it\nemphasizes the main benefit of DL.\nIn the field of ML, DL, due to its considerable success, is currently one of the most\nprominent research trends. In this paper, an overview of DL is presented that adopts\nvarious perspectives such as the main concepts, architectures, challenges, applications,\ncomputational tools and evolution matrix. Convolutional neural network (CNN) is one\nof the most popular and used of DL networks [19, 20]. Because of CNN, DL is very popular nowadays. The main advantage of CNN compared to its predecessors is that it automatically detects the significant features without any human supervision which made\nit the most used. Therefore, we have dug in deep with CNN by presenting the main\n\nPage 3 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\ncomponents of it. Furthermore, we have elaborated in detail the most common CNN\narchitectures, starting with the AlexNet network and ending with the High-Resolution\nnetwork (HR.Net).\nSeveral published DL review papers have been presented in the last few years. However, all of them have only been addressed one side focusing on one application or topic\nsuch as the review of CNN architectures [21], DL for classification of plant diseases [22],\nDL for object detection [23], DL applications in medical image analysis [24], and etc.\nAlthough these reviews present good topics, they do not provide a full understanding of\nDL topics such as concepts, detailed research gaps, computational tools, and DL applications. First, It is required to understand DL aspects including concepts, challenges, and\napplications then going deep in the applications. To achieve that, it requires extensive\ntime and a large number of research papers to learn about DL including research gaps\nand applications. Therefore, we propose a deep review of DL to provide a more suitable starting point from which to develop a full understanding of DL from one review\npaper. The motivation behinds our review was to cover the most important aspect of DL\nincluding open challenges, applications, and computational tools perspective. Furthermore, our review can be the first step towards other DL topics.\nThe main aim of this review is to present the most important aspects of DL to make it\neasy for researchers and students to have a clear image of DL from single review paper.\nThis review will further advance DL research by helping people discover more about\nrecent developments in the field. Researchers would be allowed to decide the more suitable direction of work to be taken in order to provide more accurate alternatives to the\nfield. Our contributions are outlined as follows:\n•\t This is the first review that almost provides a deep survey of the most important\naspects of deep learning. This review helps researchers and students to have a good\nunderstanding from one paper.\n•\t We explain CNN in deep which the most popular deep learning algorithm by\ndescribing the concepts, theory, and state-of-the-art architectures.\n•\t We review current challenges (limitations) of Deep Learning including lack of training data, Imbalanced Data, Interpretability of data, Uncertainty scaling, Catastrophic\nforgetting, Model compression, Overfitting, Vanishing gradient problem, Exploding\nGradient Problem, and Underspecification. We additionally discuss the proposed\nsolutions tackling these issues.\n•\t We provide an exhaustive list of medical imaging applications with deep learning by\ncategorizing them based on the tasks by starting with classification and ending with\nregistration.\n•\t We discuss the computational approaches (CPU, GPU, FPGA) by comparing the\ninfluence of each tool on deep learning algorithms.\nThe rest of the paper is organized as follows: “Survey methodology” section describes\nThe survey methodology. “Background” section presents the background. “Classification\nof DL approaches” section defines the classification of DL approaches. “Types of DL networks” section displays types of DL networks. “CNN architectures” section shows CNN\nArchitectures. “Challenges (limitations) of deep learning and alternate solutions” section\n\nPage 4 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\ndetails the challenges of DL and alternate solutions. “Applications of deep learning” section outlines the applications of DL. “Computational approaches” section explains the\ninfluence of computational approaches (CPU, GPU, FPGA) on DL. “Evaluation metrics” section presents the evaluation metrics. “Frameworks and datasets” section lists\nframeworks and datasets. “Summary and conclusion” section presents the summary and\nconclusion.\nSurvey methodology\nWe have reviewed the significant research papers in the field published during 2010–\n2020, mainly from the years of 2020 and 2019 with some papers from 2021. The main\nfocus was papers from the most reputed publishers such as IEEE, Elsevier, MDPI,\nNature, ACM, and Springer. Some papers have been selected from ArXiv. We have\nreviewed more than 300 papers on various DL topics. There are 108 papers from the\nyear 2020, 76 papers from the year 2019, and 48 papers from the year 2018. This indicates that this review focused on the latest publications in the field of DL. The selected\npapers were analyzed and reviewed to (1) list and define the DL approaches and network\ntypes, (2) list and explain CNN architectures, (3) present the challenges of DL and suggest the alternate solutions, (4) assess the applications of DL, (5) assess computational\napproaches. The most keywords used for search criteria for this review paper are (“Deep\nLearning”), (“Machine Learning”), (“Convolution Neural Network”), (“Deep Learning”\nAND “Architectures”), ((“Deep Learning”) AND (“Image”) AND (“detection” OR “classification” OR “segmentation” OR “Localization”)), (“Deep Learning” AND “detection” OR\n“classification” OR “segmentation” OR “Localization”), (“Deep Learning” AND “CPU”\nOR “GPU” OR “FPGA”), (“Deep Learning” AND “Transfer Learning”), (“Deep Learning” AND “Imbalanced Data”), (“Deep Learning” AND “Interpretability of data”), (“Deep\nLearning” AND “Overfitting”), (“Deep Learning” AND “Underspecification”). Figure 1\nshows our search structure of the survey paper. Table 1 presents the details of some of\nthe journals that have been cited in this review paper.\nBackground\nThis section will present a background of DL. We begin with a quick introduction to\nDL, followed by the difference between DL and ML. We then show the situations that\nrequire DL. Finally, we present the reasons for applying DL.\nDL, a subset of ML (Fig. 2), is inspired by the information processing patterns found\nin the human brain. DL does not require any human-designed rules to operate; rather,\nit uses a large amount of data to map the given input to specific labels. DL is designed\nusing numerous layers of algorithms (artificial neural networks, or ANNs), each of which\nprovides a different interpretation of the data that has been fed to them [18, 25].\nAchieving the classification task using conventional ML techniques requires several\nsequential steps, specifically pre-processing, feature extraction, wise feature selection,\nlearning, and classification. Furthermore, feature selection has a great impact on the\nperformance of ML techniques. Biased feature selection may lead to incorrect discrimination between classes. Conversely, DL has the ability to automate the learning of feature sets for several tasks, unlike conventional ML methods [18, 26]. DL enables learning\nand classification to be achieved in a single shot (Fig. 3). DL has become an incredibly\n\nPage 5 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\npopular type of ML algorithm in recent years due to the huge growth and evolution of\nthe field of big data [27, 28]. It is still in continuous development regarding novel performance for several ML tasks [22, 29–31] and has simplified the improvement of many\nlearning fields [32, 33], such as image super-resolution [34], object detection [35, 36],\nand image recognition [30, 37]. Recently, DL performance has come to exceed human\nperformance on tasks such as image classification (Fig. 4).\nNearly all scientific fields have felt the impact of this technology. Most industries and\nbusinesses have already been disrupted and transformed through the use of DL. The\nleading technology and economy-focused companies around the world are in a race to\nimprove DL. Even now, human-level performance and capability cannot exceed that the\nperformance of DL in many areas, such as predicting the time taken to make car deliveries, decisions to certify loan requests, and predicting movie ratings [38]. The winners\nof the 2019 “Nobel Prize” in computing, also known as the Turing Award, were three\npioneers in the field of DL (Yann LeCun, Geoffrey Hinton, and Yoshua Bengio) [39].\nAlthough a large number of goals have been achieved, there is further progress to be\nmade in the DL context. In fact, DL has the ability to enhance human lives by providing additional accuracy in diagnosis, including estimating natural disasters [40], the discovery of new drugs [41], and cancer diagnosis [42–44]. Esteva et al. [45] found that a\nDL network has the same ability to diagnose the disease as twenty-one board-certified\ndermatologists using 129,450 images of 2032 diseases. Furthermore, in grading prostate\nFig. 1  Search framework\n\nPage 6 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\ncancer, US board-certified general pathologists achieved an average accuracy of 61%,\nwhile the Google AI [44] outperformed these specialists by achieving an average accuracy of 70%. In 2020, DL is playing an increasingly vital role in early diagnosis of the\nnovel coronavirus (COVID-19) [29, 46–48]. DL has become the main tool in many hospitals around the world for automatic COVID-19 classification and detection using chest\nTable 1  Some of the journals have been cited in this review paper\nJournal\nIF 2019 CiteScore 2019 Publisher Journal homepage\nPattern Recognition\n7.196\n13.1\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\npatte​rn-​recog​nition\nPattern Recognition Letter\n3.255\n6.3\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\npatte​rn-​recog​nition-​lette​rs\nArtificial Intelligence Review\n5.747\n9.1\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n10462?​refer​er=​www.​sprin​geron​\nline.​com\nExpert Systems with Applications\n5.452\n11\nElsevier\nhttps://​www.​scien​cedir​ect.​com/​journ​\nal/​expert-​syste​ms-​with-​appli​catio​ns\nNeurocomputing\n4.438\n9.5\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\nneuro​compu​ting\nNature Medicine\n36.130\n45.9\nNature\nhttps://​www.​nature.​com/​nm/\nNature\n42.779\n51\nNature\nhttps://​www.​nature.​com/\nJournal of Big Data\n–\n6.1\nSpringer\nhttps://​journ​alofb​igdata.​sprin​gerop​\nen.​com/\nMultimedia Tools and Applications\n2.313\n3.7\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n11042\nComputer Methods and Programs\nin Biomedicine\n3.632\n7.5\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\ncompu​ter-​metho​ds-​and-​progr​ams-​\nin-​biome​dicine\nMachine Learning\n2.672\n5.0\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n10994\nMachine Vision and Applications\n1.605\n4.2\nSpringer\nhttps://​www.​sprin​ger.​com/​journ​al/​\n138\nMedical Image Analysis\n11.148\n17.2\nElsevier\nhttps://​www.​scien​cedir​ect.​com/​journ​\nal/​medic​al-​image-​analy​sis\nIEEE Access\n3.745\n3.9\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​62876​39\nIEEE Transactions on Knowledge\nand Data Engineering\n4.935\n12.0\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​69\nNature Communications\n12.121\n18.1\nNature\nhttps://​www.​nature.​com/​ncomms/\nIEEE Transactions on Intelligent\nTransportation Systems\n6.319\n12.7\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​6979\nMethods\n3.812\n8.0\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\nmetho​ds\nACM Journal on Emerging Tech‑\nnologies in Computing Systems\n1.652\n4.3\nACM\nhttps://​dl.​acm.​org/​journ​al/​jetc\nACM Computing Surveys\n6.319\n12.7\nACM\nhttps://​dl.​acm.​org/​journ​al/​csur\nApplied Soft Computing\n5.472\n10.2\nElsevier\nhttps://​www.​journ​als.​elsev​ier.​com/​\nappli​ed-​soft-​compu​ting\nElectronics\n2.412\n1.9\nMDPI\nhttps://​www.​mdpi.​com/​journ​al/​elect​\nronics\nApplied Sciences\n2.474\n2.4\nMDPI\nhttps://​www.​mdpi.​com/​journ​al/​\nappls​ci\nIEEE Transactions on Industrial\nInformatics\n9.112\n13.9\nIEEE\nhttps://​ieeex​plore.​ieee.​org/​xpl/​Recen​\ntIssue.​jsp?​punum​ber=​9424\n\nPage 7 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nFig. 2  Deep learning family\nFig. 3  The difference between deep learning and traditional machine learning\n\nPage 8 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nX-ray images or other types of images. We end this section by the saying of AI pioneer\nGeoffrey Hinton “Deep learning is going to be able to do everything”.\nWhen to apply deep learning\nMachine intelligence is useful in many situations which is equal or better than human\nexperts in some cases [49–52], meaning that DL can be a solution to the following\nproblems:\n•\t Cases where human experts are not available.\n•\t Cases where humans are unable to explain decisions made using their expertise (language understanding, medical decisions, and speech recognition).\n•\t Cases where the problem solution updates over time (price prediction, stock preference, weather prediction, and tracking).\n•\t Cases where solutions require adaptation based on specific cases (personalization,\nbiometrics).\n•\t Cases where size of the problem is extremely large and exceeds our inadequate reasoning abilities (sentiment analysis, matching ads to Facebook, calculation webpage\nranks).\nWhy deep learning?\nSeveral performance features may answer this question, e.g\n1.\t Universal Learning Approach: Because DL has the ability to perform in approximately all application domains, it is sometimes referred to as universal learning.\n2.\t Robustness: In general, precisely designed features are not required in DL techniques. Instead, the optimized features are learned in an automated fashion related\nFig. 4  Deep learning performance compared to human\n\nPage 9 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nto the task under consideration. Thus, robustness to the usual changes of the input\ndata is attained.\n3.\t Generalization: Different data types or different applications can use the same DL\ntechnique, an approach frequently referred to as transfer learning (TL) which\nexplained in the latter section. Furthermore, it is a useful approach in problems\nwhere data is insufficient.\n4.\t Scalability: DL is highly scalable. ResNet [37], which was invented by Microsoft,\ncomprises 1202 layers and is frequently applied at a supercomputing scale. Lawrence Livermore National Laboratory (LLNL), a large enterprise working on evolving\nframeworks for networks, adopted a similar approach, where thousands of nodes can\nbe implemented [53].\nClassification of DL approaches\nDL techniques are classified into three major categories: unsupervised, partially supervised (semi-supervised) and supervised. Furthermore, deep reinforcement learning\n(DRL), also known as RL, is another type of learning technique, which is mostly considered to fall into the category of partially supervised (and occasionally unsupervised)\nlearning techniques.\nDeep supervised learning\nThis technique deals with labeled data. When considering such a technique, the environs\nhave a collection of inputs and resultant outputs (xt, yt) ∼ρ . For instance, the smart\nagent guesses\nif the input is xt and will obtain\nas a loss value. Next,\nthe network parameters are repeatedly updated by the agent to obtain an improved estimate for the preferred outputs. Following a positive training outcome, the agent acquires\nthe ability to obtain the right solutions to the queries from the environs. For DL, there\nare several supervised learning techniques, such as recurrent neural networks (RNNs),\nconvolutional neural networks (CNNs), and deep neural networks (DNNs). In addition,\nthe RNN category includes gated recurrent units (GRUs) and long short-term memory\n(LSTM) approaches. The main advantage of this technique is the ability to collect data\nor generate a data output from the prior knowledge. However, the disadvantage of this\ntechnique is that decision boundary might be overstrained when training set doesn’t\nown samples that should be in a class. Overall, this technique is simpler than other techniques in the way of learning with high performance.\nDeep semi‑supervised learning\nIn this technique, the learning process is based on semi-labeled datasets. Occasionally,\ngenerative adversarial networks (GANs) and DRL are employed in the same way as this\ntechnique. In addition, RNNs, which include GRUs and LSTMs, are also employed for\npartially supervised learning. One of the advantages of this technique is to minimize\nthe amount of labeled data needed. On other the hand, One of the disadvantages of this\ntechnique is irrelevant input feature present training data could furnish incorrect decisions. Text document classifier is one of the most popular example of an application of\n\nPage 10 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nsemi-supervised learning. Due to difficulty of obtaining a large amount of labeled text\ndocuments, semi-supervised learning is ideal for text document classification task.\nDeep unsupervised learning\nThis technique makes it possible to implement the learning process in the absence\nof available labeled data (i.e. no labels are required). Here, the agent learns the significant features or interior representation required to discover the unidentified\nstructure or relationships in the input data. Techniques of generative networks,\ndimensionality reduction and clustering are frequently counted within the category\nof unsupervised learning. Several members of the DL family have performed well on\nnon-linear dimensionality reduction and clustering tasks; these include restricted\nBoltzmann machines, auto-encoders and GANs as the most recently developed techniques. Moreover, RNNs, which include GRUs and LSTM approaches, have also\nbeen employed for unsupervised learning in a wide range of applications. The main\ndisadvantages of unsupervised learning are unable to provide accurate information\nconcerning data sorting and computationally complex. One of the most popular\nunsupervised learning approaches is clustering [54].\nDeep reinforcement learning\nReinforcement Learning operates on interacting with the environment, while supervised learning operates on provided sample data. This technique was developed in\n2013 with Google Deep Mind [55]. Subsequently, many enhanced techniques dependent on reinforcement learning were constructed. For example, if the input environment samples: xt ∼ρ , agent predict:\nand the received cost of the agent is\n, P here is the unknown probability distribution, then the environment asks a question to the agent. The answer it gives is a noisy score. This method\nis sometimes referred to as semi-supervised learning. Based on this concept, several\nsupervised and unsupervised techniques were developed. In comparison with traditional supervised techniques, performing this learning is much more difficult, as no\nstraightforward loss function is available in the reinforcement learning technique. In\naddition, there are two essential differences between supervised learning and reinforcement learning: first, there is no complete access to the function, which requires\noptimization, meaning that it should be queried via interaction; second, the state\nbeing interacted with is founded on an environment, where the input xt is based on\nthe preceding actions [9, 56].\nFor solving a task, the selection of the type of reinforcement learning that needs to\nbe performed is based on the space or the scope of the problem. For example, DRL is\nthe best way for problems involving many parameters to be optimized. By contrast,\nderivative-free reinforcement learning is a technique that performs well for problems\nwith limited parameters. Some of the applications of reinforcement learning are business strategy planning and robotics for industrial automation. The main drawback of\nReinforcement Learning is that parameters may influence the speed of learning. Here\nare the main motivations for utilizing Reinforcement Learning:\n\nPage 11 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n•\t It assists you to identify which action produces the highest reward over a longer\nperiod.\n•\t It assists you to discover which situation requires action.\n•\t It also enables it to figure out the best approach for reaching large rewards.\n•\t Reinforcement Learning also gives the learning agent a reward function.\nReinforcement Learning can’t utilize in all the situation such as:\n•\t In case there is sufficient data to resolve the issue with supervised learning techniques.\n•\t Reinforcement Learning is computing-heavy and time-consuming. Specially when\nthe workspace is large.\nTypes of DL networks\nThe most famous types of deep learning networks are discussed in this section: these\ninclude recursive neural networks (RvNNs), RNNs, and CNNs. RvNNs and RNNs were\nbriefly explained in this section while CNNs were explained in deep due to the importance of this type. Furthermore, it is the most used in several applications among other\nnetworks.\nRecursive neural networks\nRvNN can achieve predictions in a hierarchical structure also classify the outputs utilizing compositional vectors [57]. Recursive auto-associative memory (RAAM) [58] is\nthe primary inspiration for the RvNN development. The RvNN architecture is generated for processing objects, which have randomly shaped structures like graphs or trees.\nThis approach generates a fixed-width distributed representation from a variable-size\nrecursive-data structure. The network is trained using an introduced back-propagation\nthrough structure (BTS) learning system [58]. The BTS system tracks the same technique as the general-back propagation algorithm and has the ability to support a treelike\nstructure. Auto-association trains the network to regenerate the input-layer pattern at\nthe output layer. RvNN is highly effective in the NLP context. Socher et al. [59] introduced RvNN architecture designed to process inputs from a variety of modalities. These\nauthors demonstrate two applications for classifying natural language sentences: cases\nwhere each sentence is split into words and nature images, and cases where each image\nis separated into various segments of interest. RvNN computes a likely pair of scores for\nmerging and constructs a syntactic tree. Furthermore, RvNN calculates a score related\nto the merge plausibility for every pair of units. Next, the pair with the largest score is\nmerged within a composition vector. Following every merge, RvNN generates (a) a larger\narea of numerous units, (b) a compositional vector of the area, and (c) a label for the\nclass (for instance, a noun phrase will become the class label for the new area if two units\nare noun words). The compositional vector for the entire area is the root of the RvNN\ntree structure. An example RvNN tree is shown in Fig. 5. RvNN has been employed in\nseveral applications [60–62].\n\nPage 12 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nRecurrent neural networks\nRNNs are a commonly employed and familiar algorithm in the discipline of DL [63–\n65]. RNN is mainly applied in the area of speech processing and NLP contexts [66,\n67]. Unlike conventional networks, RNN uses sequential data in the network. Since\nthe embedded structure in the sequence of the data delivers valuable information, this\nfeature is fundamental to a range of different applications. For instance, it is important to understand the context of the sentence in order to determine the meaning of\na specific word in it. Thus, it is possible to consider the RNN as a unit of short-term\nmemory, where x represents the input layer, y is the output layer, and s represents the\nstate (hidden) layer. For a given input sequence, a typical unfolded RNN diagram is\nillustrated in Fig. 6. Pascanu et al. [68] introduced three different types of deep RNN\ntechniques, namely “Hidden-to-Hidden”, “Hidden-to-Output”, and “Input-to-Hidden”.\nA deep RNN is introduced that lessens the learning difficulty in the deep network and\nbrings the benefits of a deeper RNN based on these three techniques.\nHowever, RNN’s sensitivity to the exploding gradient and vanishing problems represent one of the main issues with this approach [69]. More specifically, during the\ntraining process, the reduplications of several large or small derivatives may cause the\ngradients to exponentially explode or decay. With the entrance of new inputs, the network stops thinking about the initial ones; therefore, this sensitivity decays over time.\nFurthermore, this issue can be handled using LSTM [70]. This approach offers recurrent connections to memory blocks in the network. Every memory block contains a\nnumber of memory cells, which have the ability to store the temporal states of the\nnetwork. In addition, it contains gated units for controlling the flow of information.\nIn very deep networks [37], residual connections also have the ability to considerably\nreduce the impact of the vanishing gradient issue which explained in later sections.\nFig. 5  An example of RvNN tree\n\nPage 13 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nCNN is considered to be more powerful than RNN. RNN includes less feature compatibility when compared to CNN.\nConvolutional neural networks\nIn the field of DL, the CNN is the most famous and commonly employed algorithm [30,\n71–75]. The main benefit of CNN compared to its predecessors is that it automatically\nidentifies the relevant features without any human supervision [76]. CNNs have been\nextensively applied in a range of different fields, including computer vision [77], speech\nprocessing [78], Face Recognition [79], etc. The structure of CNNs was inspired by neurons in human and animal brains, similar to a conventional neural network. More specifically, in a cat’s brain, a complex sequence of cells forms the visual cortex; this sequence\nis simulated by the CNN [80]. Goodfellow et al. [28] identified three key benefits of the\nCNN: equivalent representations, sparse interactions, and parameter sharing. Unlike\nconventional fully connected (FC) networks, shared weights and local connections in\nthe CNN are employed to make full use of 2D input-data structures like image signals.\nThis operation utilizes an extremely small number of parameters, which both simplifies\nthe training process and speeds up the network. This is the same as in the visual cortex cells. Notably, only small regions of a scene are sensed by these cells rather than the\nwhole scene (i.e., these cells spatially extract the local correlation available in the input,\nlike local filters over the input).\nA commonly used type of CNN, which is similar to the multi-layer perceptron (MLP),\nconsists of numerous convolution layers preceding sub-sampling (pooling) layers, while\nthe ending layers are FC layers. An example of CNN architecture for image classification\nis illustrated in Fig. 7.\nThe input x of each layer in a CNN model is organized in three dimensions: height,\nwidth, and depth, or m × m × r , where the height (m) is equal to the width. The depth\nis also referred to as the channel number. For example, in an RGB image, the depth (r) is\nequal to three. Several kernels (filters) available in each convolutional layer are denoted\nby k and also have three dimensions ( n × n × q ), similar to the input image; here,\nFig. 6  Typical unfolded RNN diagram\n\nPage 14 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nhowever, n must be smaller than m, while q is either equal to or smaller than r. In addition, the kernels are the basis of the local connections, which share similar parameters\n(bias bk and weight W k ) for generating k feature maps hk with a size of ( m −n −1 ) each\nand are convolved with input, as mentioned above. The convolution layer calculates a\ndot product between its input and the weights as in Eq. 1, similar to NLP, but the inputs\nare undersized areas of the initial image size. Next, by applying the nonlinearity or an\nactivation function to the convolution-layer output, we obtain the following:\nThe next step is down-sampling every feature map in the sub-sampling layers. This leads\nto a reduction in the network parameters, which accelerates the training process and\nin turn enables handling of the overfitting issue. For all feature maps, the pooling function (e.g. max or average) is applied to an adjacent area of size p × p , where p is the\nkernel size. Finally, the FC layers receive the mid- and low-level features and create the\nhigh-level abstraction, which represents the last-stage layers as in a typical neural network. The classification scores are generated using the ending layer [e.g. support vector\nmachines (SVMs) or softmax]. For a given instance, every score represents the probability of a specific class.\nBenefits of employing CNNs\nThe benefits of using CNNs over other traditional neural networks in the computer\nvision environment are listed as follows:\n1.\t The main reason to consider CNN is the weight sharing feature, which reduces the\nnumber of trainable network parameters and in turn helps the network to enhance\ngeneralization and to avoid overfitting.\n2.\t Concurrently learning the feature extraction layers and the classification layer causes\nthe model output to be both highly organized and highly reliant on the extracted features.\n(1)\nhk = f (W k ∗x + bk)\nFig. 7  An example of CNN architecture for image classification\n\nPage 15 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n3.\t Large-scale network implementation is much easier with CNN than with other neural networks.\nCNN layers\nThe CNN architecture consists of a number of layers (or so-called multi-building blocks).\nEach layer in the CNN architecture, including its function, is described in detail below.\n1.\t Convolutional Layer: In CNN architecture, the most significant component is the\nconvolutional layer. It consists of a collection of convolutional filters (so-called kernels). The input image, expressed as N-dimensional metrics, is convolved with these\nfilters to generate the output feature map.\n•\t Kernel definition: A grid of discrete numbers or values describes the kernel.\nEach value is called the kernel weight. Random numbers are assigned to act as\nthe weights of the kernel at the beginning of the CNN training process. In addition, there are several different methods used to initialize the weights. Next, these\nweights are adjusted at each training era; thus, the kernel learns to extract significant features.\n•\t Convolutional Operation: Initially, the CNN input format is described. The\nvector format is the input of the traditional neural network, while the multichanneled image is the input of the CNN. For instance, single-channel is the\nformat of the gray-scale image, while the RGB image format is three-channeled.\nTo understand the convolutional operation, let us take an example of a 4 × 4\ngray-scale image with a 2 × 2 random weight-initialized kernel. First, the kernel slides over the whole image horizontally and vertically. In addition, the dot\nproduct between the input image and the kernel is determined, where their\ncorresponding values are multiplied and then summed up to create a single scalar value, calculated concurrently. The whole process is then repeated until no\nfurther sliding is possible. Note that the calculated dot product values represent\nthe feature map of the output. Figure 8 graphically illustrates the primary calculations executed at each step. In this figure, the light green color represents\nthe 2 × 2 kernel, while the light blue color represents the similar size area of the\ninput image. Both are multiplied; the end result after summing up the resulting\nproduct values (marked in a light orange color) represents an entry value to the\noutput feature map.\n\nHowever, padding to the input image is not applied in the previous example,\nwhile a stride of one (denoted for the selected step-size over all vertical or horizontal locations) is applied to the kernel. Note that it is also possible to use\nanother stride value. In addition, a feature map of lower dimensions is obtained\nas a result of increasing the stride value.\n\nOn the other hand, padding is highly significant to determining border size\ninformation related to the input image. By contrast, the border side-features\nmoves carried away very fast. By applying padding, the size of the input image\n\nPage 16 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nwill increase, and in turn, the size of the output feature map will also increase.\nCore Benefits of Convolutional Layers.\n•\t Sparse Connectivity: Each neuron of a layer in FC neural networks links with\nall neurons in the following layer. By contrast, in CNNs, only a few weights are\navailable between two adjacent layers. Thus, the number of required weights or\nconnections is small, while the memory required to store these weights is also\nFig. 8  The primary calculations executed at each step of convolutional layer\n\nPage 17 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nsmall; hence, this approach is memory-effective. In addition, matrix operation\nis computationally much more costly than the dot (.) operation in CNN.\n•\t Weight Sharing: There are no allocated weights between any two neurons of\nneighboring layers in CNN, as the whole weights operate with one and all\npixels of the input matrix. Learning a single group of weights for the whole\ninput will significantly decrease the required training time and various costs,\nas it is not necessary to learn additional weights for each neuron.\n2.\t Pooling Layer: The main task of the pooling layer is the sub-sampling of the feature\nmaps. These maps are generated by following the convolutional operations. In other\nwords, this approach shrinks large-size feature maps to create smaller feature maps.\nConcurrently, it maintains the majority of the dominant information (or features) in\nevery step of the pooling stage. In a similar manner to the convolutional operation,\nboth the stride and the kernel are initially size-assigned before the pooling operation\nis executed. Several types of pooling methods are available for utilization in various\npooling layers. These methods include tree pooling, gated pooling, average pooling,\nmin pooling, max pooling, global average pooling (GAP), and global max pooling.\nThe most familiar and frequently utilized pooling methods are the max, min, and\nGAP pooling. Figure 9 illustrates these three pooling operations.\n\nSometimes, the overall CNN performance is decreased as a result; this represents\nthe main shortfall of the pooling layer, as this layer helps the CNN to determine\nwhether or not a certain feature is available in the particular input image, but focuses\nexclusively on ascertaining the correct location of that feature. Thus, the CNN model\nmisses the relevant information.\n3.\t Activation Function (non-linearity) Mapping the input to the output is the core function of all types of activation function in all types of neural network. The input value\nis determined by computing the weighted summation of the neuron input along with\nits bias (if present). This means that the activation function makes the decision as to\nwhether or not to fire a neuron with reference to a particular input by creating the\ncorresponding output.\nFig. 9  Three types of pooling operations\n\nPage 18 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nNon-linear activation layers are employed after all layers with weights (so-called\nlearnable layers, such as FC layers and convolutional layers) in CNN architecture.\nThis non-linear performance of the activation layers means that the mapping of input\nto output will be non-linear; moreover, these layers give the CNN the ability to learn\nextra-complicated things. The activation function must also have the ability to differentiate, which is an extremely significant feature, as it allows error back-propagation\nto be used to train the network. The following types of activation functions are most\ncommonly used in CNN and other deep neural networks.\n•\t Sigmoid: The input of this activation function is real numbers, while the output is\nrestricted to between zero and one. The sigmoid function curve is S-shaped and\ncan be represented mathematically by Eq. 2.\n•\t Tanh: It is similar to the sigmoid function, as its input is real numbers, but the output is restricted to between − 1 and 1. Its mathematical representation is in Eq. 3.\n•\t ReLU: The mostly commonly used function in the CNN context. It converts the\nwhole values of the input to positive numbers. Lower computational load is the\nmain benefit of ReLU over the others. Its mathematical representation is in Eq. 4.\nOccasionally, a few significant issues may occur during the use of ReLU. For\ninstance, consider an error back-propagation algorithm with a larger gradient\nflowing through it. Passing this gradient within the ReLU function will update the\nweights in a way that makes the neuron certainly not activated once more. This\nissue is referred to as “Dying ReLU”. Some ReLU alternatives exist to solve such\nissues. The following discusses some of them.\n•\t Leaky ReLU: Instead of ReLU down-scaling the negative inputs, this activation\nfunction ensures these inputs are never ignored. It is employed to solve the Dying\nReLU problem. Leaky ReLU can be represented mathematically as in Eq. 5.\nNote that the leak factor is denoted by m. It is commonly set to a very small value,\nsuch as 0.001.\n•\t Noisy ReLU: This function employs a Gaussian distribution to make ReLU noisy.\nIt can be represented mathematically as in Eq. 6.\n(2)\nf (x)sigm =\n1\n1 + e−x\n(3)\nf (x)tanh = ex −e−x\nex + e−x\n(4)\nf (x)ReLU = max(0, x)\n(5)\nf (x)LeakyReLU =\n\nx,\nif\nx > 0\nmx, x ≤0\n\n(6)\nf (x)NoisyReLU = max(x + Y ), with Y ∼N(0, σ(x))\n\nPage 19 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n•\t Parametric Linear Units: This is mostly the same as Leaky ReLU. The main difference is that the leak factor in this function is updated through the model training\nprocess. The parametric linear unit can be represented mathematically as in Eq. 7.\nNote that the learnable weight is denoted as a.\n4.\t Fully Connected Layer: Commonly, this layer is located at the end of each CNN\narchitecture. Inside this layer, each neuron is connected to all neurons of the previous layer, the so-called Fully Connected (FC) approach. It is utilized as the CNN\nclassifier. It follows the basic method of the conventional multiple-layer perceptron\nneural network, as it is a type of feed-forward ANN. The input of the FC layer comes\nfrom the last pooling or convolutional layer. This input is in the form of a vector,\nwhich is created from the feature maps after flattening. The output of the FC layer\nrepresents the final CNN output, as illustrated in Fig. 10.\n5.\t Loss Functions: The previous section has presented various layer-types of CNN\narchitecture. In addition, the final classification is achieved from the output layer,\nwhich represents the last layer of the CNN architecture. Some loss functions are utilized in the output layer to calculate the predicted error created across the training\nsamples in the CNN model. This error reveals the difference between the actual output and the predicted one. Next, it will be optimized through the CNN learning process.\n\nHowever, two parameters are used by the loss function to calculate the error. The\nCNN estimated output (referred to as the prediction) is the first parameter. The\nactual output (referred to as the label) is the second parameter. Several types of loss\nfunction are employed in various problem types. The following concisely explains\nsome of the loss function types.\n(7)\nf (x)ParametricLinear =\n\nx, if x > 0\nax,\nx ≤0\n\nFig. 10  Fully connected layer\n\nPage 20 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n(a)\t Cross-Entropy or Softmax Loss Function: This function is commonly employed\nfor measuring the CNN model performance. It is also referred to as the log\nloss function. Its output is the probability p ∈{0, 1} . In addition, it is usually\nemployed as a substitution of the square error loss function in multi-class classification problems. In the output layer, it employs the softmax activations to\ngenerate the output within a probability distribution. The mathematical representation of the output class probability is Eq. 8.\nHere, eai represents the non-normalized output from the preceding layer, while\nN represents the number of neurons in the output layer. Finally, the mathematical representation of cross-entropy loss function is Eq. 9.\n(b)\t Euclidean Loss Function: This function is widely used in regression problems.\nIn addition, it is also the so-called mean square error. The mathematical expression of the estimated Euclidean loss is Eq. 10.\n(c)\t Hinge Loss Function: This function is commonly employed in problems related\nto binary classification. This problem relates to maximum-margin-based classification; this is mostly important for SVMs, which use the hinge loss function,\nwherein the optimizer attempts to maximize the margin around dual objective\nclasses. Its mathematical formula is Eq. 11.\nThe margin m is commonly set to 1. Moreover, the predicted output is denoted\nas pi , while the desired output is denoted as yi.\nRegularization to CNN\nFor CNN models, over-fitting represents the central issue associated with obtaining\nwell-behaved generalization. The model is entitled over-fitted in cases where the model\nexecutes especially well on training data and does not succeed on test data (unseen data)\nwhich is more explained in the latter section. An under-fitted model is the opposite; this\ncase occurs when the model does not learn a sufficient amount from the training data.\nThe model is referred to as “just-fitted” if it executes well on both training and testing\ndata. These three types are illustrated in Fig. 11. Various intuitive concepts are used to\nhelp the regularization to avoid over-fitting; more details about over-fitting and underfitting are discussed in latter sections.\n(8)\npi =\neai\nN\nk=1 ea\nk\n(9)\nH(p, y) = −\n\ni\nyi log(pi)\nwhere\ni ∈[1, N]\n(10)\nH(p, y) =\n1\n2N\nN\n\ni=1\n(pi −yi)2\n(11)\nH(p, y) =\nN\n\ni=1\nmax(0, m −(2yi −1)pi)\n\nPage 21 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n1.\t Dropout: This is a widely utilized technique for generalization. During each training\nepoch, neurons are randomly dropped. In doing this, the feature selection power is\ndistributed equally across the whole group of neurons, as well as forcing the model to\nlearn different independent features. During the training process, the dropped neuron will not be a part of back-propagation or forward-propagation. By contrast, the\nfull-scale network is utilized to perform prediction during the testing process.\n2.\t Drop-Weights: This method is highly similar to dropout. In each training epoch, the\nconnections between neurons (weights) are dropped rather than dropping the neurons; this represents the only difference between drop-weights and dropout.\n3.\t Data Augmentation: Training the model on a sizeable amount of data is the easiest\nway to avoid over-fitting. To achieve this, data augmentation is used. Several techniques are utilized to artificially expand the size of the training dataset. More details\ncan be found in the latter section, which describes the data augmentation techniques.\n4.\t Batch Normalization: This method ensures the performance of the output activations\n[81]. This performance follows a unit Gaussian distribution. Subtracting the mean\nand dividing by the standard deviation will normalize the output at each layer. While\nit is possible to consider this as a pre-processing task at each layer in the network, it\nis also possible to differentiate and to integrate it with other networks. In addition, it\nis employed to reduce the “internal covariance shift” of the activation layers. In each\nlayer, the variation in the activation distribution defines the internal covariance shift.\nThis shift becomes very high due to the continuous weight updating through training, which may occur if the samples of the training data are gathered from numerous dissimilar sources (for example, day and night images). Thus, the model will consume extra time for convergence, and in turn, the time required for training will also\nincrease. To resolve this issue, a layer representing the operation of batch normalization is applied in the CNN architecture.\n\nThe advantages of utilizing batch normalization are as follows:\n•\t It prevents the problem of vanishing gradient from arising.\n•\t It can effectively control the poor weight initialization.\n•\t It significantly reduces the time required for network convergence (for large-scale\ndatasets, this will be extremely useful).\n•\t It struggles to decrease training dependency across hyper-parameters.\nFig. 11  Over-fitting and under-fitting issues\n\nPage 22 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n•\t Chances of over-fitting are reduced, since it has a minor influence on regularization.\nOptimizer selection\nThis section discusses the CNN learning process. Two major issues are included in the\nlearning process: the first issue is the learning algorithm selection (optimizer), while the\nsecond issue is the use of many enhancements (such as AdaDelta, Adagrad, and momentum) along with the learning algorithm to enhance the output.\nLoss functions, which are founded on numerous learnable parameters (e.g. biases,\nweights, etc.) or minimizing the error (variation between actual and predicted output),\nare the core purpose of all supervised learning algorithms. The techniques of gradientbased learning for a CNN network appear as the usual selection. The network parameters should always update though all training epochs, while the network should also look\nfor the locally optimized answer in all training epochs in order to minimize the error.\nThe learning rate is defined as the step size of the parameter updating. The training epoch represents a complete repetition of the parameter update that involves the\ncomplete training dataset at one time. Note that it needs to select the learning rate\nwisely so that it does not influence the learning process imperfectly, although it is a\nhyper-parameter.\nGradient Descent or Gradient-based learning algorithm: To minimize the training error, this algorithm repetitively updates the network parameters through every\ntraining epoch. More specifically, to update the parameters correctly, it needs to compute the objective function gradient (slope) by applying a first-order derivative with\nrespect to the network parameters. Next, the parameter is updated in the reverse\ndirection of the gradient to reduce the error. The parameter updating process is performed though network back-propagation, in which the gradient at every neuron is\nback-propagated to all neurons in the preceding layer. The mathematical representation of this operation is as Eq. 12.\nThe final weight in the current training epoch is denoted by wijt , while the weight in the\npreceding (t −1) training epoch is denoted wijt−1 . The learning rate is η and the prediction error is E. Different alternatives of the gradient-based learning algorithm are available and commonly employed; these include the following:\n1.\t Batch Gradient Descent: During the execution of this technique [82], the network\nparameters are updated merely one time behind considering all training datasets via\nthe network. In more depth, it calculates the gradient of the whole training set and\nsubsequently uses this gradient to update the parameters. For a small-sized dataset,\nthe CNN model converges faster and creates an extra-stable gradient using BGD.\nSince the parameters are changed only once for every training epoch, it requires a\nsubstantial amount of resources. By contrast, for a large training dataset, additional\n(12)\nwijt = wijt−1 −\u001fwijt,\nwijt = η ∗∂E\n∂wij\n\nPage 23 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\ntime is required for converging, and it could converge to a local optimum (for nonconvex instances).\n2.\t Stochastic Gradient Descent: The parameters are updated at each training sample in\nthis technique [83]. It is preferred to arbitrarily sample the training samples in every\nepoch in advance of training. For a large-sized training dataset, this technique is both\nmore memory-effective and much faster than BGD. However, because it is frequently\nupdated, it takes extremely noisy steps in the direction of the answer, which in turn\ncauses the convergence behavior to become highly unstable.\n3.\t Mini-batch Gradient Descent: In this approach, the training samples are partitioned\ninto several mini-batches, in which every mini-batch can be considered an undersized collection of samples with no overlap between them [84]. Next, parameter\nupdating is performed following gradient computation on every mini-batch. The\nadvantage of this method comes from combining the advantages of both BGD and\nSGD techniques. Thus, it has a steady convergence, more computational efficiency\nand extra memory effectiveness. The following describes several enhancement techniques in gradient-based learning algorithms (usually in SGD), which further powerfully enhance the CNN training process.\n4.\t Momentum: For neural networks, this technique is employed in the objective function. It enhances both the accuracy and the training speed by summing the computed gradient at the preceding training step, which is weighted via a factor \u001f (known\nas the momentum factor). However, it therefore simply becomes stuck in a local\nminimum rather than a global minimum. This represents the main disadvantage of\ngradient-based learning algorithms. Issues of this kind frequently occur if the issue\nhas no convex surface (or solution space).\n\nTogether with the learning algorithm, momentum is used to solve this issue, which\ncan be expressed mathematically as in Eq. 13.\nThe weight increment in the current t′th training epoch is denoted as \u001fwijt , while\nη is the learning rate, and the weight increment in the preceding (t −1)′th training\nepoch. The momentum factor value is maintained within the range 0 to 1; in turn,\nthe step size of the weight updating increases in the direction of the bare minimum\nto minimize the error. As the value of the momentum factor becomes very low, the\nmodel loses its ability to avoid the local bare minimum. By contrast, as the momentum factor value becomes high, the model develops the ability to converge much\nmore rapidly. If a high value of momentum factor is used together with LR, then the\nmodel could miss the global bare minimum by crossing over it.\n\nHowever, when the gradient varies its direction continually throughout the training\nprocess, then the suitable value of the momentum factor (which is a hyper-parameter) causes a smoothening of the weight updating variations.\n5.\t Adaptive Moment Estimation (Adam): It is another optimization technique or learning algorithm that is widely used. Adam [85] represents the latest trends in deep\n(13)\nwijt =\n\nη ∗∂E\n∂wij\n\n+ (\u001f ∗\u001fwijt−1)\n\nPage 24 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nlearning optimization. This is represented by the Hessian matrix, which employs a\nsecond-order derivative. Adam is a learning strategy that has been designed specifically for training deep neural networks. More memory efficient and less computational power are two advantages of Adam. The mechanism of Adam is to calculate\nadaptive LR for each parameter in the model. It integrates the pros of both Momentum and RMSprop. It utilizes the squared gradients to scale the learning rate as\nRMSprop and it is similar to the momentum by using the moving average of the gradient. The equation of Adam is represented in Eq. 14.\nDesign of algorithms (backpropagation)\nLet’s start with a notation that refers to weights in the network unambiguously. We\ndenote wh\nij to be the weight for the connection from ith input or (neuron at (h −1)th)\nto the jt neuron in the hth layer. So, Fig. 12 shows the weight on a connection from the\nneuron in the first layer to another neuron in the next layer in the network.\nWhere w2\n11 has represented the weight from the first neuron in the first layer to the\nfirst neuron in the second layer, based on that the second weight for the same neuron\nwill be w2\n21 which means is the weight comes from the second neuron in the previous\nlayer to the first layer in the next layer which is the second in this net. Regarding the\nbias, since the bias is not the connection between the neurons for the layers, so it is\neasily handled each neuron must have its own bias, some network each layer has a\ncertain bias. It can be seen from the above net that each layer has its own bias. Each\nnetwork has the parameters such as the no of the layer in the net, the number of the\nneurons in each layer, no of the weight (connection) between the layers, the no of\nconnection can be easily determined based on the no of neurons in each layer, for\nexample, if there are ten input fully connect with two neurons in the next layer then\n(14)\nwijt = wijt−1 −\nη\n\nE[δ2]t+ ∈\n∗\nE[δ2]t\nFig. 12  MLP structure\n\nPage 25 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nthe number of connection between them is (10 ∗2 = 20 connection, weights), how\nthe error is defined, and the weight is updated, we will imagine there is there are two\nlayers in our neural network,\nwhere d is the label of induvial input ith and y is the output of the same individual input.\nBackpropagation is about understanding how to change the weights and biases in a network based on the changes of the cost function (Error). Ultimately, this means computing the partial derivatives ∂E/∂wh\nij and ∂E/∂bh\nj . But to compute those, a local variable is\nintroduced, δ1\nj which is called the local error in the jth neuron in the hth layer. Based on\nthat local error Backpropagation will give the procedure to compute ∂E/∂wh\nij and ∂E/∂bh\nj\nhow the error is defined, and the weight is updated, we will imagine there is there are\ntwo layers in our neural network that is shown in Fig. 13.\nOutput error for δ1\nj each 1 = 1 : L where L is no. of neuron in output\nwhere e(k) is the error of the epoch k as shown in Eq. (2) and ϑ′\nvj(k)\n\nis the derivate of\nthe activation function for vj at the output.\nBackpropagate the error at all the rest layer except the output\nwhere δ1\nj (k) is the output error and wh+1\njl\n(k) is represented the weight after the layer\nwhere the error need to obtain.\nAfter finding the error at each neuron in each layer, now we can update the weight\nin each layer based on Eqs. (16) and (17).\nImproving performance of CNN\nBased on our experiments in different DL applications [86–88]. We can conclude the\nmost active solutions that may improve the performance of CNN are:\n(15)\nerror = 1/2\n\ndi −yi\n2\n(16)\nδ1\nj (k) = (−1)e(k)ϑ′\nvj(k)\n\n(17)\nδh\nj (k) = ϑ′\nvj(k)\n\nL\n\nl=1\nδ1\nj\nwh+1\njl\n(k)\nFig. 13  Neuron activation functions\n\nPage 26 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n•\t Expand the dataset with data augmentation or use transfer learning (explained in latter sections).\n•\t Increase the training time.\n•\t Increase the depth (or width) of the model.\n•\t Add regularization.\n•\t Increase hyperparameters tuning.\nCNN architectures\nOver the last 10 years, several CNN architectures have been presented [21, 26]. Model\narchitecture is a critical factor in improving the performance of different applications.\nVarious modifications have been achieved in CNN architecture from 1989 until today.\nSuch modifications include structural reformulation, regularization, parameter optimizations, etc. Conversely, it should be noted that the key upgrade in CNN performance\noccurred largely due to the processing-unit reorganization, as well as the development\nof novel blocks. In particular, the most novel developments in CNN architectures were\nperformed on the use of network depth. In this section, we review the most popular\nCNN architectures, beginning from the AlexNet model in 2012 and ending at the HighResolution (HR) model in 2020. Studying these architectures features (such as input size,\ndepth, and robustness) is the key to help researchers to choose the suitable architecture\nfor the their target task. Table 2 presents the brief overview of CNN architectures.\nAlexNet\nThe history of deep CNNs began with the appearance of LeNet [89] (Fig. 14). At that\ntime, the CNNs were restricted to handwritten digit recognition tasks, which cannot\nbe scaled to all image classes. In deep CNN architecture, AlexNet is highly respected\n[30], as it achieved innovative results in the fields of image recognition and classification.\nKrizhevesky et  al. [30] first proposed AlexNet and consequently improved the CNN\nlearning ability by increasing its depth and implementing several parameter optimization strategies. Figure 15 illustrates the basic design of the AlexNet architecture.\nThe learning ability of the deep CNN was limited at this time due to hardware\nrestrictions. To overcome these hardware limitations, two GPUs (NVIDIA GTX 580)\nwere used in parallel to train AlexNet. Moreover, in order to enhance the applicability of the CNN to different image categories, the number of feature extraction stages\nwas increased from five in LeNet to seven in AlexNet. Regardless of the fact that depth\nenhances generalization for several image resolutions, it was in fact overfitting that represented the main drawback related to the depth. Krizhevesky et al. used Hinton’s idea to\naddress this problem [90, 91]. To ensure that the features learned by the algorithm were\nextra robust, Krizhevesky et  al.’s algorithm randomly passes over several transformational units throughout the training stage. Moreover, by reducing the vanishing gradient\nproblem, ReLU [92] could be utilized as a non-saturating activation function to enhance\nthe rate of convergence [93]. Local response normalization and overlapping subsampling\nwere also performed to enhance the generalization by decreasing the overfitting. To\nimprove on the performance of previous networks, other modifications were made by\nusing large-size filters (5 × 5 and 11 × 11) in the earlier layers. AlexNet has considerable\n\nPage 27 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nTable 2  Brief overview of CNN architectures\nModel\nMain finding\nDepth\nDataset\nError rate\nInput size\nYear\nAlexNet\nUtilizes Dropout\nand ReLU\n8\nImageNet\n16.4\n227 × 227 × 3\n2012\nNIN\nNew layer, called\n‘mlpconv’, utilizes\nGAP\n3\nCIFAR-10, CIFAR100, MNIST\n10.41, 35.68, 0.45\n32 × 32 × 3\n2013\nZfNet\nVisualization idea\nof middle layers\n8\nImageNet\n11.7\n224 × 224 × 3\n2014\nVGG\nIncreased depth,\nsmall filter size\n16, 19\nImageNet\n7.3\n224 × 224 × 3\n2014\nGoogLeNet\nIncreased\ndepth,block\nconcept, differ‑\nent filter size,\nconcatenation\nconcept\n22\nImageNet\n6.7\n224 × 224 × 3\n2015\nInception-V3\nUtilizes small\nfiltersize, better\nfeature represen‑\ntation\n48\nImageNet\n3.5\n229 × 229 × 3\n2015\nHighway\nPresented the mul‑\ntipath concept\n19, 32\nCIFAR-10\n7.76\n32 × 32 × 3\n2015\nInception-V4\nDivided transform\nand integration\nconcepts\n70\nImageNet\n3.08\n229 × 229 × 3\n2016\nResNet\nRobust against\noverfitting due\nto symmetry\nmapping-based\nskip links\n152\nImageNet\n3.57\n224 × 224 × 3\n2016\nInception-ResNetv2\nIntroduced the\nconcept of\nresidual links\n164\nImageNet\n3.52\n229 × 229 × 3\n2016\nFractalNet\nIntroduced the\nconcept of\nDrop-Path as\nregularization\n40,80\nCIFAR-10\n4.60\n32 × 32 × 3\n2016\nCIFAR-100\n18.85\nWideResNet\nDecreased the\ndepth and\nincreased the\nwidth\n28\nCIFAR-10\n3.89\n32 × 32 × 3\n2016\nCIFAR-100\n18.85\nXception\nA depthwise con‑\nvolutionfollowed\nby a pointwise\nconvolution\n71\nImageNet\n0.055\n229 × 229 × 3\n2017\nResidual attention\nneural network\nPresented the\nattention tech‑\nnique\n452\nCIFAR-10, CIFAR100\n3.90, 20.4\n40 × 40 × 3\n2017\nSqueeze-and-exci‑\ntation networks\nModeled inter‑\ndependencies\nbetween chan‑\nnels\n152\nImageNet\n2.25\n229 × 229 × 3\n2017\n224 × 224 × 3\n320 × 320 × 3\nDenseNet\nBlocks of layers;\nlayers connected\nto each other\n201\nCIFAR-10, CIFAR100,ImageNet\n3.46, 17.18, 5.54\n224 × 224 × 3\n2017\nCompetitive\nsqueeze and\nexcitation net‑\nwork\nBoth residual and\nidentity map‑\npings utilized\nto rescale the\nchannel\n152\nCIFAR-10\n3.58\n32 × 32 × 3\n2018\nCIFAR-100\n18.47\n\nPage 28 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nsignificance in the recent CNN generations, as well as beginning an innovative research\nera in CNN applications.\nNetwork‑in‑network\nThis network model, which has some slight differences from the preceding models,\nintroduced two innovative concepts [94]. The first was employing multiple layers of\nTable 2  (continued)\nModel\nMain finding\nDepth\nDataset\nError rate\nInput size\nYear\nMobileNet-v2\nInverted residual\nstructure\n53\nImageNet\n–\n224 × 224 × 3\n2018\nCapsuleNet\nPays attention to\nspecial relation‑\nships between\nfeatures\n3\nMNIST\n0.00855\n28 × 28 × 1\n2018\nHRNetV2\nHigh-resolution\nrepresentations\n–\nImageNet\n5.4\n224 × 224 × 3\n2020\nFig. 14  The architecture of LeNet\nFig. 15  The architecture of AlexNet\n\nPage 29 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nperception convolution. These convolutions are executed using a 1×1 filter, which supports the addition of extra nonlinearity in the networks. Moreover, this supports enlarging the network depth, which may later be regularized using dropout. For DL models,\nthis idea is frequently employed in the bottleneck layer. As a substitution for a FC layer,\nthe GAP is also employed, which represents the second novel concept and enables a significant reduction in the number of model parameters. In addition, GAP considerably\nupdates the network architecture. Generating a final low-dimensional feature vector\nwith no reduction in the feature maps dimension is possible when GAP is used on a\nlarge feature map [95, 96]. Figure 16 shows the structure of the network.\nZefNet\nBefore 2013, the CNN learning mechanism was basically constructed on a trial-anderror basis, which precluded an understanding of the precise purpose following the\nenhancement. This issue restricted the deep CNN performance on convoluted images. In\nresponse, Zeiler and Fergus introduced DeconvNet (a multilayer de-convolutional neural network) in 2013 [97]. This method later became known as ZefNet, which was developed in order to quantitively visualize the network. Monitoring the CNN performance\nvia understanding the neuron activation was the purpose of the network activity visualization. However, Erhan et al. utilized this exact concept to optimize deep belief network\n(DBN) performance by visualizing the features of the hidden layers [98]. Moreover, in\naddition to this issue, Le et al. assessed the deep unsupervised auto-encoder (AE) performance by visualizing the created classes of the image using the output neurons [99].\nBy reversing the operation order of the convolutional and pooling layers, DenconvNet\noperates like a forward-pass CNN. Reverse mapping of this kind launches the convolutional layer output backward to create visually observable image shapes that accordingly\ngive the neural interpretation of the internal feature representation learned at each layer\n[100]. Monitoring the learning schematic through the training stage was the key concept underlying ZefNet. In addition, it utilized the outcomes to recognize an ability issue\ncoupled with the model. This concept was experimentally proven on AlexNet by applying DeconvNet. This indicated that only certain neurons were working, while the others\nwere out of action in the first two layers of the network. Furthermore, it indicated that\nthe features extracted via the second layer contained aliasing objects. Thus, Zeiler and\nFergus changed the CNN topology due to the existence of these outcomes. In addition,\nthey executed parameter optimization, and also exploited the CNN learning by decreasing the stride and the filter sizes in order to retain all features of the initial two convolutional layers. An improvement in performance was accordingly achieved due to this\nFig. 16  The architecture of network-in-network\n\nPage 30 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nrearrangement in CNN topology. This rearrangement proposed that the visualization of\nthe features could be employed to identify design weaknesses and conduct appropriate\nparameter alteration. Figure 17 shows the structure of the network.\nVisual geometry group (VGG)\nAfter CNN was determined to be effective in the field of image recognition, an easy and\nefficient design principle for CNN was proposed by Simonyan and Zisserman. This innovative design was called Visual Geometry Group (VGG). A multilayer model [101], it featured nineteen more layers than ZefNet [97] and AlexNet [30] to simulate the relations\nof the network representational capacity in depth. Conversely, in the 2013-ILSVRC competition, ZefNet was the frontier network, which proposed that filters with small sizes\ncould enhance the CNN performance. With reference to these results, VGG inserted a\nlayer of the heap of 3 × 3 filters rather than the 5 × 5 and 11 × 11 filters in ZefNet. This\nshowed experimentally that the parallel assignment of these small-size filters could produce the same influence as the large-size filters. In other words, these small-size filters\nmade the receptive field similarly efficient to the large-size filters (7 × 7 and 5 × 5) . By\ndecreasing the number of parameters, an extra advantage of reducing computational\ncomplication was achieved by using small-size filters. These outcomes established a\nnovel research trend for working with small-size filters in CNN. In addition, by inserting\n1 × 1 convolutions in the middle of the convolutional layers, VGG regulates the network\ncomplexity. It learns a linear grouping of the subsequent feature maps. With respect\nto network tuning, a max pooling layer [102] is inserted following the convolutional\nlayer, while padding is implemented to maintain the spatial resolution. In general, VGG\nobtained significant results for localization problems and image classification. While it\ndid not achieve first place in the 2014-ILSVRC competition, it acquired a reputation due\nto its enlarged depth, homogenous topology, and simplicity. However, VGG’s computational cost was excessive due to its utilization of around 140 million parameters, which\nrepresented its main shortcoming. Figure 18 shows the structure of the network.\nGoogLeNet\nIn the 2014-ILSVRC competition, GoogleNet (also called Inception-V1) emerged as the\nwinner [103]. Achieving high-level accuracy with decreased computational cost is the\ncore aim of the GoogleNet architecture. It proposed a novel inception block (module)\nconcept in the CNN context, since it combines multiple-scale convolutional transformations by employing merge, transform, and split functions for feature extraction. Figure 19 illustrates the inception block architecture. This architecture incorporates filters\nFig. 17  The architecture of ZefNet\n\nPage 31 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nof different sizes ( 5 × 5, 3 × 3, and 1 × 1 ) to capture channel information together with\nspatial information at diverse ranges of spatial resolution. The common convolutional\nlayer of GoogLeNet is substituted by small blocks using the same concept of networkin-network (NIN) architecture [94], which replaced each layer with a micro-neural network. The GoogLeNet concepts of merge, transform, and split were utilized, supported\nby attending to an issue correlated with different learning types of variants existing in a\nsimilar class of several images. The motivation of GoogLeNet was to improve the efficiency of CNN parameters, as well as to enhance the learning capacity. In addition, it\nregulates the computation by inserting a 1 × 1 convolutional filter, as a bottleneck layer,\nahead of using large-size kernels. GoogleNet employed sparse connections to overcome\nthe redundant information problem. It decreased cost by neglecting the irrelevant channels. It should be noted here that only some of the input channels are connected to some\nof the output channels. By employing a GAP layer as the end layer, rather than utilizing a\nFC layer, the density of connections was decreased. The number of parameters was also\nsignificantly decreased from 40 to 5 million parameters due to these parameter tunings.\nThe additional regularity factors used included the employment of RmsProp as optimizer and batch normalization [104]. Furthermore, GoogleNet proposed the idea of auxiliary learners to speed up the rate of convergence. Conversely, the main shortcoming of\nGoogleNet was its heterogeneous topology; this shortcoming requires adaptation from\none module to another. Other shortcomings of GoogleNet include the representation\nFig. 18  The architecture of VGG\nFig. 19  The basic structure of Google Block\n\nPage 32 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\njam, which substantially decreased the feature space in the following layer, and in turn\noccasionally leads to valuable information loss.\nHighway network\nIncreasing the network depth enhances its performance, mainly for complicated tasks.\nBy contrast, the network training becomes difficult. The presence of several layers in\ndeeper networks may result in small gradient values of the back-propagation of error at\nlower layers. In 2015, Srivastava et al. [105] suggested a novel CNN architecture, called\nHighway Network, to overcome this issue. This approach is based on the cross-connectivity concept. The unhindered information flow in Highway Network is empowered by\ninstructing two gating units inside the layer. The gate mechanism concept was motivated\nby LSTM-based RNN [106, 107]. The information aggregation was conducted by merging the information of the ıth −k layers with the next ıth layer to generate a regularization impact, which makes the gradient-based training of the deeper network very simple.\nThis empowers the training of networks with more than 100 layers, such as a deeper\nnetwork of 900 layers with the SGD algorithm. A Highway Network with a depth of fifty\nlayers presented an improved rate of convergence, which is better than thin and deep\narchitectures at the same time [108]. By contrast, [69] empirically demonstrated that\nplain Net performance declines when more than ten hidden layers are inserted. It should\nbe noted that even a Highway Network 900 layers in depth converges much more rapidly\nthan the plain network.\nResNet\nHe et al. [37] developed ResNet (Residual Network), which was the winner of ILSVRC 2015. Their objective was to design an ultra-deep network free of the vanishing\ngradient issue, as compared to the previous networks. Several types of ResNet were\ndeveloped based on the number of layers (starting with 34 layers and going up to 1202\nlayers). The most common type was ResNet50, which comprised 49 convolutional layers plus a single FC layer. The overall number of network weights was 25.5 M, while\nthe overall number of MACs was 3.9 M. The novel idea of ResNet is its use of the\nbypass pathway concept, as shown in Fig. 20, which was employed in Highway Nets to\naddress the problem of training a deeper network in 2015. This is illustrated in Fig. 20,\nwhich contains the fundamental ResNet block diagram. This is a conventional feedforward network plus a residual connection. The residual layer output can be identified as the (l −1)th outputs, which are delivered from the preceding layer (xl −1) .\nAfter executing different operations [such as convolution using variable-size filters,\nor batch normalization, before applying an activation function like ReLU on (xl −1) ],\nthe output is F(xl −1) . The ending residual output is xl , which can be mathematically\nrepresented as in Eq. 18.\nThere are numerous basic residual blocks included in the residual network. Based on\nthe type of the residual network architecture, operations in the residual block are also\nchanged [37].\n(18)\nxl = F(xl −1) + xl −1\n\nPage 33 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nIn comparison to the highway network, ResNet presented shortcut connections\ninside layers to enable cross-layer connectivity, which are parameter-free and dataindependent. Note that the layers characterize non-residual functions when a gated\nshortcut is closed in the highway network. By contrast, the individuality shortcuts are\nnever closed, while the residual information is permanently passed in ResNet. Furthermore, ResNet has the potential to prevent the problems of gradient diminishing,\nas the shortcut connections (residual links) accelerate the deep network convergence.\nResNet was the winner of the 2015-ILSVRC championship with 152 layers of depth;\nthis represents 8 times the depth of VGG and 20 times the depth of AlexNet. In comparison with VGG, it has lower computational complexity, even with enlarged depth.\nInception: ResNet and Inception‑V3/4\nSzegedy et  al. [103, 109, 110] proposed Inception-ResNet and Inception-V3/4 as\nupgraded types of Inception-V1/2. The concept behind Inception-V3 was to minimize\nthe computational cost with no effect on the deeper network generalization. Thus,\nSzegedy et  al. used asymmetric small-size filters ( 1 × 5 and 1 × 7 ) rather than largesize filters ( 7 × 7 and 5 × 5 ); moreover, they utilized a bottleneck of 1 × 1 convolution\nprior to the large-size filters [110]. These changes make the operation of the traditional\nFig. 20  The block diagram for ResNet\n\nPage 34 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nconvolution very similar to cross-channel correlation. Previously, Lin et al. utilized the\n1 × 1 filter potential in NIN architecture [94]. Subsequently, [110] utilized the same\nidea in an intelligent manner. By using 1 × 1 convolutional operation in Inception-V3,\nthe input data are mapped into three or four isolated spaces, which are smaller than the\ninitial input spaces. Next, all of these correlations are mapped in these smaller spaces\nthrough common 5 × 5 or 3 × 3 convolutions. By contrast, in Inception-ResNet, Szegedy\net al. bring together the inception block and the residual learning power by replacing the\nfilter concatenation with the residual connection [111]. Szegedy et al. empirically demonstrated that Inception-ResNet (Inception-4 with residual connections) can achieve a\nsimilar generalization power to Inception-V4 with enlarged width and depth and without residual connections. Thus, it is clearly illustrated that using residual connections in\ntraining will significantly accelerate the Inception network training. Figure 21 shows The\nbasic block diagram for Inception Residual unit.\nDenseNet\nTo solve the problem of the vanishing gradient, DenseNet was presented, following the\nsame direction as ResNet and the Highway network [105, 111, 112]. One of the drawbacks of ResNet is that it clearly conserves information by means of preservative individuality transformations, as several layers contribute extremely little or no information.\nIn addition, ResNet has a large number of weights, since each layer has an isolated group\nof weights. DenseNet employed cross-layer connectivity in an improved approach to\naddress this problem [112–114]. It connected each layer to all layers in the network\nusing a feed-forward approach. Therefore, the feature maps of each previous layer were\nemployed to input into all of the following layers. In traditional CNNs, there are l connections between the previous layer and the current layer, while in DenseNet, there are\nl(l+1)\n2\ndirect connections. DenseNet demonstrates the influence of cross-layer depth\nFig. 21  The basic block diagram for Inception Residual unit\n\nPage 35 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nwise-convolutions. Thus, the network gains the ability to discriminate clearly between\nthe added and the preserved information, since DenseNet concatenates the features of\nthe preceding layers rather than adding them. However, due to its narrow layer structure, DenseNet becomes parametrically high-priced in addition to the increased number\nof feature maps. The direct admission of all layers to the gradients via the loss function\nenhances the information flow all across the network. In addition, this includes a regularizing impact, which minimizes overfitting on tasks alongside minor training sets. Figure 22 shows the architecture of DenseNet Network.\nResNext\nResNext is an enhanced version of the Inception Network [115]. It is also known as the\nAggregated Residual Transform Network. Cardinality, which is a new term presented by\n[115], utilized the split, transform, and merge topology in an easy and effective way. It\ndenotes the size of the transformation set as an extra dimension [116–118]. However, the\nInception network manages network resources more efficiently, as well as enhancing the\nlearning ability of the conventional CNN. In the transformation branch, different spatial\nembeddings (employing e.g. 5 × 5 , 3 × 3 , and 1 × 1 ) are used. Thus, customizing each\nlayer is required separately. By contrast, ResNext derives its characteristic features from\nResNet, VGG, and Inception. It employed the VGG deep homogenous topology with\nthe basic architecture of GoogleNet by setting 3 × 3 filters as spatial resolution inside\nthe blocks of split, transform, and merge. Figure 23 shows the ResNext building blocks.\nResNext utilized multi-transformations inside the blocks of split, transform, and merge,\nas well as outlining such transformations in cardinality terms. The performance is significantly improved by increasing the cardinality, as Xie et al. showed. The complexity\nFig. 22  The architecture of DenseNet Network (adopted from [112])\n\nPage 36 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nof ResNext was regulated by employing 1 × 1 filters (low embeddings) ahead of a 3 × 3\nconvolution. By contrast, skipping connections are used for optimized training [115].\nWideResNet\nThe feature reuse problem is the core shortcoming related to deep residual networks,\nsince certain feature blocks or transformations contribute a very small amount to learning. Zagoruyko and Komodakis [119] accordingly proposed WideResNet to address this\nproblem. These authors advised that the depth has a supplemental influence, while the\nresidual units convey the core learning ability of deep residual networks. WideResNet\nutilized the residual block power via making the ResNet wider instead of deeper [37]. It\nenlarged the width by presenting an extra factor, k, which handles the network width.\nIn other words, it indicated that layer widening is a highly successful method of performance enhancement compared to deepening the residual network. While enhanced\nrepresentational capacity is achieved by deep residual networks, these networks also\nhave certain drawbacks, such as the exploding and vanishing gradient problems, feature\nreuse problem (inactivation of several feature maps), and the time-intensive nature of\nthe training. He et al. [37] tackled the feature reuse problem by including a dropout in\neach residual block to regularize the network in an efficient manner. In a similar manner,\nutilizing dropouts, Huang et al. [120] presented the stochastic depth concept to solve the\nslow learning and gradient vanishing problems. Earlier research was focused on increasing the depth; thus, any small enhancement in performance required the addition of\nseveral new layers. When comparing the number of parameters, WideResNet has twice\nthat of ResNet, as an experimental study showed. By contrast, WideResNet presents an\nimproved method for training relative to deep networks [119]. Note that most architectures prior to residual networks (including the highly effective VGG and Inception) were\nwider than ResNet. Thus, wider residual networks were established once this was determined. However, inserting a dropout between the convolutional layers (as opposed to\nwithin the residual block) made the learning more effective in WideResNet [121, 122].\nPyramidal Net\nThe depth of the feature map increases in the succeeding layer due to the deep stacking of multi-convolutional layers, as shown in previous deep CNN architectures such as\nFig. 23  The basic block diagram for the ResNext building blocks\n\nPage 37 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nResNet, VGG, and AlexNet. By contrast, the spatial dimension reduces, since a sub-sampling follows each convolutional layer. Thus, augmented feature representation is recompensed by decreasing the size of the feature map. The extreme expansion in the depth\nof the feature map, alongside the spatial information loss, interferes with the learning\nability in the deep CNNs. ResNet obtained notable outcomes for the issue of image classification. Conversely, deleting a convolutional block—in which both the number of\nchannel and spatial dimensions vary (channel depth enlarges, while spatial dimension\nreduces)—commonly results in decreased classifier performance. Accordingly, the stochastic ResNet enhanced the performance by decreasing the information loss accompanying the residual unit drop. Han et al. [123] proposed Pyramidal Net to address the\nResNet learning interference problem. To address the depth enlargement and extreme\nreduction in spatial width via ResNet, Pyramidal Net slowly enlarges the residual unit\nwidth to cover the most feasible places rather than saving the same spatial dimension\ninside all residual blocks up to the appearance of the down-sampling. It was referred to\nas Pyramidal Net due to the slow enlargement in the feature map depth based on the\nup-down method. Factor l, which was determined by Eq. 19, regulates the depth of the\nfeature map.\nHere, the dimension of the lth residual unit is indicated by dl ; moreover, n indicates the\noverall number of residual units, the step factor is indicated by \u001f , and the depth increase\nis regulated by the factor\nn , which uniformly distributes the weight increase across the\ndimension of the feature map. Zero-padded identity mapping is used to insert the residual connections among the layers. In comparison to the projection-based shortcut connections, zero-padded identity mapping requires fewer parameters, which in turn leads\nto enhanced generalization [124]. Multiplication- and addition-based widening are two\ndifferent approaches used in Pyramidal Nets for network widening. More specifically,\nthe first approach (multiplication) enlarges geometrically, while the second one (addition) enlarges linearly [92]. The main problem associated with the width enlargement is\nthe growth in time and space required related to the quadratic time.\nXception\nExtreme inception architecture is the main characteristic of Xception. The main idea\nbehind Xception is its depthwise separable convolution [125]. The Xception model\nadjusted the original inception block by making it wider and exchanging a single\ndimension ( 3 × 3 ) followed by a 1 × 1 convolution to reduce computational complexity. Figure 24 shows the Xception block architecture. The Xception network becomes\nextra computationally effective through the use of the decoupling channel and spatial\ncorrespondence. Moreover, it first performs mapping of the convolved output to the\nembedding short dimension by applying 1 × 1 convolutions. It then performs k spatial\ntransformations. Note that k here represents the width-defining cardinality, which is\nobtained via the transformations number in Xception. However, the computations were\nmade simpler in Xception by distinctly convolving each channel around the spatial axes.\n(19)\ndl =\n\n16\nif l = 1\n\ndl−1 +\nn\n\nif 2 ≤l ≤n + 1\n\nPage 38 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nThese axes are subsequently used as the 1 × 1 convolutions (pointwise convolution) for\nperforming cross-channel correspondence. The 1 × 1 convolution is utilized in Xception\nto regularize the depth of the channel. The traditional convolutional operation in Xception utilizes a number of transformation segments equivalent to the number of channels;\nInception, moreover, utilizes three transformation segments, while traditional CNN\narchitecture utilizes only a single transformation segment. Conversely, the suggested\nXception transformation approach achieves extra learning efficiency and better performance but does not minimize the number of parameters [126, 127].\nResidual attention neural network\nTo improve the network feature representation, Wang et al. [128] proposed the Residual\nAttention Network (RAN). Enabling the network to learn aware features of the object is\nthe main purpose of incorporating attention into the CNN. The RAN consists of stacked\nresidual blocks in addition to the attention module; hence, it is a feed-forward CNN.\nHowever, the attention module is divided into two branches, namely the mask branch\nand trunk branch. These branches adopt a top-down and bottom-up learning strategy\nrespectively. Encapsulating two different strategies in the attention model supports topdown attention feedback and fast feed-forward processing in only one particular feedforward process. More specifically, the top-down architecture generates dense features\nto make inferences about every aspect. Moreover, the bottom-up feedforward architecture generates low-resolution feature maps in addition to robust semantic information.\nRestricted Boltzmann machines employed a top-down bottom-up strategy as in previously proposed studies [129]. During the training reconstruction phase, Goh et al. [130]\nused the mechanism of top-down attention in deep Boltzmann machines (DBMs) as a\nregularizing factor. Note that the network can be globally optimized using a top-down\nlearning strategy in a similar manner, where the maps progressively output to the input\nthroughout the learning process [129–132].\nIncorporating the attention concept with convolutional blocks in an easy way was used\nby the transformation network, as obtained in a previous study [133]. Unfortunately,\nthese are inflexible, which represents the main problem, along with their inability to be\nFig. 24  The basic block diagram for the Xception block architecture\n\nPage 39 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nused for varying surroundings. By contrast, stacking multi-attention modules has made\nRAN very effective at recognizing noisy, complex, and cluttered images. RAN’s hierarchical organization gives it the capability to adaptively allocate a weight for every feature\nmap depending on its importance within the layers. Furthermore, incorporating three\ndistinct levels of attention (spatial, channel, and mixed) enables the model to use this\nability to capture the object-aware features at these distinct levels.\nConvolutional block attention module\nThe importance of the feature map utilization and the attention mechanism is certified\nvia SE-Network and RAN [128, 134, 135]. The convolutional block attention (CBAM)\nmodule, which is a novel attention-based CNN, was first developed by Woo et al. [136].\nThis module is similar to SE-Network and simple in design. SE-Network disregards the\nobject’s spatial locality in the image and considers only the channels’ contribution during\nthe image classification. Regarding object detection, object spatial location plays a significant role. The convolutional block attention module sequentially infers the attention\nmaps. More specifically, it applies channel attention preceding the spatial attention to\nobtain the refined feature maps. Spatial attention is performed using 1 × 1 convolution\nand pooling functions, as in the literature. Generating an effective feature descriptor can\nbe achieved by using a spatial axis along with the pooling of features. In addition, generating a robust spatial attention map is possible, as CBAM concatenates the max pooling and average pooling operations. In a similar manner, a collection of GAP and max\npooling operations is used to model the feature map statistics. Woo et al. [136] demonstrated that utilizing GAP will return a sub-optimized inference of channel attention,\nwhereas max pooling provides an indication of the distinguishing object features. Thus,\nthe utilization of max pooling and average pooling enhances the network’s representational power. The feature maps improve the representational power, as well as facilitating\na focus on the significant portion of the chosen features. The expression of 3D attention\nmaps through a serial learning procedure assists in decreasing the computational cost\nand the number of parameters, as Woo et al. [136] experimentally proved. Note that any\nCNN architecture can be simply integrated with CBAM.\nConcurrent spatial and channel excitation mechanism\nTo make the work valid for segmentation tasks, Roy et al. [137, 138] expanded Hu et al.\n[134] effort by adding the influence of spatial information to the channel information.\nRoy et al. [137, 138] presented three types of modules: (1) channel squeeze and excitation with concurrent channels (scSE); (2) exciting spatially and squeezing channel-wise\n(sSE); (3) exciting channel-wise and squeezing spatially (cSE). For segmentation purposes, they employed auto-encoder-based CNNs. In addition, they suggested inserting\nmodules following the encoder and decoder layers. To specifically highlight the objectspecific feature maps, they further allocated attention to every channel by expressing a\nscaling factor from the channel and spatial information in the first module (scSE). In the\nsecond module (sSE), the feature map information has lower importance than the spatial\nlocality, as the spatial information plays a significant role during the segmentation process. Therefore, several channel collections are spatially divided and developed so that\n\nPage 40 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nthey can be employed in segmentation. In the final module (cSE), a similar SE-block concept is used. Furthermore, the scaling factor is derived founded on the contribution of\nthe feature maps within the object detection [137, 138].\nCapsuleNet\nCNN is an efficient technique for detecting object features and achieving well-behaved\nrecognition performance in comparison with innovative handcrafted feature detectors.\nA number of restrictions related to CNN are present, meaning that the CNN does not\nconsider certain relations, orientation, size, and perspectives of features. For instance,\nwhen considering a face image, the CNN does not count the various face components\n(such as mouth, eyes, nose, etc.) positions, and will incorrectly activate the CNN neurons and recognize the face without taking specific relations (such as size, orientation\netc.) into account. At this point, consider a neuron that has probability in addition to\nfeature properties such as size, orientation, perspective, etc. A specific neuron/capsule of\nthis type has the ability to effectively detect the face along with different types of information. Thus, many layers of capsule nodes are used to construct the capsule network.\nAn encoding unit, which contains three layers of capsule nodes, forms the CapsuleNet\nor CapsNet (the initial version of the capsule networks).\nFor example, the MNIST architecture comprises 28 × 28 images, applying 256 filters\nof size 9 × 9 and with stride 1. The 28 −9 + 1 = 20 is the output plus 256 feature maps.\nNext, these outputs are input to the first capsule layer, while producing an 8D vector\nrather than a scalar; in fact, this is a modified convolution layer. Note that a stride 2\nwith 9 × 9 filters is employed in the first convolution layer. Thus, the dimension of the\noutput is (20 −9)/2 + 1 = 6 . The initial capsules employ 8 × 32 filters, which generate\n32 × 8 × 6 × 6 (32 for groups, 8 for neurons, while 6 × 6 is the neuron size).\nFigure 25 represents the complete CapsNet encoding and decoding processes. In the\nCNN context, a max-pooling layer is frequently employed to handle the translation\nchange. It can detect the feature moves in the event that the feature is still within the\nmax-pooling window. This approach has the ability to detect the overlapped features;\nFig. 25  The complete CapsNet encoding and decoding processes\n\nPage 41 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nthis is highly significant in detection and segmentation operations, since the capsule\ninvolves the weighted features sum from the preceding layer.\nIn conventional CNNs, a particular cost function is employed to evaluate the global\nerror that grows toward the back throughout the training process. Conversely, in such\ncases, the activation of a neuron will not grow further once the weight between two\nneurons turns out to be zero. Instead of a single size being provided with the complete cost function in repetitive dynamic routing alongside the agreement, the signal\nis directed based on the feature parameters. Sabour et al. [139] provides more details\nabout this architecture. When using MNIST to recognize handwritten digits, this\ninnovative CNN architecture gives superior accuracy. From the application perspective, this architecture has extra suitability for segmentation and detection approaches\nwhen compared with classification approaches [140–142].\nHigh‑resolution network (HRNet)\nHigh-resolution representations are necessary for position-sensitive vision tasks,\nsuch as semantic segmentation, object detection, and human pose estimation. In the\npresent up-to-date frameworks, the input image is encoded as a low-resolution representation using a subnetwork that is constructed as a connected series of high-to-low\nresolution convolutions such as VGGNet and ResNet. The low-resolution representation is then recovered to become a high-resolution one. Alternatively, high-resolution representations are maintained during the entire process using a novel network,\nreferred to as a High-Resolution Network (HRNet) [143, 144]. This network has two\nprincipal features. First, the convolution series of high-to-low resolutions are connected in parallel. Second, the information across the resolutions are repeatedly\nexchanged. The advantage achieved includes getting a representation that is more\naccurate in the spatial domain and extra-rich in the semantic domain. Moreover,\nHRNet has several applications in the fields of object detection, semantic segmentation, and human pose prediction. For computer vision problems, the HRNet represents a more robust backbone. Figure 26 illustrates the general architecture of HRNet.\nChallenges (limitations) of deep learning and alternate solutions\nWhen employing DL, several difficulties are often taken into consideration. Those\nmore challenging are listed next and several possible alternatives are accordingly\nprovided.\nFig. 26  The general architecture of HRNet\n\nPage 42 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nTraining data\nDL is extremely data-hungry considering it also involves representation learning [145,\n146]. DL demands an extensively large amount of data to achieve a well-behaved performance model, i.e. as the data increases, an extra well-behaved performance model\ncan be achieved (Fig. 27). In most cases, the available data are sufficient to obtain a\ngood performance model. However, sometimes there is a shortage of data for using\nDL directly [87]. To properly address this issue, three suggested methods are available. The first involves the employment of the transfer-learning concept after data is\ncollected from similar tasks. Note that while the transferred data will not directly augment the actual data, it will help in terms of both enhancing the original input representation of data and its mapping function [147]. In this way, the model performance\nis boosted. Another technique involves employing a well-trained model from a similar\ntask and fine-tuning the ending of two layers or even one layer based on the limited\noriginal data. Refer to [148, 149] for a review of different transfer-learning techniques\napplied in the DL approach. In the second method, data augmentation is performed\n[150]. This task is very helpful for use in augmenting the image data, since the image\ntranslation, mirroring, and rotation commonly do not change the image label. Conversely, it is important to take care when applying this technique in some cases such\nas with bioinformatics data. For instance, when mirroring an enzyme sequence, the\noutput data may not represent the actual enzyme sequence. In the third method, the\nsimulated data can be considered for increasing the volume of the training set. It is\noccasionally possible to create simulators based on the physical process if the issue is\nwell understood. Therefore, the result will involve the simulation of as much data as\nneeded. Processing the data requirement for DL-based simulation is obtained as an\nexample in Ref. [151].\nTransfer learning\nRecent research has revealed a widespread use of deep CNNs, which offer groundbreaking support for answering many classification problems. Generally speaking,\ndeep CNN models require a sizable volume of data to obtain good performance. The\nFig. 27  The performance of DL regarding the amount of data\n\nPage 43 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\ncommon challenge associated with using such models concerns the lack of training\ndata. Indeed, gathering a large volume of data is an exhausting job, and no successful\nsolution is available at this time. The undersized dataset problem is therefore currently solved using the TL technique [148, 149], which is highly efficient in addressing the lack of training data issue. The mechanism of TL involves training the CNN\nmodel with large volumes of data. In the next step, the model is fine-tuned for training on a small request dataset.\nThe student-teacher relationship is a suitable approach to clarifying TL. Gathering\ndetailed knowledge of the subject is the first step [152]. Next, the teacher provides a\n“course” by conveying the information within a “lecture series” over time. Put simply,\nthe teacher transfers the information to the student. In more detail, the expert (teacher)\ntransfers the knowledge (information) to the learner (student). Similarly, the DL network\nis trained using a vast volume of data, and also learns the bias and the weights during the\ntraining process. These weights are then transferred to different networks for retraining\nor testing a similar novel model. Thus, the novel model is enabled to pre-train weights\nrather than requiring training from scratch. Figure 28 illustrates the conceptual diagram\nof the TL technique.\n1.\t Pre-trained models: Many CNN models, e.g. AlexNet [30], GoogleNet [103], and\nResNet [37], have been trained on large datasets such as ImageNet for image recognition purposes. These models can then be employed to recognize a different task\nwithout the need to train from scratch. Furthermore, the weights remain the same\napart from a few learned features. In cases where data samples are lacking, these\nmodels are very useful. There are many reasons for employing a pre-trained model.\nFirst, training large models on sizeable datasets requires high-priced computational\npower. Second, training large models can be time-consuming, taking up to multiple\nweeks. Finally, a pre-trained model can assist with network generalization and speed\nup the convergence.\nFig. 28  The conceptual diagram of the TL technique\n\nPage 44 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n2.\t A research problem using pre-trained models: Training a DL approach requires a\nmassive number of images. Thus, obtaining good performance is a challenge under\nthese circumstances. Achieving excellent outcomes in image classification or recognition applications, with performance occasionally superior to that of a human,\nbecomes possible through the use of deep convolutional neural networks (DCNNs)\nincluding several layers if a huge amount of data is available [37, 148, 153]. However, avoiding overfitting problems in such applications requires sizable datasets and\nproperly generalizing DCNN models. When training a DCNN model, the dataset\nsize has no lower limit. However, the accuracy of the model becomes insufficient\nin the case of the utilized model has fewer layers, or if a small dataset is used for\ntraining due to over- or under-fitting problems. Due to they have no ability to utilize the hierarchical features of sizable datasets, models with fewer layers have poor\naccuracy. It is difficult to acquire sufficient training data for DL models. For example, in medical imaging and environmental science, gathering labelled datasets is\nvery costly [148]. Moreover, the majority of the crowdsourcing workers are unable to\nmake accurate notes on medical or biological images due to their lack of medical or\nbiological knowledge. Thus, ML researchers often rely on field experts to label such\nimages; however, this process is costly and time consuming. Therefore, producing\nthe large volume of labels required to develop flourishing deep networks turns out\nto be unfeasible. Recently, TL has been widely employed to address the later issue.\nNevertheless, although TL enhances the accuracy of several tasks in the fields of pattern recognition and computer vision [154, 155], there is an essential issue related to\nthe source data type used by the TL as compared to the target dataset. For instance,\nenhancing the medical image classification performance of CNN models is achieved\nby training the models using the ImageNet dataset, which contains natural images\n[153]. However, such natural images are completely dissimilar from the raw medical\nimages, meaning that the model performance is not enhanced. It has further been\nproven that TL from different domains does not significantly affect performance on\nmedical imaging tasks, as lightweight models trained from scratch perform nearly\nas well as standard ImageNet-transferred models [156]. Therefore, there exists scenarios in which using pre-trained models do not become an affordable solution. In\n2020, some researchers have utilized same-domain TL and achieved excellent results\n[86–88, 157]. Same-domain TL is an approach of using images that look similar to\nthe target dataset for training. For example, using X-ray images of different chest diseases to train the model, then fine-tuning and training it on chest X-ray images for\nCOVID-19 diagnosis. More details about same-domain TL and how to implement\nthe fine-tuning process can be found in [87].\nData augmentation techniques\nIf the goal is to increase the amount of available data and avoid the overfitting issue, data\naugmentation techniques are one possible solution [150, 158, 159]. These techniques are\ndata-space solutions for any limited-data problem. Data augmentation incorporates a\ncollection of methods that improve the attributes and size of training datasets. Thus, DL\n\nPage 45 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nnetworks can perform better when these techniques are employed. Next, we list some\ndata augmentation alternate solutions.\n1.\t Flipping: Flipping the vertical axis is a less common practice than flipping the horizontal one. Flipping has been verified as valuable on datasets like ImageNet and\nCIFAR-10. Moreover, it is highly simple to implement. In addition, it is not a labelconserving transformation on datasets that involve text recognition (such as SVHN\nand MNIST).\n2.\t Color space: Encoding digital image data is commonly used as a dimension tensor\n( height × width × colorchannels ). Accomplishing augmentations in the color space\nof the channels is an alternative technique, which is extremely workable for implementation. A very easy color augmentation involves separating a channel of a particular color, such as Red, Green, or Blue. A simple way to rapidly convert an image\nusing a single-color channel is achieved by separating that matrix and inserting additional double zeros from the remaining two color channels. Furthermore, increasing or decreasing the image brightness is achieved by using straightforward matrix\noperations to easily manipulate the RGB values. By deriving a color histogram that\ndescribes the image, additional improved color augmentations can be obtained.\nLighting alterations are also made possible by adjusting the intensity values in histograms similar to those employed in photo-editing applications.\n3.\t Cropping: Cropping a dominant patch of every single image is a technique employed\nwith combined dimensions of height and width as a specific processing step for\nimage data. Furthermore, random cropping may be employed to produce an impact\nsimilar to translations. The difference between translations and random cropping is\nthat translations conserve the spatial dimensions of this image, while random cropping reduces the input size [for example from (256, 256) to (224, 224)]. According to\nthe selected reduction threshold for cropping, the label-preserving transformation\nmay not be addressed.\n4.\t Rotation: When rotating an image left or right from within 0 to 360 degrees around\nthe axis, rotation augmentations are obtained. The rotation degree parameter greatly\ndetermines the suitability of the rotation augmentations. In digit recognition tasks,\nsmall rotations (from 0 to 20 degrees) are very helpful. By contrast, the data label\ncannot be preserved post-transformation when the rotation degree increases.\n5.\t Translation: To avoid positional bias within the image data, a very useful transformation is to shift the image up, down, left, or right. For instance, it is common that the\nwhole dataset images are centered; moreover, the tested dataset should be entirely\nmade up of centered images to test the model. Note that when translating the initial\nimages in a particular direction, the residual space should be filled with Gaussian or\nrandom noise, or a constant value such as 255 s or 0 s. The spatial dimensions of the\nimage post-augmentation are preserved using this padding.\n6.\t Noise injection This approach involves injecting a matrix of arbitrary values. Such\na matrix is commonly obtained from a Gaussian distribution. Moreno-Barea et al.\n[160] employed nine datasets to test the noise injection. These datasets were taken\nfrom the UCI repository [161]. Injecting noise within images enables the CNN to\nlearn additional robust features.\n\nPage 46 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nHowever, highly well-behaved solutions for positional biases available within the\ntraining data are achieved by means of geometric transformations. To separate the\ndistribution of the testing data from the training data, several prospective sources\nof bias exist. For instance, when all faces should be completely centered within the\nframes (as in facial recognition datasets), the problem of positional biases emerges.\nThus, geometric translations are the best solution. Geometric translations are helpful\ndue to their simplicity of implementation, as well as their effective capability to disable the positional biases. Several libraries of image processing are available, which\nenables beginning with simple operations such as rotation or horizontal flipping.\nAdditional training time, higher computational costs, and additional memory are\nsome shortcomings of geometric transformations. Furthermore, a number of geometric transformations (such as arbitrary cropping or translation) should be manually observed to ensure that they do not change the image label. Finally, the biases\nthat separate the test data from the training data are more complicated than transitional and positional changes. Hence, it is not trivial answering to when and where\ngeometric transformations are suitable to be applied.\nImbalanced data\nCommonly, biological data tend to be imbalanced, as negative samples are much more\nnumerous than positive ones [162–164]. For example, compared to COVID-19-positive\nX-ray images, the volume of normal X-ray images is very large. It should be noted that\nundesirable results may be produced when training a DL model using imbalanced data.\nThe following techniques are used to solve this issue. First, it is necessary to employ the\ncorrect criteria for evaluating the loss, as well as the prediction result. In considering\nthe imbalanced data, the model should perform well on small classes as well as larger\nones. Thus, the model should employ area under curve (AUC) as the resultant loss as\nwell as the criteria [165]. Second, it should employ the weighted cross-entropy loss,\nwhich ensures the model will perform well with small classes if it still prefers to employ\nthe cross-entropy loss. Simultaneously, during model training, it is possible either to\ndown-sample the large classes or up-sample the small classes. Finally, to make the data\nbalanced as in Ref. [166], it is possible to construct models for every hierarchical level,\nas a biological system frequently has hierarchical label space. However, the effect of the\nimbalanced data on the performance of the DL model has been comprehensively investigated. In addition, to lessen the problem, the most frequently used techniques were\nalso compared. Nevertheless, note that these techniques are not specified for biological\nproblems.\nInterpretability of data\nOccasionally, DL techniques are analyzed to act as a black box. In fact, they are interpretable. The need for a method of interpreting DL, which is used to obtain the valuable motifs and patterns recognized by the network, is common in many fields, such as\nbioinformatics [167]. In the task of disease diagnosis, it is not only required to know the\ndisease diagnosis or prediction results of a trained DL model, but also how to enhance\nthe surety of the prediction outcomes, as the model makes its decisions based on these\nverifications [168]. To achieve this, it is possible to give a score of importance for every\n\nPage 47 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nportion of the particular example. Within this solution, back-propagation-based techniques or perturbation-based approaches are used [169]. In the perturbation-based\napproaches, a portion of the input is changed and the effect of this change on the model\noutput is observed [170–173]. This concept has high computational complexity, but it is\nsimple to understand. On the other hand, to check the score of the importance of various input portions, the signal from the output propagates back to the input layer in the\nback-propagation-based techniques. These techniques have been proven valuable in\n[174]. In different scenarios, various meanings can represent the model interpretability.\nUncertainty scaling\nCommonly, the final prediction label is not the only label required when employing DL\ntechniques to achieve the prediction; the score of confidence for every inquiry from the\nmodel is also desired. The score of confidence is defined as how confident the model is\nin its prediction [175]. Since the score of confidence prevents belief in unreliable and\nmisleading predictions, it is a significant attribute, regardless of the application scenario.\nIn biology, the confidence score reduces the resources and time expended in proving\nthe outcomes of the misleading prediction. Generally speaking, in healthcare or similar\napplications, the uncertainty scaling is frequently very significant; it helps in evaluating\nautomated clinical decisions and the reliability of machine learning-based disease-diagnosis [176, 177]. Because overconfident prediction can be the output of different DL\nmodels, the score of probability (achieved from the softmax output of the direct-DL) is\noften not in the correct scale [178]. Note that the softmax output requires post-scaling\nto achieve a reliable probability score. For outputting the probability score in the correct scale, several techniques have been introduced, including Bayesian Binning into\nQuantiles (BBQ) [179], isotonic regression [180], histogram binning [181], and the legendary Platt scaling [182]. More specifically, for DL techniques, temperature scaling\nwas recently introduced, which achieves superior performance compared to the other\ntechniques.\nCatastrophic forgetting\nThis is defined as incorporating new information into a plain DL model, made possible\nby interfering with the learned information. For instance, consider a case where there\nare 1000 types of flowers and a model is trained to classify these flowers, after which\na new type of flower is introduced; if the model is fine-tuned only with this new class,\nits performance will become unsuccessful with the older classes [183, 184]. The logical\ndata are continually collected and renewed, which is in fact a highly typical scenario in\nmany fields, e.g. Biology. To address this issue, there is a direct solution that involves\nemploying old and new data to train an entirely new model from scratch. This solution\nis time-consuming and computationally intensive; furthermore, it leads to an unstable\nstate for the learned representation of the initial data. At this time, three different types\nof ML techniques, which have not catastrophic forgetting, are made available to solve\nthe human brain problem founded on the neurophysiological theories [185, 186]. Techniques of the first type are founded on regularizations such as EWC [183] Techniques\nof the second type employ rehearsal training techniques and dynamic neural network\n\nPage 48 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\narchitecture like iCaRL [187, 188]. Finally, techniques of the third type are founded on\ndual-memory learning systems [189]. Refer to [190–192] in order to gain more details.\nModel compression\nTo obtain well-trained models that can still be employed productively, DL models have\nintensive memory and computational requirements due to their huge complexity and\nlarge numbers of parameters [193, 194]. One of the fields that is characterized as dataintensive is the field of healthcare and environmental science. These needs reduce the\ndeployment of DL in limited computational-power machines, mainly in the healthcare\nfield. The numerous methods of assessing human health and the data heterogeneity have\nbecome far more complicated and vastly larger in size [195]; thus, the issue requires\nadditional computation [196]. Furthermore, novel hardware-based parallel processing\nsolutions such as FPGAs and GPUs [197–199] have been developed to solve the computation issues associated with DL. Recently, numerous techniques for compressing\nthe DL models, designed to decrease the computational issues of the models from the\nstarting point, have also been introduced. These techniques can be classified into four\nclasses. In the first class, the redundant parameters (which have no significant impact on\nmodel performance) are reduced. This class, which includes the famous deep compression method, is called parameter pruning [200]. In the second class, the larger model\nuses its distilled knowledge to train a more compact model; thus, it is called knowledge\ndistillation [201, 202]. In the third class, compact convolution filters are used to reduce\nthe number of parameters [203]. In the final class, the information parameters are estimated for preservation using low-rank factorization [204]. For model compression, these\nclasses represent the most representative techniques. In [193], it has been provided a\nmore comprehensive discussion about the topic.\nOverfitting\nDL models have excessively high possibilities of resulting in data overfitting at the training stage due to the vast number of parameters involved, which are correlated in a complex manner. Such situations reduce the model’s ability to achieve good performance on\nthe tested data [90, 205]. This problem is not only limited to a specific field, but involves\ndifferent tasks. Therefore, when proposing DL techniques, this problem should be fully\nconsidered and accurately handled. In DL, the implied bias of the training process\nenables the model to overcome crucial overfitting problems, as recent studies suggest\n[205–208]. Even so, it is still necessary to develop techniques that handle the overfitting problem. An investigation of the available DL algorithms that ease the overfitting\nproblem can categorize them into three classes. The first class acts on both the model\narchitecture and model parameters and includes the most familiar approaches, such as\nweight decay [209], batch normalization [210], and dropout [90]. In DL, the default technique is weight decay [209], which is used extensively in almost all ML algorithms as a\nuniversal regularizer. The second class works on model inputs such as data corruption\nand data augmentation [150, 211]. One reason for the overfitting problem is the lack\nof training data, which makes the learned distribution not mirror the real distribution.\nData augmentation enlarges the training data. By contrast, marginalized data corruption improves the solution exclusive to augmenting the data. The final class works on the\n\nPage 49 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nmodel output. A recently proposed technique penalizes the over-confident outputs for\nregularizing the model [178]. This technique has demonstrated the ability to regularize\nRNNs and CNNs.\nVanishing gradient problem\nIn general, when using backpropagation and gradient-based learning techniques along\nwith ANNs, largely in the training stage, a problem called the vanishing gradient problem arises [212–214]. More specifically, in each training iteration, every weight of the\nneural network is updated based on the current weight and is proportionally relative to\nthe partial derivative of the error function. However, this weight updating may not occur\nin some cases due to a vanishingly small gradient, which in the worst case means that no\nextra training is possible and the neural network will stop completely. Conversely, similarly to other activation functions, the sigmoid function shrinks a large input space to a\ntiny input space. Thus, the derivative of the sigmoid function will be small due to large\nvariation at the input that produces a small variation at the output. In a shallow network,\nonly some layers use these activations, which is not a significant issue. While using more\nlayers will lead the gradient to become very small in the training stage, in this case, the\nnetwork works efficiently. The back-propagation technique is used to determine the gradients of the neural networks. Initially, this technique determines the network derivatives of each layer in the reverse direction, starting from the last layer and progressing\nback to the first layer. The next step involves multiplying the derivatives of each layer\ndown the network in a similar manner to the first step. For instance, multiplying N small\nderivatives together when there are N hidden layers employs an activation function such\nas the sigmoid function. Hence, the gradient declines exponentially while propagating\nback to the first layer. More specifically, the biases and weights of the first layers cannot\nbe updated efficiently during the training stage because the gradient is small. Moreover,\nthis condition decreases the overall network accuracy, as these first layers are frequently\ncritical to recognizing the essential elements of the input data. However, such a problem\ncan be avoided through employing activation functions. These functions lack the squishing property, i.e., the ability to squish the input space to within a small space. By mapping\nX to max, the ReLU [91] is the most popular selection, as it does not yield a small derivative that is employed in the field. Another solution involves employing the batch normalization layer [81]. As mentioned earlier, the problem occurs once a large input space\nis squashed into a small space, leading to vanishing the derivative. Employing batch normalization degrades this issue by simply normalizing the input, i.e., the expression |x|\ndoes not accomplish the exterior boundaries of the sigmoid function. The normalization\nprocess makes the largest part of it come down in the green area, which ensures that the\nderivative is large enough for further actions. Furthermore, faster hardware can tackle\nthe previous issue, e.g. that provided by GPUs. This makes standard back-propagation\npossible for many deeper layers of the network compared to the time required to recognize the vanishing gradient problem [215].\n\nPage 50 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nExploding gradient problem\nOpposite to the vanishing problem is the one related to gradient. Specifically, large error\ngradients are accumulated during back-propagation [216–218]. The latter will lead to\nextremely significant updates to the weights of the network, meaning that the system\nbecomes unsteady. Thus, the model will lose its ability to learn effectively. Grosso modo,\nmoving backward in the network during back-propagation, the gradient grows exponentially by repetitively multiplying gradients. The weight values could thus become incredibly large and may overflow to become a not-a-number (NaN) value. Some potential\nsolutions include:\n1.\t Using different weight regularization techniques.\n2.\t Redesigning the architecture of the network model.\nUnderspecification\nIn 2020, a team of computer scientists at Google has identified a new challenge called\nunderspecification [219]. ML models including DL models often show surprisingly poor\nbehavior when they are tested in real-world applications such as computer vision, medical imaging, natural language processing, and medical genomics. The reason behind the\nweak performance is due to underspecification. It has been shown that small modifications can force a model towards a completely different solution as well as lead to different predictions in deployment domains. There are different techniques of addressing\nunderspecification issue. One of them is to design “stress tests” to examine how good\na model works on real-world data and to find out the possible issues. Nevertheless,\nthis demands a reliable understanding of the process the model can work inaccurately.\nThe team stated that “Designing stress tests that are well-matched to applied requirements, and that provide good “coverage” of potential failure modes is a major challenge”.\nUnderspecification puts major constraints on the credibility of ML predictions and may\nrequire some reconsidering over certain applications. Since ML is linked to human by\nserving several applications such as medical imaging and self-driving cars, it will require\nproper attention to this issue.\nApplications of deep learning\nPresently, various DL applications are widespread around the world. These applications include healthcare, social network analysis, audio and speech processing (like recognition and enhancement), visual data processing methods (such as multimedia data\nanalysis and computer vision), and NLP (translation and sentence classification), among\nothers (Fig. 29) [220–224]. These applications have been classified into five categories:\nclassification, localization, detection, segmentation, and registration. Although each of\nthese tasks has its own target, there is fundamental overlap in the pipeline implementation of these applications as shown in Fig. 30. Classification is a concept that categorizes\na set of data into classes. Detection is used to locate interesting objects in an image with\nconsideration given to the background. In detection, multiple objects, which could be\nfrom dissimilar classes, are surrounded by bounding boxes. Localization is the concept\n\nPage 51 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nused to locate the object, which is surrounded by a single bounding box. In segmentation (semantic segmentation), the target object edges are surrounded by outlines, which\nalso label them; moreover, fitting a single image (which could be 2D or 3D) onto another\nrefers to registration. One of the most important and wide-ranging DL applications are\nin healthcare [225–230]. This area of research is critical due to its relation to human\nlives. Moreover, DL has shown tremendous performance in healthcare. Therefore, we\ntake DL applications in the medical image analysis field as an example to describe the DL\napplications.\nFig. 29  Examples of DL applications\nFig. 30  Workflow of deep learning tasks\n\nPage 52 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nClassification\nComputer-Aided Diagnosis (CADx) is another title sometimes used for classification. Bharati et al. [231] used a chest X-ray dataset for detecting lung diseases based\non a CNN. Another study attempted to read X-ray images by employing CNN [232].\nIn this modality, the comparative accessibility of these images has likely enhanced the\nprogress of DL. [233] used an improved pre-trained GoogLeNet CNN containing more\nthan 150,000 images for training and testing processes. This dataset was augmented\nfrom 1850 chest X-rays. The creators reorganized the image orientation into lateral and\nfrontal views and achieved approximately 100% accuracy. This work of orientation classification has clinically limited use. As a part of an ultimately fully automated diagnosis workflow, it obtained the data augmentation and pre-trained efficiency in learning\nthe metadata of relevant images. Chest infection, commonly referred to as pneumonia, is extremely treatable, as it is a commonly occurring health problem worldwide.\nConversely, Rajpurkar et al. [234] utilized CheXNet, which is an improved version of\nDenseNet [112] with 121 convolution layers, for classifying fourteen types of disease.\nThese authors used the CheXNet14 dataset [235], which comprises 112,000 images. This\nnetwork achieved an excellent performance in recognizing fourteen different diseases.\nIn particular, pneumonia classification accomplished a 0.7632 AUC score using receiver\noperating characteristics (ROC) analysis. In addition, the network obtained better than\nor equal to the performance of both a three-radiologist panel and four individual radiologists. Zuo et al. [236] have adopted CNN for candidate classification in lung nodule.\nShen et al. [237] employed both Random Forest (RF) and SVM classifiers with CNNs to\nclassify lung nodules. They employed two convolutional layers with each of the three\nparallel CNNs. The LIDC-IDRI (Lung Image Database Consortium) dataset, which contained 1010-labeled CT lung scans, was used to classify the two types of lung nodules\n(malignant and benign). Different scales of the image patches were used by every CNN\nto extract features, while the output feature vector was constructed using the learned\nfeatures. Next, these vectors were classified into malignant or benign using either the\nRF classifier or SVM with radial basis function (RBF) filter. The model was robust to\nvarious noisy input levels and achieved an accuracy of 86% in nodule classification.\nConversely, the model of [238] interpolates the image data missing between PET and\nMRI images using 3D CNNs. The Alzheimer Disease Neuroimaging Initiative (ADNI)\ndatabase, containing 830 PET and MRI patient scans, was utilized in their work. The\nPET and MRI images are used to train the 3D CNNs, first as input and then as output.\nFurthermore, for patients who have no PET images, the 3D CNNs utilized the trained\nimages to rebuild the PET images. These rebuilt images approximately fitted the actual\ndisease recognition outcomes. However, this approach did not address the overfitting\nissues, which in turn restricted their technique in terms of its possible capacity for generalization. Diagnosing normal versus Alzheimer’s disease patients has been achieved\nby several CNN models [239, 240]. Hosseini-Asl et al. [241] attained 99% accuracy for\nup-to-date outcomes in diagnosing normal versus Alzheimer’s disease patients. These\nauthors applied an auto-encoder architecture using 3D CNNs. The generic brain features were pre-trained on the CADDementia dataset. Subsequently, the outcomes of\nthese learned features became inputs to higher layers to differentiate between patient\nscans of Alzheimer’s disease, mild cognitive impairment, or normal brains based on the\n\nPage 53 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nADNI dataset and using fine-tuned deep supervision techniques. The architectures of\nVGGNet and RNNs, in that order, were the basis of both VOXCNN and ResNet models\ndeveloped by Korolev et al. [242]. They also discriminated between Alzheimer’s disease\nand normal patients using the ADNI database. Accuracy was 79% for Voxnet and 80%\nfor ResNet. Compared to Hosseini-Asl’s work, both models achieved lower accuracies.\nConversely, the implementation of the algorithms was simpler and did not require feature hand-crafting, as Korolev declared. In 2020, Mehmood et al. [240] trained a developed CNN-based network called “SCNN” with MRI images for the tasks of classification\nof Alzheimer’s disease. They achieved state-of-the-art results by obtaining an accuracy\nof 99.05%.\nRecently, CNN has taken some medical imaging classification tasks to different level\nfrom traditional diagnosis to automated diagnosis with tremendous performance. Examples of these tasks are diabetic foot ulcer (DFU) (as normal and abnormal (DFU) classes)\n[87, 243–246], sickle cells anemia (SCA) (as normal, abnormal (SCA), and other blood\ncomponents) [86, 247], breast cancer by classify hematoxylin–eosin-stained breast\nbiopsy images into four classes: invasive carcinoma, in-situ carcinoma, benign tumor\nand normal tissue [42, 88, 248–252], and multi-class skin cancer classification [253–255].\nIn 2020, CNNs are playing a vital role in early diagnosis of the novel coronavirus\n(COVID-2019). CNN has become the primary tool for automatic COVID-19 diagnosis\nin many hospitals around the world using chest X-ray images [256–260]. More details\nabout the classification of medical imaging applications can be found in [226, 261–265].\nLocalization\nAlthough applications in anatomy education could increase, the practicing clinician is\nmore likely to be interested in the localization of normal anatomy. Radiological images\nare independently examined and described outside of human intervention, while localization could be applied in completely automatic end-to-end applications [266–268].\nZhao et al. [269] introduced a new deep learning-based approach to localize pancreatic\ntumor in projection X-ray images for image-guided radiation therapy without the need\nfor fiducials. Roth et al. [270] constructed and trained a CNN using five convolutional\nlayers to classify around 4000 transverse-axial CT images. These authors used five categories for classification: legs, pelvis, liver, lung, and neck. After data augmentation techniques were applied, they achieved an AUC score of 0.998 and the classification error\nrate of the model was 5.9%. For detecting the positions of the spleen, kidney, heart, and\nliver, Shin et al. [271] employed stacked auto-encoders on 78 contrast-improved MRI\nscans of the stomach area containing the kidneys or liver. Temporal and spatial domains\nwere used to learn the hierarchal features. Based on the organs, these approaches\nachieved detection accuracies of 62–79%. Sirazitdinov et al. [268] presented an aggregate of two convolutional neural networks, namely RetinaNet and Mask R-CNN for\npneumonia detection and localization.\nDetection\nComputer-Aided Detection (CADe) is another method used for detection. For both the\nclinician and the patient, overlooking a lesion on a scan may have dire consequences.\n\nPage 54 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nThus, detection is a field of study requiring both accuracy and sensitivity [272–274].\nChouhan et al. [275] introduced an innovative deep learning framework for the detection of pneumonia by adopting the idea of transfer learning. Their approach obtained\nan accuracy of 96.4% with a recall of 99.62% on unseen data. In the area of COVID19 and pulmonary disease, several convolutional neural network approaches have been\nproposed for automatic detection from X-ray images which showed an excellent performance [46, 276–279].\nIn the area of skin cancer, there several applications were introduced for the detection\ntask [280–282]. Thurnhofer-Hemsi et al. [283] introduced a deep learning approach for\nskin cancer detection by fine-tuning five state-of-art convolutional neural network models. They addressed the issue of a lack of training data by adopting the ideas of transfer\nlearning and data augmentation techniques. DenseNet201 network has shown superior\nresults compared to other models.\nAnother interesting area is that of histopathological images, which are progressively\ndigitized. Several papers have been published in this field [284–290]. Human pathologists read these images laboriously; they search for malignancy markers, such as a high\nindex of cell proliferation, using molecular markers (e.g. Ki-67), cellular necrosis signs,\nabnormal cellular architecture, enlarged numbers of mitotic figures denoting augmented\ncell replication, and enlarged nucleus-to-cytoplasm ratios. Note that the histopathological slide may contain a huge number of cells (up to the thousands). Thus, the risk of\ndisregarding abnormal neoplastic regions is high when wading through these cells at\nexcessive levels of magnification. Ciresan et al. [291] employed CNNs of 11–13 layers\nfor identifying mitotic figures. Fifty breast histology images from the MITOS dataset\nwere used. Their technique attained recall and precision scores of 0.7 and 0.88 respectively. Sirinukunwattana et al. [292] utilized 100 histology images of colorectal adenocarcinoma to detect cell nuclei using CNNs. Roughly 30,000 nuclei were hand-labeled for\ntraining purposes. The novelty of this approach was in the use of Spatially Constrained\nCNN. This CNN detects the center of nuclei using the surrounding spatial context and\nspatial regression. Instead of this CNN, Xu et al. [293] employed a stacked sparse autoencoder (SSAE) to identify nuclei in histological slides of breast cancer, achieving 0.83\nand 0.89 recall and precision scores respectively. In this field, they showed that unsupervised learning techniques are also effectively utilized. In medical images, Albarquoni\net  al. [294] investigated the problem of insufficient labeling. They crowd-sourced the\nactual mitoses labeling in the histology images of breast cancer (from amateurs online).\nSolving the recurrent issue of inadequate labeling during the analysis of medical images\ncan be achieved by feeding the crowd-sourced input labels into the CNN. This method\nsignifies a remarkable proof-of-concept effort. In 2020, Lei et al. [285] introduced the\nemployment of deep convolutional neural networks for automatic identification of\nmitotic candidates from histological sections for mitosis screening. They obtained the\nstate-of-the-art detection results on the dataset of the International Pattern Recognition\nConference (ICPR) 2012 Mitosis Detection Competition.\nSegmentation\nAlthough MRI and CT image segmentation research includes different organs such\nas knee cartilage, prostate, and liver, most research work has concentrated on brain\n\nPage 55 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nsegmentation, particularly tumors [295–300]. This issue is highly significant in surgical\npreparation to obtain the precise tumor limits for the shortest surgical resection. During surgery, excessive sacrificing of key brain regions may lead to neurological shortfalls\nincluding cognitive damage, emotionlessness, and limb difficulty. Conventionally, medical anatomical segmentation was done by hand; more specifically, the clinician draws\nout lines within the complete stack of the CT or MRI volume slice by slice. Thus, it is\nperfect for implementing a solution that computerizes this painstaking work. Wadhwa\net  al. [301] presented a brief overview on brain tumor segmentation of MRI images.\nAkkus et al. [302] wrote a brilliant review of brain MRI segmentation that addressed\nthe different metrics and CNN architectures employed. Moreover, they explain several\ncompetitions in detail, as well as their datasets, which included Ischemic Stroke Lesion\nSegmentation (ISLES), Mild Traumatic brain injury Outcome Prediction (MTOP), and\nBrain Tumor Segmentation (BRATS).\nChen et  al. [299] proposed convolutional neural networks for precise brain tumor\nsegmentation. The approach that they employed involves several approaches for better\nfeatures learning including the DeepMedic model, a novel dual-force training scheme,\na label distribution-based loss function, and Multi-Layer Perceptron-based post-processing. They conducted their method on the two most modern brain tumor segmentation datasets, i.e., BRATS 2017 and BRATS 2015 datasets. Hu et al. [300] introduced the\nbrain tumor segmentation method by adopting a multi-cascaded convolutional neural\nnetwork (MCCNN) and fully connected conditional random fields (CRFs). The achieved\nresults were excellent compared with the state-of-the-art methods.\nMoeskops et al. [303] employed three parallel-running CNNs, each of which had a 2D\ninput patch of dissimilar size, for segmenting and classifying MRI brain images. These\nimages, which include 35 adults and 22 pre-term infants, were classified into various tissue categories such as cerebrospinal fluid, grey matter, and white matter. Every patch\nconcentrates on capturing various image aspects with the benefit of employing three\ndissimilar sizes of input patch; here, the bigger sizes incorporated the spatial features,\nwhile the lowest patch sizes concentrated on the local textures. In general, the algorithm\nhas Dice coefficients in the range of 0.82–0.87 and achieved a satisfactory accuracy.\nAlthough 2D image slices are employed in the majority of segmentation research, Milletrate et al. [304] implemented 3D CNN for segmenting MRI prostate images. Furthermore, they used the PROMISE2012 challenge dataset, from which fifty MRI scans were\nused for training and thirty for testing. The U-Net architecture of Ronnerberger et al.\n[305] inspired their V-net. This model attained a 0.869 Dice coefficient score, the same\nas the winning teams in the competition. To reduce overfitting and create the model of\na deeper 11-convolutional layer CNN, Pereira et al. [306] applied intentionally smallsized filters of 3x3. Their model used MRI scans of 274 gliomas (a type of brain tumor)\nfor training. They achieved first place in the 2013 BRATS challenge, as well as second\nplace in the BRATS challenge 2015. Havaei et al. [307] also considered gliomas using\nthe 2013 BRATS dataset. They investigated different 2D CNN architectures. Compared\nto the winner of BRATS 2013, their algorithm worked better, as it required only 3 min\nto execute rather than 100 min. The concept of cascaded architecture formed the basis\nof their model. Thus, it is referred to as an InputCascadeCNN. Employing FC Conditional Random Fields (CRFs), atrous spatial pyramid pooling, and up-sampled filters\n\nPage 56 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nwere techniques introduced by Chen et al. [308]. These authors aimed to enhance the\naccuracy of localization and enlarge the field of view of every filter at a multi-scale. Their\nmodel, DeepLab, attained 79.7% mIOU (mean Intersection Over Union). In the PASCAL VOC-2012 image segmentation, their model obtained an excellent performance.\nRecently, the Automatic segmentation of COVID-19 Lung Infection from CT Images\nhelps to detect the development of COVID-19 infection by employing several deep\nlearning techniques [309–312].\nRegistration\nUsually, given two input images, the four main stages of the canonical procedure of the\nimage registration task are [313, 314]:\n•\t Target Selection: it illustrates the determined input image that the second counterpart input image needs to remain accurately superimposed to.\n•\t Feature Extraction: it computes the set of features extracted from each input image.\n•\t Feature Matching: it allows finding similarities between the previously obtained features.\n•\t Pose Optimization: it is aimed to minimize the distance between both input images.\nThen, the result of the registration procedure is the suitable geometric transformation\n(e.g. translation, rotation, scaling, etc.) that provides both input images within the same\ncoordinate system in a way the distance between them is minimal, i.e. their level of\nsuperimposition/overlapping is optimal. It is out of the scope of this work to provide an\nextensive review of this topic. Nevertheless, a short summary is accordingly introduced\nnext.\nCommonly, the input images for the DL-based registration approach could be in various forms, e.g. point clouds, voxel grids, and meshes. Additionally, some techniques\nallow as inputs the result of the Feature Extraction or Matching steps in the canonical\nscheme. Specifically, the outcome could be some data in a particular form as well as the\nresult of the steps from the classical pipeline (feature vector, matching vector, and transformation). Nevertheless, with the newest DL-based methods, a novel conceptual type\nof ecosystem issues. It contains acquired characteristics about the target, materials, and\ntheir behavior that can be registered with the input data. Such a conceptual ecosystem is\nformed by a neural network and its training manner, and it could be counted as an input\nto the registration approach. Nevertheless, it is not an input that one might adopt in\nevery registration situation since it corresponds to an interior data representation.\nFrom a DL view-point, the interpretation of the conceptual design enables differentiating the input data of a registration approach into defined or non-defined models. In\nparticular, the illustrated phases are models that depict particular spatial data (e.g. 2D or\n3D) while a non-defined one is a generalization of a data set created by a learning system.\nYumer et al. [315] developed a framework in which the model acquires characteristics of\nobjects, meaning ready to identify what a more sporty car seems like or a more comfy\nchair is, also adjusting a 3D model to fit those characteristics while maintaining the main\ncharacteristics of the primary data. Likewise, a fundamental perspective of the unsupervised learning method introduced by Ding et al. [316] is that there is no target for the\n\nPage 57 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nregistration approach. In this instance, the network is able of placing each input point\ncloud in a global space, solving SLAM issues in which many point clouds have to be registered rigidly. On the other hand, Mahadevan [317] proposed the combination of two\nconceptual models utilizing the growth of Imagination Machines to give flexible artificial intelligence systems and relationships between the learned phases through training\nschemes that are not inspired on labels and classifications. Another practical application\nof DL, especially CNNs, to image registration is the 3D reconstruction of objects. Wang\net al. [318] applied an adversarial way using CNNs to rebuild a 3D model of an object\nfrom its 2D image. The network learns many objects and orally accomplishes the registration between the image and the conceptual model. Similarly, Hermoza et al. [319] also\nutilize the GAN network for prognosticating the absent geometry of damaged archaeological objects, providing the reconstructed object based on a voxel grid format and a\nlabel selecting its class.\nDL for medical image registration has numerous applications, which were listed by\nsome review papers [320–322]. Yang et al. [323] implemented stacked convolutional layers as an encoder-decoder approach to predict the morphing of the input pixel into its\nlast formation using MRI brain scans from the OASIS dataset. They employed a registration model known as Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nand attained remarkable enhancements in computation time. Miao et al. [324] used synthetic X-ray images to train a five-layer CNN to register 3D models of a trans-esophageal\nprobe, a hand implant, and a knee implant onto 2D X-ray images for pose estimation.\nThey determined that their model achieved an execution time of 0.1 s, representing\nan important enhancement against the conventional registration techniques based on\nintensity; moreover, it achieved effective registrations 79–99% of the time. Li et al. [325]\nintroduced a neural network-based approach for the non-rigid 2D–3D registration of\nthe lateral cephalogram and the volumetric cone-beam CT (CBCT) images.\nComputational approaches\nFor computationally exhaustive applications, complex ML and DL approaches have rapidly been identified as the most significant techniques and are widely used in different\nfields. The development and enhancement of algorithms aggregated with capabilities of\nwell-behaved computational performance and large datasets make it possible to effectively execute several applications, as earlier applications were either not possible or difficult to take into consideration.\nCurrently, several standard DNN configurations are available. The interconnection\npatterns between layers and the total number of layers represent the main differences\nbetween these configurations. The Table 2 illustrates the growth rate of the overall number of layers over time, which seems to be far faster than the “Moore’s Law growth rate”.\nIn normal DNN, the number of layers grew by around 2.3× each year in the period from\n2012 to 2016. Recent investigations of future ResNet versions reveal that the number of\nlayers can be extended up to 1000. However, an SGD technique is employed to fit the\nweights (or parameters), while different optimization techniques are employed to obtain\nparameter updating during the DNN training process. Repetitive updates are required to\nenhance network accuracy in addition to a minorly augmented rate of enhancement. For\nexample, the training process using ImageNet as a large dataset, which contains more\n\nPage 58 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nthan 14 million images, along with ResNet as a network model, take around 30K to 40K\nrepetitions to converge to a steady solution. In addition, the overall computational load,\nas an upper-level prediction, may exceed 1020 FLOPS when both the training set size\nand the DNN complexity increase.\nPrior to 2008, boosting the training to a satisfactory extent was achieved by using\nGPUs. Usually, days or weeks are needed for a training session, even with GPU support. By contrast, several optimization strategies were developed to reduce the extensive\nlearning time. The computational requirements are believed to increase as the DNNs\ncontinuously enlarge in both complexity and size.\nIn addition to the computational load cost, the memory bandwidth and capacity have\na significant effect on the entire training performance, and to a lesser extent, deduction.\nMore specifically, the parameters are distributed through every layer of the input data,\nthere is a sizeable amount of reused data, and the computation of several network layers\nexhibits an excessive computation-to-bandwidth ratio. By contrast, there are no distributed parameters, the amount of reused data is extremely small, and the additional FC\nlayers have an extremely small computation-to-bandwidth ratio. Table 3 presents a comparison between different aspects related to the devices. In addition, the table is established to facilitate familiarity with the tradeoffs by obtaining the optimal approach for\nconfiguring a system based on either FPGA, GPU, or CPU devices. It should be noted\nthat each has corresponding weaknesses and strengths; accordingly, there are no clear\none-size-fits-all solutions.\nAlthough GPU processing has enhanced the ability to address the computational\nchallenges related to such networks, the maximum GPU (or CPU) performance is not\nachieved, and several techniques or models have turned out to be strongly linked to\nbandwidth. In the worst cases, the GPU efficiency is between 15 and 20% of the maximum theoretical performance. This issue is required to enlarge the memory bandwidth\nusing high-bandwidth stacked memory. Next, different approaches based on FPGA,\nGPU, and CPU are accordingly detailed.\nTable 3  A comparison between different aspects related to the devices\nFeature\nAssessment\nLeader\nDevelopment\nCPU is the easiest to program, then GPU, then FPGA\nCPU\nSize\nBoth FPGA and CPU have smaller volume solutions due to their lower\npower consumption\nFPGA-CPU\nCustomization\nBroader flexibility is provided by FPGA\nFPGA\nEase of change\nEasier way to vary application functionality is provided by GPU and CPU\nGPU-CPU\nBackward compatibility Transferring RTL to novel FPGA requires additional work. Furthermore, GPU\nhas less stable architecture than CPU\nCPU\nInterfaces\nSeveral varieties of interfaces can be implemented using FPGA\nFPGA\nProcessing/$\nFPGA configurability assists utilization in wider acceleration space. Due to\nthe considerable processing abilities, GPU wins\nFPGA-GPU\nProcessing/watt\nCustomized designs can be optimized\nFPGA\nTiming latency\nImplemented FPGA algorithm offers deterministic timing, which is in turn\nmuch faster than GPU\nFPGA\nLarge data analysis\nFPGA performs well for inline processing, while CPU supports storage\ncapabilities and largest memory\nFPGA-CPU\nDCNN inference\nFPGA has lower latency and can be customized\nFPGA\nDCNN training\nGreater float-point capabilities provided by GPU\nGPU\n\nPage 59 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\nCPU‑based approach\nThe well-behaved performance of the CPU nodes usually assists robust network connectivity, storage abilities, and large memory. Although CPU nodes are more commonpurpose than those of FPGA or GPU, they lack the ability to match them in unprocessed\ncomputation facilities, since this requires increased network ability and a larger memory\ncapacity.\nGPU‑based approach\nGPUs are extremely effective for several basic DL primitives, which include greatly\nparallel-computing operations such as activation functions, matrix multiplication, and\nconvolutions [326–330]. Incorporating HBM-stacked memory into the up-to-date\nGPU models significantly enhances the bandwidth. This enhancement allows numerous\nprimitives to efficiently utilize all computational resources of the available GPUs. The\nimprovement in GPU performance over CPU performance is usually 10-20:1 related to\ndense linear algebra operations.\nMaximizing parallel processing is the base of the initial GPU programming model. For\nexample, a GPU model may involve up to sixty-four computational units. There are four\nSIMD engines per each computational layer, and each SIMD has sixteen floating-point\ncomputation lanes. The peak performance is 25 TFLOPS (fp16) and 10 TFLOPS (fp32)\nas the percentage of the employment approaches 100%. Additional GPU performance\nmay be achieved if the addition and multiply functions for vectors combine the inner\nproduction instructions for matching primitives related to matrix operations.\nFor DNN training, the GPU is usually considered to be an optimized design, while for\ninference operations, it may also offer considerable performance improvements.\nFPGA‑based approach\nFPGA is wildly utilized in various tasks including deep learning [199, 247, 331–334].\nInference accelerators are commonly implemented utilizing FPGA. The FPGA can be\neffectively configured to reduce the unnecessary or overhead functions involved in GPU\nsystems. Compared to GPU, the FPGA is restricted to both weak-behaved floating-point\nperformance and integer inference. The main FPGA aspect is the capability to dynamically reconfigure the array characteristics (at run-time), as well as the capability to configure the array by means of effective design with little or no overhead.\nAs mentioned earlier, the FPGA offers both performance and latency for every watt it\ngains over GPU and CPU in DL inference operations. Implementation of custom highperformance hardware, pruned networks, and reduced arithmetic precision are three\nfactors that enable the FPGA to implement DL algorithms and to achieve FPGA with\nthis level of efficiency. In addition, FPGA may be employed to implement CNN overlay engines with over 80% efficiency, eight-bit accuracy, and over 15 TOPs peak performance; this is used for a few conventional CNNs, as Xillinx and partners demonstrated\nrecently. By contrast, pruning techniques are mostly employed in the LSTM context. The\nsizes of the models can be efficiently minimized by up to 20×, which provides an important benefit during the implementation of the optimal solution, as MLP neural processing demonstrated. A recent study in the field of implementing fixed-point precision and\n\nPage 60 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\ncustom floating-point has revealed that lowering the 8-bit is extremely promising; moreover, it aids in supplying additional advancements to implementing peak performance\nFPGA related to the DNN models.\nEvaluation metrics\nEvaluation metrics adopted within DL tasks play a crucial role in achieving the optimized classifier [335]. They are utilized within a usual data classification procedure\nthrough two main stages: training and testing. It is utilized to optimize the classification\nalgorithm during the training stage. This means that the evaluation metric is utilized to\ndiscriminate and select the optimized solution, e.g., as a discriminator, which can generate an extra-accurate forecast of upcoming evaluations related to a specific classifier.\nFor the time being, the evaluation metric is utilized to measure the efficiency of the created classifier, e.g. as an evaluator, within the model testing stage using hidden data. As\ngiven in Eq. 20, TN and TP are defined as the number of negative and positive instances,\nrespectively, which are successfully classified. In addition, FN and FP are defined as the\nnumber of misclassified positive and negative instances respectively. Next, some of the\nmost well-known evaluation metrics are listed below.\n1.\t Accuracy: Calculates the ratio of correct predicted classes to the total number of\nsamples evaluated (Eq. 20).\n2.\t Sensitivity or Recall: Utilized to calculate the fraction of positive patterns that are\ncorrectly classified (Eq. 21).\n3.\t Specificity: Utilized to calculate the fraction of negative patterns that are correctly\nclassified (Eq. 22).\n4.\t Precision: Utilized to calculate the positive patterns that are correctly predicted by all\npredicted patterns in a positive class (Eq. 23).\n5.\t F1-Score: Calculates the harmonic average between recall and precision rates\n(Eq. 24).\n6.\t J Score: This metric is also called Youdens J statistic. Eq. 25 represents the metric.\n(20)\nAccuracy =\nTP + TN\nTP + TN + FP + FN\n(21)\nSensitivity =\nTP\nTP + FN\n(22)\nSpeciﬁcity =\nTN\nFP + TN\n(23)\nPrecision =\nTP\nTP + FP\n(24)\nF1score = 2 × Precision × Recall\nPrecision + Recall\n\nPage 61 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n7.\t False Positive Rate (FPR): This metric refers to the possibility of a false alarm ratio as\ncalculated in Eq. 26\n8.\t Area Under the ROC Curve: AUC is a common ranking type metric. It is utilized to\nconduct comparisons between learning algorithms [336–338], as well as to construct\nan optimal learning model [339, 340]. In contrast to probability and threshold metrics, the AUC value exposes the entire classifier ranking performance. The following\nformula is used to calculate the AUC value for two-class problem [341] (Eq. 27)\nHere, Sp represents the sum of all positive ranked samples. The number of negative\nand positive samples is denoted as nn and np , respectively. Compared to the accuracy metrics, the AUC value was verified empirically and theoretically, making it\nvery helpful for identifying an optimized solution and evaluating the classifier performance through classification training.\n\nWhen considering the discrimination and evaluation processes, the AUC performance was brilliant. However, for multiclass issues, the AUC computation is primarily cost-effective when discriminating a large number of created solutions. In addition, the time complexity for computing the AUC is O\n\n|C|2 n log n\n\nwith respect to\nthe Hand and Till AUC model [341] and O\n\n|C| n log n\n\naccording to Provost and\nDomingo’s AUC model [336].\nFrameworks and datasets\nSeveral DL frameworks and datasets have been developed in the last few years. various frameworks and libraries have also been used in order to expedite the work with\ngood results. Through their use, the training process has become easier. Table 4 lists\nthe most utilized frameworks and libraries.\nBased on the star ratings on Github, as well as our own background in the field,\nTensorFlow is deemed the most effective and easy to use. It has the ability to work on\nseveral platforms. (Github is one of the biggest software hosting sites, while Github\nstars refer to how well-regarded a project is on the site). Moreover, there are several\nother benchmark datasets employed for different DL tasks. Some of these are listed in\nTable 5.\nSummary and conclusion\nFinally, it is mandatory the inclusion of a brief discussion by gathering all the relevant\ndata provided along this extensive research. Next, an itemized analysis is presented in\norder to conclude our review and exhibit the future directions.\n(25)\nJscore = Sensitivity + Speciﬁcity −1\n(26)\nFPR = 1 −Speciﬁcity\n(27)\nAUC = Sp −np(nn + 1)/2\nnpnn\n\nPage 62 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n•\t DL already experiences difficulties in simultaneously modeling multi-complex\nmodalities of data. In recent DL developments, another common approach is that of\nmultimodal DL.\n•\t DL requires sizeable datasets (labeled data preferred) to predict unseen data and to\ntrain the models. This challenge turns out to be particularly difficult when real-time\ndata processing is required or when the provided datasets are limited (such as in the\nTable 5  Benchmark datasets\nDataset\nNum. of classes Applications\nLink to dataset\nImageNet\n1000\nImage classification, object\nlocalization, object detection,\netc.\nhttp://​www.​image-​net.​org/\nCIFAR10/100\n10/100\nImage classification\nhttps://​www.​cs.​toron​to.​edu/​~kriz/​\ncifar.​html\nMNIST\n10\nClassification of handwritten\ndigits\nhttp://​yann.​lecun.​com/​exdb/​\nmnist/\nPascal VOC\n20\nImage classification, segmenta‑\ntion, object detection\nhttp://​host.​robots.​ox.​ac.​uk/​pascal/​\nVOC/​voc20​12/\nMicrosoft COCO\n80\nObject detection, semantic\nsegmentation\nhttps://​cocod​ataset.​org/#​home\nYFCC100M\n8M\nVideo and image understanding\nhttp://​proje​cts.​dfki.​unikl.​de/​yfcc1​\n00m/\nYouTube-8M\n4716\nVideo classification\nhttps://​resea​rch.​google.​com/​\nyoutu​be8m/\nUCF-101\n101\nHuman action detection\nhttps://​www.​crcv.​ucf.​edu/​data/​\nUCF101.​php\nKinetics\n400\nHuman action detection\nhttps://​deepm​ind.​com/​resea​rch/​\nopen-​source/​kinet​ics\nGoogle Open Images\n350\nImage classification, segmenta‑\ntion, object detection\nhttps://​stora​ge.​googl​eapis.​com/​\nopeni​mages/​web/​index.​html\nCalTech101\n101\nClassification\nhttp://​www.​vision.​calte​ch.​edu/​\nImage_​Datas​ets/​Calte​ch101/\nLabeled Faces in the Wild –\nFace recognition\nhttp://​vis-​www.​cs.​umass.​edu/​lfw/\nMIT-67 scene dataset\n67\nIndoor scene recognition\nhttp://​web.​mit.​edu/​torra​lba/​\nwww/​indoor.​htm\nTable 4  List of the most common frameworks and libraries\nFramework\nLicense\nCore language\nYear of release\nHomepages\nTensorFlow\nApache 2.0\nC++ & Python\n2015\nhttps://​www.​tenso​rflow.​org/\nKeras\nMIT\nPython\n2015\nhttps://​keras.​io/\nCaffe\nBSD\nC++\n2015\nhttp://​caffe.​berke​leyvi​sion.​org/\nMatConvNet\nOxford\nMATLAB\n2014\nhttp://​www.​vlfeat.​org/​matco​nvnet/\nMXNet\nApache 2.0\nC++\n2015\nhttps://​github.​com/​dmlc/​mxnet\nCNTK\nMIT\nC++\n2016\nhttps://​github.​com/​Micro​soft/​CNTK\nTheano\nBSD\nPython\n2008\nhttp://​deepl​earni​ng.​net/​softw​are/​theano/\nTorch\nBSD\nC & Lua\n2002\nhttp://​torch.​ch/\nDL4j\nApache 2.0\nJava\n2014\nhttps://​deepl​earni​ng4j.​org/\nGluon\nAWS Microsoft\nC++\n2017\nhttps://​github.​com/​gluon-​api/​gluon-​api/\nOpenDeep\nMIT\nPython\n2017\nhttp://​www.​opend​eep.​org/\n\nPage 63 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\ncase of healthcare data). To alleviate this issue, TL and data augmentation have been\nresearched over the last few years.\n•\t Although ML slowly transitions to semi-supervised and unsupervised learning to\nmanage practical data without the need for manual human labeling, many of the current deep-learning models utilize supervised learning.\n•\t The CNN performance is greatly influenced by hyper-parameter selection. Any small\nchange in the hyper-parameter values will affect the general CNN performance.\nTherefore, careful parameter selection is an extremely significant issue that should be\nconsidered during optimization scheme development.\n•\t Impressive and robust hardware resources like GPUs are required for effective CNN\ntraining. Moreover, they are also required for exploring the efficiency of using CNN\nin smart and embedded systems.\n•\t In the CNN context, ensemble learning [342, 343] represents a prospective research\narea. The collection of different and multiple architectures will support the model\nin improving its generalizability across different image categories through extracting\nseveral levels of semantic image representation. Similarly, ideas such as new activation functions, dropout, and batch normalization also merit further investigation.\n•\t The exploitation of depth and different structural adaptations is significantly\nimproved in the CNN learning capacity. Substituting the traditional layer configuration with blocks results in significant advances in CNN performance, as has been\nshown in the recent literature. Currently, developing novel and efficient block architectures is the main trend in new research models of CNN architectures. HRNet is\nonly one example that shows there are always ways to improve the architecture.\n•\t It is expected that cloud-based platforms will play an essential role in the future\ndevelopment of computational DL applications. Utilizing cloud computing offers\na solution to handling the enormous amount of data. It also helps to increase efficiency and reduce costs. Furthermore, it offers the flexibility to train DL architectures.\n•\t With the recent development in computational tools including a chip for neural networks and a mobile GPU, we will see more DL applications on mobile devices. It will\nbe easier for users to use DL.\n•\t Regarding the issue of lack of training data, It is expected that various techniques of\ntransfer learning will be considered such as training the DL model on large unlabeled\nimage datasets and next transferring the knowledge to train the DL model on a small\nnumber of labeled images for the same task.\n•\t Last, this overview provides a starting point for the community of DL being interested in the field of DL. Furthermore, researchers would be allowed to decide the\nmore suitable direction of work to be taken in order to provide more accurate alternatives to the field.\nAcknowledgements\nWe would like to thank the professors from the Queensland University of Technology and the University of Information\nTechnology and Communications who gave their feedback on the paper.\nAuthors’ contributions\nConceptualization: LA, and JZ; methodology: LA, JZ, and JS; software: LA, and MAF; validation: LA, JZ, MA, and LF; formal\nanalysis: LA, JZ, YD, and JS; investigation: LA, and JZ; resources: LA, JZ, and MAF; data curation: LA, and OA.; writing–origi‑\nnal draft preparation: LA, and OA; writing—review and editing: LA, JZ, AJH, AA, YD, OA, JS, MAF, MA, and LF; visualization:\n\nPage 64 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\nLA, and MAF; supervision: JZ, and YD; project administration: JZ, YD, and JS; funding acquisition: LA, AJH, AA, and YD. All\nauthors read and approved the final manuscript.\nFunding\nThis research received no external funding.\nAvailability of data and materials\nNot applicable.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 School of Computer Science, Queensland University of Technology, Brisbane, QLD 4000, Australia. 2 Control and Sys‑\ntems Engineering Department, University of Technology, Baghdad 10001, Iraq. 3 Electrical Engineering Technical College,\nMiddle Technical University, Baghdad 10001, Iraq. 4 Faculty of Electrical Engineering & Computer Science, University\nof Missouri, Columbia, MO 65211, USA. 5 AlNidhal Campus, University of Information Technology & Communications,\nBaghdad 10001, Iraq. 6 Department of Computer Science, University of Jaén, 23071 Jaén, Spain. 7 College of Computer\nScience and Information Technology, University of Sumer, Thi Qar 64005, Iraq. 8 School of Engineering, Manchester\nMetropolitan University, Manchester M1 5GD, UK.\nReceived: 21 January 2021  Accepted: 22 March 2021\nReferences\n\n1.\t Rozenwald MB, Galitsyna AA, Sapunov GV, Khrameeva EE, Gelfand MS. A machine learning framework for the\nprediction of chromatin folding in Drosophila using epigenetic features. PeerJ Comput Sci. 2020;6:307.\n\n2.\t Amrit C, Paauw T, Aly R, Lavric M. Identifying child abuse through text mining and machine learning. Expert Syst\nAppl. 2017;88:402–18.\n\n3.\t Hossain E, Khan I, Un-Noor F, Sikander SS, Sunny MSH. Application of big data and machine learning in smart grid,\nand associated security concerns: a review. IEEE Access. 2019;7:13960–88.\n\n4.\t Crawford M, Khoshgoftaar TM, Prusa JD, Richter AN, Al Najada H. Survey of review spam detection using machine\nlearning techniques. J Big Data. 2015;2(1):23.\n\n5.\t Deldjoo Y, Elahi M, Cremonesi P, Garzotto F, Piazzolla P, Quadrana M. Content-based video recommendation\nsystem based on stylistic visual features. J Data Semant. 2016;5(2):99–113.\n\n6.\t Al-Dulaimi K, Chandran V, Nguyen K, Banks J, Tomeo-Reyes I. Benchmarking hep-2 specimen cells classifica‑\ntion using linear discriminant analysis on higher order spectra features of cell shape. Pattern Recogn Lett.\n2019;125:534–41.\n\n7.\t Liu W, Wang Z, Liu X, Zeng N, Liu Y, Alsaadi FE. A survey of deep neural network architectures and their applica‑\ntions. Neurocomputing. 2017;234:11–26.\n\n8.\t Pouyanfar S, Sadiq S, Yan Y, Tian H, Tao Y, Reyes MP, Shyu ML, Chen SC, Iyengar S. A survey on deep learning: algo‑\nrithms, techniques, and applications. ACM Comput Surv (CSUR). 2018;51(5):1–36.\n\n9.\t Alom MZ, Taha TM, Yakopcic C, Westberg S, Sidike P, Nasrin MS, Hasan M, Van Essen BC, Awwal AA, Asari VK. A\nstate-of-the-art survey on deep learning theory and architectures. Electronics. 2019;8(3):292.\n10.\t Potok TE, Schuman C, Young S, Patton R, Spedalieri F, Liu J, Yao KT, Rose G, Chakma G. A study of complex deep\nlearning networks on high-performance, neuromorphic, and quantum computers. ACM J Emerg Technol Comput\nSyst (JETC). 2018;14(2):1–21.\n11.\t Adeel A, Gogate M, Hussain A. Contextual deep learning-based audio-visual switching for speech enhancement\nin real-world environments. Inf Fusion. 2020;59:163–70.\n12.\t Tian H, Chen SC, Shyu ML. Evolutionary programming based deep learning feature selection and network con‑\nstruction for visual data classification. Inf Syst Front. 2020;22(5):1053–66.\n13.\t Young T, Hazarika D, Poria S, Cambria E. Recent trends in deep learning based natural language processing. IEEE\nComput Intell Mag. 2018;13(3):55–75.\n14.\t Koppe G, Meyer-Lindenberg A, Durstewitz D. Deep learning for small and big data in psychiatry. Neuropsychop‑\nharmacology. 2021;46(1):176–90.\n15.\t Dalal N, Triggs B. Histograms of oriented gradients for human detection. In: 2005 IEEE computer society confer‑\nence on computer vision and pattern recognition (CVPR’05), vol. 1. IEEE; 2005. p. 886–93.\n16.\t Lowe DG. Object recognition from local scale-invariant features. In: Proceedings of the seventh IEEE international\nconference on computer vision, vol. 2. IEEE; 1999. p. 1150–7.\n17.\t Wu L, Hoi SC, Yu N. Semantics-preserving bag-of-words models and applications. IEEE Trans Image Process.\n2010;19(7):1908–20.\n\nPage 65 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n18.\t LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436–44.\n19.\t Yao G, Lei T, Zhong J. A review of convolutional-neural-network-based action recognition. Pattern Recogn Lett.\n2019;118:14–22.\n20.\t Dhillon A, Verma GK. Convolutional neural network: a review of models, methodologies and applications to object\ndetection. Prog Artif Intell. 2020;9(2):85–112.\n21.\t Khan A, Sohail A, Zahoora U, Qureshi AS. A survey of the recent architectures of deep convolutional neural net‑\nworks. Artif Intell Rev. 2020;53(8):5455–516.\n22.\t Hasan RI, Yusuf SM, Alzubaidi L. Review of the state of the art of deep learning for plant diseases: a broad analysis\nand discussion. Plants. 2020;9(10):1302.\n23.\t Xiao Y, Tian Z, Yu J, Zhang Y, Liu S, Du S, Lan X. A review of object detection based on deep learning. Multimed\nTools Appl. 2020;79(33):23729–91.\n24.\t Ker J, Wang L, Rao J, Lim T. Deep learning applications in medical image analysis. IEEE Access. 2017;6:9375–89.\n25.\t Zhang Z, Cui P, Zhu W. Deep learning on graphs: a survey. IEEE Trans Knowl Data Eng. 2020. https://​doi.​org/​10.​\n1109/​TKDE.​2020.​29813​33.\n26.\t Shrestha A, Mahmood A. Review of deep learning algorithms and architectures. IEEE Access. 2019;7:53040–65.\n27.\t Najafabadi MM, Villanustre F, Khoshgoftaar TM, Seliya N, Wald R, Muharemagic E. Deep learning applications and\nchallenges in big data analytics. J Big Data. 2015;2(1):1.\n28.\t Goodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning, vol. 1. Cambridge: MIT press; 2016.\n29.\t Shorten C, Khoshgoftaar TM, Furht B. Deep learning applications for COVID-19. J Big Data. 2021;8(1):1–54.\n30.\t Krizhevsky A, Sutskever I, Hinton GE. Imagenet classification with deep convolutional neural networks. Commun\nACM. 2017;60(6):84–90.\n31.\t Bhowmick S, Nagarajaiah S, Veeraraghavan A. Vision and deep learning-based algorithms to detect and quantify\ncracks on concrete surfaces from uav videos. Sensors. 2020;20(21):6299.\n32.\t Goh GB, Hodas NO, Vishnu A. Deep learning for computational chemistry. J Comput Chem. 2017;38(16):1291–307.\n33.\t Li Y, Zhang T, Sun S, Gao X. Accelerating flash calculation through deep learning methods. J Comput Phys.\n2019;394:153–65.\n34.\t Yang W, Zhang X, Tian Y, Wang W, Xue JH, Liao Q. Deep learning for single image super-resolution: a brief review.\nIEEE Trans Multimed. 2019;21(12):3106–21.\n35.\t Tang J, Li S, Liu P. A review of lane detection methods based on deep learning. Pattern Recogn. 2020;111:107623.\n36.\t Zhao ZQ, Zheng P, Xu ST, Wu X. Object detection with deep learning: a review. IEEE Trans Neural Netw Learn Syst.\n2019;30(11):3212–32.\n37.\t He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference\non computer vision and pattern recognition; 2016. p. 770–8.\n38.\t Ng A. Machine learning yearning: technical strategy for AI engineers in the era of deep learning. 2019. https://​\nwww.​mlyea​rning.​org.\n39.\t Metz C. Turing award won by 3 pioneers in artificial intelligence. The New York Times. 2019;27.\n40.\t Nevo S, Anisimov V, Elidan G, El-Yaniv R, Giencke P, Gigi Y, Hassidim A, Moshe Z, Schlesinger M, Shalev G, et al. Ml\nfor flood forecasting at scale; 2019. arXiv preprint arXiv:​1901.​09583.\n41.\t Chen H, Engkvist O, Wang Y, Olivecrona M, Blaschke T. The rise of deep learning in drug discovery. Drug Discov\nToday. 2018;23(6):1241–50.\n42.\t Benhammou Y, Achchab B, Herrera F, Tabik S. Breakhis based breast cancer automatic diagnosis using deep learn‑\ning: taxonomy, survey and insights. Neurocomputing. 2020;375:9–24.\n43.\t Wulczyn E, Steiner DF, Xu Z, Sadhwani A, Wang H, Flament-Auvigne I, Mermel CH, Chen PHC, Liu Y, Stumpe MC.\nDeep learning-based survival prediction for multiple cancer types using histopathology images. PLoS ONE.\n2020;15(6):e0233678.\n44.\t Nagpal K, Foote D, Liu Y, Chen PHC, Wulczyn E, Tan F, Olson N, Smith JL, Mohtashamian A, Wren JH, et al. Develop‑\nment and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer. NPJ Digit\nMed. 2019;2(1):1–10.\n45.\t Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Dermatologist-level classification of skin cancer\nwith deep neural networks. Nature. 2017;542(7639):115–8.\n46.\t Brunese L, Mercaldo F, Reginelli A, Santone A. Explainable deep learning for pulmonary disease and coronavirus\nCOVID-19 detection from X-rays. Comput Methods Programs Biomed. 2020;196(105):608.\n47.\t Jamshidi M, Lalbakhsh A, Talla J, Peroutka Z, Hadjilooei F, Lalbakhsh P, Jamshidi M, La Spada L, Mirmozafari M,\nDehghani M, et al. Artificial intelligence and COVID-19: deep learning approaches for diagnosis and treatment.\nIEEE Access. 2020;8:109581–95.\n48.\t Shorfuzzaman M, Hossain MS. Metacovid: a siamese neural network framework with contrastive loss for n-shot\ndiagnosis of COVID-19 patients. Pattern Recogn. 2020;113:107700.\n49.\t Carvelli L, Olesen AN, Brink-Kjær A, Leary EB, Peppard PE, Mignot E, Sørensen HB, Jennum P. Design of a deep\nlearning model for automatic scoring of periodic and non-periodic leg movements during sleep validated against\nmultiple human experts. Sleep Med. 2020;69:109–19.\n50.\t De Fauw J, Ledsam JR, Romera-Paredes B, Nikolov S, Tomasev N, Blackwell S, Askham H, Glorot X, O’Donoghue\nB, Visentin D, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat Med.\n2018;24(9):1342–50.\n51.\t Topol EJ. High-performance medicine: the convergence of human and artificial intelligence. Nat Med.\n2019;25(1):44–56.\n52.\t Kermany DS, Goldbaum M, Cai W, Valentim CC, Liang H, Baxter SL, McKeown A, Yang G, Wu X, Yan F, et al. Identify‑\ning medical diagnoses and treatable diseases by image-based deep learning. Cell. 2018;172(5):1122–31.\n53.\t Van Essen B, Kim H, Pearce R, Boakye K, Chen B. Lbann: livermore big artificial neural network HPC toolkit. In:\nProceedings of the workshop on machine learning in high-performance computing environments; 2015. p. 1–6.\n54.\t Saeed MM, Al Aghbari Z, Alsharidah M. Big data clustering techniques based on spark: a literature review. PeerJ\nComput Sci. 2020;6:321.\n\nPage 66 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n55.\t Mnih V, Kavukcuoglu K, Silver D, Rusu AA, Veness J, Bellemare MG, Graves A, Riedmiller M, Fidjeland AK, Ostrovski\nG, et al. Human-level control through deep reinforcement learning. Nature. 2015;518(7540):529–33.\n56.\t Arulkumaran K, Deisenroth MP, Brundage M, Bharath AA. Deep reinforcement learning: a brief survey. IEEE Signal\nProcess Mag. 2017;34(6):26–38.\n57.\t Socher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng AY, Potts C. Recursive deep models for semantic compo‑\nsitionality over a sentiment treebank. In: Proceedings of the 2013 conference on empirical methods in natural\nlanguage processing; 2013. p. 1631–42.\n58.\t Goller C, Kuchler A. Learning task-dependent distributed representations by backpropagation through structure.\nIn: Proceedings of international conference on neural networks (ICNN’96), vol 1. IEEE; 1996. p. 347–52.\n59.\t Socher R, Lin CCY, Ng AY, Manning CD. Parsing natural scenes and natural language with recursive neural net‑\nworks. In: ICML; 2011.\n60.\t Louppe G, Cho K, Becot C, Cranmer K. QCD-aware recursive neural networks for jet physics. J High Energy Phys.\n2019;2019(1):57.\n61.\t Sadr H, Pedram MM, Teshnehlab M. A robust sentiment analysis method based on sequential combination of\nconvolutional and recursive neural networks. Neural Process Lett. 2019;50(3):2745–61.\n62.\t Urban G, Subrahmanya N, Baldi P. Inner and outer recursive neural networks for chemoinformatics applications. J\nChem Inf Model. 2018;58(2):207–11.\n63.\t Hewamalage H, Bergmeir C, Bandara K. Recurrent neural networks for time series forecasting: current status and\nfuture directions. Int J Forecast. 2020;37(1):388–427.\n64.\t Jiang Y, Kim H, Asnani H, Kannan S, Oh S, Viswanath P. Learn codes: inventing low-latency codes via recurrent\nneural networks. IEEE J Sel Areas Inf Theory. 2020;1(1):207–16.\n65.\t John RA, Acharya J, Zhu C, Surendran A, Bose SK, Chaturvedi A, Tiwari N, Gao Y, He Y, Zhang KK, et al. Optogenetics\ninspired transition metal dichalcogenide neuristors for in-memory deep recurrent neural networks. Nat Commun.\n2020;11(1):1–9.\n66.\t Batur Dinler Ö, Aydin N. An optimal feature parameter set based on gated recurrent unit recurrent neural net‑\nworks for speech segment detection. Appl Sci. 2020;10(4):1273.\n67.\t Jagannatha AN, Yu H. Structured prediction models for RNN based sequence labeling in clinical text. In: Proceed‑\nings of the conference on empirical methods in natural language processing. conference on empirical methods\nin natural language processing, vol. 2016, NIH Public Access; 2016. p. 856.\n68.\t Pascanu R, Gulcehre C, Cho K, Bengio Y. How to construct deep recurrent neural networks. In: Proceedings of the\nsecond international conference on learning representations (ICLR 2014); 2014.\n69.\t Glorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. In: Proceedings of\nthe thirteenth international conference on artificial intelligence and statistics; 2010. p. 249–56.\n70.\t Gao C, Yan J, Zhou S, Varshney PK, Liu H. Long short-term memory-based deep recurrent neural networks for\ntarget tracking. Inf Sci. 2019;502:279–96.\n71.\t Zhou DX. Theory of deep convolutional neural networks: downsampling. Neural Netw. 2020;124:319–27.\n72.\t Jhong SY, Tseng PY, Siriphockpirom N, Hsia CH, Huang MS, Hua KL, Chen YY. An automated biometric identifica‑\ntion system using CNN-based palm vein recognition. In: 2020 international conference on advanced robotics and\nintelligent systems (ARIS). IEEE; 2020. p. 1–6.\n73.\t Al-Azzawi A, Ouadou A, Max H, Duan Y, Tanner JJ, Cheng J. Deepcryopicker: fully automated deep neural network\nfor single protein particle picking in cryo-EM. BMC Bioinform. 2020;21(1):1–38.\n74.\t Wang T, Lu C, Yang M, Hong F, Liu C. A hybrid method for heartbeat classification via convolutional neural net‑\nworks, multilayer perceptrons and focal loss. PeerJ Comput Sci. 2020;6:324.\n75.\t Li G, Zhang M, Li J, Lv F, Tong G. Efficient densely connected convolutional neural networks. Pattern Recogn.\n2021;109:107610.\n76.\t Gu J, Wang Z, Kuen J, Ma L, Shahroudy A, Shuai B, Liu T, Wang X, Wang G, Cai J, et al. Recent advances in convolu‑\ntional neural networks. Pattern Recogn. 2018;77:354–77.\n77.\t Fang W, Love PE, Luo H, Ding L. Computer vision for behaviour-based safety in construction: a review and future\ndirections. Adv Eng Inform. 2020;43:100980.\n78.\t Palaz D, Magimai-Doss M, Collobert R. End-to-end acoustic modeling using convolutional neural networks for\nhmm-based automatic speech recognition. Speech Commun. 2019;108:15–32.\n79.\t Li HC, Deng ZY, Chiang HH. Lightweight and resource-constrained learning network for face recognition with\nperformance optimization. Sensors. 2020;20(21):6114.\n80.\t Hubel DH, Wiesel TN. Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex. J\nPhysiol. 1962;160(1):106.\n81.\t Ioffe S, Szegedy C. Batch normalization: accelerating deep network training by reducing internal covariate shift;\n2015. arXiv preprint arXiv:​1502.​03167.\n82.\t Ruder S. An overview of gradient descent optimization algorithms; 2016. arXiv preprint arXiv:​1609.​04747.\n83.\t Bottou L. Large-scale machine learning with stochastic gradient descent. In: Proceedings of COMPSTAT’2010.\nSpringer; 2010. p. 177–86.\n84.\t Hinton G, Srivastava N, Swersky K. Neural networks for machine learning lecture 6a overview of mini-batch gradi‑\nent descent. Cited on. 2012;14(8).\n85.\t Zhang Z. Improved Adam optimizer for deep neural networks. In: 2018 IEEE/ACM 26th international symposium\non quality of service (IWQoS). IEEE; 2018. p. 1–2.\n86.\t Alzubaidi L, Fadhel MA, Al-Shamma O, Zhang J, Duan Y. Deep learning models for classification of red blood cells\nin microscopy images to aid in sickle cell anemia diagnosis. Electronics. 2020;9(3):427.\n87.\t Alzubaidi L, Fadhel MA, Al-Shamma O, Zhang J, Santamaría J, Duan Y, Oleiwi SR. Towards a better understanding of\ntransfer learning for medical imaging: a case study. Appl Sci. 2020;10(13):4523.\n88.\t Alzubaidi L, Al-Shamma O, Fadhel MA, Farhan L, Zhang J, Duan Y. Optimizing the performance of breast cancer\nclassification by employing the same domain transfer learning from hybrid deep convolutional neural network\nmodel. Electronics. 2020;9(3):445.\n\nPage 67 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n89.\t LeCun Y, Jackel LD, Bottou L, Cortes C, Denker JS, Drucker H, Guyon I, Muller UA, Sackinger E, Simard P, et al. Learn‑\ning algorithms for classification: a comparison on handwritten digit recognition. Neural Netw Stat Mech Perspect.\n1995;261:276.\n90.\t Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: a simple way to prevent neural net‑\nworks from overfitting. J Mach Learn Res. 2014;15(1):1929–58.\n91.\t Dahl GE, Sainath TN, Hinton GE. Improving deep neural networks for LVCSR using rectified linear units and drop‑\nout. In: 2013 IEEE international conference on acoustics, speech and signal processing. IEEE; 2013. p. 8609–13.\n92.\t Xu B, Wang N, Chen T, Li M. Empirical evaluation of rectified activations in convolutional network; 2015. arXiv\npreprint arXiv:​1505.​00853.\n93.\t Hochreiter S. The vanishing gradient problem during learning recurrent neural nets and problem solutions. Int J\nUncertain Fuzziness Knowl Based Syst. 1998;6(02):107–16.\n94.\t Lin M, Chen Q, Yan S. Network in network; 2013. arXiv preprint arXiv:​1312.​4400.\n95.\t Hsiao TY, Chang YC, Chou HH, Chiu CT. Filter-based deep-compression with global average pooling for convolu‑\ntional networks. J Syst Arch. 2019;95:9–18.\n96.\t Li Z, Wang SH, Fan RR, Cao G, Zhang YD, Guo T. Teeth category classification via seven-layer deep convolutional\nneural network with max pooling and global average pooling. Int J Imaging Syst Technol. 2019;29(4):577–83.\n97.\t Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. In: European conference on computer\nvision. Springer; 2014. p. 818–33.\n98.\t Erhan D, Bengio Y, Courville A, Vincent P. Visualizing higher-layer features of a deep network. Univ Montreal.\n2009;1341(3):1.\n99.\t Le QV. Building high-level features using large scale unsupervised learning. In: 2013 IEEE international conference\non acoustics, speech and signal processing. IEEE; 2013. p. 8595–8.\n100.\t Grün F, Rupprecht C, Navab N, Tombari F. A taxonomy and library for visualizing learned features in convolutional\nneural networks; 2016. arXiv preprint arXiv:​1606.​07757.\n101.\t Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition; 2014. arXiv pre‑\nprint arXiv:​1409.​1556.\n102.\t Ranzato M, Huang FJ, Boureau YL, LeCun Y. Unsupervised learning of invariant feature hierarchies with applications\nto object recognition. In: 2007 IEEE conference on computer vision and pattern recognition. IEEE; 2007. p. 1–8.\n103.\t Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V, Rabinovich A. Going deeper with\nconvolutions. In: Proceedings of the IEEE conference on computer vision and pattern recognition; 2015. p. 1–9.\n104.\t Bengio Y, et al. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization; 2015. arXiv:​1502.​\n04390corr abs/1502.04390\n105.\t Srivastava RK, Greff K, Schmidhuber J. Highway networks; 2015. arXiv preprint arXiv:​1505.​00387.\n106.\t Kong W, Dong ZY, Jia Y, Hill DJ, Xu Y, Zhang Y. Short-term residential load forecasting based on LSTM recurrent\nneural network. IEEE Trans Smart Grid. 2017;10(1):841–51.\n107.\t Ordóñez FJ, Roggen D. Deep convolutional and LSTM recurrent neural networks for multimodal wearable activity\nrecognition. Sensors. 2016;16(1):115.\n108.\t CireşAn D, Meier U, Masci J, Schmidhuber J. Multi-column deep neural network for traffic sign classification. Neural\nNetw. 2012;32:333–8.\n109.\t Szegedy C, Ioffe S, Vanhoucke V, Alemi A. Inception-v4, inception-resnet and the impact of residual connections\non learning; 2016. arXiv preprint arXiv:​1602.​07261.\n110.\t Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inception architecture for computer vision. In:\nProceedings of the IEEE conference on computer vision and pattern recognition; 2016. p. 2818–26.\n111.\t Wu S, Zhong S, Liu Y. Deep residual learning for image steganalysis. Multimed Tools Appl. 2018;77(9):10437–53.\n112.\t Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connected convolutional networks. In: Proceedings of\nthe IEEE conference on computer vision and pattern recognition; 2017. p. 4700–08.\n113.\t Rubin J, Parvaneh S, Rahman A, Conroy B, Babaeizadeh S. Densely connected convolutional networks for detec‑\ntion of atrial fibrillation from short single-lead ECG recordings. J Electrocardiol. 2018;51(6):S18-21.\n114.\t Kuang P, Ma T, Chen Z, Li F. Image super-resolution with densely connected convolutional networks. Appl Intell.\n2019;49(1):125–36.\n115.\t Xie S, Girshick R, Dollár P, Tu Z, He K. Aggregated residual transformations for deep neural networks. In: Proceed‑\nings of the IEEE conference on computer vision and pattern recognition; 2017. p. 1492–500.\n116.\t Su A, He X, Zhao X. Jpeg steganalysis based on ResNeXt with gauss partial derivative filters. Multimed Tools Appl.\n2020;80(3):3349–66.\n117.\t Yadav D, Jalal A, Garlapati D, Hossain K, Goyal A, Pant G. Deep learning-based ResNeXt model in phycological stud‑\nies for future. Algal Res. 2020;50:102018.\n118.\t Han W, Feng R, Wang L, Gao L. Adaptive spatial-scale-aware deep convolutional neural network for high-resolu‑\ntion remote sensing imagery scene classification. In: IGARSS 2018-2018 IEEE international geoscience and remote\nsensing symposium. IEEE; 2018. p. 4736–9.\n119.\t Zagoruyko S, Komodakis N. Wide residual networks; 2016. arXiv preprint arXiv:​1605.​07146.\n120.\t Huang G, Sun Y, Liu Z, Sedra D, Weinberger KQ. Deep networks with stochastic depth. In: European conference on\ncomputer vision. Springer; 2016. p. 646–61.\n121.\t Huynh HT, Nguyen H. Joint age estimation and gender classification of Asian faces using wide ResNet. SN Comput\nSci. 2020;1(5):1–9.\n122.\t Takahashi R, Matsubara T, Uehara K. Data augmentation using random image cropping and patching for deep\ncnns. IEEE Trans Circuits Syst Video Technol. 2019;30(9):2917–31.\n123.\t Han D, Kim J, Kim J. Deep pyramidal residual networks. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition; 2017. p. 5927–35.\n124.\t Wang Y, Wang L, Wang H, Li P. End-to-end image super-resolution via deep and shallow convolutional networks.\nIEEE Access. 2019;7:31959–70.\n\nPage 68 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n125.\t Chollet F. Xception: Deep learning with depthwise separable convolutions. In: Proceedings of the IEEE conference\non computer vision and pattern recognition; 2017. p. 1251–8.\n126.\t Lo WW, Yang X, Wang Y. An xception convolutional neural network for malware classification with transfer learn‑\ning. In: 2019 10th IFIP international conference on new technologies, mobility and security (NTMS). IEEE; 2019. p.\n1–5.\n127.\t Rahimzadeh M, Attar A. A modified deep convolutional neural network for detecting COVID-19 and pneumo‑\nnia from chest X-ray images based on the concatenation of xception and resnet50v2. Inform Med Unlocked.\n2020;19:100360.\n128.\t Wang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X. Residual attention network for image classification.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 3156–64.\n129.\t Salakhutdinov R, Larochelle H. Efficient learning of deep boltzmann machines. In: Proceedings of the thirteenth\ninternational conference on artificial intelligence and statistics; 2010. p. 693–700.\n130.\t Goh H, Thome N, Cord M, Lim JH. Top-down regularization of deep belief networks. Adv Neural Inf Process Syst.\n2013;26:1878–86.\n131.\t Guan J, Lai R, Xiong A, Liu Z, Gu L. Fixed pattern noise reduction for infrared images based on cascade residual\nattention CNN. Neurocomputing. 2020;377:301–13.\n132.\t Bi Q, Qin K, Zhang H, Li Z, Xu K. RADC-Net: a residual attention based convolution network for aerial scene clas‑\nsification. Neurocomputing. 2020;377:345–59.\n133.\t Jaderberg M, Simonyan K, Zisserman A, et al. Spatial transformer networks. In: Advances in neural information\nprocessing systems. San Mateo: Morgan Kaufmann Publishers; 2015. p. 2017–25.\n134.\t Hu J, Shen L, Sun G. Squeeze-and-excitation networks. In: Proceedings of the IEEE conference on computer vision\nand pattern recognition; 2018. p. 7132–41.\n135.\t Mou L, Zhu XX. Learning to pay attention on spectral domain: a spectral attention module-based convolutional\nnetwork for hyperspectral image classification. IEEE Trans Geosci Remote Sens. 2019;58(1):110–22.\n136.\t Woo S, Park J, Lee JY, So Kweon I. CBAM: Convolutional block attention module. In: Proceedings of the European\nconference on computer vision (ECCV); 2018. p. 3–19.\n137.\t Roy AG, Navab N, Wachinger C. Concurrent spatial and channel ‘squeeze & excitation’ in fully convolutional net‑\nworks. In: International conference on medical image computing and computer-assisted intervention. Springer;\n2018. p. 421–9.\n138.\t Roy AG, Navab N, Wachinger C. Recalibrating fully convolutional networks with spatial and channel “squeeze and\nexcitation’’ blocks. IEEE Trans Med Imaging. 2018;38(2):540–9.\n139.\t Sabour S, Frosst N, Hinton GE. Dynamic routing between capsules. In: Advances in neural information processing\nsystems. San Mateo: Morgan Kaufmann Publishers; 2017. p. 3856–66.\n140.\t Arun P, Buddhiraju KM, Porwal A. Capsulenet-based spatial-spectral classifier for hyperspectral images. IEEE J Sel\nTopics Appl Earth Obs Remote Sens. 2019;12(6):1849–65.\n141.\t Xinwei L, Lianghao X, Yi Y. Compact video fingerprinting via an improved capsule net. Syst Sci Control Eng.\n2020;9:1–9.\n142.\t Ma B, Li X, Xia Y, Zhang Y. Autonomous deep learning: a genetic DCNN designer for image classification. Neuro‑\ncomputing. 2020;379:152–61.\n143.\t Wang J, Sun K, Cheng T, Jiang B, Deng C, Zhao Y, Liu D, Mu Y, Tan M, Wang X, et al. Deep high-resolution repre‑\nsentation learning for visual recognition. IEEE Trans Pattern Anal Mach Intell. 2020. https://​doi.​org/​10.​1109/​TPAMI.​\n2020.​29836​86.\n144.\t Cheng B, Xiao B, Wang J, Shi H, Huang TS, Zhang L. Higherhrnet: scale-aware representation learning for bottomup human pose estimation. In: CVPR 2020; 2020. https://​www.​micro​soft.​com/​en-​us/​resea​rch/​publi​cation/​highe​\nrhrnet-​scale-​aware-​repre​senta​tion-​learn​ing-​for-​bottom-​up-​human-​pose-​estim​ation/.\n145.\t Karimi H, Derr T, Tang J. Characterizing the decision boundary of deep neural networks; 2019. arXiv preprint arXiv:​\n1912.​11460.\n146.\t Li Y, Ding L, Gao X. On the decision boundary of deep neural networks; 2018. arXiv preprint arXiv:​1808.​05385.\n147.\t Yosinski J, Clune J, Bengio Y, Lipson H. How transferable are features in deep neural networks? In: Advances in\nneural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2014. p. 3320–8.\n148.\t Tan C, Sun F, Kong T, Zhang W, Yang C, Liu C. A survey on deep transfer learning. In: International conference on\nartificial neural networks. Springer; 2018. p. 270–9.\n149.\t Weiss K, Khoshgoftaar TM, Wang D. A survey of transfer learning. J Big Data. 2016;3(1):9.\n150.\t Shorten C, Khoshgoftaar TM. A survey on image data augmentation for deep learning. J Big Data. 2019;6(1):60.\n151.\t Wang F, Wang H, Wang H, Li G, Situ G. Learning from simulation: an end-to-end deep-learning approach for com‑\nputational ghost imaging. Opt Express. 2019;27(18):25560–72.\n152.\t Pan W. A survey of transfer learning for collaborative recommendation with auxiliary data. Neurocomputing.\n2016;177:447–53.\n153.\t Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L. Imagenet: a large-scale hierarchical image database. In: 2009 IEEE\nconference on computer vision and pattern recognition. IEEE; 2009. p. 248–55.\n154.\t Cook D, Feuz KD, Krishnan NC. Transfer learning for activity recognition: a survey. Knowl Inf Syst. 2013;36(3):537–56.\n155.\t Cao X, Wang Z, Yan P, Li X. Transfer learning for pedestrian detection. Neurocomputing. 2013;100:51–7.\n156.\t Raghu M, Zhang C, Kleinberg J, Bengio S. Transfusion: understanding transfer learning for medical imaging. In:\nAdvances in neural information processing systems. San Mateo: Morgan Kaufmann Publishers; 2019. p. 3347–57.\n157.\t Pham TN, Van Tran L, Dao SVT. Early disease classification of mango leaves using feed-forward neural network and\nhybrid metaheuristic feature selection. IEEE Access. 2020;8:189960–73.\n158.\t Saleh AM, Hamoud T. Analysis and best parameters selection for person recognition based on gait model using\nCNN algorithm and image augmentation. J Big Data. 2021;8(1):1–20.\n159.\t Hirahara D, Takaya E, Takahara T, Ueda T. Effects of data count and image scaling on deep learning training. PeerJ\nComput Sci. 2020;6:312.\n\nPage 69 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n160.\t Moreno-Barea FJ, Strazzera F, Jerez JM, Urda D, Franco L. Forward noise adjustment scheme for data augmenta‑\ntion. In: 2018 IEEE symposium series on computational intelligence (SSCI). IEEE; 2018. p. 728–34.\n161.\t Dua D, Karra Taniskidou E. Uci machine learning repository. Irvine: University of california. School of Information\nand Computer Science; 2017. http://​archi​ve.​ics.​uci.​edu/​ml\n162.\t Johnson JM, Khoshgoftaar TM. Survey on deep learning with class imbalance. J Big Data. 2019;6(1):27.\n163.\t Yang P, Zhang Z, Zhou BB, Zomaya AY. Sample subset optimization for classifying imbalanced biological data. In:\nPacific-Asia conference on knowledge discovery and data mining. Springer; 2011. p. 333–44.\n164.\t Yang P, Yoo PD, Fernando J, Zhou BB, Zhang Z, Zomaya AY. Sample subset optimization techniques for imbalanced\nand ensemble learning problems in bioinformatics applications. IEEE Trans Cybern. 2013;44(3):445–55.\n165.\t Wang S, Sun S, Xu J. Auc-maximized deep convolutional neural fields for sequence labeling 2015. arXiv preprint\narXiv:​1511.​05265.\n166.\t Li Y, Wang S, Umarov R, Xie B, Fan M, Li L, Gao X. Deepre: sequence-based enzyme EC number prediction by deep\nlearning. Bioinformatics. 2018;34(5):760–9.\n167.\t Li Y, Huang C, Ding L, Li Z, Pan Y, Gao X. Deep learning in bioinformatics: introduction, application, and perspective\nin the big data era. Methods. 2019;166:4–21.\n168.\t Choi E, Bahadori MT, Sun J, Kulas J, Schuetz A, Stewart W. Retain: An interpretable predictive model for healthcare\nusing reverse time attention mechanism. In: Advances in neural information processing systems. San Mateo:\nMorgan Kaufmann Publishers; 2016. p. 3504–12.\n169.\t Ching T, Himmelstein DS, Beaulieu-Jones BK, Kalinin AA, Do BT, Way GP, Ferrero E, Agapow PM, Zietz M, Hoff‑\nman MM, et al. Opportunities and obstacles for deep learning in biology and medicine. J R Soc Interface.\n2018;15(141):20170,387.\n170.\t Zhou J, Troyanskaya OG. Predicting effects of noncoding variants with deep learning-based sequence model. Nat\nMethods. 2015;12(10):931–4.\n171.\t Pokuri BSS, Ghosal S, Kokate A, Sarkar S, Ganapathysubramanian B. Interpretable deep learning for guided\nmicrostructure-property explorations in photovoltaics. NPJ Comput Mater. 2019;5(1):1–11.\n172.\t Ribeiro MT, Singh S, Guestrin C. “Why should I trust you?” explaining the predictions of any classifier. In: Proceed‑\nings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining; 2016. p.\n1135–44.\n173.\t Wang L, Nie R, Yu Z, Xin R, Zheng C, Zhang Z, Zhang J, Cai J. An interpretable deep-learning architecture of\ncapsule networks for identifying cell-type gene expression programs from single-cell RNA-sequencing data. Nat\nMach Intell. 2020;2(11):1–11.\n174.\t Sundararajan M, Taly A, Yan Q. Axiomatic attribution for deep networks; 2017. arXiv preprint arXiv:​1703.​01365.\n175.\t Platt J, et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood meth‑\nods. Adv Large Margin Classif. 1999;10(3):61–74.\n176.\t Nair T, Precup D, Arnold DL, Arbel T. Exploring uncertainty measures in deep networks for multiple sclerosis lesion\ndetection and segmentation. Med Image Anal. 2020;59:101557.\n177.\t Herzog L, Murina E, Dürr O, Wegener S, Sick B. Integrating uncertainty in deep neural networks for MRI based\nstroke analysis. Med Image Anal. 2020;65:101790.\n178.\t Pereyra G, Tucker G, Chorowski J, Kaiser Ł, Hinton G. Regularizing neural networks by penalizing confident output\ndistributions; 2017. arXiv preprint arXiv:​1701.​06548.\n179.\t Naeini MP, Cooper GF, Hauskrecht M. Obtaining well calibrated probabilities using bayesian binning. In: Proceed‑\nings of the... AAAI conference on artificial intelligence. AAAI conference on artificial intelligence, vol. 2015. NIH\nPublic Access; 2015. p. 2901.\n180.\t Li M, Sethi IK. Confidence-based classifier design. Pattern Recogn. 2006;39(7):1230–40.\n181.\t Zadrozny B, Elkan C. Obtaining calibrated probability estimates from decision trees and Naive Bayesian classifiers.\nIn: ICML, vol. 1, Citeseer; 2001. p. 609–16.\n182.\t Steinwart I. Consistency of support vector machines and other regularized kernel classifiers. IEEE Trans Inf Theory.\n2005;51(1):128–42.\n183.\t Lee K, Lee K, Shin J, Lee H. Overcoming catastrophic forgetting with unlabeled data in the wild. In: Proceedings of\nthe IEEE international conference on computer vision; 2019. p. 312–21.\n184.\t Shmelkov K, Schmid C, Alahari K. Incremental learning of object detectors without catastrophic forgetting. In:\nProceedings of the IEEE international conference on computer vision; 2017. p. 3400–09.\n185.\t Zenke F, Gerstner W, Ganguli S. The temporal paradox of Hebbian learning and homeostatic plasticity. Curr Opin\nNeurobiol. 2017;43:166–76.\n186.\t Andersen N, Krauth N, Nabavi S. Hebbian plasticity in vivo: relevance and induction. Curr Opin Neurobiol.\n2017;45:188–92.\n187.\t Zheng R, Chakraborti S. A phase ii nonparametric adaptive exponentially weighted moving average control chart.\nQual Eng. 2016;28(4):476–90.\n188.\t Rebuffi SA, Kolesnikov A, Sperl G, Lampert CH. ICARL: Incremental classifier and representation learning. In: Pro‑\nceedings of the IEEE conference on computer vision and pattern recognition; 2017. p. 2001–10.\n189.\t Hinton GE, Plaut DC. Using fast weights to deblur old memories. In: Proceedings of the ninth annual conference of\nthe cognitive science society; 1987. p. 177–86.\n190.\t Parisi GI, Kemker R, Part JL, Kanan C, Wermter S. Continual lifelong learning with neural networks: a review. Neural\nNetw. 2019;113:54–71.\n191.\t Soltoggio A, Stanley KO, Risi S. Born to learn: the inspiration, progress, and future of evolved plastic artificial neural\nnetworks. Neural Netw. 2018;108:48–67.\n192.\t Parisi GI, Tani J, Weber C, Wermter S. Lifelong learning of human actions with deep neural network self-organiza‑\ntion. Neural Netw. 2017;96:137–49.\n193.\t Cheng Y, Wang D, Zhou P, Zhang T. Model compression and acceleration for deep neural networks: the principles,\nprogress, and challenges. IEEE Signal Process Mag. 2018;35(1):126–36.\n\nPage 70 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n194.\t Wiedemann S, Kirchhoffer H, Matlage S, Haase P, Marban A, Marinč T, Neumann D, Nguyen T, Schwarz H, Wiegand\nT, et al. Deepcabac: a universal compression algorithm for deep neural networks. IEEE J Sel Topics Signal Process.\n2020;14(4):700–14.\n195.\t Mehta N, Pandit A. Concurrence of big data analytics and healthcare: a systematic review. Int J Med Inform.\n2018;114:57–65.\n196.\t Esteva A, Robicquet A, Ramsundar B, Kuleshov V, DePristo M, Chou K, Cui C, Corrado G, Thrun S, Dean J. A guide to\ndeep learning in healthcare. Nat Med. 2019;25(1):24–9.\n197.\t Shawahna A, Sait SM, El-Maleh A. Fpga-based accelerators of deep learning networks for learning and classifica‑\ntion: a review. IEEE Access. 2018;7:7823–59.\n198.\t Min Z. Public welfare organization management system based on FPGA and deep learning. Microprocess\nMicrosyst. 2020;80:103333.\n199.\t Al-Shamma O, Fadhel MA, Hameed RA, Alzubaidi L, Zhang J. Boosting convolutional neural networks performance\nbased on fpga accelerator. In: International conference on intelligent systems design and applications. Springer;\n2018. p. 509–17.\n200.\t Han S, Mao H, Dally WJ. Deep compression: compressing deep neural networks with pruning, trained quantization\nand huffman coding; 2015. arXiv preprint arXiv:​1510.​00149.\n201.\t Chen Z, Zhang L, Cao Z, Guo J. Distilling the knowledge from handcrafted features for human activity recognition.\nIEEE Trans Ind Inform. 2018;14(10):4334–42.\n202.\t Hinton G, Vinyals O, Dean J. Distilling the knowledge in a neural network; 2015. arXiv preprint arXiv:​1503.​02531.\n203.\t Lenssen JE, Fey M, Libuschewski P. Group equivariant capsule networks. In: Advances in neural information pro‑\ncessing systems. San Mateo: Morgan Kaufmann Publishers; 2018. p. 8844–53.\n204.\t Denton EL, Zaremba W, Bruna J, LeCun Y, Fergus R. Exploiting linear structure within convolutional networks for\nefficient evaluation. In: Advances in neural information processing systems. San Mateo: Morgan Kaufmann Pub‑\nlishers; 2014. p. 1269–77.\n205.\t Xu Q, Zhang M, Gu Z, Pan G. Overfitting remedy by sparsifying regularization on fully-connected layers of CNNs.\nNeurocomputing. 2019;328:69–74.\n206.\t Zhang C, Bengio S, Hardt M, Recht B, Vinyals O. Understanding deep learning requires rethinking generalization.\nCommun ACM. 2018;64(3):107–15.\n207.\t Xu X, Jiang X, Ma C, Du P, Li X, Lv S, Yu L, Ni Q, Chen Y, Su J, et al. A deep learning system to screen novel coronavi‑\nrus disease 2019 pneumonia. Engineering. 2020;6(10):1122–9.\n208.\t Sharma K, Alsadoon A, Prasad P, Al-Dala’in T, Nguyen TQV, Pham DTH. A novel solution of using deep learning for\nleft ventricle detection: enhanced feature extraction. Comput Methods Programs Biomed. 2020;197:105751.\n209.\t Zhang G, Wang C, Xu B, Grosse R. Three mechanisms of weight decay regularization; 2018. arXiv preprint arXiv:​\n1810.​12281.\n210.\t Laurent C, Pereyra G, Brakel P, Zhang Y, Bengio Y. Batch normalized recurrent neural networks. In: 2016 IEEE interna‑\ntional conference on acoustics, speech and signal processing (ICASSP), IEEE; 2016. p. 2657–61.\n211.\t Salamon J, Bello JP. Deep convolutional neural networks and data augmentation for environmental sound clas‑\nsification. IEEE Signal Process Lett. 2017;24(3):279–83.\n212.\t Wang X, Qin Y, Wang Y, Xiang S, Chen H. ReLTanh: an activation function with vanishing gradient resistance for\nSAE-based DNNs and its application to rotating machinery fault diagnosis. Neurocomputing. 2019;363:88–98.\n213.\t Tan HH, Lim KH. Vanishing gradient mitigation with deep learning neural network optimization. In: 2019 7th\ninternational conference on smart computing & communications (ICSCC). IEEE; 2019. p. 1–4.\n214.\t MacDonald G, Godbout A, Gillcash B, Cairns S. Volume-preserving neural networks: a solution to the vanishing\ngradient problem; 2019. arXiv preprint arXiv:​1911.​09576.\n215.\t Mittal S, Vaishay S. A survey of techniques for optimizing deep learning on GPUs. J Syst Arch. 2019;99:101635.\n216.\t Kanai S, Fujiwara Y, Iwamura S. Preventing gradient explosions in gated recurrent units. In: Advances in neural\ninformation processing systems. San Mateo: Morgan Kaufmann Publishers; 2017. p. 435–44.\n217.\t Hanin B. Which neural net architectures give rise to exploding and vanishing gradients? In: Advances in neural\ninformation processing systems. San Mateo: Morgan Kaufmann Publishers; 2018. p. 582–91.\n218.\t Ribeiro AH, Tiels K, Aguirre LA, Schön T. Beyond exploding and vanishing gradients: analysing RNN training using\nattractors and smoothness. In: International conference on artificial intelligence and statistics, PMLR; 2020. p.\n2370–80.\n219.\t D’Amour A, Heller K, Moldovan D, Adlam B, Alipanahi B, Beutel A, Chen C, Deaton J, Eisenstein J, Hoffman MD, et al.\nUnderspecification presents challenges for credibility in modern machine learning; 2020. arXiv preprint arXiv:​\n2011.​03395.\n220.\t Chea P, Mandell JC. Current applications and future directions of deep learning in musculoskeletal radiology.\nSkelet Radiol. 2020;49(2):1–15.\n221.\t Wu X, Sahoo D, Hoi SC. Recent advances in deep learning for object detection. Neurocomputing. 2020;396:39–64.\n222.\t Kuutti S, Bowden R, Jin Y, Barber P, Fallah S. A survey of deep learning applications to autonomous vehicle control.\nIEEE Trans Intell Transp Syst. 2020;22:712–33.\n223.\t Yolcu G, Oztel I, Kazan S, Oz C, Bunyak F. Deep learning-based face analysis system for monitoring customer inter‑\nest. J Ambient Intell Humaniz Comput. 2020;11(1):237–48.\n224.\t Jiao L, Zhang F, Liu F, Yang S, Li L, Feng Z, Qu R. A survey of deep learning-based object detection. IEEE Access.\n2019;7:128837–68.\n225.\t Muhammad K, Khan S, Del Ser J, de Albuquerque VHC. Deep learning for multigrade brain tumor classification in\nsmart healthcare systems: a prospective survey. IEEE Trans Neural Netw Learn Syst. 2020;32:507–22.\n226.\t Litjens G, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, Van Der Laak JA, Van Ginneken B, Sánchez CI. A\nsurvey on deep learning in medical image analysis. Med Image Anal. 2017;42:60–88.\n227.\t Mukherjee D, Mondal R, Singh PK, Sarkar R, Bhattacharjee D. Ensemconvnet: a deep learning approach for\nhuman activity recognition using smartphone sensors for healthcare applications. Multimed Tools Appl.\n2020;79(41):31663–90.\n\nPage 71 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n228.\t Zeleznik R, Foldyna B, Eslami P, Weiss J, Alexander I, Taron J, Parmar C, Alvi RM, Banerji D, Uno M, et al. Deep\nconvolutional neural networks to predict cardiovascular risk from computed tomography. Nature Commun.\n2021;12(1):1–9.\n229.\t Wang J, Liu Q, Xie H, Yang Z, Zhou H. Boosted efficientnet: detection of lymph node metastases in breast cancer\nusing convolutional neural networks. Cancers. 2021;13(4):661.\n230.\t Yu H, Yang LT, Zhang Q, Armstrong D, Deen MJ. Convolutional neural networks for medical image analysis:\nstate-of-the-art, comparisons, improvement and perspectives. Neurocomputing. 2021. https://​doi.​org/​10.​1016/j.​\nneucom.​2020.​04.​157.\n231.\t Bharati S, Podder P, Mondal MRH. Hybrid deep learning for detecting lung diseases from X-ray images. Inform Med\nUnlocked. 2020;20:100391.\n232.\t Dong Y, Pan Y, Zhang J, Xu W. Learning to read chest X-ray images from 16000+ examples using CNN. In: 2017\nIEEE/ACM international conference on connected health: applications, systems and engineering technologies\n(CHASE). IEEE; 2017. p. 51–7.\n233.\t Rajkomar A, Lingam S, Taylor AG, Blum M, Mongan J. High-throughput classification of radiographs using deep\nconvolutional neural networks. J Digit Imaging. 2017;30(1):95–101.\n234.\t Rajpurkar P, Irvin J, Zhu K, Yang B, Mehta H, Duan T, Ding D, Bagul A, Langlotz C, Shpanskaya K, et al. Chexnet:\nradiologist-level pneumonia detection on chest X-rays with deep learning; 2017. arXiv preprint arXiv:​1711.​05225.\n235.\t Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-ray8: Hospital-scale chest X-ray database and bench‑\nmarks on weakly-supervised classification and localization of common thorax diseases. In: Proceedings of the IEEE\nconference on computer vision and pattern recognition; 2017. p. 2097–106.\n236.\t Zuo W, Zhou F, Li Z, Wang L. Multi-resolution CNN and knowledge transfer for candidate classification in lung\nnodule detection. IEEE Access. 2019;7:32510–21.\n237.\t Shen W, Zhou M, Yang F, Yang C, Tian J. Multi-scale convolutional neural networks for lung nodule classification. In:\nInternational conference on information processing in medical imaging. Springer; 2015. p. 588–99.\n238.\t Li R, Zhang W, Suk HI, Wang L, Li J, Shen D, Ji S. Deep learning based imaging data completion for improved brain\ndisease diagnosis. In: International conference on medical image computing and computer-assisted intervention.\nSpringer; 2014. p. 305–12.\n239.\t Wen J, Thibeau-Sutre E, Diaz-Melo M, Samper-González J, Routier A, Bottani S, Dormont D, Durrleman S, Burgos N,\nColliot O, et al. Convolutional neural networks for classification of Alzheimer’s disease: overview and reproducible\nevaluation. Med Image Anal. 2020;63:101694.\n240.\t Mehmood A, Maqsood M, Bashir M, Shuyuan Y. A deep siamese convolution neural network for multi-class clas‑\nsification of Alzheimer disease. Brain Sci. 2020;10(2):84.\n241.\t Hosseini-Asl E, Ghazal M, Mahmoud A, Aslantas A, Shalaby A, Casanova M, Barnes G, Gimel’farb G, Keynton R,\nEl-Baz A. Alzheimer’s disease diagnostics by a 3d deeply supervised adaptable convolutional network. Front Biosci.\n2018;23:584–96.\n242.\t Korolev S, Safiullin A, Belyaev M, Dodonova Y. Residual and plain convolutional neural networks for 3D brain MRI\nclassification. In: 2017 IEEE 14th international symposium on biomedical imaging (ISBI 2017). IEEE; 2017. p. 835–8.\n243.\t Alzubaidi L, Fadhel MA, Oleiwi SR, Al-Shamma O, Zhang J. DFU_QUTNet: diabetic foot ulcer classification using\nnovel deep convolutional neural network. Multimed Tools Appl. 2020;79(21):15655–77.\n244.\t Goyal M, Reeves ND, Davison AK, Rajbhandari S, Spragg J, Yap MH. Dfunet: convolutional neural networks for\ndiabetic foot ulcer classification. IEEE Trans Emerg Topics Comput Intell. 2018;4(5):728–39.\n245.\t Yap MH., Hachiuma R, Alavi A, Brungel R, Goyal M, Zhu H, Cassidy B, Ruckert J, Olshansky M, Huang X, et al. Deep\nlearning in diabetic foot ulcers detection: a comprehensive evaluation; 2020. arXiv preprint arXiv:​2010.​03341.\n246.\t Tulloch J, Zamani R, Akrami M. Machine learning in the prevention, diagnosis and management of diabetic foot\nulcers: a systematic review. IEEE Access. 2020;8:198977–9000.\n247.\t Fadhel MA, Al-Shamma O, Alzubaidi L, Oleiwi SR. Real-time sickle cell anemia diagnosis based hardware accelera‑\ntor. In: International conference on new trends in information and communications technology applications,\nSpringer; 2020. p. 189–99.\n248.\t Debelee TG, Kebede SR, Schwenker F, Shewarega ZM. Deep learning in selected cancers’ image analysis—a survey.\nJ Imaging. 2020;6(11):121.\n249.\t Khan S, Islam N, Jan Z, Din IU, Rodrigues JJC. A novel deep learning based framework for the detection and clas‑\nsification of breast cancer using transfer learning. Pattern Recogn Lett. 2019;125:1–6.\n250.\t Alzubaidi L, Hasan RI, Awad FH, Fadhel MA, Alshamma O, Zhang J. Multi-class breast cancer classification by a\nnovel two-branch deep convolutional neural network architecture. In: 2019 12th international conference on\ndevelopments in eSystems engineering (DeSE). IEEE; 2019. p. 268–73.\n251.\t Roy K, Banik D, Bhattacharjee D, Nasipuri M. Patch-based system for classification of breast histology images using\ndeep learning. Comput Med Imaging Gr. 2019;71:90–103.\n252.\t Hameed Z, Zahia S, Garcia-Zapirain B, Javier Aguirre J, María Vanegas A. Breast cancer histopathology image clas‑\nsification using an ensemble of deep learning models. Sensors. 2020;20(16):4373.\n253.\t Hosny KM, Kassem MA, Foaud MM. Skin cancer classification using deep learning and transfer learning. In: 2018\n9th Cairo international biomedical engineering conference (CIBEC). IEEE; 2018. p. 90–3.\n254.\t Dorj UO, Lee KK, Choi JY, Lee M. The skin cancer classification using deep convolutional neural network. Multimed\nTools Appl. 2018;77(8):9909–24.\n255.\t Kassem MA, Hosny KM, Fouad MM. Skin lesions classification into eight classes for ISIC 2019 using deep convolu‑\ntional neural network and transfer learning. IEEE Access. 2020;8:114822–32.\n256.\t Heidari M, Mirniaharikandehei S, Khuzani AZ, Danala G, Qiu Y, Zheng B. Improving the performance of CNN to\npredict the likelihood of COVID-19 using chest X-ray images with preprocessing algorithms. Int J Med Inform.\n2020;144:104284.\n257.\t Al-Timemy AH, Khushaba RN, Mosa ZM, Escudero J. An efficient mixture of deep and machine learning models for\nCOVID-19 and tuberculosis detection using X-ray images in resource limited settings 2020. arXiv preprint arXiv:​\n2007.​08223.\n\nPage 72 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n258.\t Abraham B, Nair MS. Computer-aided detection of COVID-19 from X-ray images using multi-CNN and Bayesnet\nclassifier. Biocybern Biomed Eng. 2020;40(4):1436–45.\n259.\t Nour M, Cömert Z, Polat K. A novel medical diagnosis model for COVID-19 infection detection based on deep\nfeatures and Bayesian optimization. Appl Soft Comput. 2020;97:106580.\n260.\t Mallio CA, Napolitano A, Castiello G, Giordano FM, D’Alessio P, Iozzino M, Sun Y, Angeletti S, Russano M, Santini D,\net al. Deep learning algorithm trained with COVID-19 pneumonia also identifies immune checkpoint inhibitor\ntherapy-related pneumonitis. Cancers. 2021;13(4):652.\n261.\t Fourcade A, Khonsari R. Deep learning in medical image analysis: a third eye for doctors. J Stomatol Oral Maxillofac\nSurg. 2019;120(4):279–88.\n262.\t Guo Z, Li X, Huang H, Guo N, Li Q. Deep learning-based image segmentation on multimodal medical imaging.\nIEEE Trans Radiat Plasma Med Sci. 2019;3(2):162–9.\n263.\t Thakur N, Yoon H, Chong Y. Current trends of artificial intelligence for colorectal cancer pathology image analysis:\na systematic review. Cancers. 2020;12(7):1884.\n264.\t Lundervold AS, Lundervold A. An overview of deep learning in medical imaging focusing on MRI. Zeitschrift für\nMedizinische Physik. 2019;29(2):102–27.\n265.\t Yadav SS, Jadhav SM. Deep convolutional neural network based medical image classification for disease diagnosis.\nJ Big Data. 2019;6(1):113.\n266.\t Nehme E, Freedman D, Gordon R, Ferdman B, Weiss LE, Alalouf O, Naor T, Orange R, Michaeli T, Shechtman Y. Deep‑\nSTORM3D: dense 3D localization microscopy and PSF design by deep learning. Nat Methods. 2020;17(7):734–40.\n267.\t Zulkifley MA, Abdani SR, Zulkifley NH. Pterygium-Net: a deep learning approach to pterygium detection and\nlocalization. Multimed Tools Appl. 2019;78(24):34563–84.\n268.\t Sirazitdinov I, Kholiavchenko M, Mustafaev T, Yixuan Y, Kuleev R, Ibragimov B. Deep neural network ensemble for\npneumonia localization from a large-scale chest X-ray database. Comput Electr Eng. 2019;78:388–99.\n269.\t Zhao W, Shen L, Han B, Yang Y, Cheng K, Toesca DA, Koong AC, Chang DT, Xing L. Markerless pancreatic tumor\ntarget localization enabled by deep learning. Int J Radiat Oncol Biol Phys. 2019;105(2):432–9.\n270.\t Roth HR, Lee CT, Shin HC, Seff A, Kim L, Yao J, Lu L, Summers RM. Anatomy-specific classification of medical images\nusing deep convolutional nets. In: 2015 IEEE 12th international symposium on biomedical imaging (ISBI). IEEE;\n2015. p. 101–4.\n271.\t Shin HC, Orton MR, Collins DJ, Doran SJ, Leach MO. Stacked autoencoders for unsupervised feature learn‑\ning and multiple organ detection in a pilot study using 4D patient data. IEEE Trans Pattern Anal Mach Intell.\n2012;35(8):1930–43.\n272.\t Li Z, Dong M, Wen S, Hu X, Zhou P, Zeng Z. CLU-CNNs: object detection for medical images. Neurocomputing.\n2019;350:53–9.\n273.\t Gao J, Jiang Q, Zhou B, Chen D. Convolutional neural networks for computer-aided detection or diagnosis in\nmedical image analysis: an overview. Math Biosci Eng. 2019;16(6):6536.\n274.\t Lumini A, Nanni L. Review fair comparison of skin detection approaches on publicly available datasets. Expert Syst\nAppl. 2020. https://​doi.​org/​10.​1016/j.​eswa.​2020.​113677.\n275.\t Chouhan V, Singh SK, Khamparia A, Gupta D, Tiwari P, Moreira C, Damaševičius R, De Albuquerque VHC. A novel\ntransfer learning based approach for pneumonia detection in chest X-ray images. Appl Sci. 2020;10(2):559.\n276.\t Apostolopoulos ID, Mpesiana TA. COVID-19: automatic detection from X-ray images utilizing transfer learning with\nconvolutional neural networks. Phys Eng Sci Med. 2020;43(2):635–40.\n277.\t Mahmud T, Rahman MA, Fattah SA. CovXNet: a multi-dilation convolutional neural network for automatic COVID19 and other pneumonia detection from chest X-ray images with transferable multi-receptive feature optimiza‑\ntion. Comput Biol Med. 2020;122:103869.\n278.\t Tayarani-N MH. Applications of artificial intelligence in battling against COVID-19: a literature review. Chaos Soli‑\ntons Fractals. 2020;142:110338.\n279.\t Toraman S, Alakus TB, Turkoglu I. Convolutional capsnet: a novel artificial neural network approach to detect\nCOVID-19 disease from X-ray images using capsule networks. Chaos Solitons Fractals. 2020;140:110122.\n280.\t Dascalu A, David E. Skin cancer detection by deep learning and sound analysis algorithms: a prospective clinical\nstudy of an elementary dermoscope. EBioMedicine. 2019;43:107–13.\n281.\t Adegun A, Viriri S. Deep learning techniques for skin lesion analysis and melanoma cancer detection: a survey of\nstate-of-the-art. Artif Intell Rev. 2020;54:1–31.\n282.\t Zhang N, Cai YX, Wang YY, Tian YT, Wang XL, Badami B. Skin cancer diagnosis based on optimized convolutional\nneural network. Artif Intell Med. 2020;102:101756.\n283.\t Thurnhofer-Hemsi K, Domínguez E. A convolutional neural network framework for accurate skin cancer detection.\nNeural Process Lett. 2020. https://​doi.​org/​10.​1007/​s11063-​020-​10364-y.\n284.\t Jain MS, Massoud TF. Predicting tumour mutational burden from histopathological images using multiscale deep\nlearning. Nat Mach Intell. 2020;2(6):356–62.\n285.\t Lei H, Liu S, Elazab A, Lei B. Attention-guided multi-branch convolutional neural network for mitosis detection\nfrom histopathological images. IEEE J Biomed Health Inform. 2020;25(2):358–70.\n286.\t Celik Y, Talo M, Yildirim O, Karabatak M, Acharya UR. Automated invasive ductal carcinoma detection based using\ndeep transfer learning with whole-slide images. Pattern Recogn Lett. 2020;133:232–9.\n287.\t Sebai M, Wang X, Wang T. Maskmitosis: a deep learning framework for fully supervised, weakly supervised, and\nunsupervised mitosis detection in histopathology images. Med Biol Eng Comput. 2020;58:1603–23.\n288.\t Sebai M, Wang T, Al-Fadhli SA. Partmitosis: a partially supervised deep learning framework for mitosis detection in\nbreast cancer histopathology images. IEEE Access. 2020;8:45133–47.\n289.\t Mahmood T, Arsalan M, Owais M, Lee MB, Park KR. Artificial intelligence-based mitosis detection in breast cancer\nhistopathology images using faster R-CNN and deep CNNs. J Clin Med. 2020;9(3):749.\n290.\t Srinidhi CL, Ciga O, Martel AL. Deep neural network models for computational histopathology: a survey. Med\nImage Anal. 2020;67:101813.\n\nPage 73 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n\n291.\t Cireşan DC, Giusti A, Gambardella LM, Schmidhuber J. Mitosis detection in breast cancer histology images with\ndeep neural networks. In: International conference on medical image computing and computer-assisted inter‑\nvention. Springer; 2013. p. 411–8.\n292.\t Sirinukunwattana K, Raza SEA, Tsang YW, Snead DR, Cree IA, Rajpoot NM. Locality sensitive deep learning\nfor detection and classification of nuclei in routine colon cancer histology images. IEEE Trans Med Imaging.\n2016;35(5):1196–206.\n293.\t Xu J, Xiang L, Liu Q, Gilmore H, Wu J, Tang J, Madabhushi A. Stacked sparse autoencoder (SSAE) for nuclei detec‑\ntion on breast cancer histopathology images. IEEE Trans Med Imaging. 2015;35(1):119–30.\n294.\t Albarqouni S, Baur C, Achilles F, Belagiannis V, Demirci S, Navab N. Aggnet: deep learning from crowds for mitosis\ndetection in breast cancer histology images. IEEE Trans Med Imaging. 2016;35(5):1313–21.\n295.\t Abd-Ellah MK, Awad AI, Khalaf AA, Hamed HF. Two-phase multi-model automatic brain tumour diagnosis\nsystem from magnetic resonance images using convolutional neural networks. EURASIP J Image Video Process.\n2018;2018(1):97.\n296.\t Thaha MM, Kumar KPM, Murugan B, Dhanasekeran S, Vijayakarthick P, Selvi AS. Brain tumor segmentation using\nconvolutional neural networks in MRI images. J Med Syst. 2019;43(9):294.\n297.\t Talo M, Yildirim O, Baloglu UB, Aydin G, Acharya UR. Convolutional neural networks for multi-class brain disease\ndetection using MRI images. Comput Med Imaging Gr. 2019;78:101673.\n298.\t Gabr RE, Coronado I, Robinson M, Sujit SJ, Datta S, Sun X, Allen WJ, Lublin FD, Wolinsky JS, Narayana PA. Brain and\nlesion segmentation in multiple sclerosis using fully convolutional neural networks: a large-scale study. Mult Scler\nJ. 2020;26(10):1217–26.\n299.\t Chen S, Ding C, Liu M. Dual-force convolutional neural networks for accurate brain tumor segmentation. Pattern\nRecogn. 2019;88:90–100.\n300.\t Hu K, Gan Q, Zhang Y, Deng S, Xiao F, Huang W, Cao C, Gao X. Brain tumor segmentation using multi-cascaded\nconvolutional neural networks and conditional random field. IEEE Access. 2019;7:92615–29.\n301.\t Wadhwa A, Bhardwaj A, Verma VS. A review on brain tumor segmentation of MRI images. Magn Reson Imaging.\n2019;61:247–59.\n302.\t Akkus Z, Galimzianova A, Hoogi A, Rubin DL, Erickson BJ. Deep learning for brain MRI segmentation: state of the\nart and future directions. J Digit Imaging. 2017;30(4):449–59.\n303.\t Moeskops P, Viergever MA, Mendrik AM, De Vries LS, Benders MJ, Išgum I. Automatic segmentation of MR brain\nimages with a convolutional neural network. IEEE Trans Med Imaging. 2016;35(5):1252–61.\n304.\t Milletari F, Navab N, Ahmadi SA. V-net: Fully convolutional neural networks for volumetric medical image segmen‑\ntation. In: 2016 fourth international conference on 3D vision (3DV). IEEE; 2016. p. 565–71.\n305.\t Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In: Interna‑\ntional conference on medical image computing and computer-assisted intervention. Springer; 2015. p. 234–41.\n306.\t Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutional neural networks in MRI images.\nIEEE Trans Med Imaging. 2016;35(5):1240–51.\n307.\t Havaei M, Davy A, Warde-Farley D, Biard A, Courville A, Bengio Y, Pal C, Jodoin PM, Larochelle H. Brain tumor seg‑\nmentation with deep neural networks. Med Image Anal. 2017;35:18–31.\n308.\t Chen LC, Papandreou G, Kokkinos I, Murphy K, Yuille AL. DeepLab: semantic image segmentation with\ndeep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Trans Pattern Anal Mach Intell.\n2017;40(4):834–48.\n309.\t Yan Q, Wang B, Gong D, Luo C, Zhao W, Shen J, Shi Q, Jin S, Zhang L, You Z. COVID-19 chest CT image segmenta‑\ntion—a deep convolutional neural network solution; 2020. arXiv preprint arXiv:​2004.​10987.\n310.\t Wang G, Liu X, Li C, Xu Z, Ruan J, Zhu H, Meng T, Li K, Huang N, Zhang S. A noise-robust framework for automatic\nsegmentation of COVID-19 pneumonia lesions from CT images. IEEE Trans Med Imaging. 2020;39(8):2653–63.\n311.\t Khan SH, Sohail A, Khan A, Lee YS. Classification and region analysis of COVID-19 infection using lung CT images\nand deep convolutional neural networks; 2020. arXiv preprint arXiv:​2009.​08864.\n312.\t Shi F, Wang J, Shi J, Wu Z, Wang Q, Tang Z, He K, Shi Y, Shen D. Review of artificial intelligence techniques in imag‑\ning data acquisition, segmentation and diagnosis for COVID-19. IEEE Rev Biomed Eng. 2020;14:4–5.\n313.\t Santamaría J, Rivero-Cejudo M, Martos-Fernández M, Roca F. An overview on the latest nature-inspired and\nmetaheuristics-based image registration algorithms. Appl Sci. 2020;10(6):1928.\n314.\t Santamaría J, Cordón O, Damas S. A comparative study of state-of-the-art evolutionary image registration meth‑\nods for 3D modeling. Comput Vision Image Underst. 2011;115(9):1340–54.\n315.\t Yumer ME, Mitra NJ. Learning semantic deformation flows with 3D convolutional networks. In: European confer‑\nence on computer vision. Springer; 2016. p. 294–311.\n316.\t Ding L, Feng C. Deepmapping: unsupervised map estimation from multiple point clouds. In: Proceedings of the\nIEEE conference on computer vision and pattern recognition; 2019. p. 8650–9.\n317.\t Mahadevan S. Imagination machines: a new challenge for artificial intelligence. AAAI. 2018;2018:7988–93.\n318.\t Wang L, Fang Y. Unsupervised 3D reconstruction from a single image via adversarial learning; 2017. arXiv preprint\narXiv:​1711.​09312.\n319.\t Hermoza R, Sipiran I. 3D reconstruction of incomplete archaeological objects using a generative adversarial\nnetwork. In: Proceedings of computer graphics international 2018. Association for Computing Machinery; 2018. p.\n5–11.\n320.\t Fu Y, Lei Y, Wang T, Curran WJ, Liu T, Yang X. Deep learning in medical image registration: a review. Phys Med Biol.\n2020;65(20):20TR01.\n321.\t Haskins G, Kruger U, Yan P. Deep learning in medical image registration: a survey. Mach Vision Appl. 2020;31(1):8.\n322.\t de Vos BD, Berendsen FF, Viergever MA, Sokooti H, Staring M, Išgum I. A deep learning framework for unsupervised\naffine and deformable image registration. Med Image Anal. 2019;52:128–43.\n323.\t Yang X, Kwitt R, Styner M, Niethammer M. Quicksilver: fast predictive image registration—a deep learning\napproach. NeuroImage. 2017;158:378–96.\n\nPage 74 of 74\nAlzubaidi et al. J Big Data      (2021) 8:53\n324.\t Miao S, Wang ZJ, Liao R. A CNN regression approach for real-time 2D/3D registration. IEEE Trans Med Imaging.\n2016;35(5):1352–63.\n325.\t Li P, Pei Y, Guo Y, Ma G, Xu T, Zha H. Non-rigid 2D–3D registration using convolutional autoencoders. In: 2020 IEEE\n17th international symposium on biomedical imaging (ISBI). IEEE; 2020. p. 700–4.\n326.\t Zhang J, Yeung SH, Shu Y, He B, Wang W. Efficient memory management for GPU-based deep learning systems;\n2019. arXiv preprint arXiv:​1903.​06631.\n327.\t Zhao H, Han Z, Yang Z, Zhang Q, Yang F, Zhou L, Yang M, Lau FC, Wang Y, Xiong Y, et al. Hived: sharing a {GPU}\ncluster for deep learning with guarantees. In: 14th {USENIX} symposium on operating systems design and imple‑\nmentation ({OSDI} 20); 2020. p. 515–32.\n328.\t Lin Y, Jiang Z, Gu J, Li W, Dhar S, Ren H, Khailany B, Pan DZ. DREAMPlace: deep learning toolkit-enabled GPU accel‑\neration for modern VLSI placement. IEEE Trans Comput Aided Des Integr Circuits Syst. 2020;40:748–61.\n329.\t Hossain S, Lee DJ. Deep learning-based real-time multiple-object detection and tracking from aerial imagery via a\nflying robot with GPU-based embedded devices. Sensors. 2019;19(15):3371.\n330.\t Castro FM, Guil N, Marín-Jiménez MJ, Pérez-Serrano J, Ujaldón M. Energy-based tuning of convolutional neural\nnetworks on multi-GPUs. Concurr Comput Pract Exp. 2019;31(21):4786.\n331.\t Gschwend D. Zynqnet: an fpga-accelerated embedded convolutional neural network; 2020. arXiv preprint arXiv:​\n2005.​06892.\n332.\t Zhang N, Wei X, Chen H, Liu W. FPGA implementation for CNN-based optical remote sensing object detection.\nElectronics. 2021;10(3):282.\n333.\t Zhao M, Hu C, Wei F, Wang K, Wang C, Jiang Y. Real-time underwater image recognition with FPGA embedded\nsystem for convolutional neural network. Sensors. 2019;19(2):350.\n334.\t Liu X, Yang J, Zou C, Chen Q, Yan X, Chen Y, Cai C. Collaborative edge computing with FPGA-based CNN accelera‑\ntors for energy-efficient and time-aware face tracking system. IEEE Trans Comput Soc Syst. 2021. https://​doi.​org/​\n10.​1109/​TCSS.​2021.​30593​18.\n335.\t Hossin M, Sulaiman M. A review on evaluation metrics for data classification evaluations. Int J Data Min Knowl\nManag Process. 2015;5(2):1.\n336.\t Provost F, Domingos P. Tree induction for probability-based ranking. Mach Learn. 2003;52(3):199–215.\n337.\t Rakotomamonyj A. Optimizing area under roc with SVMS. In: Proceedings of the European conference on artificial\nintelligence workshop on ROC curve and artificial intelligence (ROCAI 2004), 2004. p. 71–80.\n338.\t Mingote V, Miguel A, Ortega A, Lleida E. Optimization of the area under the roc curve using neural network super‑\nvectors for text-dependent speaker verification. Comput Speech Lang. 2020;63:101078.\n339.\t Fawcett T. An introduction to roc analysis. Pattern Recogn Lett. 2006;27(8):861–74.\n340.\t Huang J, Ling CX. Using AUC and accuracy in evaluating learning algorithms. IEEE Trans Knowl Data Eng.\n2005;17(3):299–310.\n341.\t Hand DJ, Till RJ. A simple generalisation of the area under the ROC curve for multiple class classification problems.\nMach Learn. 2001;45(2):171–86.\n342.\t Masoudnia S, Mersa O, Araabi BN, Vahabie AH, Sadeghi MA, Ahmadabadi MN. Multi-representational learning for\noffline signature verification using multi-loss snapshot ensemble of CNNs. Expert Syst Appl. 2019;133:317–30.\n343.\t Coupé P, Mansencal B, Clément M, Giraud R, de Senneville BD, Ta VT, Lepetit V, Manjon JV. Assemblynet: a large\nensemble of CNNs for 3D whole brain MRI segmentation. NeuroImage. 2020;219:117026.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "stats": {
    "raw_length": 238718,
    "normalized_length": 235158,
    "raw_lines": 3813,
    "normalized_lines": 3361
  }
}