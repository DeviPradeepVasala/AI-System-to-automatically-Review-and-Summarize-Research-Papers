{
  "pdf_path": "C:\\Users\\hp\\AI-System-to-automatically-Review-and-Summarize-Research-Papers\\data\\papers\\Mixed precision algorithms in numerical linear algebra.pdf",
  "pdf_name": "Mixed precision algorithms in numerical linear algebra.pdf",
  "file_hash": "37f321bf8375f88ca959273bb79dad3b975959bec339b13a8da58a0151250eb8",
  "metadata": {
    "title": "",
    "author": "",
    "subject": "",
    "keywords": "",
    "creator": "LaTeX with hyperref package",
    "producer": "pdfTeX-1.40.18",
    "creation_date": "D:20220408094217+01'00'",
    "modification_date": "D:20220609082156+00'00'"
  },
  "raw_text": "Acta Numerica (2022), pp. 347–414\nPrinted in the United Kingdom\ndoi:10.1017/S0962492922000022\nMixed precision algorithms in\nnumerical linear algebra\nNicholas J. Higham\nDepartment of Mathematics, University of Manchester,\nManchester, M13 9PL, UK\nE-mail: nick.higham@manchester.ac.uk\nTheo Mary\nSorbonne Université, CNRS, LIP6,\nParis, F-75005, France\nE-mail: theo.mary@lip6.fr\nToday’s ﬂoating-point arithmetic landscape is broader than ever. While scientiﬁc\ncomputing has traditionally used single precision and double precision ﬂoating-point\narithmetics, half precision is increasingly available in hardware and quadruple preci-\nsion is supported in software. Lower precision arithmetic brings increased speed and\nreduced communication and energy costs, but it produces results of correspondingly\nlow accuracy. Higher precisions are more expensive but can potentially provide great\nbeneﬁts, even if used sparingly. A variety of mixed precision algorithms have been\ndeveloped that combine the superior performance of lower precisions with the better\naccuracy of higher precisions. Some of these algorithms aim to provide results of\nthe same quality as algorithms running in a ﬁxed precision but at a much lower cost;\nothers use a little higher precision to improve the accuracy of an algorithm. This\nsurvey treats a broad range of mixed precision algorithms in numerical linear al-\ngebra, both direct and iterative, for problems including matrix multiplication, matrix\nfactorization, linear systems, least squares, eigenvalue decomposition and singular\nvalue decomposition. We identify key algorithmic ideas, such as iterative reﬁne-\nment, adapting the precision to the data, and exploiting mixed precision block fused\nmultiply–add operations. We also describe the possible performance beneﬁts and\nexplain what is known about the numerical stability of the algorithms. This survey\nshould be useful to a wide community of researchers and practitioners who wish to\ndevelop or beneﬁt from mixed precision numerical linear algebra algorithms.\n© The Author(s), 2022. Published by Cambridge University Press.\nThis is an Open Access article, distributed under the terms of the Creative Commons Attribution\nlicence (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution,\nand reproduction in any medium, provided the original work is properly cited.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n348\nN. J. Higham and T. Mary\nCONTENTS\n1\nIntroduction\n348\n2\nFloating-point arithmetics\n352\n3\nRounding error analysis model\n360\n4\nMatrix multiplication\n361\n5\nNon-linear equations\n364\n6\nIterative reﬁnement for Ax = b\n369\n7\nDirect methods for Ax = b\n371\n8\nIterative methods for Ax = b\n381\n9\nMixed precision orthogonalization and\nQR factorization\n385\n10 Least squares problems\n388\n11 Eigenvalue decomposition\n389\n12 Singular value decomposition\n392\n13 Multiword arithmetic\n393\n14 Adaptive precision algorithms\n396\n15 Miscellany\n399\nAcknowledgements\n401\nReferences\n401\n1. Introduction\nTraditionally, scientiﬁc computing has been carried out in double precision arith-\nmetic, which nowadays corresponds to a 64-bit ﬂoating-point number format. It\nhas long been recognized that single precision computations have advantages over\ndouble precision ones, not just because single precision arithmetic is typically twice\nas fast as double precision arithmetic but also because single precision data requires\nhalf as much storage as double precision data and has half the memory transfer\ncosts. Of course, single precision computations will generally provide only single\nprecision accuracy. Whether this is suﬃcient for a given application depends on\nthe application, and the answer can be diﬀerent even for diﬀerent computations\nwithin the same ﬁeld: see Section 1.2.2.\nModern hardware increasingly supports half precision arithmetic, which is at-\ntractive compared with single and double precisions because of its speed, its lower\nenergy usage and its reduced storage and data movement costs.\nAs we now have three precisions of ﬂoating-point arithmetic in hardware, as\nwell as quadruple precision arithmetic in software, we are in an intrinsically mixed\nprecision world, where precisions can be judiciously chosen in order to make the\nbest use of our computational resources.\nIn this work we survey mixed precision algorithms in numerical linear algebra.\nRelevant work goes back to the beginning of the digital computer area, but most\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n349\ncontributions in this area have been made in the last couple of decades. An earlier\nsurvey of the same areas is that of Abdelfattah et al. (2021a).\n1.1. Mixed precision versus multiprecision\nWe use the following terminology.\n• A mixed precision algorithm uses two or more precisions chosen from a\nsmall number of available precisions, which are typically half, single and\ndouble precision, provided in hardware, and quadruple precision, provided in\nsoftware.\n• A multiprecision algorithm uses one or more arbitrary precisions, which may\nbe problem-dependent and are provided in software. The precision at which\nresults are returned may be ﬁxed (for example, double precision) or may be\na parameter. See Section 2.5 for details of some available multiprecision\narithmetics.\nThe term variable precision is sometimes used in place of\nmultiprecision.\nThis survey is concerned with mixed precision algorithms, but we will brieﬂy\ndiscuss some multiprecision algorithms in Section 15.2.\n1.2. Applications\nMixed precision algorithms are being used, or considered for use in a wide variety of\napplications, some of which involve computations at very large scale. We mention\nsome examples here in order to illustrate the diﬀerent motivations for using mixed\nprecision arithmetic and the possible beneﬁts in real-life applications.\n1.2.1. Simulations\nIdomura, Ina, Ali and Imamura (2020) carry out plasma turbulence simulations for\nthe next generation experimental fusion reactor ITER on the Fugaku and Summit\nsupercomputers.\nTheir code for integrating the gyrokinetic partial diﬀerential\nequation (PDE) in double precision involves the solution of linear systems by a\nKrylov method. The authors show that using a communication-avoiding version\nof the Krylov method with a half precision (fp16) version of the preconditioner\nresults in speedups over the original solver by a factor of approximately 2–3.\nYang et al. (2019) implement in TensorFlow a Monte Carlo simulation of the\nIsing model on a two-dimensional lattice and run it on Google Tensor Processing\nUnits (TPUs). They ﬁnd that single precision can be replaced by half precision\n(bﬂoat16) without any loss of accuracy, enabling larger lattices to be simulated\nbecause of the lower memory requirement of half precision.\n1.2.2. Climate modelling and weather forecasting\nIn climate modelling and weather forecasting, codes have traditionally used double\nprecision variables, but in recent years the use of lower precisions has been extens-\nively investigated (Palmer 2014). The lesser data movement and faster execution\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n350\nN. J. Higham and T. Mary\nof lower precision arithmetic oﬀers the possibility of using reﬁned spatial grids\nrepresented in lower precision, allowing higher resolution simulations with no in-\ncrease in run time, potentially improving the output of a model. The observations\non which a model is built have low precision, so it can be argued that variables\ndo not need to be represented in double precision (Tintó Prims et al. 2019), and\nthis argument is also supported by the notion that the grid parametrizations should\nbe stochastic (Palmer 2020). Moreover, many model components have uncertain-\nties that can be much larger than the level of double precision (Dawson, Düben,\nMacLeod and Palmer 2018). Codes in this area can consist of millions of lines of\nFortran (Bauer et al. 2021), so changing the precisions of variables and assessing\nthe eﬀect of the changes is not an easy task.\nVáňa et al. (2017) show that almost all double precision variables in the In-\ntegrated Forecast System of the European Centre for Medium-Range Weather\nForecasts can be converted to single precision with no noticeable loss in accuracy\nand a gain in speed of about 40%. A beneﬁt of running the code in lower precision\nwas found to be that it revealed places where the code could be made more robust.\nHarvey and Verseghy (2015) had a diﬀerent experience with their code for a land\nsurface model, where running in single precision instead of double did not provide\nsuﬃcient accuracy for some of the land depths and timescales of interest.\nA weather and climate simulation code called the Uniﬁed Model (UM) is used\nby the Met Oﬃce for both operational numerical weather prediction and climate\nmodelling. The code carries out time integration of a system of PDEs, which\ninvolves at each time step the solution of a linear system with a banded, time-\nvarying non-symmetric matrix of size 3.5 × 108. The system is solved by the\npreconditioned BiCGstab algorithm, with a convergence test requiring a residual\nof norm 10−4 relative to the right-hand side. The UM is coded in double precision\nand is memory-bound (that is, its execution time is determined by the speed at which\ndata is transferred from main memory to the arithmetic units rather than by the speed\nof the ﬂoating-point arithmetic). Maynard and Walters (2019) implemented the\nlinear system solution almost entirely in single precision, with the same convergence\ntolerance, obtaining close to a factor 2 speedup of the solver, which they attribute\nto the reduction in data movement costs. To alleviate some stability issues they use\na mixed precision summation algorithm that is essentially the FABsum algorithm\nof Blanchard, Higham and Mary (2020a), which is discussed in Section 4.2. The\nmixed precision solver is now used in operational forecasts.\n1.2.3. Machine learning\nLow precision arithmetic has become widely used in machine learning in the last\nfew years because it has been found experimentally that algorithms can run faster\nwith certain parts executed in low precision, with little or no deterioration in the\nquality of the results. Dean (2020) gives three characteristics of deep learning\nmodels that make specialized hardware suitable for running them.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n351\nFirst, they are very tolerant of reduced-precision computations. Second, the computations\nperformed by most models are simply diﬀerent compositions of a relatively small handful\nof operations like matrix multiplies, vector operations, application of convolutional kernels,\nand other dense linear algebra calculations . . . Third, many of the mechanisms developed\nover the past 40 years to enable general-purpose programs to run with high performance on\nmodern CPUs . . . are unnecessary for machine learning computations. So, the opportunity\nexists to build computational hardware that is specialized for dense, low-precision linear\nalgebra, and not much else, but is still programmable at the level of specifying programs\nas diﬀerent compositions of mostly linear algebra-style operations.\nThe special-purpose hardware being referred to here can be classiﬁed as either\nﬁeld programmable gate arrays (FPGAs) or application-speciﬁc integrated circuits\n(ASICs), and it may use ﬁxed-point arithmetic, ﬂoating-point arithmetic or an\nintermediate between the two called block ﬂoating-point arithmetic (which was in\nuse in the 1960s (Wilkinson 1963, Wang et al. 2019)).\nVariables of diﬀerent precisions arise in machine learning algorithms from quant-\nization, the process of reducing the number of bits per operand. The limiting case is\nbinarization, in which a number has just two possible values, 0 and 1. Quantization\nis applied in various ways, including during training or on a trained model.\nOne of the ﬁrst papers to popularize the use of low precision arithmetic in deep\nlearning is by Courbariaux, Bengio and David (2015), who ﬁnd that ‘very low\nprecision is suﬃcient not just for running trained networks but also for training\nthem.’\nSeveral reasons have been suggested by numerical analysts for the success of low\nprecision ﬂoating-point arithmetic in machine learning. Scheinberg (2016) argues\nthat in machine learning we are solving the wrong problem, namely a surrogate\nfor the original optimization problem, so we do not need an accurate solution. It\ncan also be argued that low precision arithmetic provides regularization and that\nthis is beneﬁcial to machine learning, perhaps by leading to ﬂat minima rather than\nnarrow minima.\nIn machine learning one often updates a parameter φ by a sequence of small\nquantities hi through a recurrence φ(i+1) ←φ(i) + hi, i = 1: n. If hi is of absolute\nvalue less than half the spacing of the ﬂoating-point numbers around φ(i), which is\nmore likely in low precision arithmetic, then φ(i) + hi rounds to φ(i) with round to\nnearest, so the information in hi is lost, and if this happens for many i then the error\nin φ(n+1) can be large. This phenomenon is called stagnation. It can be avoided by\nusing stochastic rounding in place of round to nearest (Connolly, Higham and Mary\n2021, Croci et al. 2022). Stochastic rounding is a randomized form of rounding\nthat rounds to the next larger or next smaller ﬂoating-point number with probability\nproportional to 1 minus the distance to those ﬂoating-point numbers. An early use in\ndeep learning was by Gupta, Agrawal, Gopalakrishnan and Narayanan (2015), who\nﬁnd that ‘deep networks can be trained using only 16-bit wide ﬁxed-point number\nrepresentation when using stochastic rounding, and incur little to no degradation\nin the classiﬁcation accuracy.’\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n352\nN. J. Higham and T. Mary\n1.2.4. HPL-AI Mixed Precision Benchmark\nThe HPL-AI Mixed Precision Benchmark1 is intended to measure supercomputer\nperformance on AI-type workloads. It solves a double precision non-singular linear\nsystem Ax = b of order n using an LU factorization without pivoting computed in\nhalf precision and it reﬁnes the solution using preconditioned GMRES in double\nprecision. As of November 2021, the world record execution rate for the benchmark\nis 2.0 ExaFlop/s (2 × 1018 ﬂoating-point operations per second), where most of\nthe operations are half precision ones, for a matrix of size 16 957 440, which was\nachieved by the Fugaku supercomputer in Japan (Kudo, Nitadori, Ina and Imamura\n2020a,b). The choice of matrix A for the benchmark is critical, as it must be cheap\nto compute, have a controlled condition number, and have a numerically stable\nLU factorization without pivoting; a class of matrices having these properties is\nderived by Fasi and Higham (2021).\n2. Floating-point arithmetics\nSupport for more than one precision of ﬂoating-point arithmetic, provided in hard-\nware or software, has existed throughout the history of digital computing.\nA\nlandmark was the Fortran 66 standard (ANSI 1966), which included the real and\ndouble precision data types and so made it possible to write portable programs that\nused two precisions.\nSome early machines that supported d-digit but not 2d-digit arithmetic oﬀered\nthe ability to accumulate an inner product of d-digit vectors in a 2d-digit accu-\nmulator, only rounding back to d digits after the ﬁnal addition. This mode of\ncomputation was discussed by von Neumann and Goldstine (1947, Section 2.3)\nand was exploited by Wilkinson (1948, 1961) on the ACE computer and by Moler\n(1967) on the IBM 7094. Even earlier, desk calculating machines such as the\nBrunsviga oﬀered accumulators with more digits than the input or the registers\n(Croarken 1985, Section 1.2.1).\nUp to the mid 1980s, most computers used for scientiﬁc computing oﬀered both\nsingle precision and double precision ﬂoating-point arithmetic, but the formats of\nthe precisions varied greatly between machines. For example, a double precision\nnumber had a 96-bit signiﬁcand on the Cray-1, a 53-bit signiﬁcand on the DEC VAX\n(G format) and a 14-hexadecimal digit signiﬁcand on the IBM 3090. This lack\nof uniformity, and more importantly the diﬀering properties of the arithmetics,\nhindered the development of software intended to perform consistently across\ndiﬀerent machines.\n2.1. IEEE arithmetics\nA major breakthrough for scientiﬁc computing was the publication of the ANSI/\nIEEE Standard 754-1985 for Binary Floating-Point Arithmetic (IEEE 1985), which\n1 https://icl.bitbucket.io/hpl-ai/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n353\nTable 2.1. Parameters for ﬁve ﬂoating-point arithmetics: number of bits in sig-\nniﬁcand (including implicit most signiﬁcant bit) and exponent (sig, exp); unit\nroundoﬀu; smallest positive (subnormal) number xs\nmin; smallest positive normal-\nized number xmin; and largest ﬁnite number xmax. The last four columns are given\nto three signiﬁcant ﬁgures. In Intel’s bﬂoat16 speciﬁcation subnormal numbers are\nnot supported (Intel Corporation 2018).\n(sig, exp)\nu\nxs\nmin\nxmin\nxmax\nbﬂoat16\n(8, 8)\n3.91 × 10−3\n9.18 × 10−41\n1.18 × 10−38\n3.39 × 1038\nfp16\n(11, 5)\n4.88 × 10−4\n5.96 × 10−8\n6.10 × 10−5\n6.55 × 104\nfp32\n(24, 8)\n5.96 × 10−8\n1.40 × 10−45\n1.18 × 10−38\n3.40 × 1038\nfp64\n(53, 11)\n1.11 × 10−16\n4.94 × 10−324\n2.22 × 10−308\n1.80 × 10308\nfp128\n(113, 15)\n9.63 × 10−35\n6.48 × 10−4966\n3.36 × 10−4932\n1.19 × 104932\nprovided binary ﬂoating-point formats and precise rules for carrying out arithmetic\non them. The standard had been carefully designed over several years by a com-\nmittee of experts from academia and industry, and it brought much-needed order\nto computer arithmetic (Kahan 1981). Within a few years virtually all computer\nmanufacturers had adopted it.\nThe 1985 standard prescribed two ﬂoating-point number formats: 32-bit single\nprecision and 64-bit double precision. A 2008 revision (IEEE 2008) added a 128-\nbit quadruple precision format and a 16-bit half precision format, the latter deﬁned\nas a storage format only rather than for computation. The half precision format was\nmotivated by the emergence of support for 16-bit formats on graphical processing\nunits (GPUs), where these formats were used for graphics and gaming.\nTo deﬁne the IEEE formats we recall that a ﬂoating-point number system is a\nﬁnite subset F = F(β, t, emin, emax) of R whose elements have the form\nx = ±m × βe−t+1.\n(2.1)\nHere, β is the base, which is 2 on virtually all current computers. The integer t is the\nprecision and the integer e is the exponent, which lies in the range emin ≤e ≤emax,\nand the IEEE standard requires that emin = 1−emax. The signiﬁcand m is an integer\nsatisfying 0 ≤m ≤βt −1. To ensure a unique representation for each non-zero\nx ∈F it is assumed that m ≥βt−1 if x , 0, so that the system is normalized.\nThe largest and smallest positive numbers in the system are xmax = βemax(β−β1−t)\nand xmin = βemin, respectively. Two other important quantities are u = 1\n2 β1−t, the\nunit roundoﬀ, and ϵ = β1−t, the machine epsilon, which is the distance from 1 to\nthe next larger ﬂoating-point number.\nNumbers with e = emin and 0 < m < βt−1 are called subnormal numbers. They\nhave the minimal exponent but fewer than t digits of precision. They form an\nequally spaced grid between 0 and the smallest normalized number.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n354\nN. J. Higham and T. Mary\nSign\nExponent\nSigniﬁcand\n5 bits\n10 (+1) bits\nfp16\nSign\nExponent\nSigniﬁcand\n8 bits\n7 (+1) bits\nbﬂoat16\nFigure 2.1. Comparison of the 16-bit bﬂoat16 and fp16 ﬂoating-point number\nformats. The ‘+1’ refers to the implicit leading bit of the signiﬁcand, which is not\nstored.\nThe parameters for the IEEE formats are given in Table 2.1. We refer to these\nformats as ‘fpxy’, where the integer ‘xy’ speciﬁes the number of bits in a ﬂoating-\npoint number (the IEEE standard uses the terminology ‘binaryxy’).\nWhat is perhaps most striking is the great diﬀerence in ranges [xs\nmin, xmax]\nbetween the formats, and especially the narrow range [xmin, xmax] ≈[6 × 10−5, 6 ×\n104] for fp16. This means that a large proportion of fp32 and fp64 numbers are\nnot representable as ﬁnite, non-zero fp16 numbers; as we will see, this means that\ncareful scaling is needed in mixed precision algorithms that use fp16.\n2.2. Other arithmetics\nA ﬂoating-point number format called bﬂoat16 was proposed by researchers in the\nGoogle Brain artiﬁcial intelligence research group. Like fp16, it is a 16-bit format,\nbut it allocates bits between the signiﬁcand and exponent diﬀerently: as illustrated\nin Figure 2.1, bﬂoat16 allocates 8 bits for the signiﬁcand and 8 bits for the exponent\nversus 11 bits for the signiﬁcand 5 bits for the exponent for fp16. As shown in\nTable 2.1, the range of bﬂoat16 is very similar to that of fp32 (but not identical,\nbecause of its narrower signiﬁcand), which means that overﬂow in converting to\nbﬂoat16 from higher precisions is much less likely than for fp16. The drawback\nof bﬂoat16 is its low precision: about three decimal digits of precision versus four\nfor fp16. Bﬂoat16 has been taken up by Intel (Intel Corporation 2018), Arm and\nNVIDIA (beginning with the Ampere architecture).\nThere is no generally accepted 8-bit quarter precision format, though suggestions\nhave been made by Moler (2017) (and implemented in MATLAB by Moler2),\nTagliavini et al. (2018) and Wang et al. (2018).\nDouble-double arithmetic is a form of quadruple precision arithmetic in which\na quadruple precision number is represented as the unevaluated sum of two double\nprecision numbers, one representing the higher-order bits of the signiﬁcand and\n2 http://mathworks.com/matlabcentral/ﬁleexchange/59085-cleve-laboratory\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n355\nthe other the lower-order bits (Muller et al. 2018, Section 14.1). Double-double\narithmetic has a long history going back to the 1960s (Li et al. 2002). It is a\n‘poor man’s quadruple precision’ in that it is slightly less accurate than quadruple\nprecision, has roughly the same range as double precision, and does not inherit\nall the desirable properties of the underlying double precision arithmetic (Joldes,\nMuller and Popescu 2017).\nOther ﬂoating-point formats have been proposed for speciﬁc applications. For\nexample, a 9-bit format with a 4-bit signiﬁcand and a 5-bit exponent is proposed\nby O’uchi et al. (2018) for use in deep learning.\n2.3. Availability in hardware and software\nIEEE single and double precision arithmetic began to be widely supported in\nhardware in the late 1980s and early 1990s. In fact, the Intel 8087 coprocessor,\nproduced before the standard was published, partly supported it. In principle, single\nprecision arithmetic operations should run twice as fast as their double precision\ncounterparts, and single precision variables have the beneﬁt of requiring half the\nstorage of double precision ones, resulting in less data movement, but on Intel chips\nsingle precision had no speed advantage over double precision until the late 1990s,\nwhen Streaming SIMD Extensions (SSE) instructions were introduced (Langou\net al. 2006).\nIEEE fp16 arithmetic began to be supported on NVIDIA GPUs in the Max-\nwell architectures (Jetson TX1, 2014) and was included in the subsequent Pascal\n(P100, 2016), Volta (V100, 2017), Turing (T4, 2018) and Ampere (A100, 2020)\narchitectures. Fp16 is also supported on AMD GPUs in the GCN and CDNA\narchitectures.\nBﬂoat16 is supported on Google’s TPUs (Norrie et al. 2021), the NVIDIA\nA100 GPU (Choquette et al. 2021, NVIDIA Corporation 2020), the ARM NEON\narchitecture (ARM 2018) and Armv8-A architecture (ARM 2019), the Fujitsu\nA64FX ARM processor (Dongarra 2020, Sato et al. 2020) and the Intel Xeon\nCooper Lake processors. It is not only high-end devices that support half precision:\nthe Raspberry Pi, which uses the Armv8-A architecture, supports Bﬂoat16 (Groote,\nMorel, Schmaltz and Watkins 2021, Section 7.2.1).\nThe future Chinese Sunway exascale computer is scheduled to have double\nprecision arithmetic running at 1 ExaFlop/s and half precision arithmetic running\nat 4 ExaFlop/s (Gao et al. 2021, Section 4).\nQuadruple precision arithmetic is available almost exclusively in software. It\nis supported by some compilers, such as the GNU Compiler Collection (GCC),3\nand in MATLAB through the Symbolic Math Toolbox4 and the Multiprecision\nComputing Toolbox.5 Quadruple precision arithmetic is supported in hardware on\n3 https://gcc.gnu.org/\n4 http://www.mathworks.co.uk/products/symbolic/\n5 http://www.advanpix.com\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n356\nN. J. Higham and T. Mary\nthe IBM Power9 processor (Trader 2016) and the IBM z13 processor (Lichtenau,\nCarlough and Mueller 2016).\nA set of extended and mixed precision Basic Linear Algebra Subprograms\n(BLAS) known as the XBLAS6 provides extended and mixed precision counter-\nparts of selected level 1, 2 and 3 BLAS (Li et al. 2002).\nThey use extended\nprecision internally, deﬁned to mean a precision at least 1.5 times as accurate as\ndouble precision and wider than 80 bits. The input and output arguments remain\nsingle or double precision variables, but some arguments can be of mixed type (real\nor complex) as well as mixed precision (single or double), and the main visible\ndiﬀerence is an extra input argument that speciﬁes the precision at which internal\ncomputations are to be performed. A reference implementation is provided that\nemploys the double-double format described in the previous subsection.\n2.4. Block fused multiply–adds\nSince the 1990s some processors have provided a fused multiply–add (FMA)\noperation that computes x + yz with just one rounding error instead of two: x + yz\nis essentially computed exactly and then rounded. The motivation for an FMA is\nspeed, as it can be implemented in a pipelined fashion so that it takes about the\nsame time as a single multiplication or addition (Muller et al. 2018, Section 3.4.2).\nIn recent years, mixed precision block FMAs (also known as mixed precision\nmatrix multiply–accumulate accelerators) have become available in hardware. In\ngeneral, such a device takes as input matrices A ∈Rb1×b, B ∈Rb×b2 and C ∈\nRb1×b2, where A and B are provided in a given precision ulow and C is either in\nprecision ulow or in a higher precision uhigh, and computes\nD\n|{z}\nulow or uhigh\n=\nC\n|{z}\nulow or uhigh\n+\nA\n|{z}\nulow\nB\n|{z}\nulow\n,\n(2.2)\nreturning D in precision ulow or uhigh.\nThe tensor cores in the NVIDIA Volta and Turing architectures have b1 = b =\nb2 = 4. They require the matrices A and B to be in the fp16 format, C and the result\nD can be in fp16 or fp32, and internal computations are done in fp32 (Appleyard\nand Yokim 2017). Pictorially, we have\nD\n=\nC\n+\nA\nB\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|          {z          }\nfp16 or fp32\n=\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|          {z          }\nfp16 or fp32\n+\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|          {z          }\nfp16\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|          {z          }\nfp16\n.\n6 https://netlib.org/xblas/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n357\nTable 2.2. Processing units or architectures equipped with mixed precision block\nfused multiply–add accelerators. Matrix dimensions are expressed as b1 × b × b2,\nwhere the matrix product is of a b1 × b matrix with a b × b2 matrix. The input\nand output precisions ulow and uhigh are deﬁned in (2.2). Sources: ARM (2020),\nChoquette et al. (2021), Jouppi et al. (2020, 2021), Norrie et al. (2021).\nYear of release\nDevice\nMatrix dimensions\nulow\nuhigh\n2016\nGoogle TPU v2\n128 × 128 × 128\nbﬂoat16\nfp32\n2017\nGoogle TPU v3\n128 × 128 × 128\nbﬂoat16\nfp32\n2020\nGoogle TPU v4i\n128 × 128 × 128\nbﬂoat16\nfp32\n2017\nNVIDIA V100\n4 × 4 × 4\nfp16\nfp32\n2018\nNVIDIA T4\n4 × 4 × 4\nfp16\nfp32\n2019\nARMv8.6-A\n2 × 4 × 2\nbﬂoat16\nfp32\n2020\nNVIDIA A100\n8 × 8 × 4\nbﬂoat16\nfp32\n8 × 8 × 4\nfp16\nfp32\n8 × 4 × 4\nTensorFloat-32\nfp32\n2 × 4 × 2\nfp64\nfp64\nThe Ampere architecture oﬀers a wider choice of input data types for the tensor\ncores, including bﬂoat16 and fp32 (NVIDIA Corporation 2020).\nOther instances of block FMAs are the matrix units (MXUs) available on Google\nTPUs (Jouppi et al. 2020, 2021, Wang and Kanwar 2019). They use bﬂoat16 rather\nthan fp16 as the low precision format and operate on square matrices of dimension\n128. Google TPUs are not commercially available.\nTable 2.2 summarizes the properties of block FMAs available in some current\nhardware. We note that the details of the computations, such as rounding modes,\nnormalization of intermediate results and whether subnormal numbers are suppor-\nted, are generally not available and so must be inferred from experimentation. Fasi,\nHigham, Mikaitis and Pranesh (2021) investigate NVIDIA tensor cores; among\ntheir ﬁndings is that the inner products within the matrix multiplications use round\ntowards zero for the additions and can be non-monotonic.\n2.5. Multiprecision arithmetic\nMultiprecision ﬂoating-point arithmetic is a built-in feature of Maple7 and Mathem-\natica8 as well as the open-source PARI/GP9 and Sage10 computer algebra systems.\n7 http://www.maplesoft.com\n8 http://www.wolfram.com\n9 http://pari.math.u-bordeaux.fr\n10 http://www.sagemath.org\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n358\nN. J. Higham and T. Mary\nIt is available in MATLAB through the Symbolic Math Toolbox and the Multi-\nprecision Computing Toolbox.\nThe programming language Julia11 (Bezanson,\nEdelman, Karpinski and Shah 2017) supports multiprecision ﬂoating-point num-\nbers by means of the built-in data type BigFloat. For other languages third-party\nlibraries are available.\nPython: mpmath12 (Johansson et al. 2013) and SymPy13 (Meurer et al. 2017).\nC: the GNU Multiple Precision Arithmetic Library14 and the GNU MPFR Lib-\nrary15 (Fousse et al. 2007).\nC++: the BOOST libraries.16\nC++ and Fortran: the ARPREC library (Bailey, Hida, Li and Thompson 2002).\nC++: the MPFUN2020 library (Bailey 2021).17\nThe GNU MPFR Library is used in some of the software mentioned above, and\ninterfaces to it are available for several programming languages. It was originally\nintended for high precisions, though recent work has improved its eﬃciency for\nfp64 and fp128 (Lefèvre and Zimmermann 2017). As the documentation notes,18\nthe default exponent range is wide and ‘subnormal numbers are not implemented\n(but can be emulated).’\nNakata (2021) has produced a multiprecision version of the LAPACK library\ncalled MPLAPACK by translating the LAPACK source code from Fortran to C++.\nMPLAPACK has several options for the underlying arithmetic, including quad-\nruple precision provided by GCC, double-double arithmetic, quad-double arith-\nmetic (which represents a number as the unevaluated sum of four double precision\nnumbers, so has about twice the precision of quadruple precision), the GNU Mul-\ntiple Precision Arithmetic Library and the GNU MPFR Library. The test results\nreported in Nakata (2021) indicate a roughly 1:5:10 ratio of the time for double\nprecision arithmetic, double-double arithmetic and quadruple precision arithmetic\nfor matrix multiplication on an AMD Ryzen multicore CPU.\n2.6. Simulating diﬀerent precisions\nWhen developing mixed precision algorithms one may not have access in hardware\nto all the precisions of interest. Or one may wish to experiment with ﬂoating-\npoint formats not yet supported in hardware.\nIt is therefore useful to be able\n11 http://julialang.org\n12 http://mpmath.org\n13 http://www.sympy.org\n14 http://gmplib.org/\n15 http://www.mpfr.org\n16 http://www.boost.org\n17 https://www.davidhbailey.com/dhbsoftware/\n18 https://www.mpfr.org/mpfr-current/mpfr.html\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n359\nto simulate arithmetics of diﬀerent precisions using arithmetic of a given higher\nprecision available in hardware.\nThis capability has proved particularly useful\nfor half precision arithmetic, since initially fp16 was available only on GPUs and\nbﬂoat16 on Google TPUs. In addition to half precision, support for other binary\nformats speciﬁed by the user via the number of bits in the signiﬁcand and the\nexponent is desirable, as well as support for diﬀerent rounding modes.\nThese\nfeatures diﬀerentiate the simulations from the multiprecision arithmetics described\nin the previous section, some of which are parametrized by the number of base 10\ndigits.\nThe MATLAB function chop19 of Higham and Pranesh (2019) rounds the ele-\nments of a matrix stored in single precision or double precision to a lower precision\nusing one of several forms of rounding, with the result stored in the original preci-\nsion. The target format for the rounding is speciﬁed by the number of bits in the\nsigniﬁcand and the maximum value of the exponent. The bﬂoat16, fp16 and fp32\nformats are built-in. Subnormal numbers can be included or not. Six rounding\nmodes are supported: round to nearest using round to even last bit to break ties (the\ndefault), round towards plus inﬁnity, round towards minus inﬁnity, round towards\nzero, and two forms of stochastic rounding. The chop function makes it easy to\nadapt existing codes to mixed precision by wrapping statements in calls to chop,\nand since the chop function is vectorized, few calls to it are typically needed for\nlinear algebra codes.\nThe library CPFloat20 by Fasi and Mikaitis (2020) oﬀers similar functionality\nto chop for C. It comes with a MEX interface to MATLAB, and calling CPFloat\ncan be faster than calling chop for large matrices. Fasi and Mikaitis (2020) oﬀer\na comparison with some other available packages for simulating low precision\nﬂoating-point arithmetics.\nAnother approach to simulation is to provide a new storage class and overload\noperators to do arithmetic on the class. The fp16 half precision MATLAB class of\nMoler (2017)21 introduces a new data type fp16 that implements the fp16 storage\nformat and overloads some basic functions for fp16 arguments. Arithmetic in this\nclass is slow because of both the overhead of object orientation in MATLAB and\nthe cost of converting to and from the fp16 storage format. Moler has also written\na class vfp16 that allows the partitioning of a 16-bit word between signiﬁcand\nand exponent to be varied, in particular allowing bﬂoat16 to be simulated (Moler\n2019). This class also allows subnormals to be included or not and FMAs to be\ndone within the inner products inside a matrix multiplication.\n19 https://github.com/higham/chop\n20 https://github.com/mfasi/cpﬂoat\n21 http://mathworks.com/matlabcentral/ﬁleexchange/59085-cleve-laboratory\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n360\nN. J. Higham and T. Mary\nHalf precision simulations are available in some other languages.\nJulia: the built-in ﬂoat16 (fp16) datatype and the bﬂoat16 package22 provide sim-\nulations of these half precision arithmetics.\nC++: a header ﬁle for fp16 is available.23\nPython: NumPy provides a ﬂoat16 data type.24\nThe rpe (reduced ﬂoating-point precision) library of Dawson and Düben (2017)\nprovides a derived type and overloaded operators for Fortran and was developed\nfor use in weather and climate modelling. It emulates the speciﬁed precision but\nin general uses the exponent range of double precision.\nWith all these simulations it is important to realize that one might obtain more\naccurate results than for a true low precision computation because certain operations\nmay be done in higher precision. For example, a language that supports matrix\noperations and provides a half precision data type may implement half precision\nmatrix operations by doing them at higher precision and rounding to half precision.\nFor a detailed discussion of the resulting diﬀerences in accuracy, see Higham and\nPranesh (2019, Section 3).\n3. Rounding error analysis model\nWe denote by fl the operator that rounds a real number into the ﬂoating-point\nnumber system F whose elements are given by (2.1). We recall that if x is in the\nrange of F,\nfl(x) = x(1 + δ),\n|δ| ≤u,\nwhere u is the unit roundoﬀ(Higham 2002, Theorem 2.2).\nUnless otherwise\nstated, when the argument of fl is an expression expr, fl(expr) denotes the result of\nevaluating that expression in ﬂoating-point arithmetic.\nWe will use the standard model of ﬂoating-point arithmetic (Higham 2002,\nSec 2.2), which states that\nfl(x op y) = (x op y)(1 + δ),\n|δ| ≤u,\nop = +, −, ∗, /.\n(3.1)\nThis model is certainly satisﬁed by IEEE arithmetic (in the absence of underﬂow\nor overﬂow), which deﬁnes fl(x op y) to be the rounded exact value.\nA constant that appears in rounding error analyses is\nγn =\nnu\n1 −nu\n(nu < 1).\nWe will use the notation u16, u32, u64 and u128 to denote the unit roundoﬀs\ncorresponding to IEEE arithmetics with the indicated word sizes. These values are\ngiven in the third column of Table 2.1.\n22 https://github.com/JuliaMath/BFloat16s.jl\n23 http://half.sourceforge.net/\n24 https://numpy.org/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n361\nThe rounding error bounds we state in this paper are mostly worst-case bounds\nand can be very pessimistic. For blocked algorithms, worst-case bounds that are\nsmaller by a factor equal to the block size can be obtained for many algorithms,\nas explained by Higham (2021). Moreover, under suitable assumptions on the\nrounding errors, probabilistic bounds with constants that are the square roots of\nthe constants in the worst-case bounds can be obtained; see Section 4.3. These\nobservations are important because for low precisions a constant nu (say) in a\nworst-case rounding error bound can exceed 1 even for very modest n.\n4. Matrix multiplication\nIn this section we consider the computation of C = AB, where A ∈Rm×n and\nB ∈Rn×p are two general matrices. If all operations are carried out in a uni-\nform precision u, the computed b\nC satisﬁes the standard bound (Higham 2002,\nSection 3.5)\n| b\nC −C| ≤γn|A||B|,\n(4.1)\nwhere |A| denotes the matrix of absolute values, (|aij|).\nThe presence of the dimension n in bound (4.1), which reﬂects the fact that\nrounding errors accumulate along the inner dimension, may prevent the compu-\ntation from achieving suﬃcient accuracy when n is large or u is large. Various\napproaches have therefore been proposed to reduce the eﬀect of error accumulation,\nand mixed precision arithmetic is at the heart of several of them.\n4.1. Using block FMAs\nA matrix product can be computed with the aid of a block FMA (2.2). We will\nassume that the internal computations are done at precision uhigh.\nBlock FMAs can be chained together by taking the output D at precision uhigh\nand using it as the input C to a subsequent FMA. Block FMAs thereby provide a\nnatural way to mitigate error accumulation, as accumulation occurs at the level of\nuhigh, not ulow. The required conversion of A and B to ulow is the only source of\nerror of order ulow, and it does not depend on the matrix dimensions.\nAlgorithm 4.1 shows how to use a block FMA to compute a general matrix\nproduct. Three precisions are in play: the working precision u and the precisions\nulow and uhigh, where uhigh ≤ulow.\nThe following error bound, a special case of Blanchard et al. (2020b, The-\norem 3.2), describes the result of Algorithm 4.1.\nTheorem 4.1.\nLet the product C = AB of A ∈Rm×n and B ∈Rn×t, given in\nprecision u, be evaluated by Algorithm 4.1, where q = n/b. The computed b\nC\nsatisﬁes\n| b\nC −C| ≤(2ulow + u2\nlow + nuhigh + O(uhighulow))|A||B|.\n(4.2)\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n362\nN. J. Higham and T. Mary\nAlgorithm 4.1. Let A ∈Rm×n and B ∈Rn×t, given in precision u, be partitioned\ninto b1 × b blocks Aij and b× b2 blocks Bij, respectively, where p = m/b1, q = n/b\nand r = t/b2 are assumed to be integers. This algorithm performs the matrix\nmultiplication C = AB using a block FMA.\n1\neA ←fllow(A), eB ←fllow(B)\n2 for i = 1: p\n3\nfor j = 1: r\n4\nCij = 0\n5\nfor ℓ= 1: q\n6\nCompute Cij = Cij + eAiℓeBℓj using a block FMA with output\nat precision uhigh.\n7\nend\n8\nConvert Cij to precision u.\n9\nend\n10 end\nTheorem 4.1 is applicable to NVIDIA Volta and Turing tensor cores with b = 4,\nulow = u16 and uhigh = u32. The theorem is also applicable to the latest Ampere\ngeneration of NVIDIA GPUs, where A and B can also be stored in bﬂoat16 or\ntﬂoat32 arithmetics.25\nWe note that a more general error analysis is given in Blanchard et al. (2020b,\nTheorem 3.2) that allows for a diﬀerent precision in the internal block FMA eval-\nuation.\nOptimized low precision BLAS are available in vendor libraries, such as in\nNVIDIA’s cuBLAS library. Open-source implementations are also available, such\nas that of San Juan et al. (2021), who target the ARM v8.2 architecture, and\nAbdelfattah, Tomov and Dongarra (2019a), who provide batched multiplication\nroutines for NVIDIA GPUs. A batched operation is one in which many independent\noperations on small matrices are grouped together and carried out by a single\nroutine, and the batched BLAS standard described by Abdelfattah et al. (2021b)\nincludes half precision and quadruple precision data types.\n4.2. Blocked summation\nIn the absence of block FMA hardware with internal computations in higher pre-\ncision, reduced error bounds can still be achieved by changing the summation\nalgorithm used to compute each element cij = Ín\nk=1 aikbk j. In particular, blocked\nalgorithms, which are widely used in numerical linear algebra, compute the sum\ns = Ín\nk=1 xk by grouping summands xk into blocks of size b. Partial sums of b\n25 Tﬂoat32 is a format introduced by NVIDIA for use in tensor cores that has the range of fp32 and\nthe precision of fp16.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n363\nsummands are ﬁrst computed independently, before being combined into the ﬁnal\nresult. By doing so, the term γn in the error bound (4.1) is reduced to γb+n/b−1,\nbecause rounding errors incurred in diﬀerent blocks do not accumulate. Indeed,\nin forming cij, precisely (n/b)(b −1) = n −n/b of the additions are carried out in\ncomputing the partial sums, and these account for the term bu in the bound. Only\nthe last n/b −1 additions account for the error term (n/b −1)u. This observation\ncreates an opportunity for mixed precision arithmetic: by computing these last\nn/b −1 additions in higher precision (say, in precision u2), we can obtain an error\nbound independent of n to ﬁrst order. The next result is a special case of Blanchard\net al. (2020a, Theorem 4.2).\nTheorem 4.2.\nLet the product C = AB of A ∈Rm×n and B ∈Rn×p be evaluated\nby computing the inner products cij = Ín\nk=1 aikbk j by blocks of size b, where\npartial sums of each block are computed in precision u before being combined in\nprecision u2. The computed b\nC satisﬁes\n| b\nC −C| ≤((b + 1)u + (n/b + b2 −1)u2 + O(u3))|A||B|.\n(4.3)\nThis mixed precision summation algorithm is an instance of the FABsum al-\ngorithm (Blanchard et al. 2020a), which computes the partial sums with a fast\nsummation and combines them with an accurate summation. The reduction of the\nerror bound is achieved at a modest extra cost, because most of the additions are\nstill carried out in precision u.\nFasi et al. (2022) implement FABsum on NVIDIA GPUs using the CUTLASS26\nlibrary to improve the accuracy of multiword matrix multiplication (see Section 13).\nTheir implementation achieves an improved performance–accuracy tradeoﬀcom-\npared with cuBLAS: depending on the choice of block size and precisions, FABsum\ncan be either as fast as cuBLAS with fp16 tensor cores, but more accurate, or as\naccurate as cuBLAS with fp32 arithmetic, but faster.\n4.3. Probabilistic analyses\nThe bounds (4.1)–(4.3) are worst-case bounds and they do not reﬂect the fact that\nrounding errors of opposite signs can partially cancel each other. Under some\nassumptions on the rounding errors, probabilistic error analyses (Connolly et al.\n2021, Connolly and Higham 2022, Higham and Mary 2019a, 2020, Ipsen and Zhou\n2020) show that the dimension-dependent constants in the bounds can be replaced\nby their square roots. The underlying assumptions of these analyses are not always\nsatisﬁed; one way to enforce them is to use stochastic rounding (Connolly et al.\n2021).\nIn the case where the matrices A and B are generated by sampling their entries\nfrom a random distribution, the sharper analysis of Higham and Mary (2020) shows\nthat the means of the entries play an important role. Speciﬁcally, for zero-mean\n26 https://github.com/nvidia/cutlass\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n364\nN. J. Higham and T. Mary\ndata, the error bound is independent of n. Therefore, given a general matrix, a\nnatural idea is to shift its entries so that they have zero mean. Computing the\nproduct C = AB of the shifted matrices and shifting back the result can provide a\nmuch more accurate result (Higham and Mary 2020, Theorem 4.2). Shifting back\nthe result has negligible cost for large dimensions, but it must be carried out in\nhigher precision.\n4.4. Multiword matrix multiplication with block FMAs\nThe emergence of block FMA hardware that allows for accumulation in higher\nprecision (see Section 2.4) has provided new opportunities to eﬃciently implement\nmatrix multiplication using multiword arithmetic, such as double-fp16 arithmetic,\nwhich approximates fp32 arithmetic by representing numbers as the unevaluated\nsum of two fp16 numbers. These approaches are described in Section 13.\n4.5. Data-driven matrix–vector product\nRecently, various inner product and matrix–vector product algorithms have been\nproposed based on the idea of storing each element in a precision adapted to its\nmagnitude. These approaches are described in Section 14.4.\n5. Non-linear equations\nConsider a system of non-linear algebraic equations\nF(x) = 0,\nF : Rn →Rn.\n(5.1)\nMany problems of interest can be formulated in this form, so we consider the use of\nmixed precision arithmetic in this general context before specializing to particular\nproblems.\nSuppose we have an iteration xk+1 = g(xk) that generates a sequence of vectors\nx0, x1, . . . converging to a solution x∗. An obvious idea is to use on each iteration\narithmetic of the lowest available precision that equals or exceeds the accuracy of\nthe iterates. Therefore we use low precision arithmetic for the early iterations and\nincrease the precision as the iteration proceeds, until the last few iterations are done\nat the working precision. The justiﬁcation is that the rounding errors committed\non the early iterations should be dominated by the inherent iteration errors. If the\niteration is globally convergent this approach should produce a solution of quality\nas good as if the working precision were used throughout, because such an iteration\ndamps out errors, and each iterate xk can be regarded as restarting the iteration\nwith a new starting value.\nThe possible gain in speed from using mixed precision arithmetic in this way\ndepends on the number of iterations required, which in turn depends on the rate\nof convergence and on the cost per iteration. Consider a quadratically convergent\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n365\niteration and a working precision of double. If at the end of the ﬁrst iteration the\nerror is 10−1, the subsequent errors will ideally be 10−2, 10−4, 10−8 and 10−16.\nWe might carry out the ﬁrst three iterations at half precision, the fourth at single\nprecision and the ﬁfth at double precision. Assuming each iteration requires the\nsame number of operations and a ratio of 1:2:4 for the costs of half, single and double\nprecision arithmetic, the overall cost will be a fraction (3/4 + 1/2 + 1)/5 = 9/20\nof the cost of carrying out all the iterations in double precision. A greater speedup\nwill be obtained if a greater proportion of the iterations are carried out at lower\nprecisions, as will be the case if the initial non-asymptotic convergence phase is\nlong, but in this case alternative iterations or methods might be more eﬃcient. The\nassumption that each iteration requires the same number of operations may not\nhold, as we will see in Section 6, and this is why greater speedups are possible.\nVarying the precisions in the way just described does not always work.\nIn\nparticular, it fails for iterations for matrix functions that are not self-correcting,\nsuch as the Newton iteration for the unitary polar factor (a solution to X∗X = I):\nXk+1 = (Xk + X−∗\nk )/2, X0 = A ∈Cn×n (Higham 1986; Higham 2008, Chapter 8).\nThe iteration formula is independent of A, so if we perturb Xk →Xk + Ek with\na general Ek with ∥Ek∥≫u (as opposed to the specially structured errors that\nappear in the exact arithmetic iteration), then important information about A has\nbeen lost and convergence to a matrix with error of order u cannot be obtained.\n5.1. Newton’s method\nNewton’s method is an excellent method for solving (5.1) and it includes various\nparticular methods of interest as special cases. Therefore we will carry out an\nanalysis of Newton’s method in mixed precision arithmetic. Because the analysis\nis general, it may be suboptimal for particular problems, but it will reveal features\ncommon to all.\nWe suppose that F is continuously diﬀerentiable and denote by J its Jacobian\nmatrix (∂Fi/∂xj). Given a starting vector x0, Newton’s method for (5.1) generates\na sequence {xi} deﬁned by\nJ(xi)(xi+1 −xi) = −F(xi),\ni ≥0.\n(5.2)\nAs is well known, under appropriate conditions xi converges to a solution x∗\nfrom any starting vector x0 suﬃciently close to x∗, and the rate of convergence\nis quadratic if J(x∗) is non-singular (Dennis and Schnabel 1983, Theorem 5.2.1).\nWe consider the mixed precision implementation of Newton’s method given in\nAlgorithm 5.1. Here, we evaluate f in a possibly higher precision ur and solve\nfor the update di at a possibly lower precision uℓ(hoping that the resulting errors\nare damped out). In this and the later algorithms, imax is a limit on the number of\niterations and ‘or until converged’ means that the iteration will be terminated if an\nunspeciﬁed convergence test based on the residual or an estimate of the forward\nerror is satisﬁed.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n366\nN. J. Higham and T. Mary\nAlgorithm 5.1. Newton’s method for F(x) = 0 with starting vector x1, in preci-\nsions uℓ, u and ur (ur ≤u ≤uℓ).\n1 for i = 1: imax or until converged\n2\nCompute fi = F(xi) in precision ur.\n3\nSolve J(xi)di = −fi in precision uℓ.\n4\nxi+1 = xi + di at precision u.\n5 end\nAccounting for rounding and approximation errors, we can write the computed\niterates bxi as\nbxi+1 = bxi −(J(bxi) + Ei)−1(F(bxi) + ei) + ϵi.\n(5.3)\nThe error terms are explained as follows.\n• ei is the error made in computing F(bxi), and we assume that there is a function\nψ depending on F, bxi, u and ur such that\n∥ei∥≤u∥F(bxi)∥+ ψ(F, bxi, u, ur).\n(5.4)\n• Ei combines the error incurred in forming J(bxi) with the backward error for\nsolving the linear system for di. We assume that\n∥Ei∥≤φ(F, bxi, n, uℓ, u),\n(5.5)\nfor some function φ that reﬂects both the (in)stability of the linear system\nsolver and the error made when approximating or forming J(bxi). In practice,\nwe certainly have φ(F, bxi, n, uℓ, u) ≥u∥J(bxi)∥.\n• ϵi is the rounding error made when adding the correction bdi to bxi, so\n∥ϵi∥≤u(∥bxi∥+ ∥bdi∥).\nThe norm is any absolute vector norm (one for which ∥|v|∥= ∥v∥for all v) and\nthe corresponding subordinate matrix norm.\nNote that (5.3) is a very general model, and with a suitable choice of Ei it\nyields modiﬁed Newton methods, in which the Jacobian is held constant for several\niterations in order to reduce the cost of the method.\nWe wish to know how the precisions aﬀect (a) suﬃcient conditions for con-\nvergence and (b) the limiting accuracy and limiting residual, that is, how small\n∥x∗−bxi∥and ∥F(bxi)∥are guaranteed to become as i increases, where x∗is the\nsolution to which the iteration would converge in the absence of errors.\nWe will assume that J is Lipschitz continuous with constant θL, that is,\n∥J(v) −J(w)∥≤θL∥v −w∥\nfor all v, w ∈Rn.\nAnalyses of the eﬀects of diﬀerent sources of error on Newton’s method are\navailable in the literature, for example in Kelley (1995, Section 5.4) and Kelley\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n367\n(2022). Most useful for our purposes are results of Tisseur (2001). The results\nwere originally stated for the situation where just two precisions are in use (uℓ= u),\nbut they are general enough to support a third precision uℓas well. The ﬁrst result\nbounds the limiting accuracy. Here we use the condition number κ(A) = ∥A∥∥A−1∥.\nTheorem 5.1 (Tisseur).\nAssume that there is an x∗such that F(x∗) = 0 and\nJ∗= J(x∗) is non-singular with\nκ(J∗)u ≤1\n8.\n(5.6)\nAssume also that for φ in (5.5),\n∥J(bxi)−1∥φ(F, bxi, n, uℓ, u) ≤1\n8\nfor all i.\n(5.7)\nThen, for all x0 such that\nθL∥J−1\n∗∥∥x0 −x∗∥≤1\n8,\n(5.8)\nNewton’s method in ﬂoating-point arithmetic generates a sequence {bxi} satisfying\n∥bxi+1 −x∗∥≤αi∥bxi −x∗∥+ βi,\n(5.9)\nwhere\nαi ≈∥J(bxi)−1Ei∥+ ∥J−1\n∗∥∥bxi −x∗∥+ κ(J∗)u,\nβi ≈∥J−1\n∗∥∥ψ(F, bxi, u, ur)∥+ u∥x∗∥,\nand the normwise relative error decreases until the ﬁrst i for which\n∥bxi+1 −x∗∥\n∥x∗∥\n≈∥J−1\n∗∥\n∥x∗∥ψ(F, x∗, u, ur) + u.\n(5.10)\nAs a check, we note that in the absence of errors, the terms u, ψ(F, v, u, ur)\nand φ(F, v, n, uℓ, u) are all zero and thus Theorem 5.1 implies local quadratic\nconvergence of Newton’s method.\nIn words, Theorem 5.1 says that if J(x∗) is not too ill-conditioned, the Jacobian\nevaluation and the solver are not too inaccurate, the Lipschitz constant θL is not\ntoo large and the initial guess x0 is not too far from x∗, then the limiting accuracy is\nproportional to the condition of the Jacobian at the solution and the accuracy with\nwhich the residual is evaluated. Note that the function φ does not appear in (5.10),\nwhich shows that errors in forming J and solving the linear system do not aﬀect\nthe limiting accuracy, provided they are not too large. The αi term in (5.9) shows\nthat these errors do, however, aﬀect the rate of convergence, and that this rate is\nessentially independent of ur.\nThe next result bounds the limiting residual.\nTheorem 5.2 (Tisseur).\nUnder the assumptions of Theorem 5.1, if\nθL∥J−1\n∗∥(∥J−1\n∗∥ψ(F, x∗, u, ur) + u∥x∗∥) ≤1\n8,\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n368\nN. J. Higham and T. Mary\nthen, for all x0 such that (5.8) holds, the sequence {∥F(bxi)∥} of residual norms\ngenerated by Newton’s method in ﬂoating-point arithmetic decreases until\n∥F(bxi+1)∥≈ψ(F, bxi, u, ur) + u∥J(bxi)∥∥bxi∥.\n(5.11)\nTheorem 5.2 shows that, under very similar conditions to those in Theorem 5.1,\nthe limiting residual is at the level of the error made in computing the residual plus\nthe term u∥J(bxi)∥∥bxi∥. This latter term is inevitable: from the Taylor series\nF(x∗+ ∆x∗) = F(x∗) + J(x∗)∆x∗+ O(∥∆x∗∥2),\nwe see that merely rounding the exact solution to ex∗= x∗+ ∆x∗, so that ∥∆x∗∥≤\nu∥x∗∥, gives\n∥F(ex∗)∥≲∥J(x∗)∥∥∆x∗∥≤u∥J(x∗)∥∥x∗∥.\nJust as for the limiting accuracy, the limiting residual does not depend on the errors\nin evaluating J or in solving the linear systems.\nSince the limiting accuracy and limiting residual both depend on ψ, Theorems 5.1\nand 5.2 conﬁrm the folklore that Newton’s method must be provided with good\nfunction values if it is to work well in practice.\nAs an application, we consider a linear system F(x) = b −Ax = 0, where\nA ∈Rn×n is non-singular. In principle, Newton’s method converges in one step,\nbut in the presence of errors it becomes an iterative method, namely iterative\nreﬁnement. Here, we have θL = 0. Computing F at precision ur and rounding to\nprecision u gives\nψ(F, bxi, u, ur) ≈γr\nn+1(∥b∥+ ∥A∥∥bxi∥),\nwhere γr\nn+1 = (n + 1)ur/(1 −(n + 1)ur). Hence Theorem 5.1 shows a limiting\naccuracy\n∥bxi+1 −x∗∥\n∥x∗∥\n≈∥A−1∥\n∥x∗∥γr\nn+1(∥b∥+ ∥A∥∥bxi∥) + u\n≲2κ(A)γr\nn+1 + u,\nsince ∥bxi∥≈∥x∗∥and ∥b∥≤∥A∥∥x∗∥. Hence, if ur = u2, the limiting accuracy is\nof order u for κ(A) < u−1. Theorem 5.2 gives a limiting residual\n∥b −Abxi+1∥≈γr\nn+1(∥b∥+ ∥A∥∥bxi∥) + u∥A∥∥bxi∥\n≲(nur + u)(∥b∥+ ∥A∥∥bxi∥),\nwhich means a backward error of order nur + u. Both theorems require (5.7) to\nhold, and since we expect φ to be proportional to uℓfor a solver in precision uℓ,\nthis condition is essentially of the form cnκ(A)uℓ< 1 for some constant cn.\nThis very general analysis of Newton’s method for Ax = b provides signiﬁcant\ninsight into mixed precision iterative reﬁnement, even though we have not speciﬁed\nthe details of the solver. We will derive more speciﬁc and detailed results in the\nnext section.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n369\n6. Iterative reﬁnement for Ax = b\nWe consider the general iterative reﬁnement algorithm given in Algorithm 6.1 for\nsolving a non-singular linear system Ax = b, based on the use of precisions ur ≤u\nand uℓ≥u in addition to the working precision u. The method used to solve for\nthe update vector di on line 3 is arbitrary; we will specialize to particular solvers\nin the following sections.\nFor a discussion of stopping tests see Higham (2002, Section 12.3).\nAlgorithm 6.1. Given a non-singular matrix A ∈Rn×n, b ∈Rn and an initial\napproximation x1, this algorithm uses iterative reﬁnement to solve Ax = b. The\nalgorithm uses three precisions satisfying ur ≤u ≤uℓ.\n1 for i = 1: imax or until converged\n2\nCompute ri = b −Axi in precision ur.\n3\nSolve Adi = ri at precision uℓ.\n4\nUpdate xi+1 = xi + di in precision u.\n5 end\nWe denote the relative error in the solution computed on line 3 by\nξi = ∥di −bdi∥∞\n∥di∥∞\n.\n(6.1)\nLet\nµi =\n∥A(xi −bxi)∥∞\n∥A∥∞∥x1 −bxi∥∞\n≤1,\n(6.2)\nφi = 2 min(cond(A), κ∞(A)µi)uℓ+ ξi,\n(6.3)\nwhere the condition number cond(A) = ∥|A−1||A| ∥∞. We also need the condition\nnumber\ncond(A, x) = ∥|A−1||A||x| ∥∞\n∥x∥∞\n.\nNote that cond(A, x) ≤cond(A) ≤κ∞(A). The next result is by Carson and Higham\n(2018, Corollary 3.3).\nTheorem 6.1.\nLet Algorithm 6.1 be applied with any x1 to a linear system Ax = b,\nwhere A ∈Rn×n is non-singular. As long as φi in (6.3) is suﬃciently less than\n1, the forward error is reduced on the ith iteration by a factor of approximately φi\nuntil an iterate bx is produced for which\n∥bx −x∥∞\n∥x∥∞\n≲u + 4p cond(A, x)ur,\n(6.4)\nwhere p is the maximum number of non-zeros in any row of [A b].\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n370\nN. J. Higham and T. Mary\nTheorem 6.1 shows that the limiting accuracy (6.4) depends on the precisions\nu and ur but does not depend on the precision uℓ, on x1, or on how the system\nAdi = ri at line 3 is solved, provided that it is solved with some relative accuracy\nξi ≪1.\nThe limiting accuracy (6.4) motivates the use of extended precision in the com-\nputation of the residual. Indeed, if we set ur = u2, we obtain a limiting accuracy of\norder u, independent of the conditioning of the problem as long as cond(A, x)u ≤1.\n6.1. Historical development\n6.1.1. Traditional iterative reﬁnement\nIterative reﬁnement was programmed on a digital computer by Wilkinson (1948,\np. 111 ﬀ.), using LU factorization with partial pivoting as the solver. Wilkinson,\nand subsequent authors, took advantage in computing the residual of the ability\nof many machines of the time to accumulate inner products at twice the working\nprecision at little or no extra cost (as discussed at the start of Section 2). The\nmethod was also used by Wilkinson and colleagues on desk calculating machines,\nmaking use of their extra length accumulators in computing residuals (Fox, Huskey\nand Wilkinson 1948).\nIterative reﬁnement with extra precision residuals fell out of favour in the 1970s\nbecause machines began to lack the ability to accumulate inner products in extra\nprecision. Indeed the LINPACK library did not include it because it could not be\nimplemented in a portable way in Fortran (Dongarra, Bunch, Moler and Stewart\n1979).\n6.1.2. Fixed precision iterative reﬁnement\nAs the traditional form of iterative reﬁnement declined in popularity, another\nusage came to the fore: ﬁxed precision reﬁnement, in which only one precision is\nused. Jankowski and Woźniakowski (1977) proved that an arbitrary linear equation\nsolver is made normwise backward stable by the use of ﬁxed precision iterative\nreﬁnement, as long as the solver is not too unstable to begin with and A is not too\nill-conditioned. Skeel (1980) analysed ﬁxed precision iterative reﬁnement for LU\nfactorization with partial pivoting and showed that one step of reﬁnement yields a\nsmall componentwise backward error under suitable conditions. Higham (1991)\nextended the componentwise backward error analysis of ﬁxed precision iterative\nreﬁnement to a general solver, and Higham (1997) gave an analysis that covers the\ntraditional and ﬁxed precision forms and a general solver.\n6.1.3. Iterative reﬁnement with lower precision solves\nIn the 2000s, hardware emerged in which fp32 arithmetic was much faster than\nfp64 arithmetic, such as Intel chips with SSE instructions (a factor about 2) and the\nSony/Toshiba/IBM (STI) Cell processor (a factor up to 14) (Kurzak and Dongarra\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n371\n2007). Motivated by this speed diﬀerence, Langou et al. (2006) proposed a new\nusage of iterative reﬁnement in which LU factors computed at a precision lower\nthan the working precision (speciﬁcally, single versus double precision) are used\nto solve on line 3 in Algorithm 6.1.\nCarson and Higham (2017) showed how preconditioned GMRES can be ex-\nploited on line 3 in Algorithm 6.1, giving an algorithm called GMRES-based\niterative reﬁnement (GMRES-IR). Carson and Higham (2018) proposed a three-\nprecision version of iterative reﬁnement, essentially Algorithm 6.1, and gave de-\ntailed convergence analysis for the backward error (normwise and componentwise)\nand the forward error.\nAmestoy et al. (2021b) extended the analysis of Carson and Higham (2018) to a\nﬁve-precision form of GMRES-IR.\nMore details of these works are given in Sections 7 and 8.\n6.2. Specialized applications\nMuch work has been done on specializing iterative reﬁnement to particular contexts.\nWe mention just a few examples.\nGovaerts and Pryce (1990) develop and analyse an iterative reﬁnement-based\nalgorithm for solving bordered linear systems Ax = b of order n in which a\nblack box solver is assumed to be available for systems involving the submatrix\nA(1: n −1, 1: n −1). One application is to numerical continuation problems.\nIn some structured problems the elements of A are never formed and so resid-\nuals cannot be computed in the usual way via matrix–vector multiplication. An\nexample is when A is a Vandermonde matrix and a fast O(n2) ﬂops algorithm\ntailored to the structure is being used. Higham (1988) develops algorithms for\nsolving Vandermonde-like systems where aij = pi−1(αj), with {pi(x)}n−1\ni=0 a set of\npolynomials satisfying a three-term recurrence relation, such as orthogonal poly-\nnomials. The algorithms are numerically unstable for the Chebyshev polynomials,\nbut one step of iterative reﬁnement at the working precision is found to give stabil-\nity. The residual is evaluated by a nested multiplication algorithm for orthogonal\npolynomials.\nBy steadily increasing the precision during the iterative reﬁnement process it is\npossible to compute solutions to arbitrarily high accuracy, assuming that arithmetic\nof suitable precisions is available.\nThis idea, ﬁrst suggested in an exercise by\nStewart (1973, pp. 206–207) has been investigated by Kiełbasiński (1981) and\nSmoktunowicz and Sokolnicka (1984).\n7. Direct methods for Ax = b\nIn this section we discuss the solution of linear systems by direct methods based\non a factorization of the matrix.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n372\nN. J. Higham and T. Mary\n7.1. LU factorization-based iterative reﬁnement\nAlgorithm 7.1 is a version of Algorithm 6.1 based on an LU factorization of A,\nhereinafter referred to as LU-IR. The LU factorization is computed in precision\nuℓand is used to compute the initial solution x1 and solve the update equation on\nline 5.\nThe only line of the algorithm that costs O(n3) ﬂops is the ﬁrst line, as the\nsubstitutions cost only O(n2) ﬂops. The factorization is carried out at precision uℓ,\nso if uℓ≫u, then if the iteration converges quickly the algorithm is potentially\nsigniﬁcantly faster than solving Ax = b by LU factorization at precision u.\nAlgorithm 7.1 (LU-IR). Given a non-singular matrix A ∈Rn×n and b ∈Rn, this\nalgorithm uses LU factorization-based iterative reﬁnement with three precisions\nsatisfying ur ≤u ≤uℓ, to solve Ax = b.\n1 Compute the factorization A = LU in precision uℓ.\n2 Solve LUx1 = b by substitution in precision uℓ.\n3 for i = 1: imax or until converged\n4\nCompute ri = b −Axi in precision ur.\n5\nSolve LUdi = ri by substitution in precision uℓ.\n6\nUpdate xi+1 = xi + di in precision u.\n7 end\nStandard error analysis (Higham 2002, Theorem 9.4) shows that solving a lin-\near system by substitution in precision uℓwith LU factors computed in precision\nuℓachieves a relative error bounded approximately by 3n∥|A−1||bL|| b\nU|∥uℓ. The-\norem 6.1 therefore yields the following result.\nTheorem 7.1.\nLet LU-IR (Algorithm 7.1) be applied to a linear system Ax = b,\nwhere A ∈Rn×n is non-singular. If ∥|A−1||bL|| b\nU|∥uℓis suﬃciently less than 1, then\nthe algorithm produces an iterate bx satisfying (6.4).\nAs mentioned in Section 6, the traditional and ﬁxed precision forms of iterative\nreﬁnement use an LU factorization computed by LU factorization with partial\npivoting with uℓ= u.\nWith such a stable LU factorization, the convergence\ncondition in Theorem 7.1 reduces to κ(A)u ≪1. However, as already mentioned,\nunstable solvers can still lead to convergence. In the context of LU-IR, two main\napproaches have been proposed that introduce potential instability in an attempt to\nincrease speed.\nThe ﬁrst approach is to use a potentially unstable LU factorization in precision\nu, where the instability can come from diﬀerent sources. For example, using a\nweaker form of pivoting to accelerate the factorization and preserve the sparsity of\nthe matrix, such as static pivoting (Li and Demmel 1998, Arioli, Duﬀ, Gratton and\nPralet 2007), can still lead to the convergence of LU-IR. Several sparse direct solv-\ners incorporate static pivoting strategies as an option, such as MUMPS (Amestoy,\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n373\nDuﬀ, L’Excellent and Koster 2001, Amestoy, Buttari, L’Excellent and Mary 2019),\nwhich implements the approach proposed by Duﬀand Pralet (2007), or as the\ndefault, such as SuperLU_DIST (Li and Demmel 2003) and PARDISO (Schenk,\nGärtner, Fichtner and Stricker 2001). Other potentially unstable, but faster, factor-\nizations have been combined with iterative reﬁnement to remedy their instability,\nsuch as incomplete LU factorization (Zlatev 1982) or Cholesky factorization for\nquasideﬁnite systems (Gill, Saunders and Shinnerl 1996).\nThe second approach is to use an LU factorization in lower precision uℓ> u.\nIf the LU factorization algorithm is numerically stable, convergence is guaranteed\nprovided that κ(A)uℓ≪1, as noted above. This approach is attractive because\nmost of the work (O(n3) ﬂops for dense systems) is done in the factorization phase;\nthe iterative phase (O(n2) ﬂops) has negligible cost for large n, as long as the\nnumber of iterations remains reasonable. Thus, asymptotically, we may expect the\nspeed of the entire solution to be determined by the speed of the lower precision\narithmetic. Using the Cell processor, Langou et al. (2006) solve double precision\nlinear systems (u = u64) with speedups of up to a factor of 8 over a double precision\nLU factorization by using LU-IR (Algorithm 7.1) with uℓ= u32 and ur = u.\nFurther experimental results are reported by Buttari et al. (2007) for dense linear\nsystems and by Buttari et al. (2008) for sparse ones. See Baboulin et al. (2009) for\nan overview of the methods developed in this period.\nIterative reﬁnement with LU factorization in lower precision has also been ex-\nploited on FPGAs (Sun, Peterson and Storaasli 2008).\nThe popularity of LU-IR with a lower precision factorization grew again with the\nemergence of half precision arithmetic (fp16 and bﬂoat16). Indeed, half precision\narithmetic is at least four times faster than double precision arithmetic, and possibly\nmuch more than that on some hardware, notably on NVIDIA tensor cores (see\nSection 7.3). Haidar, Wu, Tomov and Dongarra (2017) provide the ﬁrst evaluation\nof the potential of half precision for iterative reﬁnement, obtaining speedups of up\nto 2.7 on an NVIDIA P100 GPU using LU-IR with u = ur = u64 and uℓ= u16.\nKudo et al. (2020a,b) implement LU-IR on the Fugaku supercomputer, which is\nequipped with ARM-based Fujitsu A64FX processors that support fp16 arithmetic,\nfor use in the HPL-AI Mixed Precision Benchmark (see Section 1.2.4).\nLU-IR with a half precision factorization can only guarantee convergence for\nwell-conditioned problems: the condition κ(A)uℓ≪1 translates to κ(A) ≪2000\nin fp16 and κ(A) ≪300 in bﬂoat16. Two main approaches have been proposed\nto extend the applicability of half precision iterative reﬁnement to a wider range\nof problems: the ﬁrst uses a more accurate solver on line 5 of Algorithm 7.1 (see\nSection 7.2) and the second uses more accurate hardware such as tensor cores (see\nSection 7.3).\n7.2. GMRES-based iterative reﬁnement\nGMRES-IR (Carson and Higham 2017), mentioned in Section 6.1.3, is described\nin Algorithm 7.2.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n374\nN. J. Higham and T. Mary\nAlgorithm 7.2 (GMRES-IR). Given a non-singular matrix A ∈Rn×n and b ∈Rn,\nthis algorithm solves Ax = b using by GMRES-IR in ﬁve precisions: ur, ug, up, u\nand uℓ.\n1 Compute the factorization A = LU in precision uℓ.\n2 Solve LUx1 = b by substitution in precision uℓ.\n3 for i = 1: imax or until converged\n4\nCompute ri = b −Axi in precision ur.\n5\nSolve U−1L−1Adi = U−1L−1ri by GMRES in precision ug, performing\nthe products with U−1L−1A in precision up.\n6\nCompute xi+1 = xi + di in precision u.\n7 end\nNote that the application of the preconditioner on line 5 involves a multiplication\nby A and substitutions with the LU factors. The results quoted below assume\nthe use of a backward stable implementation of GMRES, such as MGS-GMRES\n(Paige, Rozložník and Strakoš 2006).\n7.2.1. High accuracy solution of ill-conditioned systems\nCarson and Higham (2017) proposed GMRES-IR with two precisions, u = uℓand\nur = ug = up = u2, and were interested in solving ill-conditioned systems to high\naccuracy. They showed that the quantity µi deﬁned in (6.2) tends to be small in\nthe early iterations and gradually grows to order 1 as the iteration proceeds. This\nmeans that in φi in (6.3) the min term is negligible in the early iterations and so\nφi ≈ξi. Carson and Higham (2017) also show that ξi ≈u as long as κ(A) is not\nmuch larger than u−1. Hence Theorem 6.1 guarantees that a limiting accuracy of\norder u will be achieved. In other words, by using a small amount of computation\nat twice the working precision it is possible to solve Ax = b to full accuracy even\nif A is numerically singular!\nIt is important to emphasize that standard methods, such as even the singular\nvalue decomposition (SVD), will not in general yield an accurate solution to an\nill-conditioned system. GMRES-IR computes an accurate solution to the update\nequation, which is relatively well-conditioned thanks to the preconditioning and\nwhich has an accurate right-hand side. The behaviour of µi is also crucial, and it\nhad not been previously been proved or exploited, though Wilkinson (1977) did\nmake an observation that is equivalent to saying that the µi increase with i.\n7.2.2. Exploiting low precision LU factors of an ill-conditioned matrix\nAs mentioned in Section 7.1, one of the main limitations of LU-IR (Algorithm 7.1)\nis that its success is guaranteed only when κ(A)uℓ≪1. If the LU factorization is\ncomputed in low precision, LU-IR is therefore limited to well-conditioned matrices.\nIn this setting, GMRES-IR becomes particularly useful.\nIndeed, even though\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n375\nGMRES-IR was originally intended to solve linear systems nearly singular to the\nworking precision u, as described in the previous subsection, Carson and Higham\n(2018) subsequently proposed using it to exploit LU factors computed in a precision\nuℓ(potentially much) lower than the working precision u. They assume that the\npreconditioner is applied in precision ug = up = u2. Amestoy et al. (2021b) relax\nthis requirement by allowing the products with U−1L−1A to be carried out in a\nprecision up possibly lower than u2, and additionally allow the rest of the GMRES\ncomputations to be performed in a precision ug possibly lower than u. This results\nin the ﬁve-precision algorithm described in Algorithm 7.2. To obtain convergence\nguarantees for this algorithm, Amestoy et al. (2021b) generalize the analysis of\nPaige et al. (2006) on the backward stability of GMRES to a two-precision GMRES\nwith LU preconditioning, and they prove the following theorem.\nTheorem 7.2.\nLet GMRES-IR (Algorithm 7.2) be applied to a linear system\nAx = b, where A ∈Rn×n is non-singular. If κ(A)2u2\nℓ(ug + κ(A)up) is suﬃciently\nless than 1, then the algorithm produces an iterate bx satisfying (6.4).\nAmestoy et al. (2021b) consider the thousands of possible combinations of the\nﬁve precisions in Algorithm 7.2 and narrow down the choice to a few combinations\nof practical interest, from among which one can balance accuracy, robustness and\nperformance. The bounds on κ(A) for convergence guarantees are not always sharp,\nso it can be diﬃcult to decide which variant should be preferred for a particular\nproblem. To address this issue, Oktay and Carson (2022) propose a multistage\niterative reﬁnement that switches to increasingly robust but also more expensive\nvariants by monitoring key quantities during the iterative process.\nHaidar, Tomov, Dongarra and Higham (2018b) and Haidar et al. (2020) imple-\nment GMRES-IR with just two precisions: the factorization is in half precision\n(uℓ) and the rest of the operations are in double precision (ug = up = ur = u).\nThey show that for several matrices where LU-IR takes a large number of iterations\nto converge, GMRES-IR can still converge in a small number of iterations and\nthus retains an attractive performance boost compared with LU-IR with a single\nprecision factorization.\nHigham and Mary (2019b) propose a new preconditioner that builds upon the low\nprecision LU factors and exploits a low-rank approximation to speed up GMRES-\nIR.\n7.3. Harnessing tensor cores\nNVIDIA tensor cores present two beneﬁts for iterative reﬁnement compared with\nstandard half precision arithmetic on NVIDIA GPUs. The ﬁrst is that they are\nsigniﬁcantly faster and so computing the LU factorization with tensor cores provides\nmore room to amortize the cost of the iterations in the iterative phase. The second\nbeneﬁt is their improved accuracy since, as discussed in Section 2.4, tensor cores\naccumulate intermediate operations in fp32 arithmetic.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n376\nN. J. Higham and T. Mary\nTensor cores can carry out arbitrarily sized matrix products using Algorithm 4.1\nand so can be naturally exploited by standard blocked LU factorization algorithms,\nwhich mostly consist of matrix–matrix products. Haidar et al. (2018b) propose\nan algorithm that harnesses tensor cores to accelerate the updates of the trailing\nsubmatrix, which account for the O(n3) ﬂops of the factorization; the remaining\nO(n2) ﬂops are carried out by standard ﬂoating-point units in fp32 arithmetic.\nBlanchard et al. (2020b, Theorem 4.4) analyse this algorithm and prove that it\npossesses a reduced backward error bound of order u16 + nu32 instead of the\nstandard bound nu16 of an LU factorization entirely in fp16 arithmetic.\nUsing their mixed precision LU factorizationalgorithmwithinLU-IRorGMRES-\nIR, Haidar et al. (2018b) are able to solve linear systems with fp64 accuracy at a\nspeed of up to 24 TFLOPS on an NVIDIA V100 GPU, which represents a speedup\nof 4 over a double precision solver. Moreover, for some of the more diﬃcult matrices\nin their test set, the solution entirely in fp16 arithmetic requires many iterations\nor does not converge at all, whereas the algorithm using tensor cores maintains a\nfast convergence. This shows that the accuracy boost of tensor cores can strongly\nimprove the convergence of iterative reﬁnement. The use of half precision and/or\ntensor cores also improves the energy eﬃciency of the solution, reducing power\nconsumption by up to a factor of 5 (Haidar et al. 2018a). See Haidar et al. (2020)\nfor a more complete discussion of iterative reﬁnement with tensor cores, and Ab-\ndelfattah, Tomov and Dongarra (2019b) for an extension of these approaches to\ncomplex matrices. These ideas are implemented in the MAGMA library,27 in the\nNVIDIA cuSOLVER library28 and also in the SLATE library29 (Charara et al.\n2020), which targets machines with large numbers of cores and multiple hardware\naccelerators per node.\nIn addition to its speed and energy beneﬁts, fp16 arithmetic can also be used to\nreduce memory consumption and data movement. However, special care has to be\ntaken not to lose the accuracy boost of tensor cores. Indeed, tensor cores carry out\ncomputations internally in fp32 arithmetic, and so to beneﬁt from their improved\naccuracy the input matrix C in (2.2) needs to be stored in fp32. Lopez and Mary\n(2020) propose a modiﬁcation of the above approaches of Haidar et al., based on a\nleft-looking Crout factorization that allows them to store the matrix in fp16 while\naccumulating computations in fp32 buﬀers of controlled size. As a result, memory\nconsumption is halved and data movement costs are greatly reduced, making the\nfactorization faster by up to a factor of 2 on NVIDIA V100 GPUs.\n7.4. Scaling strategies\nA limitation of iterative reﬁnement with fp16 as the low precision arithmetic is the\nnarrow range of the arithmetic: as seen in Table 2.1, numbers of magnitude outside\n27 https://icl.utk.edu/magma/\n28 https://developer.nvidia.com/cusolver\n29 https://icl.utk.edu/slate/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n377\nthe interval [xs\nmin, xmax] = [5.96 × 10−8, 6.55 × 104] are not representable and will\nunderﬂow or overﬂow when converted to fp16. Moreover, numbers of magnitude\nsmaller than xmin = 6.10 × 10−5 are often ﬂushed to zero in practice, to avoid the\npossibly heavy performance penalty of handling subnormal numbers.\nIn the LU factorization of a matrix A in fp16 arithmetic, overﬂow and underﬂow\nmay occur during the initial conversion of A to fp16 and also during the LU\nfactorization itself. In particular, note that even LU factorization algorithms using\ntensor cores that keep the original matrix in fp32 are not immune to overﬂow and\nunderﬂow, since the LU factors must be converted to fp16.\nOne way to deal with overﬂow in rounding A to fp16 is to replace any element\naij that overﬂows by sign(aij)θxmax, where θ ∈(0, 1] is a parameter. We will refer\nto this as the overﬂow mapping strategy. This approach is used in Haidar et al.\n(2017, 2018a,b).\nHigham, Pranesh and Zounon (2019) suggest Algorithm 7.3, which uses a two-\nsided diagonal scaling at the working precision and only rounds to fp16 once all\nthe matrix elements do not exceed xmax. The algorithm applies row and column\nequilibration, which produces a matrix eA in which every row and column has\nmaximum element in modulus equal to 1. Then it scales eA so that the maximum\nelement in modulus of the scaled matrix is θxmax and rounds to fp16. Here, θ is\nintended to be reasonably close to 1, in order to maximize the use of the limited\nfp16 range and keep the numbers away from the subnormal zone. If A is symmetric,\na symmetry-preserving two-sided scaling of Knight, Ruiz and Uçar (2014) can be\nused instead of row and column equilibration.\nAlgorithm 7.3. This algorithm rounds A ∈Rn×n to the fp16 matrix A(h), scaling\nall elements to avoid overﬂow. θ ∈(0, 1] is a parameter.\n1 R = diag(∥A(i, : )∥−1\n∞)\n2\neA = RA\n% eA is row equilibrated.\n3 S = diag(∥eA(:, j)∥−1\n∞)\n4\neA = eAS\n% eA is now row and column equilibrated.\n5 Let β be the maximum magnitude of any entry of eA.\n6 µ = θxmax/β\n7 A(h) = flh(µeA)\nHow should θ be chosen? The main requirement is that there is no overﬂow in\nthe LU factorization, which means that we need θ ≤ρ−1\nn , where ρn is the growth\nfactor for LU factorization on A. With partial pivoting, ρn is typically not large,\nso one might take θ = 0.1, as used in Higham et al. (2019).\nHowever, large\ngrowth factors can occur, notably for ‘randsvd matrices’ having one small singular\nvalue (Higham, Higham and Pranesh 2021), and this led to poor performance with\nθ = 0.1 in one of the experiments in Haidar et al. (2020, Section 14(b)).\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n378\nN. J. Higham and T. Mary\nHigham et al. (2019) show experimentally that compared with the overﬂow\nmapping strategy, Algorithm 7.3 leads to faster and more reliable convergence of\nGMRES-IR on badly scaled matrices.\nUnless the LU factors are converted to fp32 precision at the end of the fac-\ntorization (or are already available in fp32, such as when using tensor cores), the\nsubstitution operations must also be performed in fp16 arithmetic, and are therefore\nvulnerable to overﬂow and underﬂow, especially as the elements of the residual vec-\ntor ri on line 4 of Algorithm 7.1 must eventually become of order u(∥A∥∥x∥+ ∥b∥),\nand so are likely to underﬂow in fp16. Techniques for avoiding overﬂow in solving\ntriangular systems can be found in Anderson (1991) and Demmel and Li (1994)\n(these are used in the LAPACK subroutine xLATRS), and they can be combined\nwith the simple scaling suggested by Carson and Higham (2018, Section 6) and\nLuszczek, Yamazaki and Dongarra (2019).\n7.5. Exploiting symmetry and positive deﬁniteness\nSuppose, now, that A ∈Rn×n is symmetric positive deﬁnite. In principle, LU-\nIR can be adapted in a straightforward way by replacing LU factorization with\nCholesky factorization. However, there is a problem to overcome: a matrix that\nhas elements stored in a given precision and is symmetric positive deﬁnite may lose\ndeﬁniteness when rounded to a lower precision, and in Algorithm 7.1 we round\nA to precision uℓon the ﬁrst step. We can guarantee to preserve deﬁniteness in\nthe rounding only if κ2(A)uℓ< 1, which is a severe restriction if we are using\nhalf precision. Higham and Pranesh (2021) suggest Algorithm 7.4, which scales\nand shifts in order to ensure a successful Cholesky factorization. The two-sided\nscaling H = D−1AD−1, where D = diag(a1/2\nii ), produces a unit diagonal matrix\nwith oﬀ-diagonal elements bounded in magnitude by 1. This matrix is then shifted\nby an amount intended to lift the smallest eigenvalue suﬃciently above zero, and a\nmultiplicative factor θ is applied that plays the same role as that in Algorithm 7.3.\nAs explained by Higham and Pranesh (2021), shifting H by a multiple of I is better\nthan shifting A by a multiple of I, as it is equivalent to shifting A by a multiple of\ndiag(aii) and so it makes the same relative perturbation to each diagonal element\nof A.\nHigham and Pranesh (2021) give perturbation analysis and error analysis that\nsuggests taking c ≈n2 in Algorithm 7.4, but they ﬁnd this is too pessimistic in\npractice. They recommend taking c as a small constant and found c = 2 to work\nwell in practice, with no need for the doubling on line 8. They use this idea with\nan appropriate modiﬁcation of GMRES-IR in which GMRES is applied to the\npreconditioned update equation M Adi = Mri, where M = µD−1R−1R−T D−1.\nNote that since A is symmetric positive deﬁnite it is more natural to use the\nconjugate gradient (CG) method instead of GMRES, but the supporting rounding\nerror analysis works only for GMRES, because it relies on the backward stabil-\nity of GMRES and preconditioned CG is not guaranteed to be backward stable\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n379\nAlgorithm 7.4. Given a symmetric positive deﬁnite A ∈Rn×n in precision u, this\nalgorithm computes an approximate Cholesky factorization RTR ≈µD−1AD−1 at\nprecision uℓ> u, where D = diag(a1/2\nii ). The scalar θ ∈(0, 1] and the positive\ninteger c are parameters.\n1 D = diag(a1/2\nii ), H = D−1AD−1\n% Set hii ≡1 instead of computing it.\n2 G = H + cuℓI\n3 β = 1 + cuℓ\n4 µ = θxmax/β\n5 Aℓ= flℓ(µG)\n6 Attempt Cholesky factorization Aℓ= RTR in precision uℓ.\n7 if Cholesky factorization failed\n8\nc ←2c, goto line 2\n9 end\n(Greenbaum 1997, eq. (34)). However, Higham and Pranesh (2021) ﬁnd that in\ntheir experiments CG works as well as GMRES.\nAlgorithm 7.4 has been implemented on an NVIDIA V100 GPU by Abdelfattah,\nTomov and Dongarra (2020), eﬀectively taking uℓ= u16, u = ur = u64, with\nCholesky factorization computed in mixed fp16 and fp32 precisions. With matrices\nof dimensions up to 42 000, they obtained speedups of up to 4.7 over a double\nprecision solver.\n7.6. Sparse matrix considerations\nSparsity presents both opportunities and obstacles to the use of iterative reﬁnement.\nOn the one hand, while LU factorization of dense matrices tends to run twice as\nfast in single precision as in double precision, this speedup may not be attained for\nsparse matrices, for two reasons explained by Zounon, Higham, Lucas and Tisseur\n(2022). The ﬁrst reason is that real-life sparse double precision matrices, such as\nmany of those in the SuiteSparse Matrix Collection30 (Davis and Hu 2011), can\nhave elements of widely varying magnitudes. While the matrix elements usually ﬁt\ninto the range of single precision numbers, LU factorization can generate cascading\nﬁll-ins in which small multipliers combine to produce subnormal numbers. This\ncan cause a signiﬁcant performance loss because ﬂoating-point operations on sub-\nnormal numbers can be very slow. A cure is to set a compiler ﬂag to ﬂush subnormal\nnumbers to zero. The second reason why LU factorization of a sparse matrix in\nsingle precision may not give the expected speedup over double precision is that\nthe reordering and analysis phase of the algorithm does not involve ﬂoating-point\narithmetic (Duﬀ, Erisman and Reid 2017) and so does not beneﬁt from reducing\nthe precision. Moreover, if the reordering and analysis is sequential rather than\n30 https://sparse.tamu.edu/. Previously known as the University of Florida Sparse Matrix Collection.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n380\nN. J. Higham and T. Mary\nparallelized, then increasing the number of cores increases the proportion of time\nspent on non-ﬂoating-point arithmetic computations.\nOn the other hand, some of the features of iterative reﬁnement are especially\nattractive when the matrix is sparse. First, as explained by Amestoy et al. (2022),\niterative reﬁnement with a lower precision LU factorization can lead to signiﬁcant\nmemory savings due to the fact that the LU factors of a sparse matrix are typically\nmuch denser, and unlike for dense matrices, the overhead of keeping a high precision\ncopy of the original matrix is negligible. Second, to best preserve the sparsity of\nthe matrix, sparse direct solvers often employ relaxed pivoting strategies, such as\nthreshold partial pivoting (Duﬀet al. 2017, Chapter 7) or the more aggressive static\npivoting (Li and Demmel 1998), which can lead to large growth factors; iterative\nreﬁnement can overcome any resulting numerical instability.\nAmestoy et al. (2022) develop implementations of LU-IR and GMRES-IR based\non a single precision sparse LU factorization computed with the multifrontal solver\nMUMPS and use them to solve with double precision accuracy a range of large and\nill-conditioned sparse systems coming from a variety of applications. They obtain\nreductions of up to a factor of 2 in both execution time and memory consumption\nover the double precision MUMPS solver, with LU-IR being usually faster than\nGMRES-IR, although the latter is more robust and successfully converged for all\ntest problems.\n7.7. Exploiting data sparsity\nIn many applications, the matrix possesses a so-called data sparse structure: many\nof its oﬀ-diagonal blocks have low numerical rank. In the last two decades, several\napproaches have been devised to leverage this property to accelerate the solution\nof linear solvers, such as hierarchical (H) or block low-rank (BLR) methods.\nThe low-rank approximations are computed with a truncation threshold para-\nmeter ε, which controls the accuracy of these data sparse solvers, as proved by\nHigham and Mary (2021) in the case of BLR solvers. Thus data sparse solvers\ncan be used either as direct solvers (setting ε to the target accuracy) or as precon-\nditioners to iterative methods. In particular, they can be used in conjunction with\niterative reﬁnement. Amestoy et al. (2022) use the BLR sparse solver MUMPS\n(Amestoy et al. 2019) at low accuracy with LU-IR and GMRES-IR, and obtain\nlarge performance gains with respect to the double precision solver, reducing ex-\necution time by up to 5.6× and memory consumption by up to 4.4×. Moreover,\nGMRES-IR can converge for larger values of the parameter ε than LU-IR, which\nleads to increased performance in some cases.\nIn addition to their use in lower precision, data sparse solvers can also beneﬁt\nfrom mixed precision. Indeed, data sparse matrices exhibit blocks of highly unequal\nimportance: those that correspond to weak interactions (and that are usually far\naway from the diagonal) contain less signiﬁcant information and are more resilient\nto the use of reduced precision. As a result, Abdulah et al. (2019) propose to store\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n381\nblocks that are suﬃciently far way from the diagonal in single precision instead of\ndouble precision. They apply this strategy to the Cholesky factorization of data\nsparse covariance matrices arising in geostatistical modelling, obtaining an average\n1.6× speedup. The approach is extended to also include half precision in Abdulah\net al. (2022), leading to an improved 2.6× speedup. Doucet, Ltaief, Gratadour and\nKeyes (2019) use the same approach in a diﬀerent application in computational\nastronomy (tomographic reconstructors) using only single and half precisions on\nNVIDIA V100 GPUs.\nTo go even further, in addition to storing diﬀerent blocks in diﬀerent precisions,\neach block can also use a mixed precision representation. Since most of the blocks\nof data sparse matrices exhibit rapidly decaying singular values, they are amenable\nto the mixed precision low-rank representation proposed by Amestoy et al. (2021a)\nand described in Section 12.2. Amestoy et al. (2021a) apply this approach to the\nLU factorization of BLR matrices and obtain storage and ﬂops reductions of up to\na factor of 3 using fp64, fp32 and bﬂoat16 arithmetics.\n8. Iterative methods for Ax = b\nWe outline three classes of approaches to exploit mixed precision arithmetic in\niterative methods.\nThe ﬁrst approach is to use an inner–outer scheme such as\nGMRES-IR, where the low precision is used by the inner scheme (Section 8.1).\nThe second approach is to use low precision arithmetic to compute and/or apply\nthe preconditioner in a higher precision iterative method (Section 8.2). The third\napproach is to intrinsically use mixed precision within the iterative method, such\nas for inexact Krylov methods (Section 8.3). Finally, we also comment on speciﬁc\nmethods such as communication-avoiding or multigrid methods.\n8.1. GMRES-IR without an LU factorization\nComputing an LU factorization can be expensive, especially for large, sparse\nmatrices. GMRES-IR can also be eﬀective with a cheaper preconditioner M−1,\nor with no preconditioner at all.\nIn this latter case, Algorithm 7.2 reduces to\nAlgorithm 8.1, which has the form of an inner–outer scheme: the outer loop for\niterative reﬁnement (in precision u, with the residual computed at a possibly higher\nprecision ur) and the inner loop for solving the correction equations with GMRES\n(assumed backward stable) in lower precision uℓ.\nBy Theorem 6.1 (or indeed\nTheorem 5.2), convergence to an iterate satisfying (6.4) is guaranteed as long as\nκ(A)uℓ≪1. Note that inner solvers other than GMRES can be used, and, as long\nas they are backward stable, the convergence condition κ(A)uℓ≪1 still holds.\nAlgorithm 8.1 is one form of mixed precision restarted GMRES, although to\nguarantee convergence the GMRES call on line 4 must not terminate after a ﬁxed\nnumber of iterations, but rather when a suﬃciently small residual has been achieved.\nAlgorithm 8.1 was ﬁrst described by Turner and Walker (1992), who perform\nthe inner loop in single precision (uℓ) and the outer loop in double precision\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n382\nN. J. Higham and T. Mary\nAlgorithm 8.1. GMRES-based iterative reﬁnement in three precisions for the solu-\ntion of Ax = b with no preconditioner.\n1 Choose an initial x1.\n2 for i = 1: imax or until converged\n3\nCompute ri = b −Axi in precision ur.\n4\nSolve Adi = ri by GMRES in precision uℓ.\n5\nCompute xi+1 = xi + di in precision u.\n6 end\n(u and ur), and use a ﬁxed number of inner GMRES iterations. Buttari et al.\n(2008) implement several inner–outer iterative algorithms similar to GMRES-IR\nemploying single and double precisions for the solution of sparse linear systems.\nIn particular, one version uses GMRES for the inner loop and FGMRES for the\nouter loop; this version is also studied by Baboulin et al. (2009).\nMore recent implementations of these methods, still using only single and double\nprecisions, are described by Lindquist, Luszczek and Dongarra (2020, 2022) for\nCPUs and by Loe et al. (2021a,b) for GPUs. Iwashita, Suzuki and Fukaya (2020)\npropose a restarted GMRES where the inner loop uses integer arithmetic and the\nouter loop uses ﬂoating-point arithmetic.\nTo ﬁnd a compromise between computing an LU factorization and using no\npreconditioner at all, cheaper preconditioners can be considered. Algorithm 8.2 is\nobtained by replacing U−1L−1 in Algorithm 7.2 with a general preconditioner M−1.\nAlgorithm 8.2. GMRES-based iterative reﬁnement in ﬁve precisions for the solu-\ntion of Ax = b with a general preconditioner M−1 ≈A−1 stored in precision uℓ.\n1 Compute x1 = M−1b in precision uℓ.\n2 for i = 1: imax or until converged\n3\nCompute ri = b −Axi in precision ur.\n4\nSolve M−1Adi = M−1ri by GMRES in precision ug, performing the\nproducts with M−1A in precision up.\n5\nCompute xi+1 = xi + di in precision u.\n6 end\nThere is a tradeoﬀinvolved, since a better quality preconditioner will lead to faster\nconvergence but will be more expensive to compute. More subtly, the closer M−1\nis to A−1, the more signiﬁcant the rounding errors incurred in the matrix–vector\nproducts with M−1A become. Indeed, with M−1 = U−1L−1 (LU factorization-\nbased preconditioner), we have explained in Section 7.2.2 that the products with\nU−1L−1A introduce an extra κ(A) term in the convergence condition, which can be\nattenuated by performing them in higher precision up. This error analysis has not\nbeen extended to a general preconditioner M−1 in the literature, but we can expect\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n383\nκ(A) in the error bound to be replaced by a more general term depending on both\nA and M−1.\nExamples of implementations that use a preconditioner other than a low precision\nLU factorization are found in Lindquist et al. (2020, 2022), who use GMRES-IR\npreconditioned by an incomplete LU factorization, or in Loe et al. (2021a,b), where\nblock Jacobi and polynomial preconditioners are used.\n8.2. Iterative methods with low or mixed precision preconditioner\nAnother approach to exploiting mixed precision arithmetic in iterative methods is\nto use low precision to compute and/or apply the preconditioner. If the iterative\nmethod is iterative reﬁnement, and the preconditioner is a low precision LU factor-\nization, this corresponds to LU-IR (Algorithm 7.1). The idea can be extended to\nother iterative methods or preconditioners.\nFor example, Arioli and Duﬀ(2009) show that FGMRES implemented in double\nprecision and preconditioned with an LU factorization computed in single precision\ncan give backward stability at double precision, even for ill-conditioned systems.\nBuilding on this work, Hogg and Scott (2010) implement an algorithm for sym-\nmetric indeﬁnite systems that computes a solution using a direct solver in single\nprecision, performs iterative reﬁnement using the factorization of A, and then uses\nmixed precision FGMRES preconditioned by the direct solver to solve the original\nsystem.\nGiraud, Haidar and Watson (2008) propose an fp32 domain decomposition\npreconditioner applied to an fp64 CG solver. Similarly, Emans and van der Meer\n(2012) propose the use of an fp32 algebraic multigrid method as preconditioner for\nan fp64 CG.\nAnzt et al. (2019a) and Flegar, Anzt, Cojean and Quintana-Ortí (2021) imple-\nment a block Jacobi preconditioner within the preconditioned conjugate gradient\nmethod and store the explicitly inverted diagonal blocks of the preconditioner in\nhalf, single or double precision arithmetic according to a criterion based on the\ncondition number of each block. In experiments that use the preconditioner within\na conjugate gradient solver, Flegar et al. (2021) report reductions in run time of\n10%–30% compared with a full precision implementation. Göbel, Grützmacher,\nRibizel and Anzt (2021) apply the same idea to sparse approximate inverse pre-\nconditioning with a BiCGSTAB solver. It is worth noting that in these papers the\npreconditioner does not simply use low precision throughout but is itself in mixed\nprecision.\n8.3. Mixed precision GMRES\nThe previously described approaches introduce mixed precision in GMRES either\nin the preconditioner or via an inner–outer iteration scheme. However, there are\nopportunities to exploit multiple precisions even within a non-restarted, unprecon-\nditioned GMRES.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n384\nN. J. Higham and T. Mary\nA ﬁrst approach is to use lower precision in the matrix–vector products with\nA, based on the theory of inexact Krylov methods (Giraud, Gratton and Langou\n2007, Simoncini and Szyld 2003, van den Eshof and Sleijpen 2004), which proves\nthat an increasing level of inexactness as the iteration proceeds can be tolerated\nin the matrix–vector products without degrading the achievable accuracy. This\nwas ﬁrst experimentally observed by Bouras, Frayssé and Giraud (Bouras, Frayssé\nand Giraud 2000, Bouras and Frayssé 2005). The eﬀect of inexactness on the\nconvergence rate of the method is, however, not well understood.\nIn addition, Gratton, Simon, Titley-Peloquin and Toint (2019) prove that the\northonormalization of the Krylov basis can also be performed inexactly.\nThis\nobservation is leveraged by Aliaga et al. (2020), who propose to store the Krylov\nbasis in lower precision.\n8.4. Communication-avoiding iterative methods\nOn modern computers, communication has become a signiﬁcant performance\nbottleneck. Communication-avoiding (CA) methods seek to reduce the commu-\nnication costs, sometimes at the expense of additional ﬂops, in order to achieve\nhigher performance, especially when scaling to large numbers of processors. In\nparticular, CA iterative methods often compute blocks of s iterations at a time to\nreduce synchronization costs. However, these s-step approaches are known to be\nsometimes unstable. Mixed precision arithmetic has been used to overcome this\npotential instability.\nYamazaki, Tomov, Dong and Dongarra (2015b) and Yamazaki, Tomov and\nDongarra (2015a) propose a mixed precision Cholesky–QR orthonormalization\n(described in Section 9) that they use to stabilize CA-GMRES. They show that the\nuse of this stabilized orthonormalization avoids the need to orthogonalize twice\nand speeds up the convergence of GMRES.\nCarson, Gergelits and Yamazaki (2022a) propose mixed precision s-step Lanczos\nand conjugate gradient methods that compute the Gram matrix in higher precision.\nThis allows for reducing the loss of orthogonality by a factor relating to the condition\nnumber of the s-step Krylov bases, speeding up the convergence of the method at\nthe expense of an increase of the per-iteration cost that is expected to be small in\nlatency-bound applications.\n8.5. Multigrid iterative reﬁnement\nIn addition to Krylov methods, mixed precision has also been investigated for\nmultigrid methods. The most popular approach has been to use a multigrid method\nas the inner solver for iterative reﬁnement, that is, to use Algorithm 6.1 with a\nmultigrid solver on line 3, usually run in lower precision. Single precision multigrid\nmethods have, for example, been used within double precision iterative reﬁnement\nalgorithms by Göddeke, Strzodka and Turek (2007), Goddeke and Strzodka (2011),\nSumiyoshi, Fujii, Nukada and Tanaka (2014) and Kronbichler and Ljungkvist\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n385\n(2019). More recently, Oo and Vogel (2020) also used fp16 arithmetic on V100\nGPUs. The ﬁrst error analysis of multigrid methods in this context was performed\nby McCormick, Benzaken and Tamstorf (2021), who observed that diﬀerent levels\nin the grid hierarchy should use diﬀerent precisions: coarser grids are more resilient\nto lower precisions.\nThis ‘progressive precision’ approach was applied to the\nsolution of elliptic PDEs by Tamstorf, Benzaken and McCormick (2021).\nFor more details of mixed precision multigrid algorithms see Abdelfattah et al.\n(2021a).\n8.6. Other iterative solvers\nClark et al. (2010) gave an early investigation into mixed precision implementation\nof conjugate gradients (CG) and BiCGstab solvers on GPUs, for a lattice quantum\nchromodynamics application. They used half precision for storage only, since half\nprecision computation was not available to them.\nAnzt, Dongarra and Quintana-Ortí (2015) carried out the Jacobi iterative method\nwith diﬀerent solution components represented in diﬀerent precisions, using an\ninexpensive test to decide when to increase precisions during the iteration.\n8.7. Decoupling formats for data storage and processing\nOne speciﬁc feature of exploiting reduced precision in GMRES and iterative meth-\nods more generally is that performance is often limited by the memory bandwidth.\nThis leads to the idea of storing the data in compressed form and uncompressing\nit before performing arithmetic operations on the processor. The aim is that the\ncompression reduces the data movement costs enough to outweigh the costs of\ncompressing and uncompressing. Anzt, Flegar, Grützmacher and Quintana-Ortí\n(2019b) propose this approach of decoupling the data storage format from the pro-\ncessing format, and they focus on storing the data at a lower precision than that at\nwhich the computations are performed. This approach is used in the papers men-\ntioned at the end of Section 8.2 and for level 1 and level 2 BLAS by Grützmacher,\nAnzt and Quintana-Ortí (2021). Agullo et al. (2020) propose a similar approach\nfor ﬂexible GMRES, using as compression either reduced precision or the lossy\nﬂoating-point SZ compressor (Di and Cappello 2016).\n9. Mixed precision orthogonalization and QR factorization\nThere exist many algorithms to orthogonalize a set of vectors and to carry out\nthe related task of computing the QR factorization of a matrix A ∈Rm×n, where\nwe assume m ≥n. Householder QR factorization is the most widely used and is\nunconditionally stable: it achieves a backward error and a loss of orthogonality\nof the computed b\nQ (if it is explicitly formed) both of order the unit roundoﬀu\n(Higham 2002, Section 19.3).\nHowever, Householder QR factorization oﬀers\nrelatively little parallelism and requires expensive synchronizations. Alternative\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n386\nN. J. Higham and T. Mary\nalgorithms that are more suitable for parallel computers are unfortunately also less\nstable: for example, the classical and modiﬁed Gram–Schmidt algorithms (CGS and\nMGS) lead to a loss of orthogonality of order κ(A)2u (Giraud, Langou, Rozložník\nand van den Eshof 2005) and κ(A)u, respectively. Developing orthogonalization\nalgorithms that are both parallel and stable is an active ﬁeld of research.\nIn\nthis section we discuss how mixed precision arithmetic can be used to stabilize\nor accelerate these algorithms. We refer to the recent survey by Carson, Lund,\nRozložník and Thomas (2022b) for a description of what is known about the\nstability of block Gram–Schmidt algorithms.\nYang, Fox and Sanders (2021) perform rounding error analysis of Householder\nQR factorization under a model of mixed precision computation that assumes that\ninner products are computed in high precision uhigh and then rounded to lower\nprecision ulow. This model is thus applicable to the use of block FMAs. They show\nthat the bound for the backward error, which is of order mnu in uniform precision u\n(Higham 2002, p. 361), becomes of order nulow+mnuhigh under this mixed precision\nmodel. Unlike the error bound 2ulow + nuhigh of Blanchard et al. (2020b) for LU\nfactorization with block FMAs (see Section 7.3), their bound for QR factorization\nstill grows with n at the ulow level. This is because the model assumes the result of\nthe inner products to be rounded to precision ulow at each step of the factorization.\nYang et al. (2021) also analyse a blocked version of Householder QR assuming\nthat the rounding to precision ulow takes place only once per block-column. They\nshow that the term nulow can then be replaced by Nulow, where N is the number\nof block-columns. Taking advantage of the capability of some block FMAs (such\nas NVIDIA tensor cores) of keeping the result in high precision, one can also\nimagine a Householder QR factorization which starts with the original matrix A in\nhigh precision and rounds its QR factors to low precision on the ﬂy, similarly to\nthe LU factorization algorithm proposed by Haidar et al. (2018b) and analysed by\nBlanchard et al. (2020b). We expect this algorithm to further reduce the constant\nin the error bound at the ulow level by dropping the dependence on N, although this\nis not covered by the analysis of Yang et al. (2021).\nZhang, Baharlouei and Wu (2020) implement Householder QR factorization us-\ning NVIDIA tensor cores to accelerate the matrix–matrix products, but obtain only\nmodest speedups due to the panel factorization being the performance bottleneck.\nFor this reason, they propose to switch to a recursive QR factorization employing\nMGS for the orthogonalization, which requires more ﬂops and is potentially less\nstable but makes a more intensive use of matrix–matrix operations. They also\npropose to use communication-avoiding QR (CAQR) for the panel factorizations.\nThese two modiﬁcations allow them to eﬃciently leverage the tensor core perform-\nance, signiﬁcantly accelerating the factorization. They demonstrate experimentally\nthat their algorithm can solve least squares problems with double precision accuracy\nby using the computed QR factors to precondition the CGLS iterative method.\nThe Cholesky–QR algorithm computes the QR factorization of A ∈Rm×n by a\nthree-step process.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n387\n1 Compute the matrix product B = ATA.\n2 Compute the Cholesky factorization RTR = B.\n3 Compute Q = AR−1 by a multiple right-hand side triangular solve.\nFor tall, thin matrices (m ≫n), most of the ﬂops take place at steps 1 and 3,\nwhich are very parallel and make intensive use of BLAS-3 operations. However,\nCholesky–QR in uniform precision u leads to a loss of orthogonality of order\nκ(A)2u (Stathopoulos and Wu 2002), and can fail if the Cholesky factorization\nat step 2 breaks down. Cholesky–QR can be partially stabilized by using mixed\nprecision arithmetic: Yamazaki et al. (2015a) show that, if the ﬁrst two steps above\nare carried out at precision uhigh and the third step is carried out at precision u, the\nloss of orthogonality can be bounded by (Yamazaki et al. 2015a, Theorem 3.2)\n∥I −b\nQT b\nQ∥= O\n\u0000κ(A)2(uhigh + u2) + κ(A)u\n\u0001\n.\n(9.1)\nThus, by using doubled precision (that is, uhigh = u2) for the ﬁrst two steps, the\nloss of orthogonality is O(κ(A)2u2 + κ(A)u) and is therefore of order O(κ(A)u)\nas long as κ(A) < u−1. In this context, mixed precision arithmetic can therefore\nbe used not to accelerate the algorithm but to (partially) stabilize it, by reducing\nthe loss of orthogonality by a factor of κ(A). Yamazaki et al. (2015a) implement\nthis mixed precision Cholesky–QR algorithm with fp64 as the working precision\nu, and employ double-double arithmetic for the ﬁrst two steps. Despite requiring\n8.5× more ﬂops due to the use of software-emulated arithmetic, they show that\nthe mixed precision Cholesky–QR algorithm can be only moderately slower than\nin uniform precision (about 1.4× slower in the best case) when the number of\ncolumns n to orthogonalize is small, because in this case the performance of\nCholesky–QR is memory-bound. They apply this mixed precision Cholesky–QR\nalgorithm to the solution of linear systems with a communication-avoiding GMRES\nmethod, and show that the use of this more stable Cholesky–QR algorithm avoids\nthe need for reorthogonalization and allows GMRES to converge faster, leading\nto signiﬁcant speedups. See also Yamazaki et al. (2015b) for early results on this\napproach. When the number of columns to orthogonalize is larger, the performance\nof Cholesky–QR tends to become compute-bound and the overhead associated with\nthe use of double-double arithmetic becomes more signiﬁcant. To overcome this\nissue, Yamazaki et al. (2015c) propose a block MGS method that partitions the\nmatrix into block-columns of smaller size and uses the mixed precision Cholesky–\nQR to orthogonalize each block. This method can be up to seven times faster\nthan applying mixed precision Cholesky–QR to the entire matrix, and numerical\nexperiments show that it can also be as stable, despite the lack of error analysis\nbounding the loss of orthogonality.\nA drawback of Cholesky–QR-based methods is that they can fail if the Cholesky\nfactorization breaks down (because it encounters a non-positive pivot). Break-\ndown can be avoided by shifting the matrix to ensure the success of the Cholesky\nfactorization (Fukaya et al. 2020).\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n388\nN. J. Higham and T. Mary\nAnother solution is the singular value QR (SVQR) factorization (Stathopoulos\nand Wu 2002), which takes the following steps.\n1 Compute the matrix product B = ATA.\n2 Compute the singular value decomposition UΣUT = B.\n3 Compute the QR factorization ¯QR = Σ1/2UT.\n4 Compute Q = AR−1 by multiple right-hand side triangular solve.\nSteps 2 and 3 require more ﬂops than simply computing the Cholesky factorization\nof B, but if m ≫n the overhead is negligible compared with the ﬂops required\nby steps 1 and 4.\nThe advantage of SVQR is that when B is singular to the\nworking precision, step 2 will directly identify the entire subspace of the nearly\ndependent columns and one can replace all the associated singular values with an\nappropriately large value. Stathopoulos and Wu (2002) suggest replacing singular\nvalues smaller than uσ1 with uσ1, where σ1 is the largest singular value of B. In\nuniform precision u, SVQR also suﬀers from a loss of orthogonality proportional\nto κ(A)2u. Yamazaki, Tomov and Dongarra (2016) propose a mixed precision\nversion of SVQR analogous to their mixed precision Cholesky–QR (Yamazaki\net al. 2015a), where step 4 is carried out in halved precision u1/2 compared with\nthe ﬁrst two steps. The loss of orthogonality can then be bounded by (Yamazaki\net al. 2016, Theorem 5.1)\n∥I −b\nQT b\nQ∥= O(κ(A)2u + κ(A)u1/2).\n(9.2)\nWhen κ(A) is larger than u−1/2, the use of halved precision in step 4 therefore does\nnot signiﬁcantly impact the loss of orthogonality. For smaller values of κ(A), the\nloss of orthogonality is increased but remains a factor κ(A) smaller than if SVQR\nwere carried out entirely in halved precision.\n10. Least squares problems\nConsider the linear least squares (LS) problem minx ∥Ax −b∥2, where A ∈Rm×n\nwith m ≥n has full rank. Recall that the unique LS solution is the solution of the\nnormal equations\nATAx = AT b\n(10.1)\nand that the normal equations can be rewritten as the (m + n) × (m + n) augmented\nsystem\n\u0014 I\nA\nAT 0\n\u0015 \u0014r\nx\n\u0015\n=\n\u0014b\n0\n\u0015\n.\n(10.2)\nBjörck (1967) proposed reﬁning an approximate LS solution by applying iterative\nreﬁnement to the augmented system, with residuals calculated at twice the working\nprecision, and he showed how to eﬃciently solve the augmented system given a QR\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n389\nfactorization of A. He also gave rounding error analysis for the method. Björck’s\nmethod and analysis was extended to constrained and weighted LS problems by\nGulliksson (1994).\nDemmel, Hida, Riedy and Li (2009) discuss practical implementation details\nsuch as convergence tests and how to compute error bounds, and they exploit the\nXBLAS.\nFor more on traditional and ﬁxed precision forms of iterative reﬁnement for the\nLS problem, see Björck (1996) and Higham (2002, Chapter 20).\nRecently, mixed precision algorithms for solving the LS problem have been\ndeveloped by building on GMRES-IR for square linear systems.\nHigham and Pranesh (2021) assume that A is well-conditioned and make use\nof the normal equations (10.1). Their idea is a modiﬁcation of the algorithm of\nSection 7.5 that uses GMRES-IR with Cholesky preconditioning. It chooses a\ndiagonal matrix S so that B = AS has columns of unit 2-norm, forms C = BTB\nat precision uℓ, computes the Cholesky factorization of a shifted C at precision\nuℓ, then applies GMRES-IR to the normal equations, computing the residual in\nprecision ur as ri = AT(b−Axi) and applying GMRES to the preconditioned update\nequation M ATAdi = Mri, where M = SR−1R−T S. Solving the normal equations\nis usually avoided by numerical analysts because it gives a backward error bound\nof order κ2(A)u (Higham 2002, Section 20.4) and the Cholesky factorization can\nbreak down for κ2(A) > u−1/2. Its use here is justiﬁed by the facts that A is assumed\nto be well-conditioned, the Cholesky factorization of the cross-product matrix is\nbeing used as a preconditioner rather than to compute the solution directly, and if\na block FMA is available it can be exploited in forming C, boosting the speed and\naccuracy.\nCarson, Higham and Pranesh (2020) make use of the augmented system (10.2).\nTheir method computes a QR factorization at precision uℓ, then applies GMRES-\nIR to the augmented system with a left preconditioner constructed in one of two\npossible ways from the QR factors.\nBackward error analysis given in Carson\net al. (2020), combined with the analysis of Carson and Higham (2017, 2018)\nand Amestoy et al. (2021b), shows that the method yields a forward error, and a\nbackward error for the augmented system, of order the working precision under\nreasonable assumptions.\nNumerical experiments in Carson et al. (2020) with\nvarious combinations of the three precisions show that the method behaves as\npredicted by the theory.\n11. Eigenvalue decomposition\nA natural way to reﬁne approximate solutions to the eigenvalue problem is by\nNewton’s method, and it presents opportunities for exploiting diﬀerent arithmetic\nprecisions.\nEarly references developing Newton’s method for mixed precision\niterative reﬁnement for the standard eigenvalue problem are Dongarra (1980, 1982)\nand Dongarra, Moler and Wilkinson (1983).\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n390\nN. J. Higham and T. Mary\nWe consider the generalized eigenvalue problem Ax = λBx, where A, B ∈Rn×n.\nSetting B = I gives the standard eigenvalue problem. We suppose that we have an\napproximate eigenpair that we wish to improve. We will use Newton’s method, so\nwe need to put the problem in the form of a non-linear system.\nSince an eigenvector remains an eigenvector when multiplied by a non-zero\nscalar, we need to normalize x, which we will do by requiring that eT\ns x = 1 for\nsome chosen s, where es is the unit vector with a 1 in position s. Deﬁne\nF(v) =\n\u0014\n(A −λB)x\neT\ns x −1\n\u0015\n: Cn+1 →Cn+1,\nv =\n\u0014\nx\nλ\n\u0015\n.\nThe Jacobian is\nJ(v) =\n\u0012∂Fi\n∂vj\n\u0013\n=\n\u0014\nA −λB\n−Bx\neT\ns\n0\n\u0015\n.\nIt is easy to see that ∥J(w)−J(v)∥∞≤2∥B∥∞∥w−v∥∞, so J is Lipschitz continuous\nwith constant 2∥B∥∞. Moreover, it can be shown that J is non-singular when λ is\na simple (non-multiple) eigenvalue (Tisseur 2001, Lemma 3.3).\nBy applying Theorems 5.1 and 5.2, Tisseur (2001, Section 3.2) shows that if\n(x0, λ0) is a suﬃciently good approximation to an eigenpair (x∗, λ∗), λ∗is simple, J\nis not too ill-conditioned at (x∗, λ∗), and the linear system solver is not too unstable,\nthen Newton’s method is well-deﬁned and the limiting forward error is bounded by\n∥(bxT, λ)T −(xT\n∗, λ∗)T ∥∞\n∥(xT∗, λ∗)T ∥∞\n≲cnur ∥J(v∗)−1∥∞max(∥A∥∞, ∥B∥∞) + u,\nwhere c is a small integer constant and ur is the precision in which the residual\nF(v) is evaluated. If ur = u2 this bound can be shown to reduce to cnu. Moreover,\nthe limiting backward error is bounded by\nη∞(bx, bλ) ≲cnur + u(3 + |λ|) max\n\u0012 ∥A∥∞\n∥B∥∞\n, ∥B∥∞\n∥A∥∞\n\u0013\n.\n(11.1)\nNote that as for linear systems, instability in the linear system solver does not aﬀect\nthe bounds for the limiting forward error and backward error.\nEach Newton iteration involves the solution of a linear system with the Jacobian\nmatrix evaluated at the current iterate.\nIf this is done using LU factorization\nof J(v) it costs O(n3) ﬂops per step, which is expensive.\nIf an approximate\neigendecomposition is available then this cost can be reduced to O(n2) ﬂops per\niteration. We specialize to the symmetric deﬁnite generalized eigenvalue problem\nin which A is symmetric and B is symmetric positive deﬁnite. Algorithm 11.1\nis given by Tisseur (2001, Algorithm 4.2) and is used by Davies, Higham and\nTisseur (2001) to reﬁne solutions from the Cholesky–Jacobi method, which uses\na Cholesky decomposition of B to reduce the problem to a standard symmetric\neigenvalue problem and then applies the Jacobi method.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n391\nAlgorithm 11.1. Given a symmetric A ∈Rn×n, a symmetric positive deﬁnite\nB ∈Rn×n, X ∈Rn×n and a diagonal Λ ∈Rn×n such that XTAX ≈Λ and XT BX ≈I,\nand an approximate eigenpair (x, λ) with ∥x∥∞= xs = 1, this algorithm applies\niterative reﬁnement to λ and x at a cost of O(n2) ﬂops per iteration. Computations\nare at precision u unless otherwise stated.\n1 repeat until converged\n2\nCompute r = λBx −Ax in precision ur.\n3\nDλ = Λ −λI\n4\nd = −Bx −cs, where cs is the sth column of A −λB.\n5\nv = XT d, f = XT es\n6\nCompute Givens rotations Jk in the (k, k + 1) plane, such that\nQT\n1 v : = JT\n1 . . . JT\nn−1v = ∥v∥2e1.\n7\nCompute orthogonal Q2 such that\nT = QT\n2 QT\n1 (Dλ + v f T) is upper triangular.\n8\nz = QT\n2 QT\n1 XTr\n9\nSolve Tw = z for w.\n10\nδ = Xw\n11\nλ = λ + δs, δs = 0\n12\nx = x + δ\n13 end\nNewton’s method is well suited to reﬁning a small number of eigenpairs but not\na complete eigensystem, as in the latter case it is expensive and may not converge\nfor all eigenpairs.\nTsai, Luszczek and Dongarra (2021) revisit the Newton method for the standard\nsymmetric eigenvalue problem and develop a mixed precision algorithm that trans-\nforms the matrix to tridiagonal form in single precision, computes the eigensystem\nby divide and conquer in double precision, then reﬁnes the eigenpairs in double\nprecision.\nOgita and Aishima (2018) develop an iteration for reﬁning the whole eigensystem\nof a symmetric matrix. It requires four matrix multiplications per iteration, all\nexecuted in a higher precision than the working precision. Quadratic convergence\nis proved for suﬃciently good initial approximations. The algorithm does not work\nwell when there are nearly multiple eigenvalues. The latter limitation is addressed\nin Ogita and Aishima (2019) by using further steps that work with clusters of\neigenvalues.\nPetschow, Quintana-Ortí and Bientinesi (2014) use extra precision to improve\nthe accuracy of the multiple relatively robust representations (MRRR) method for\nthe symmetric tridiagonal eigenvalue problem without sacriﬁcing performance.\nRalha (2018) considers carrying out the bisection method for symmetric tri-\ndiagonal matrices with early iterations in single precision before switching to the\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n392\nN. J. Higham and T. Mary\nworking precision of double, and develops criteria for deciding when to make the\nswitch.\nStor, Slapničar and Barlow (2015) give an algorithm for the eigendecomposition\nof symmetric arrowhead matrices that employs bisection and a shift and invert\ntechnique, and in the latter it uses arithmetic at twice the working precision for one\nelement of the inverse in order to ensure forward stability.\nTsuchida and Choe (2012) consider a trace minimization method for computing\nthe complete eigensystem of a symmetric matrix and explore running diﬀerent\nparts of the method at half the working precision. Gains of over 30% in execution\ntime are reported with little loss of accuracy.\nAlvermann et al. (2019) report on two projects that are developing eigensolvers\nbased on the (block) Jacobi–Davidson method, subspace iteration and other meth-\nods, and are using lower precision in early iterations for speed and higher precision\nwithin the orthogonalizations for robustness.\n12. Singular value decomposition\nWe now consider the singular value decomposition (SVD) of A ∈Rm×n with m ≥n:\nA = UΣVT with U ∈Rm×m and V ∈Rn×n orthogonal and Σ = diag(σi) ∈Rm×n.\n12.1. Iterative reﬁnement\nThe Newton approach to reﬁning eigenpairs can be extended to singular value\ntriples of A ∈Rm×n by using the function\nF(x) =\n\nAv −µ1u\nATu −µ2v\nuTu −1\nvTv −1\n\n,\nx =\n\nu\nv\nµ1\nµ2\n\n.\nThe Jacobian of f is\nJ(x) =\n\n−µ1I\nA\n−u\n0\nAT\n−µ2I\n0\n−v\n2uT\n0\n0\n0\n0\n2vT\n0\n0\n\n.\nThe approximate singular value is updated by (µ1 + µ2)/2.\nDongarra (1983),\nextending the work in Dongarra et al. (1983), shows how to solve systems with\nJ(x) in O(mn) ﬂops, given an SVD or bidiagonal factorization of A. Again, the\nNewton theory of Section 5 applies.\nOgita and Aishima (2020) extend their algorithm for the symmetric eigenvalue\nproblem, mentioned in the previous section, to the SVD in order to reﬁne the\ncomplete SVD; the algorithm uses higher precision and is dominated by matrix\nmultiplication.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n393\n12.2. SVD with rapidly decaying singular values\nAnother opportunity for mixed precision arithmetic arises in the case of matrices\nwith rapidly decaying singular values. Given a target accuracy ε, it is well known\nthat singular values smaller than ε and the corresponding singular vectors can be\ndropped to provide a low-rank approximation to the matrix with an error bound of\norder ε. Amestoy et al. (2021a) explain that among the singular values that remain,\nthose that are small enough can be represented, along with their associated singular\nvectors, in lower precision. For example, singular vectors associated with singular\nvalues less than ε/us, where us = 2−24 is the unit roundoﬀfor single precision, can\nbe stored in single precision, even when ε ≪us. They introduce a mixed precision\nSVD representation that uses p precisions,\nA = UΣVT = [U1 U2 . . . Up]Σ[V1 V2 . . . Vp]T,\n(12.1)\nwhere Ui and Vi are stored in precision ui, with u1 < u2 < · · · < up. They give an\nexplicit rule on how to partition U andV in order to guarantee an overall accuracy of\norder ε (Amestoy et al. 2021a, Theorem 2.2). Note that this approach is applicable\nnot only to the SVD but also to other types of rank-revealing decompositions, such\nas QR factorization with column pivoting.\nOoi et al. (2020) propose three diﬀerent methods to introduce mixed precision\narithmetic in the product of a low-rank matrix with a vector. Their method 3 is\nsimilar to the representation (12.1), which they use with fp64 and fp32 arithmetics.\nThey apply this approach to the solution of linear systems exploiting products of a\nhierarchical (H) matrix with a vector, using the iterative BiCGstab solver.\n13. Multiword arithmetic\nMultiword arithmetic is a well-known approach to enhance the accuracy of com-\nputations while employing fast arithmetic supported in hardware. It consists of\nrepresenting high precision numbers by the unevaluated sum of lower precision\nnumbers. An example is double-double arithmetic which, as mentioned in Sec-\ntion 2.2, approximates an fp128 number as the sum of two fp64 numbers and\nreplaces fp128 operations with fp64 operations.\nThe emergence of specialized hardware supporting low precision matrix multi-\nplication with high precision accumulators, such as the NVIDIA GPU tensor cores,\nprovides new opportunities for multiword arithmetic. Indeed, these units are much\nfaster than standard fp32 arithmetic (up to 8 and 16 times faster on the Volta and\nAmpere GPUs, for example). Therefore an approach to accelerate the computation\nof an fp32 matrix product C = AB is to approximate A ≈A1 + A2 as the sum\nof two fp16 matrices, and similarly B ≈B1 + B2. Then C can be computed as\nC ≈A1B1 + A1B2 + A2B1 + A2B2 using block FMAs to compute each of the AiBj\nterms using internal tensor core arithmetic at fp32 accuracy. Since there are only\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n394\nN. J. Higham and T. Mary\nfour terms (and in fact, we can reduce that number to three, as explained below),\nthis approach can potentially be much faster than standard fp32 arithmetic.\nThis approach was ﬁrst used with NVIDIA tensor cores by Markidis et al.\n(2018) to accelerate matrix products, and by Sorna et al. (2018) to accelerate\nthe fast Fourier transform (FFT). Pisha and Ligowski (2021) similarly use the\nTensorFloat32 format in computing the FFT on the NVIDIA A100 GPUs. Henry,\nTang and Heinecke (2019) describe an approach based on block FMA hardware\nusing the bﬂoat16 format instead of the fp16 one, where A and B are split into\nthree bﬂoat16 matrices, which requires nine products to compute C = AB. Finally,\nMukunoki, Ozaki, Ogita and Imamura (2020) explain how to achieve not only fp32\naccuracy but also fp64 accuracy with this approach, by using the Ozaki scheme.\nTheir approach, however, requires splitting both A and B a large number of times,\nwhich leads to several dozens if not hundreds of products. Their algorithm is\ntherefore only beneﬁcial on GPUs on which fp64 arithmetic is very slow, such as\nsome of the Turing models.\nFasi et al. (2022) generalize these approaches by considering any low precision\nulow and any number of splits p. They give Algorithm 13.1.\nAlgorithm 13.1 (multiword matrix multiplication). Thisalgorithmcomputesthe\nmatrix–matrix product C = AB using p-word arithmetic with a mixed precision\nblock FMA with precisions ulow and uhigh.\n1 for i = 1: p\n2\nAi = fllow(A −Íi−1\nk=1 Ak)\n3\nBi = fllow(B −Íi−1\nk=1 Bk)\n4 end\n5 for i = 1: p\n6\nfor j = 1: p\n7\nCompute Cij = AiBj with Algorithm 4.1.\n8\nC ←C + Cij\n9\nend\n10 end\nThe algorithm recursively computes Ai (and similarly Bj) as the residual from\nthe (i −1)-way split A ≈A1 + · · · + Ai−1 and rounds it to precision ulow, that is,\nAi = fllow\n\u0012\nA −\ni−1\nÕ\nk=1\nAk\n\u0013\nBi = fllow\n\u0012\nB −\ni−1\nÕ\nk=1\nBk\n\u0013\n\n\ni = 1: p.\n(13.1)\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n395\nThis gives the approximations\nA =\np\nÕ\ni=1\nAi + ∆A,\n|∆A| ≤up\nlow|A|,\n(13.2)\nB =\np\nÕ\ni=1\nAi + ∆B,\n|∆B| ≤up\nlow|B|.\n(13.3)\nThen, if C is approximated by the sum of the p2 products AiBj, which are computed\nby chaining calls to a block FMA with internal precision uhigh, by Theorem 4.1 we\nobtain a computed b\nC satisfying (Fasi et al. 2022)\nb\nC = AB + E,\n|E| ≲\n\u00002up\nlow + u2p\nlow + (n + p2 −1)uhigh\n\u0001\n|A||B|.\n(13.4)\nClearly, for practical choices of ulow and uhigh a small value of p is suﬃcient. For\nexample, for fp16 (ulow = 2−11) and fp32 (uhigh = 2−24), p = 2 is enough since in\nthis case u2\nlow = 4uhigh. Taking larger values of p will not signiﬁcantly improve\nthe bound (13.4) since the term (n + p2)uhigh will then dominate. For bﬂoat16\n(ulow = 2−8) and fp32, the case p = 3 is also of interest because the signiﬁcand of\none fp32 number ﬁts exactly into the signiﬁcands of three bﬂoat16 numbers.\nImportantly, in practice not all p2 products AiBj need be computed. As a result\nof the construction (13.1), the magnitude of the elements of Ai and Bj rapidly\ndecreases as we increase i and j. More precisely, we have\n|Ai| ≤ui−1\nlow(1 + ulow)|A|,\n|Bi| ≤ui−1\nlow(1 + ulow)|B|,\ni = 1: p,\nand thus\n|Ai||Bj| ≤ui+j−2\nlow\n(1 + ulow)2|A||B|.\n(13.5)\nTherefore, ignoring any product AiBj such that i + j > p + 1 only introduces an\nerror of order up\nlow or higher, which does not signiﬁcantly impact the bound (13.4).\nIndeed, by only computing the products AiBj such that i + j ≤p + 1, we obtain the\nmodiﬁed bound\nb\nC = AB + E,\n|E| ≲\n\u0012\n2up\nlow + u2p\nlow + (n + p2)uhigh +\np−1\nÕ\ni=1\n(p −i)up+i−1\nlow\n(1 + ulow)2\n\u0013\n|A||B|.\nThe constant in this bound is (p + 1)up\nlow plus higher-order terms, so to order up\nlow\nwe have only increased the constant 2 from (13.4) to p+1, and we have reduced the\nnumber of products from p2 to p(p+1)/2. Concretely, with fp32 and fp16 (p = 2),\nwe only need three products, which is less than the four used by Markidis et al.\n(2018), and with bﬂoat16 and fp32 (p = 3), we can reduce the number of products\nfrom nine to six, as already suggested by Henry et al. (2019).\nNote that further reducing the number of products (such as using two products\nfor p = 2, as attempted by Markidis et al. 2018) is possible, but the analysis tells us\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n396\nN. J. Higham and T. Mary\nit should not be beneﬁcial. Indeed, ignoring any product AiBj such that i+ j ≤p+1\nwould introduce an error of order at least up−1\nlow , and so could not be signiﬁcantly\nmore accurate than simply using p −1 splits rather than p.\nThe above analysis encompasses previously proposed algorithms, and also in-\ncludes new cases. For example, we may use a 2-way split (p = 2) with bﬂoat16\nand fp32, which requires three products rather than six (when p = 3) and delivers\nan accuracy of order 2−16 rather than 2−24.\nNote that this analysis deals with worst-case error bounds and so does not\nguarantee that multiword arithmetic with low precision block FMAs will be as\naccurate as higher precision standard arithmetic in the case where the latter does\nnot attain its worst-case error. In fact, in their experiments with NVIDIA tensor\ncores, Fasi et al. (2022) ﬁnd that double-fp16 arithmetic can be much less accurate\nthan fp32 arithmetic due to the rounding mode of these devices, which can make\nthe worst-case bounds for double-fp16 sharp. To overcome this issue, Fasi et al.\n(2022) propose the use of FABsum (see Section 4.2) to reduce the worst-case error\nbound.\n14. Adaptive precision algorithms\nSeveral mixed precision algorithms described in the previous sections share the\nsame foundation: adapt the precision to the data by using lower precisions to\nrepresent the less important or signiﬁcant parts of the data. As an example, consider\nthe computation of the sum a + b, where |b| ≪|a|. Because of the widely diﬀerent\nmagnitudes of a and b, the least signiﬁcant bits of b do not play a signiﬁcant role in\nthe computed value of the result. Indeed if we round b to eb = fllow(b) = b(1+δlow),\nwhere |δlow| ≤ulow, then\nfl(a + eb) = (a + eb)(1 + δ)\n(|δ| ≤u)\n= (a + b(1 + δlow))(1 + δ)\n= (a + b)(1 + δ)\n\u0012\n1 +\nb\na + bδlow\n\u0013\n,\nand so we have an extra term 1 + bδlow/(a + b), which is insigniﬁcant as long\nas |b|ulow ≪|a + b|u.\nTherefore b can be stored in lower precision without\nsigniﬁcantly impacting the accuracy of the computation. Moreover, if b is the\nresult of a previous computation, that computation can also be carried out in lower\nprecision. This example illustrates that computations performed on data of small\nmagnitude need not use very high precision. This is a simple but fundamental\nobservation that has given birth to several adaptive precision algorithms.\nThe\nobject of this section is to show that these algorithms share strong connections.\nAdaptive precision algorithms seek to exploit this observation by adapting the\nprecision to be inversely proportional to the weight of the data, where the weight is\ndeﬁned by some metric such as the maximum magnitude or the norm of the data.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n397\nIn numerical linear algebra algorithms, this can be done at diﬀerent levels of the\ncomputation: at the element, block, column/row or matrix levels.\n14.1. At the matrix level\nIn computations involving several matrices, we may choose to compute and store\nsome of them in lower precision. For example, in computing C = A1B1 + A2B2\nwhere |A1| ≥|A2| and |B1| ≥|B2|, if |A2||B2| ≪|A1||B1| then the matrix product\nA2B2 can be computed in lower precision than A1B1. An example where this\nsituation arises is the use of multiword arithmetic, as illustrated by (13.5). In fact\nwe have already explained that the products AiBj of highest order can be ignored;\ndata-driven analysis also shows that most of the products that cannot be ignored\ncan however be computed in lower precision. For example, with ulow as fp16 and\np = 2, the products A1B2 and A2B1 can be computed in fp16 arithmetic, because\nthe magnitude of their entries is proportional to ulow. Only the ﬁrst term A1B1\nactually needs to be computed in fp32 arithmetic. This observation is especially\nimportant when implementing multiword arithmetic on GPU tensor cores, which\nlead to heavy rounding error accumulation in the products AiBj because of their\nrounding mode: Fasi et al. (2022) explain that it is only necessary to take care of\nreducing the eﬀect of error accumulation on the A1B1 term.\n14.2. At the column level (or, equivalently, at the row level)\nGiven a matrix, we may think of storing each of its columns (or rows) in a diﬀerent\nprecision. This approach makes the most sense when dealing with matrices that\ncan be decomposed as low-rank components of rapidly decreasing norm. This can\nbe the case, for example, of SVDs or rank-revealing factorizations. In fact, the\nmixed precision truncated SVD approaches described in Section 12.2 (Amestoy\net al. 2021a, Ooi et al. 2020) are precisely based on this property: rounding errors\nintroduced by converting singular vectors to lower precision are demagniﬁed by\nthe associated singular value, and so the precision of each vector should be selected\nbased on its associated singular value.\nNote that, given the SVD UΣVT of a matrix A, we can express the matrix as\nA = Í\ni Ai with Ai = uiσivT\ni , where ∥Ai+1∥F ≤∥Ai∥F. Thus, even though the\nmatrices Ai are never formed or manipulated explicitly, the link with the matrix-\nlevel case is clear.\n14.3. At the block level\nIn some applications it pays to partition a matrix into several blocks and adapt the\nprecision to each block. For example, in Section 7.7 we described approaches where\nthe precision of each block is based on its distance to the diagonal (Abdulah et al.\n2019, 2022, Doucet et al. 2019). The success of these approaches is explained by\nthe fact that, for many data-sparse matrices, blocks distant from the diagonal tend to\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n398\nN. J. Higham and T. Mary\nhave smaller norm. Indeed, storing each block in a precision inversely proportional\nto its norm can allow for signiﬁcant gains with potentially little accuracy loss. As\nan example, consider a matrix A ∈Rpb×pb partitioned into p2 blocks Aij ∈Rb×b\nand assume we have two precisions uhigh and ulow at our disposal. Then the matrix\nbA obtained by storing blocks Aij of Frobenius norm less than uhigh∥A∥F/(pulow) in\nprecision ulow satisﬁes ∥bA −A∥F ≤uhigh∥A∥F. Thus we can store selected blocks\nof A in precision ulow and still recover a global approximation at accuracy uhigh.\nThis example trivially extends to more than two precisions.\nAnother example of an adaptive precision algorithm at the block level is the\nadaptive precision block Jacobi preconditioner discussed in Section 8.2 (Anzt et al.\n2019a, Flegar et al. 2021). In this case the precisions of the blocks are selected\nbased on their condition number rather than their norm, because this is the relevant\nmetric when applying the inverse of the blocks as part of the preconditioner.\n14.4. At the element level\nThe adaptive precision algorithms described above seek to exploit the underlying\nstructure of the data. However, the question arises as to whether it can be beneﬁcial\nto adapt the precision at the element level: that is, to allow each variable in the\ncomputation to have its own precision, without any special structure (by blocks or\nby columns, for instance). This is similar in goal to transprecision computing and\nprecision auto-tuning tools, which we brieﬂy discuss in Section 15.1.\nWhile this approach maximizes the use of reduced precision, it also destroys\nthe granularity of the computation and should therefore only be used for memory-\nbound applications, such as for sparse matrix–vector products (SpMV) y = Ax. In\nparticular, Ahmad, Sundar and Hall (2019) propose to split A as Ad + As, where\nAs contains the small non-zero elements of A and is stored in single precision,\nwhereas Ad is kept in double precision.\nMore generally, given p precisions, one could split the elements of A into\np diﬀerent matrices and compute p independent products in the corresponding\nprecision. This idea is then similar to bucket summation (Demmel and Hida 2004,\nZhu and Hayes 2009), in which summands are split into buckets based on their\nexponent. The novelty comes from summing each bucket in a diﬀerent precision.\nDiﬀenderfer, Osei-Kuﬀuor and Menon (2021) propose such a bucket algorithm for\nthe inner product that uses the four IEEE arithmetics as well as ‘perforation’, that is,\nthe option to ignore some of the smallest summands. Graillat, Jézéquel, Mary\nand Molina (2022) propose an adaptive precision sparse matrix–vector product\nalgorithm of similar spirit that, given p precisions u1 < · · · < up, splits A into p\nbuckets based on the magnitude of the elements: bucket number i contains all the\nelements whose absolute value lies in the interval [ε/ui, ε/ui+1], for a given target\naccuracy ε. They obtain speedups of up to an order of magnitude compared with\na standard product in uniform precision.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n399\n15. Miscellany\n15.1. Tuning precisions\nA very diﬀerent approach to mixed precision computing is to focus on the code\nrather than the algorithm. Given a code to solve a particular problem and a set of\narithmetics of diﬀerent precisions, one can ask what is the best selection of precision\nin which to store each variable. Here, ‘best’ means a choice that minimizes some\nsuitable performance metric subject to achieving a computed result of acceptable\nquality. The motivation is the assumption that reducing precisions means faster\nexecution and lower storage and energy requirements, though conversion between\ndiﬀerent precisions is required and adds overhead costs.\nThis problem is clearly combinatorial in nature, as if there are n variables and p\nprecisions there are pn possible implementations. Ensuring results of acceptable\nquality requires, in principle, a parametrized rounding error analysis that encapsu-\nlates all the possible input data.\nMuch research has been done on algorithms that attempt to solve this problem.\nUsually, optimization of code is done for a ‘representative’ data set, with the\nassumption that the code will be used on related data for which the quality of the\nresults will be similar. At best a local minimum of the objective function can be\nexpected. No guarantee is provided that the code with the chosen precisions will\nsatisfy error requirements across all possible input data.\nTools can be categorized as using static analysis (carried out before the code is\nrun) or dynamic analysis. Dynamic analysis tools typically instrument the compiled\ncode in order to try diﬀerent combinations of precisions, and a popular way to do\nso is via the LLVM compiler infrastructure.31\nAny attempt to reduce precisions of variables must ensure that suﬃcient range\nis maintained to avoid overﬂow and harmful underﬂow, which is particularly im-\nportant if fp16 is one of the formats, given its narrow range.\nAn example of such work is the tool Precimonious, by Rubio-González et al.\n(2013), which uses execution time as the performance metric. It takes a C program\nas input and outputs a description of the precisions to be assigned to the variables.\nThe experiments in Rubio-González et al. (2013) demonstrate a speedup of up to a\nfactor of 1.4 by replacing certain double precision variables with single precision\nones in the benchmark codes tested.\nA more recent example, focused on GPUs, is GRAM, which chooses the preci-\nsions at run time (Ho, De Silva and Wong 2021). Each block of threads is kept at\nthe same precision and a proportion α of the blocks is assigned a lower precision,\nwith a binary search used to select α. Speedups on an NVIDIA GPU of up to 1.8\nover single precision are reported, by exploiting half precision arithmetic. GRAM\ndoes not support the use of tensor cores.\n31 https://llvm.org/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n400\nN. J. Higham and T. Mary\nBrun et al. (2021) develop a tool that uses a heuristic search strategy to select\nthe precisions at which elementary functions are evaluated in a code, aiming to\nminimize the precisions subject to achieving output of a given accuracy. They\nuse the Intel Vector Mathematics functions (VM) in the Intel oneAPI Math Kernel\nLibrary, which have an input argument that allows the user to select ‘high accuracy’,\n‘low accuracy’ or ‘enhanced performance accuracy’ modes for the functions. They\nare able to obtain up to an approximate halving of the execution time on a Monte\nCarlo code that spends 70% of its time in mathematical library functions.\nPrecision tuning has also been used in climate and weather models. Tintó Prims\net al. (2019) use the rpe Fortran library that emulates reduced precision, which\nwe mentioned in Section 2.6. For two widely used ocean model codes they use a\ndivide and conquer approach to ﬁnd assignments of precisions to variables, ﬁnding\nthat half precision or single precision can be used for large portions of the codes.\nSimilar ﬁndings were made by Düben, Subramanian, Dawson and Palmer (2017)\nfor a cloud-resolving model within a general circulation model.\nIt needs to be kept in mind that simply lowering the precisions of variables in a\ncode may not be all that can be done. In some problems the choice of algorithm,\nor the algorithm itself, is precision-dependent.\nFor example, an algorithm for\ncomputing an elementary function may be built upon a rational approximation that\ndepends on the target accuracy, so that diﬀerent approximations can be used for\nhalf, single and double precision.\n15.2. Multiprecision algorithms\nMultiprecision algorithms for the matrix logarithm and the matrix exponential are\ndeveloped by Fasi and Higham (2018, 2019). These algorithms take as input the\nunit roundoﬀu of the arithmetic and then determine a suitable level of (inverse)\nscaling and squaring transformations and degree of Taylor or Padé approximants\nsuch that the functions are approximated to precision u.\nThe key algorithmic\nparameters are determined at run time, which contrasts with the state-of-the-art\nalgorithms for double precision arithmetic, where some of the parameters have\nbeen determined in advance. A similar strategy is followed by Al-Mohy, Higham\nand Liu (2022) in a multiprecision algorithm for computing for the matrix cosine\nand its Fréchet derivative.\nHigham and Liu (2021) develop a multiprecision version of the Schur–Parlett\nalgorithm for computing general analytic functions at a matrix argument. It avoids\nthe need for derivatives by computing the function of the diagonal blocks of the\nreordered and blocked Schur form by diagonalizing, at a suitable precision, a small\nrandom perturbation of each block.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n401\nAcknowledgements\nThe work of the ﬁrst author was supported by Engineering and Physical Sciences\nResearch Council grant EP/P020720/1, the Royal Society and the Exascale Comput-\ning Project (17-SC-20-SC), a collaborative eﬀort of the US Department of Energy\nOﬃce of Science and the National Nuclear Security Administration. The work of\nthe second author was supported by the InterFLOP project (ANR-20-CE46-0009)\nof the French National Agency for Research.\nWe thank Massimiliano Fasi, Sven Hammarling, Claude-Pierre Jeannerod, Man-\ntas Mikaitis and Françoise Tisseur for their comments on a draft manuscript.\nReferences\nA. Abdelfattah, H. Anzt, E. G. Boman, E. Carson, T. Cojean, J. Dongarra, A. Fox, M. Gates,\nN. J. Higham, X. S. Li, J. Loe, P. Luszczek, S. Pranesh, S. Rajamanickam, T. Ribizel,\nB. F. Smith, K. Swirydowicz, S. Thomas, S. Tomov, Y. M. Tsai and U. M. Yang (2021a),\nA survey of numerical linear algebra methods utilizing mixed-precision arithmetic, Int.\nJ. High Perform. Comput. Appl. 35, 344–369.\nA. Abdelfattah, T. Costa, J. Dongarra, M. Gates, A. Haidar, S. Hammarling, N. J. Higham,\nJ. Kurzak, P. Luszczek, S. Tomov and M. Zounon (2021b), A set of Batched Basic\nLinear Algebra Subprograms and LAPACK routines, ACM Trans. Math. Software 47,\n21.\nA. Abdelfattah, S. Tomov and J. Dongarra (2019a), Fast batched matrix multiplication\nfor small sizes using half-precision arithmetic on GPUs, in 2019 IEEE International\nParallel and Distributed Processing Symposium (IPDPS), IEEE, pp. 111–122.\nA. Abdelfattah, S. Tomov and J. Dongarra (2019b), Towards half-precision computation\nfor complex matrices: A case study for mixed-precision solvers on GPUs, in 2019\nIEEE/ACM 10th Workshop on Latest Advances in Scalable Algorithms for Large-Scale\nSystems (ScalA), IEEE, pp. 17–24.\nA. Abdelfattah, S. Tomov and J. Dongarra (2020), Investigating the beneﬁt of FP16-\nenabled mixed-precision solvers for symmetric positive deﬁnite matrices using GPUs,\nin Computational Science – ICCS 2020 (V. V. Krzhizhanovskaya et al., eds), Vol. 12138\nof Lecture Notes in Computer Science, Springer, pp. 237–250.\nS. Abdulah, Q. Cao, Y. Pei, G. Bosilca, J. Dongarra, M. G. Genton, D. E. Keyes, H. Ltaief\nand Y. Sun (2022), Accelerating geostatistical modeling and prediction with mixed-\nprecision computations: A high-productivity approach with PaRSEC, IEEE Trans.\nParallel Distrib. Syst. 33, 964–976.\nS. Abdulah, H. Ltaief, Y. Sun, M. G. Genton and D. E. Keyes (2019), Geostatistical\nmodeling and prediction using mixed precision tile Cholesky factorization, in 2019 IEEE\n26th International Conference on High Performance Computing, Data, and Analytics\n(HiPC), IEEE, pp. 152–162.\nE. Agullo, F. Cappello, S. Di, L. Giraud, X. Liang and N. Schenkels (2020), Exploring\nvariable accuracy storage through lossy compression techniques in numerical linear al-\ngebra: A ﬁrst application to ﬂexible GMRES. Research report RR-9342, Inria Bordeaux\nSud-Ouest. Available at hal-02572910v2.\nK. Ahmad, H. Sundar and M. Hall (2019), Data-driven mixed precision sparse matrix\nvector multiplication for GPUs, ACM Trans. Archit. Code Optim. 16, 51.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n402\nN. J. Higham and T. Mary\nA. H. Al-Mohy, N. J. Higham and X. Liu (2022), Arbitrary precision algorithms for\ncomputing the matrix cosine and its Fréchet derivative, SIAM J. Matrix Anal. Appl. 43,\n233–256.\nJ. I. Aliaga, H. Anzt, T. Grützmacher, E. S. Quintana-Ortí and A. E. Tomás (2020),\nCompressed basis GMRES on high performance GPUs. Available at arXiv:2009.12101.\nA. Alvermann, A. Basermann, H.-J. Bungartz, C. Carbogno, D. Ernst, H. Fehske,\nY. Futamura, M. Galgon, G. Hager, S. Huber, T. Huckle, A. Ida, A. Imakura, M. Kawai,\nS. Köcher, M. Kreutzer, P. Kus, B. Lang, H. Lederer, V. Manin, A. Marek, K. Nakajima,\nL. Nemec, K. Reuter, M. Rippl, M. Röhrig-Zöllner, T. Sakurai, M. Scheﬄer, C. Scheurer,\nF. Shahzad, D. Simoes Brambila, J. Thies and G. Wellein (2019), Beneﬁts from using\nmixed precision computations in the ELPA-AEO and ESSEX-II eigensolver projects,\nJapan J. Indust. Appl. Math. 36, 699–717.\nP. Amestoy, O. Boiteau, A. Buttari, M. Gerest, F. Jézéquel, J.-Y. L’Excellent and T. Mary\n(2021a), Mixed precision low rank approximations and their application to block low\nrank LU factorization. Available at hal-03251738.\nP. Amestoy, A. Buttari, N. J. Higham, J.-Y. L’Excellent, T. Mary and B. Vieublé (2021b),\nFive-precision GMRES-based iterative reﬁnement. MIMS EPrint 2021.5, Manchester\nInstitute for Mathematical Sciences, The University of Manchester, UK.\nP. Amestoy, A. Buttari, N. J. Higham, J.-Y. L’Excellent, T. Mary and B. Vieublé (2022),\nCombining sparse approximate factorizations with mixed precision iterative reﬁnement.\nMIMS EPrint 2022.2, Manchester Institute for Mathematical Sciences, The University\nof Manchester, UK.\nP. Amestoy, A. Buttari, J.-Y. L’Excellent and T. Mary (2019), Performance and scalability\nof the block low-rank multifrontal factorization on multicore architectures, ACM Trans.\nMath. Software 45, 2.\nP. Amestoy, I. S. Duﬀ, J.-Y. L’Excellent and J. Koster (2001), A fully asynchronous\nmultifrontal solver using distributed dynamic scheduling, SIAM J. Matrix Anal. Appl.\n23, 15–41.\nE. Anderson (1991), Robust triangular solves for use in condition estimation. Technical\nreport CS-91-142, Department of Computer Science, The University of Tennessee,\nKnoxville, TN, USA. LAPACK Working Note 36.\nANSI (1966), American National Standard FORTRAN, American National Standards In-\nstitute, New York.\nH. Anzt, J. Dongarra and E. S. Quintana-Ortí (2015), Adaptive precision solvers for sparse\nlinear systems, in Proceedings of the 3rd International Workshop on Energy Eﬃcient\nSupercomputing (E2SC ’15), ACM Press, article 2.\nH. Anzt, J. Dongarra, G. Flegar, N. J. Higham and E. S. Quintana-Ortí (2019a), Adapt-\nive precision in block-Jacobi preconditioning for iterative sparse linear system solvers,\nConcurrency Comput. Pract. Exper. 31, e4460.\nH. Anzt, G. Flegar, T. Grützmacher and E. S. Quintana-Ortí (2019b), Toward a modular\nprecision ecosystem for high-performance computing, Int. J. High Perform. Comput.\nAppl. 33, 1069–1078.\nJ. Appleyard and S. Yokim (2017), Programming tensor cores in CUDA 9. Available at\nhttps://devblogs.nvidia.com/programming-tensor-cores-cuda-9/.\nM. Arioli and I. S. Duﬀ(2009), Using FGMRES to obtain backward stability in mixed\nprecision, Electron. Trans. Numer. Anal. 33, 31–44.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n403\nM. Arioli, I. S. Duﬀ, S. Gratton and S. Pralet (2007), A note on GMRES preconditioned\nby a perturbed LDLT decomposition with static pivoting, SIAM J. Sci. Comput. 29,\n2024–2044.\nARM (2018), ARM Architecture Reference Manual. ARMv8, for ARMv8-A Architecture\nProﬁle, ARM Limited, Cambridge, UK. Version dated 31 October 2018. Original\nrelease dated 30 April 2013.\nARM (2019), Arm A64 Instruction Set Architecture Armv8, for Armv8-A Architecture\nProﬁle, ARM Limited, Cambridge, UK.\nARM (2020), Arm Architecture Reference Manual. Armv8, for Armv8-A Architecture\nProﬁle, ARM Limited, Cambridge, UK. ARM DDI 0487F.b (ID040120).\nM. Baboulin, A. Buttari, J. Dongarra, J. Kurzak, J. Langou, J. Langou, P. Luszczek and\nS. Tomov (2009), Accelerating scientiﬁc computations with mixed precision algorithms,\nComput. Phys. Comm. 180, 2526–2533.\nD. H. Bailey (2021), MPFUN2020: A new thread-safe arbitrary precision package (full\ndocumentation). Availableat https://www.davidhbailey.com/dhbpapers/mpfun2020.pdf.\nD. H. Bailey, Y. Hida, X. S. Li and B. Thompson (2002), ARPREC: An arbitrary precision\ncomputation package.\nTechnical report LBNL-53651, Lawrence Berkeley National\nLaboratory, Berkeley, CA, USA.\nP. Bauer, P. D. Dueben, T. Hoeﬂer, T. Quintino, T. C. Schulthess and N. P. Wedi (2021),\nThe digital revolution of earth-system science, Nature Comput. Sci. 1, 104–113.\nJ. Bezanson, A. Edelman, S. Karpinski and V. B. Shah (2017), Julia: A fresh approach to\nnumerical computing, SIAM Rev. 59, 65–98.\nÅ. Björck (1967), Iterative reﬁnement of linear least squares solutions I, BIT 7, 257–278.\nÅ. Björck (1996), Numerical Methods for Least Squares Problems, SIAM.\nP. Blanchard, N. J. Higham and T. Mary (2020a), A class of fast and accurate summation\nalgorithms„ SIAM J. Sci. Comput. 42, A1541–A1557.\nP. Blanchard, N. J. Higham, F. Lopez, T. Mary and S. Pranesh (2020b), Mixed precision\nblock fused multiply-add: Error analysis and application to GPU tensor cores, SIAM J.\nSci. Comput. 42, C124–C141.\nA. Bouras and V. Frayssé (2005), Inexact matrix-vector products in Krylov methods for\nsolving linear systems: A relaxation strategy, SIAM J. Matrix Anal. Appl. 26, 660–678.\nA. Bouras, V. Frayssé and L. Giraud (2000), A relaxation strategy for inner–outer linear\nsolvers in domain decomposition methods. Technical report TR/PA/00/17, CERFACS,\nToulouse, France.\nE. Brun, D. Defour, P. De Oliveira Castro, M. Iştoan, D. Mancusi, E. Petit and A. Vaquet\n(2021), A study of the eﬀects and beneﬁts of custom-precision mathematical libraries\nfor HPC codes, IEEE Trans. Emerg. Topics Comput. 9, 1467–1478.\nA. Buttari, J. Dongarra, J. Kurzak, P. Luszczek and S. Tomov (2008), Using mixed precision\nfor sparse matrix computations to enhance the performance while achieving 64-bit\naccuracy, ACM Trans. Math. Software 34, 17.\nA. Buttari, J. Dongarra, J. Langou, J. Langou, P. Luszczek and J. Kurzak (2007), Mixed\nprecision iterative reﬁnement techniques for the solution of dense linear systems, Int. J.\nHigh Perform. Comput. Appl. 21, 457–466.\nE. Carson and N. J. Higham (2017), A new analysis of iterative reﬁnement and its applica-\ntion to accurate solution of ill-conditioned sparse linear systems, SIAM J. Sci. Comput.\n39, A2834–A2856.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n404\nN. J. Higham and T. Mary\nE. Carson and N. J. Higham (2018), Accelerating the solution of linear systems by iterative\nreﬁnement in three precisions, SIAM J. Sci. Comput. 40, A817–A847.\nE. Carson, T. Gergelits and I. Yamazaki (2022a), Mixed precision s-step Lanczos and\nconjugate gradient algorithms, Numer. Linear Algebra Appl. 29, e2425.\nE. Carson, N. J. Higham and S. Pranesh (2020), Three-precision GMRES-based iterative\nreﬁnement for least squares problems, SIAM J. Sci. Comput. 42, A4063–A4083.\nE. Carson, K. Lund, M. Rozložník and S. Thomas (2022b), Block Gram–Schmidt al-\ngorithms and their stability properties, Linear Algebra Appl. 638, 150–195.\nA. Charara, M. Gates, J. Kurzak, A. YarKhan and J. Dongarra (2020), SLATE developers’\nguide. SLATE Working Note 11, Innovative Computing Laboratory, The University of\nTennessee, Knoxville, TN, US.\nJ. Choquette, W. Gandhi, O. Giroux, N. Stam and R. Krashinsky (2021), NVIDIA A100\ntensor core GPU: Performance and innovation, IEEE Micro 41, 29–35.\nM. A. Clark, R. Babich, K. Barros, R. C. Brower and C. Rebbi (2010), Solving lattice QCD\nsystems of equations using mixed precision solvers on GPUs, Comput. Phys. Comm.\n181, 1517–1528.\nM. P. Connolly and N. J. Higham (2022), Probabilistic rounding error analysis of House-\nholder QR factorization. MIMS EPrint 2022.5, Manchester Institute for Mathematical\nSciences, The University of Manchester, UK.\nM. P. Connolly, N. J. Higham and T. Mary (2021), Stochastic rounding and its probabilistic\nbackward error analysis, SIAM J. Sci. Comput. 43, A566–A585.\nM. Courbariaux, Y. Bengio and J.-P. David (2015), Training deep neural networks with\nlow precision multiplications. Available at arXiv:1412.7024v5.\nM. G. Croarken (1985), The centralization of scientiﬁc computation in Britain 1925–1955.\nPhD thesis, University of Warwick, Coventry, UK.\nM. Croci, M. Fasi, N. J. Higham, T. Mary and M. Mikaitis (2022), Stochastic rounding:\nImplementation, error analysis, and applications, Roy. Soc. Open Sci. 9, 1–25.\nP. I. Davies, N. J. Higham and F. Tisseur (2001), Analysis of the Cholesky method with\niterative reﬁnement for solving the symmetric deﬁnite generalized eigenproblem, SIAM\nJ. Matrix Anal. Appl. 23, 472–493.\nT. A. Davis and Y. Hu (2011), The University of Florida Sparse Matrix Collection, ACM\nTrans. Math. Software 38, 1.\nA. Dawson and P. D. Düben (2017), rpe v5: An emulator for reduced ﬂoating-point\nprecision in large numerical simulations, Geosci. Model Dev. 10, 2221–2230.\nA. Dawson, P. D. Düben, D. A. MacLeod and T. N. Palmer (2018), Reliable low precision\nsimulations in land surface models, Climate Dynam. 51, 2657–2666.\nJ. Dean (2020), The deep learning revolution and its implications for computer architecture\nand chip design, in 2020 IEEE International Solid-State Circuits Conference (ISSCC),\nIEEE, pp. 8–14.\nJ. Demmel and Y. Hida (2004), Accurate and eﬃcient ﬂoating point summation, SIAM J.\nSci. Comput. 25, 1214–1248.\nJ. Demmel and X. Li (1994), Faster numerical algorithms via exception handling, IEEE\nTrans. Comput. 43, 983–992.\nJ. Demmel, Y. Hida, E. J. Riedy and X. S. Li (2009), Extra-precise iterative reﬁnement for\noverdetermined least squares problems, ACM Trans. Math. Software 35, 28.\nJ. E. Dennis, Jr and R. B. Schnabel (1983), Numerical Methods for Unconstrained Optim-\nization and Nonlinear Equations, Prentice Hall. Reprinted by SIAM, 1996.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n405\nS. Di and F. Cappello (2016), Fast error-bounded lossy HPC data compression with SZ,\nin 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS),\nIEEE, pp. 730–739.\nJ. Diﬀenderfer, D. Osei-Kuﬀuor and H. Menon (2021), QDOT: Quantized dot product\nkernel for approximate high-performance computing. Available at arXiv:2105.00115.\nJ. J. Dongarra (1980), Improving the accuracy of computed matrix eigenvalues. Preprint\nANL-80-84, Mathematics and Computer Science Division, Argonne National Laborat-\nory, Argonne, IL, USA.\nJ. J. Dongarra (1982), Algorithm 589 SICEDR: A FORTRAN subroutine for improving\nthe accuracy of computed matrix eigenvalues, ACM Trans. Math. Software 8, 371–375.\nJ. J. Dongarra (1983), Improving the accuracy of computed singular values, SIAM J. Sci.\nStatist. Comput. 4, 712–719.\nJ. J. Dongarra (2020), Report on the Fujitsu Fugaku system. Technical report ICL-UT-\n20-06, Innovative Computing Laboratory, The University of Tennessee, Knoxville, TN,\nUSA.\nJ. J. Dongarra, J. R. Bunch, C. B. Moler and G. W. Stewart (1979), LINPACK Users’ Guide,\nSIAM.\nJ. J. Dongarra, C. B. Moler and J. H. Wilkinson (1983), Improving the accuracy of computed\neigenvalues and eigenvectors, SIAM J. Numer. Anal. 20, 23–45.\nN. Doucet, H. Ltaief, D. Gratadour and D. Keyes (2019), Mixed-precision tomographic\nreconstructor computations on hardware accelerators, in 2019 IEEE/ACM 9th Workshop\non Irregular Applications: Architectures and Algorithms (IA3), IEEE, pp. 31–38.\nP. D. Düben, A. Subramanian, A. Dawson and T. N. Palmer (2017), A study of reduced\nnumerical precision to make superparameterization more competitive using a hardware\nemulator in the OpenIFS model, J. Adv. Model. Earth Syst. 9, 566–584.\nI. S. Duﬀand S. Pralet (2007), Towards stable mixed pivoting strategies for the sequential\nand parallel solution of sparse symmetric indeﬁnite systems, SIAM J. Matrix Anal. Appl.\n29, 1007–1024.\nI. S. Duﬀ, A. M. Erisman and J. K. Reid (2017), Direct Methods for Sparse Matrices,\nsecond edition, Oxford University Press.\nM. Emans and A. van der Meer (2012), Mixed-precision AMG as linear equation solver\nfor deﬁnite systems, Procedia Comput. Sci. 1, 175–183.\nM. Fasi and N. J. Higham (2018), Multiprecision algorithms for computing the matrix\nlogarithm, SIAM J. Matrix Anal. Appl. 39, 472–491.\nM. Fasi and N. J. Higham (2019), An arbitrary precision scaling and squaring algorithm\nfor the matrix exponential, SIAM J. Matrix Anal. Appl. 40, 1233–1256.\nM. Fasi and N. J. Higham (2021), Matrices with tunable inﬁnity-norm condition number\nand no need for pivoting in LU factorization, SIAM J. Matrix Anal. Appl. 42, 417–435.\nM. Fasi and M. Mikaitis (2020), CPFloat: A C library for emulating low-precision arith-\nmetic. MIMS EPrint 2020.22, Manchester Institute for Mathematical Sciences, The\nUniversity of Manchester, UK.\nM. Fasi, N. J. Higham, F. Lopez, T. Mary and M. Mikaitis (2022), Matrix multiplication in\nmultiword arithmetic: Error analysis and application to GPU tensor cores. MIMS EPrint\n2022.3, Manchester Institute for Mathematical Sciences, The University of Manchester,\nUK.\nM. Fasi, N. J. Higham, M. Mikaitis and S. Pranesh (2021), Numerical behavior of NVIDIA\ntensor cores, PeerJ Comput. Sci. 7, e330.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n406\nN. J. Higham and T. Mary\nG. Flegar, H. Anzt, T. Cojean and E. S. Quintana-Ortí (2021), Adaptive precision block-\nJacobi for high performance preconditioning in the Ginkgo linear algebra software, ACM\nTrans. Math. Software 47, 1–28.\nL. Fousse, G. Hanrot, V. Lefèvre, P. Pélissier and P. Zimmermann (2007), MPFR: A\nmultiple-precision binary ﬂoating-point library with correct rounding, ACM Trans. Math.\nSoftware 33, 13.\nL. Fox, H. D. Huskey and J. H. Wilkinson (1948), The solution of algebraic linear simultan-\neous equations by punched card methods. Report, Mathematics Division, Department\nof Scientiﬁc and Industrial Research, National Physical Laboratory, Teddington, UK.\nT. Fukaya, R. Kannan, Y. Nakatsukasa, Y. Yamamoto and Y. Yanagisawa (2020), Shifted\nCholesky QR for computing the QR factorization of ill-conditioned matrices, SIAM J.\nSci. Comput. 42, A477–A503.\nJ. Gao, F. Zheng, F. Qi, Y. Ding, H. Li, H. Lu, W. He, H. Wei, L. Jin, X. Liu, D. Gong,\nF. Wang, Y. Zheng, H. Sun, Z. Zhou, Y. Liu and H. You (2021), Sunway supercomputer\narchitecture towards exascale computing: Analysis and practice, Sci. China Inform. Sci.\n64, 141101.\nP. E. Gill, M. A. Saunders and J. R. Shinnerl (1996), On the stability of Cholesky factoriz-\nation for symmetric quasideﬁnite systems, SIAM J. Matrix Anal. Appl. 17, 35–46.\nL. Giraud, S. Gratton and J. Langou (2007), Convergence in backward error of relaxed\nGMRES, SIAM J. Sci. Comput. 29, 710–728.\nL. Giraud, A. Haidar and L. T. Watson (2008), Mixed-precision preconditioners in parallel\ndomain decomposition solvers, in Domain Decomposition Methods in Science and\nEngineering XVII (U. Langer et al., eds), Vol. 60 of Lecture Notes in Computational\nScience and Engineering, Springer, pp. 357–364.\nL. Giraud, J. Langou, M. Rozložník and J. van den Eshof (2005), Rounding error analysis\nof the classical Gram–Schmidt orthogonalization process, Numer. Math. 101, 87–100.\nF. Göbel, T. Grützmacher, T. Ribizel and H. Anzt (2021), Mixed precision incomplete and\nfactorized sparse approximate inverse preconditioning on GPUs, in Euro-Par 2021: Par-\nallel Processing, Vol. 12820 of Lecture Notes in Computer Science, Springer, pp. 550–\n564.\nD. Goddeke and R. Strzodka (2011), Cyclic reduction tridiagonal solvers on GPUs applied\nto mixed-precision multigrid, IEEE Trans. Parallel Distrib. Syst. 22, 22–32.\nD. Göddeke, R. Strzodka and S. Turek (2007), Performance and accuracy of hardware-\noriented native-, emulated- and mixed-precision solvers in FEM simulations, Int. J.\nParallel Emergent Distrib. Syst. 22, 221–256.\nW. Govaerts and J. D. Pryce (1990), Block elimination with one iterative reﬁnement solves\nbordered linear systems accurately, BIT 30, 490–507.\nS. Graillat, F. Jézéquel, T. Mary and R. Molina (2022), Adaptive precision matrix–vector\nproduct. Available at hal-03561193.\nS. Gratton, E. Simon, D. Titley-Peloquin and P. Toint (2019), Exploiting variable precision\nin GMRES. Available at arXiv:1907.10550.\nA. Greenbaum (1997), Estimating the attainable accuracy of recursively computed residual\nmethods, SIAM J. Matrix Anal. Appl. 18, 535–551.\nJ. F. Groote, R. Morel, J. Schmaltz and A. Watkins (2021), Logic Gates, Circuits, Pro-\ncessors, Compilers and Computers, Springer.\nT. Grützmacher, H. Anzt and E. S. Quintana-Ortí (2021), Using Ginkgo’s memory accessor\nfor improving the accuracy of memory-bound low precision BLAS, Software Pract.\nExper. Available at doi:10.1002/spe.3041.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n407\nM. Gulliksson (1994), Iterative reﬁnement for constrained and weighted linear least squares,\nBIT 34, 239–253.\nS. Gupta, A. Agrawal, K. Gopalakrishnan and P. Narayanan (2015), Deep learning with\nlimited numerical precision, in Proceedings of the 32nd International Conference on\nMachine Learning (F. Bach and D. Blei, eds), Vol. 37 of Proceedings of Machine\nLearning Research, PMLR, pp. 1737–1746.\nA. Haidar, A. Abdelfattah, M. Zounon, P. Wu, S. Pranesh, S. Tomov and J. Dongarra\n(2018a), The design of fast and energy-eﬃcient linear solvers: On the potential of\nhalf-precision arithmetic and iterative reﬁnement techniques, in Computational Science\n– ICCS 2018 (Y. Shi et al., eds), Vol. 10860 of Lecture Notes in Computer Science,\nSpringer, pp. 586–600.\nA. Haidar, H. Bayraktar, S. Tomov, J. Dongarra and N. J. Higham (2020), Mixed-precision\niterative reﬁnement using tensor cores on GPUs to accelerate solution of linear systems,\nProc. Roy. Soc. London A 476 (2243), 20200110.\nA. Haidar, S. Tomov, J. Dongarra and N. J. Higham (2018b), Harnessing GPU tensor\ncores for fast FP16 arithmetic to speed up mixed-precision iterative reﬁnement solv-\ners, in Proceedings of the International Conference for High Performance Computing,\nNetworking, Storage, and Analysis (SC18), IEEE, article 47.\nA. Haidar, P. Wu, S. Tomov and J. Dongarra (2017), Investigating half precision arithmetic\nto accelerate dense linear system solvers, in Proceedings of the 8th Workshop on Latest\nAdvances in Scalable Algorithms for Large-Scale Systems (ScalA ’17), ACM Press,\narticle 10.\nR. Harvey and D. L. Verseghy (2015), The reliability of single precision computations in\nthe simulation of deep soil heat diﬀusion in a land surface model, Climate Dynam. 16,\n3865–3882.\nG. Henry, P. T. P. Tang and A. Heinecke (2019), Leveraging the bﬂoat16 artiﬁcial intel-\nligence datatype for higher-precision computations, in 2019 IEEE 26th Symposium on\nComputer Arithmetic (ARITH), IEEE, pp. 69–76.\nD. J. Higham, N. J. Higham and S. Pranesh (2021), Random matrices generating large\ngrowth in LU factorization with pivoting, SIAM J. Matrix Anal. Appl. 42, 185–201.\nN. J. Higham (1986), Computing the polar decomposition: With applications, SIAM J. Sci.\nStatist. Comput. 7, 1160–1174.\nN. J. Higham (1988), Fast solution of Vandermonde-like systems involving orthogonal\npolynomials, IMA J. Numer. Anal. 8, 473–486.\nN. J. Higham (1991), Iterative reﬁnement enhances the stability of QR factorization meth-\nods for solving linear equations, BIT 31, 447–468.\nN. J. Higham (1997), Iterative reﬁnement for linear systems and LAPACK, IMA J. Numer.\nAnal. 17, 495–509.\nN. J. Higham (2002), Accuracy and Stability of Numerical Algorithms, second edition,\nSIAM.\nN. J. Higham (2008), Functions of Matrices: Theory and Computation, SIAM.\nN. J. Higham (2021), Numerical stability of algorithms at extreme scale and low precisions.\nMIMS EPrint 2021.14, Manchester Institute for Mathematical Sciences, The University\nof Manchester, UK. To appear in Proc. Int. Cong. Math.\nN. J. Higham and X. Liu (2021), A multiprecision derivative-free Schur–Parlett algorithm\nfor computing matrix functions, SIAM J. Matrix Anal. Appl. 42, 1401–1422.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n408\nN. J. Higham and T. Mary\nN. J. Higham and T. Mary (2019a), A new approach to probabilistic rounding error analysis,\nSIAM J. Sci. Comput. 41, A2815–A2835.\nN. J. Higham and T. Mary (2019b), A new preconditioner that exploits low-rank approx-\nimations to factorization error, SIAM J. Sci. Comput. 41, A59–A82.\nN. J. Higham and T. Mary (2020), Sharper probabilistic backward error analysis for basic\nlinear algebra kernels with random data, SIAM J. Sci. Comput. 42, A3427–A3446.\nN. J. Higham and T. Mary (2021), Solving block low-rank linear systems by LU factorization\nis numerically stable, IMA J. Numer. Anal. Available at doi:10.1093/imanum/drab020.\nN. J. Higham and S. Pranesh (2019), Simulating low precision ﬂoating-point arithmetic,\nSIAM J. Sci. Comput. 41, C585–C602.\nN. J. Higham and S. Pranesh (2021), Exploiting lower precision arithmetic in solving\nsymmetric positive deﬁnite linear systems and least squares problems, SIAM J. Sci.\nComput. 43, A258–A277.\nN. J. Higham, S. Pranesh and M. Zounon (2019), Squeezing a matrix into half precision,\nwith an application to solving linear systems, SIAM J. Sci. Comput. 41, A2536–A2551.\nN.-M. Ho, H. De Silva and W.-F. Wong (2021), GRAM: A framework for dynamically\nmixing precisions in GPU applications, ACM Trans. Archit. Code Optim. 18, 1–24.\nJ. D. Hogg and J. A. Scott (2010), A fast and robust mixed-precision solver for the solution\nof sparse symmetric linear systems, ACM Trans. Math. Software 37, 17.\nY. Idomura, T. Ina, Y. Ali and T. Imamura (2020), Acceleration of fusion plasma turbu-\nlence simulations using the mixed-precision communication-avoiding Krylov method,\nin International Conference for High Performance Computing, Networking, Storage and\nAnalysis (SC20), IEEE, pp. 1–13.\nIEEE (1985), IEEE Standard for Binary Floating-Point Arithmetic, ANSI/IEEE Standard\n754-1985, Institute of Electrical and Electronics Engineers.\nIEEE (2008), IEEE Standard for Floating-Point Arithmetic, IEEE Std 754-2008 (Revision\nof IEEE 754-1985), Institute of Electrical and Electronics Engineers.\nIntel Corporation (2018), BFLOAT16: Hardware Numerics Deﬁnition.\nWhite paper.\nDocument number 338302-001US.\nI. C. F. Ipsen and H. Zhou (2020), Probabilistic error analysis for inner products, SIAM J.\nMatrix Anal. Appl. 41, 1726–1741.\nT. Iwashita, K. Suzuki and T. Fukaya (2020), An integer arithmetic-based sparse linear\nsolver using a GMRES method and iterative reﬁnement, in 2020 IEEE/ACM 11th Work-\nshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA), IEEE,\npp. 1–8.\nM. Jankowski and H. Woźniakowski (1977), Iterative reﬁnement implies numerical stabil-\nity, BIT 17, 303–311.\nF. Johansson et al. (2013), Mpmath: A Python library for arbitrary-precision ﬂoating-point\narithmetic. Available at http://mpmath.org.\nM. Joldes, J.-M. Muller and V. Popescu (2017), Tight and rigorous error bounds for basic\nbuilding blocks of double-word arithmetic, ACM Trans. Math. Software 44, 15res.\nN. P. Jouppi, D. H. Yoon, M. Ashcraft, M. Gottscho, T. B. Jablin, G. Kurian, J. Laudon,\nS. Li, P. Ma, X. Ma, T. Norrie, N. Patil, S. Prasad, C. Young, Z. Zhou and D. Patterson\n(2021), Ten lessons from three generations shaped Google’s TPUv4i: Industrial product,\nin 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture\n(ISCA), IEEE, pp. 1–14.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n409\nN. P. Jouppi, D. H. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young and D. Patterson\n(2020), A domain-speciﬁc supercomputer for training deep neural networks, Comm.\nAssoc. Comput. Mach. 63, 67–78.\nW. Kahan (1981), Why do we need a ﬂoating-point arithmetic standard? Technical report,\nUniversity of California, Berkeley, CA, USA.\nC. T. Kelley (1995), Iterative Methods for Linear and Nonlinear Equations, SIAM.\nC. T. Kelley (2022), Newton’s method in mixed precision, SIAM Rev. 64, 191–211.\nA. Kiełbasiński (1981), Iterative reﬁnement for linear systems in variable-precision arith-\nmetic, BIT 21, 97–103.\nP. A. Knight, D. Ruiz and B. Uçar (2014), A symmetry preserving algorithm for matrix\nscaling, SIAM J. Matrix Anal. Appl. 35, 931–955.\nM. Kronbichler and K. Ljungkvist (2019), Multigrid for matrix-free high-order ﬁnite\nelement computations on graphics processors, ACM Trans. Parallel Comput. 6, 2.\nS. Kudo, K. Nitadori, T. Ina and T. Imamura (2020a), Implementation and numerical\ntechniques for one EFlop/s HPL-AI benchmark on Fugaku, in Proceedings of the 11th\nIEEE/ACM Workshop on Latest Advances in Scalable Algorithms for Large-Scale, Vol. 1,\nIEEE, pp. 69–76.\nS. Kudo, K. Nitadori, T. Ina and T. Imamura (2020b), Prompt report on exa-scale HPL-AI\nbenchmark, in 2020 IEEE International Conference on Cluster Computing (CLUSTER),\nIEEE, pp. 418–419.\nJ. Kurzak and J. Dongarra (2007), Implementation of mixed precision in solving systems\nof linear equations on the Cell processor, Concurrency Comput. Pract. Exper. 19, 1371–\n1385.\nJ. Langou, J. Langou, P. Luszczek, J. Kurzak, A. Buttari and J. Dongarra (2006), Exploiting\nthe performance of 32 bit ﬂoating point arithmetic in obtaining 64 bit accuracy (revis-\niting iterative reﬁnement for linear systems), in Proceedings of the 2006 ACM/IEEE\nConference on Supercomputing (SC ’06), IEEE.\nV. Lefèvre and P. Zimmermann (2017), Optimized binary64 and binary128 arithmetic with\nGNU MPFR, in 2017 IEEE 24th Symposium on Computer Arithmetic (ARITH), IEEE,\npp. 18–26.\nX. S. Li and J. W. Demmel (1998), Making sparse Gaussian elimination scalable by static\npivoting, in Proceedings of the 1998 ACM/IEEE Conference on Supercomputing, IEEE,\npp. 1–17.\nX. S. Li and J. W. Demmel (2003), SuperLU_DIST: A scalable distributed-memory sparse\ndirect solver for unsymmetric linear systems, ACM Trans. Math. Software 29, 110–140.\nX. S. Li, J. W. Demmel, D. H. Bailey, G. Henry, Y. Hida, J. Iskandar, W. Kahan, S. Y.\nKang, A. Kapur, M. C. Martin, B. J. Thompson, T. Tung and D. J. Yoo (2002), Design,\nimplementation and testing of extended and mixed precision BLAS, ACM Trans. Math.\nSoftware 28, 152–205.\nC. Lichtenau, S. Carlough and S. M. Mueller (2016), Quad precision ﬂoating point on\nthe IBM z13, in 2016 IEEE 23rd Symposium on Computer Arithmetic (ARITH), IEEE,\npp. 87–94.\nN. Lindquist, P. Luszczek and J. Dongarra (2020), Improving the performance of the\nGMRES method using mixed-precision techniques, in Communications in Computer\nand Information Science (J. Nichols et al., eds), Springer, pp. 51–66.\nN. Lindquist, P. Luszczek and J. Dongarra (2022), Accelerating restarted GMRES with\nmixed precision arithmetic, IEEE Trans. Parallel Distrib. Syst. 33, 1027–1037.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n410\nN. J. Higham and T. Mary\nJ. A. Loe, C. A. Glusa, I. Yamazaki, E. G. Boman and S. Rajamanickam (2021a), Ex-\nperimental evaluation of multiprecision strategies for GMRES on GPUs, in 2021 IEEE\nInternational Parallel and Distributed Processing Symposium Workshops (IPDPSW),\nIEEE, pp. 469–478.\nJ. A. Loe, C. A. Glusa, I. Yamazaki, E. G. Boman and S. Rajamanickam (2021b), A study\nof mixed precision strategies for GMRES on GPUs. Available at arXiv:2109.01232.\nF. Lopez and T. Mary (2020), Mixed precision LU factorization on GPU tensor cores:\nReducing data movement and memory footprint. MIMS EPrint 2020.20, Manchester\nInstitute for Mathematical Sciences, The University of Manchester, UK.\nP. Luszczek, I. Yamazaki and J. Dongarra (2019), Increasing accuracy of iterative reﬁne-\nment in limited ﬂoating-point arithmetic on half-precision accelerators, in 2019 IEEE\nHigh Performance Extreme Computing Conference (HPEC), IEEE, pp. 1–6.\nS. Markidis, S. Wei Der Chien, E. Laure, I. B. Peng and J. S. Vetter (2018), NVIDIA tensor\ncore programmability, performance & precision, in 2018 IEEE International Parallel\nand Distributed Processing Symposium Workshops (IPDPSW), IEEE, pp. 522–531.\nC. M. Maynard and D. N. Walters (2019), Mixed-precision arithmetic in the ENDGame\ndynamical core of the uniﬁed model, a numerical weather prediction and climate model\ncode, Comput. Phys. Comm. 244, 69–75.\nS. F. McCormick, J. Benzaken and R. Tamstorf (2021), Algebraic error analysis for mixed-\nprecision multigrid solvers, SIAM J. Sci. Comput. 43, S392–S419.\nA. Meurer, C. P. Smith, M. Paprocki, O. ˘Certik, S. B. Kirpichev, M. Rocklin, A. Kumar,\nS. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller,\nF. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel,\nŠ. Roučka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman and A. Scopatz (2017), SymPy:\nSymbolic computing in Python, PeerJ Comput. Sci. 3, e103.\nC. B. Moler (1967), Iterative reﬁnement in ﬂoating point, J. Assoc. Comput. Mach. 14,\n316–321.\nC. B. Moler (2017), ‘Half precision’ 16-bit ﬂoating point arithmetic.\nAvailable\nat\nhttp://blogs.mathworks.com/cleve/2017/05/08/half-precision-16-bit-ﬂoating-point-\narithmetic/.\nC. B. Moler (2019), Variable format half precision ﬂoating point arithmetic.\nAvail-\nable at https://blogs.mathworks.com/cleve/2019/01/16/variable-format-half-precision-\nﬂoating-point-arithmetic/.\nD. Mukunoki, K. Ozaki, T. Ogita and T. Imamura (2020), DGEMM using tensor cores, and\nits accurate and reproducible versions, in High Performance Computing (P. Sadayappan\net al., eds), Springer, pp. 230–248.\nJ.-M. Muller, N. Brunie, F. de Dinechin, C.-P. Jeannerod, M. Joldes, V. Lefèvre,\nG. Melquiond, N. Revol and S. Torres (2018), Handbook of Floating-Point Arithmetic,\nsecond edition, Birkhäuser.\nM. Nakata (2021), MPLAPACK version 1.0.0 user manual. Available at arXiv:2109.13406.\nT. Norrie, N. Patil, D. H. Yoon, G. Kurian, S. Li, J. Laudon, C. Young, N. Jouppi and\nD. Patterson (2021), The design process for Google’s training chips: TPUv2 and TPUv3,\nIEEE Micro 41, 56–63.\nNVIDIA Corporation (2020), NVIDIA A100 Tensor Core GPU Architecture, v1.0.\nT. Ogita and K. Aishima (2018), Iterative reﬁnement for symmetric eigenvalue decompos-\nition, Japan J. Indust. Appl. Math. 35, 1007–1035.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n411\nT. Ogita and K. Aishima (2019), Iterative reﬁnement for symmetric eigenvalue decompos-\nition II: Clustered eigenvalues, Japan J. Indust. Appl. Math. 36, 435–459.\nT. Ogita and K. Aishima (2020), Iterative reﬁnement for singular value decomposition\nbased on matrix multiplication, J. Comput. Appl. Math. 369, 112512.\nE. Oktay and E. Carson (2022), Multistage mixed precision iterative reﬁnement, Numer.\nLinear Algebra Appl. Available at doi:10.1002/nla.2434.\nK. L. Oo and A. Vogel (2020), Accelerating geometric multigrid preconditioning with\nhalf-precision arithmetic on GPUs. Available at arXiv:2007.07539.\nR. Ooi, T. Iwashita, T. Fukaya, A. Ida and R. Yokota (2020), Eﬀect of mixed precision\ncomputing on H-matrix vector multiplication in BEM analysis, in Proceedings of the\nInternational Conference on High Performance Computing in Asia-Paciﬁc Region, ACM\nPress.\nS.-i. O’uchi, H. Fuketa, T. Ikegami, W. Nogami, T. Matsukawa, T. Kudoh and R. Takano\n(2018), Image-classiﬁer deep convolutional neural network training by 9-bit dedicated\nhardware to realize validation accuracy and energy eﬃciency superior to the half pre-\ncision ﬂoating point format, in 2018 IEEE International Symposium on Circuits and\nSystems (ISCAS), IEEE, pp. 1–5.\nC. C. Paige, M. Rozložník and Z. Strakoš (2006), Modiﬁed Gram–Schmidt (MGS), least\nsquares, and backward stability of MGS-GMRES, SIAM J. Matrix Anal. Appl. 28,\n264–284.\nT. N. Palmer (2014), More reliable forecasts with less precise computations: A fast-\ntrack route to cloud-resolved weather and climate simulators?, Phil. Trans. R. Soc. A\n372 (2018), 1–14.\nT. N. Palmer (2020), The physics of numerical analysis: A climate modelling case study,\nPhil. Trans. R. Soc. A 378 (2166), 1–6.\nM. Petschow, E. Quintana-Ortí and P. Bientinesi (2014), Improved accuracy and parallelism\nfor MRRR-based eigensolvers: A mixed precision approach, SIAM J. Sci. Comput. 36,\nC240–C263.\nL. Pisha and L. Ligowski (2021), Accelerating non-power-of-2 size Fourier transforms\nwith GPU tensor cores, in 2021 IEEE International Parallel and Distributed Processing\nSymposium (IPDPS), IEEE, pp. 507–516.\nR. Ralha (2018), Mixed precision bisection, Math. Comput. Sci. 12, 173–181.\nC. Rubio-González, C. Nguyen, H. D. Nguyen, J. Demmel, W. Kahan, K. Sen, D. H.\nBailey, C. Iancu and D. Hough (2013), Precimonious: Tuning assistant for ﬂoating-\npoint precision, in Proceedings of the International Conference on High Performance\nComputing, Networking, Storage and Analysis (SC ’13), ACM Press, article 27.\nP. San Juan, R. Rodríguez-Sánchez, F. D. Igual, P. Alonso-Jordá and E. S. Quintana-\nOrtí (2021), Low precision matrix multiplication for eﬃcient deep learning in NVIDIA\ncarmel processors, J. Supercomput. 77, 11257–11269.\nM. Sato, Y. Ishikawa, H. Tomita, Y. Kodama, T. Odajima, M. Tsuji, H. Yashiro, M. Aoki,\nN. Shida, I. Miyoshi, K. Hirai, A. Furuya, A. Asato, K. Morita and T. Shimizu (2020),\nCo-design for A64FX manycore processor and ‘Fugaku’, in Proceedings of the Interna-\ntional Conference for High Performance Computing, Networking, Storage and Analysis\n(SC ’20), IEEE.\nK. Scheinberg (2016), Evolution of randomness in optimization methods for supervised\nmachine learning, SIAG/OPT Views and News 24, 1–8.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n412\nN. J. Higham and T. Mary\nO. Schenk, K. Gärtner, W. Fichtner and A. Stricker (2001), PARDISO: A high-performance\nserial and parallel sparse linear solver in semiconductor device simulation, Future Gener.\nComput. Syst. 18, 69–78.\nV. Simoncini and D. B. Szyld (2003), Theory of inexact Krylov subspace methods and\napplications to scientiﬁc computing, SIAM J. Sci. Comput. 25, 454–477.\nR. D. Skeel (1980), Iterative reﬁnement implies numerical stability for Gaussian elimina-\ntion, Math. Comp. 35, 817–832.\nA. Smoktunowicz and J. Sokolnicka (1984), Binary cascades iterative reﬁnement in\ndoubled-mantissa arithmetics, BIT 24, 123–127.\nA. Sorna, X. Cheng, E. D’Azevedo, K. Won and S. Tomov (2018), Optimizing the fast\nFourier transform using mixed precision on tensor core hardware, in 2018 IEEE 25th\nInternational Conference on High Performance Computing Workshops (HiPCW), IEEE,\npp. 3–7.\nA. Stathopoulos and K. Wu (2002), A block orthogonalization procedure with constant\nsynchronization requirements, SIAM J. Sci. Comput. 23, 2165–2182.\nG. W. Stewart (1973), Introduction to Matrix Computations, Academic Press.\nN. J. Stor, I. Slapničar and J. L. Barlow (2015), Accurate eigenvalue decomposition of real\nsymmetric arrowhead matrices and applications, Linear Algebra Appl. 464, 62–89.\nY. Sumiyoshi, A. Fujii, A. Nukada and T. Tanaka (2014), Mixed-precision AMG method\nfor many core accelerators, in Proceedings of the 21st European MPI Users’ Group\nMeeting (EuroMPI/ASIA ’14), ACM Press, pp. 127–132.\nJ. Sun, G. D. Peterson and O. O. Storaasli (2008), High-performance mixed-precision\nlinear solver for FPGAs, IEEE Trans. Comput. 57, 1614–1623.\nG. Tagliavini, S. Mach, D. Rossi, A. Marongiu and L. Benin (2018), A transprecision\nﬂoating-point platform for ultra-low power computing, in 2018 Design, Automation and\nTest in Europe Conference and Exhibition (DATE), pp. 1051–1056.\nR. Tamstorf, J. Benzaken and S. F. McCormick (2021), Discretization-error-accurate\nmixed-precision multigrid solvers, SIAM J. Sci. Comput. 43, S420–S447.\nO. Tintó Prims, M. C. Acosta, A. M. Moore, M. Castrillo, K. Serradell, A. Cortés and\nF. J. Doblas-Reyes (2019), How to use mixed precision in ocean models: Exploring a\npotential reduction of numerical precision in NEMO 4.0 and ROMS 3.6, Geosci. Model\nDev. 12, 3135–3148.\nF. Tisseur (2001), Newton’s method in ﬂoating point arithmetic and iterative reﬁnement of\ngeneralized eigenvalue problems, SIAM J. Matrix Anal. Appl. 22, 1038–1057.\nT. Trader (2016), IBM advances against x86 with Power9.\nAvailable at https://www.\nhpcwire.com/2016/08/30/ibm-unveils-power9-details/.\nY. M. Tsai, P. Luszczek and J. Dongarra (2021), Mixed-precision algorithm for ﬁnding\nselected eigenvalues and eigenvectors of symmetric and Hermitian matrices. Technical\nreport ICL-UT-21-05, Innovative Computing Laboratory, The University of Tennessee,\nKnoxville, TN, USA.\nE. Tsuchida and Y.-K. Choe (2012), Iterative diagonalization of symmetric matrices in\nmixed precision and its application to electronic structure calculations, Comput. Phys.\nComm. 183, 980–985.\nK. Turner and H. F. Walker (1992), Eﬃcient high accuracy solutions with GMRES(m),\nSIAM J. Sci. Statist. Comput. 12, 815–825.\nJ. van den Eshof and G. L. G. Sleijpen (2004), Inexact Krylov subspace methods for linear\nsystems, SIAM J. Matrix Anal. Appl. 26, 125–153.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\nMixed precision algorithms in numerical linear algebra\n413\nF. Váňa, P. Düben, S. Lang, T. Palmer, M. Leutbecher, D. Salmond and G. Carver (2017),\nSingle precision in weather forecasting models: An evaluation with the IFS, Mon.\nWeather Rev. 145, 495–502.\nJ. von Neumann and H. H. Goldstine (1947), Numerical inverting of matrices of high order,\nBull. Amer. Math. Soc. 53, 1021–1099.\nE. Wang, J. J. Davis, R. Zhao, H.-C. Ng, X. Niu, W. Luk, P. Y. K. Cheung and G. A.\nConstantinides (2019), Deep neural network approximation for custom hardware, ACM\nComput. Surv. 52, 1–39.\nN. Wang, J. Choi, D. Brand, C.-Y. Chen and K. Gopalakrishnan (2018), Training deep\nneural networks with 8-bit ﬂoating point numbers, in Advances in Neural Information\nProcessing Systems 31 (S. Bengio et al., eds), Curran Associates, pp. 7686–7695.\nS. Wang and P. Kanwar (2019), BFloat16: The secret to high performance on cloud TPUs.\nAvailable at https://cloud.google.com/blog/products/ai-machine-learning/bﬂoat16-the-\nsecret-to-high-performance-on-cloud-tpus.\nJ. H. Wilkinson (1948), Progress report on the Automatic Computing Engine. Report\nMA/17/1024, Mathematics Division, Department of Scientiﬁc and Industrial Research,\nNational Physical Laboratory, Teddington, UK.\nJ. H. Wilkinson (1961), Error analysis of direct methods of matrix inversion, J. Assoc.\nComput. Mach. 8, 281–330.\nJ. H. Wilkinson (1963), Rounding Errors in Algebraic Processes, Notes on Applied Sci-\nence No. 32, Her Majesty’s Stationery Oﬃce. Also published by Prentice Hall, USA.\nReprinted by Dover, 1994.\nJ. H. Wilkinson (1977), The use of the single-precision residual in the solution of linear\nsystems. Unpublished manuscript.\nI. Yamazaki, S. Tomov and J. Dongarra (2015a), Mixed-precision Cholesky QR factoriza-\ntion and its case studies on multicore CPU with multiple GPUs, SIAM J. Sci. Comput.\n37, C307–C330.\nI. Yamazaki, S. Tomov and J. Dongarra (2016), Stability and performance of various\nsingular value QR implementations on multicore CPU with a GPU, ACM Trans. Math.\nSoftware 43, 10.\nI. Yamazaki, S. Tomov, T. Dong and J. Dongarra (2015b), Mixed-precision orthogon-\nalization scheme and adaptive step size for improving the stability and performance\nof CA-GMRES on GPUs, in High Performance Computing for Computational Science\n(VECPAR 2014) (M. Daydé et al., eds), Vol. 8969 of Lecture Notes in Computer Science,\nSpringer, pp. 17–30.\nI. Yamazaki, S. Tomov, J. Kurzak, J. Dongarra and J. Barlow (2015c), Mixed-precision\nblock Gram Schmidt orthogonalization, in Proceedings of the 6th Workshop on Latest\nAdvances in Scalable Algorithms for Large-Scale Systems (ScalA ’15), ACM Press.\nK. Yang, Y.-F. Chen, G. Roumpos, C. Colby and J. Anderson (2019), High performance\nMonte Carlo simulation of Ising model on TPU clusters, in Proceedings of the Interna-\ntional Conference for High Performance Computing, Networking, Storage and Analysis\n(SC ’19), ACM Press.\nL. M. Yang, A. Fox and G. Sanders (2021), Rounding error analysis of mixed precision\nblock Householder QR algorithms, SIAM J. Sci. Comput. 43, A1723–A1753.\nS. Zhang, E. Baharlouei and P. Wu (2020), High accuracy matrix computations on neural\nengines: A study of QR factorization and its applications, in Proceedings of the 29th\nInternational Symposium on High-Performance Parallel and Distributed Computing,\nACM Press.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n\n414\nN. J. Higham and T. Mary\nY.-K. Zhu and W. B. Hayes (2009), Correct rounding and a hybrid approach to exact\nﬂoating-point summation, SIAM J. Sci. Comput. 31, 2981–3001.\nZ. Zlatev (1982), Use of iterative reﬁnement in the solution of sparse linear systems, SIAM\nJ. Numer. Anal. 19, 381–399.\nM. Zounon, N. J. Higham, C. Lucas and F. Tisseur (2022), Performance impact of precision\nreduction in sparse linear systems solvers, PeerJ Comput. Sci. 8, e778.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n",
  "normalized_text": "Acta Numerica (2022), pp. 347–414\nPrinted in the United Kingdom\ndoi:10.1017/S0962492922000022\nMixed precision algorithms in\nnumerical linear algebra\nNicholas J. Higham\nDepartment of Mathematics, University of Manchester,\nManchester, M13 9PL, UK\nE-mail: nick.higham@manchester.ac.uk\nTheo Mary\nSorbonne Université, CNRS, LIP6,\nParis, F-75005, France\nE-mail: theo.mary@lip6.fr\nToday’s ﬂoating-point arithmetic landscape is broader than ever. While scientiﬁc\ncomputing has traditionally used single precision and double precision ﬂoating-point\narithmetics, half precision is increasingly available in hardware and quadruple precision is supported in software. Lower precision arithmetic brings increased speed and\nreduced communication and energy costs, but it produces results of correspondingly\nlow accuracy. Higher precisions are more expensive but can potentially provide great\nbeneﬁts, even if used sparingly. A variety of mixed precision algorithms have been\ndeveloped that combine the superior performance of lower precisions with the better\naccuracy of higher precisions. Some of these algorithms aim to provide results of\nthe same quality as algorithms running in a ﬁxed precision but at a much lower cost;\nothers use a little higher precision to improve the accuracy of an algorithm. This\nsurvey treats a broad range of mixed precision algorithms in numerical linear algebra, both direct and iterative, for problems including matrix multiplication, matrix\nfactorization, linear systems, least squares, eigenvalue decomposition and singular\nvalue decomposition. We identify key algorithmic ideas, such as iterative reﬁnement, adapting the precision to the data, and exploiting mixed precision block fused\nmultiply–add operations. We also describe the possible performance beneﬁts and\nexplain what is known about the numerical stability of the algorithms. This survey\nshould be useful to a wide community of researchers and practitioners who wish to\ndevelop or beneﬁt from mixed precision numerical linear algebra algorithms.\n© The Author(s), 2022. Published by Cambridge University Press.\nThis is an Open Access article, distributed under the terms of the Creative Commons Attribution\nlicence (http://creativecommons.org/licenses/by/4.0/), which permits unrestricted re-use, distribution,\nand reproduction in any medium, provided the original work is properly cited.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n348\nN. J. Higham and T. Mary\nCONTENTS\n1\nIntroduction\n348\n2\nFloating-point arithmetics\n352\n3\nRounding error analysis model\n360\n4\nMatrix multiplication\n361\n5\nNon-linear equations\n364\n6\nIterative reﬁnement for Ax = b\n369\n7\nDirect methods for Ax = b\n371\n8\nIterative methods for Ax = b\n381\n9\nMixed precision orthogonalization and\nQR factorization\n385\n10 Least squares problems\n388\n11 Eigenvalue decomposition\n389\n12 Singular value decomposition\n392\n13 Multiword arithmetic\n393\n14 Adaptive precision algorithms\n396\n15 Miscellany\n399\nAcknowledgements\n401\nReferences\n401\n1. Introduction\nTraditionally, scientiﬁc computing has been carried out in double precision arithmetic, which nowadays corresponds to a 64-bit ﬂoating-point number format. It\nhas long been recognized that single precision computations have advantages over\ndouble precision ones, not just because single precision arithmetic is typically twice\nas fast as double precision arithmetic but also because single precision data requires\nhalf as much storage as double precision data and has half the memory transfer\ncosts. Of course, single precision computations will generally provide only single\nprecision accuracy. Whether this is suﬃcient for a given application depends on\nthe application, and the answer can be diﬀerent even for diﬀerent computations\nwithin the same ﬁeld: see Section 1.2.2.\nModern hardware increasingly supports half precision arithmetic, which is attractive compared with single and double precisions because of its speed, its lower\nenergy usage and its reduced storage and data movement costs.\nAs we now have three precisions of ﬂoating-point arithmetic in hardware, as\nwell as quadruple precision arithmetic in software, we are in an intrinsically mixed\nprecision world, where precisions can be judiciously chosen in order to make the\nbest use of our computational resources.\nIn this work we survey mixed precision algorithms in numerical linear algebra.\nRelevant work goes back to the beginning of the digital computer area, but most\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n349\ncontributions in this area have been made in the last couple of decades. An earlier\nsurvey of the same areas is that of Abdelfattah et al. (2021a).\n1.1. Mixed precision versus multiprecision\nWe use the following terminology.\n• A mixed precision algorithm uses two or more precisions chosen from a\nsmall number of available precisions, which are typically half, single and\ndouble precision, provided in hardware, and quadruple precision, provided in\nsoftware.\n• A multiprecision algorithm uses one or more arbitrary precisions, which may\nbe problem-dependent and are provided in software. The precision at which\nresults are returned may be ﬁxed (for example, double precision) or may be\na parameter. See Section 2.5 for details of some available multiprecision\narithmetics.\nThe term variable precision is sometimes used in place of\nmultiprecision.\nThis survey is concerned with mixed precision algorithms, but we will brieﬂy\ndiscuss some multiprecision algorithms in Section 15.2.\n1.2. Applications\nMixed precision algorithms are being used, or considered for use in a wide variety of\napplications, some of which involve computations at very large scale. We mention\nsome examples here in order to illustrate the diﬀerent motivations for using mixed\nprecision arithmetic and the possible beneﬁts in real-life applications.\n1.2.1. Simulations\nIdomura, Ina, Ali and Imamura (2020) carry out plasma turbulence simulations for\nthe next generation experimental fusion reactor ITER on the Fugaku and Summit\nsupercomputers.\nTheir code for integrating the gyrokinetic partial diﬀerential\nequation (PDE) in double precision involves the solution of linear systems by a\nKrylov method. The authors show that using a communication-avoiding version\nof the Krylov method with a half precision (fp16) version of the preconditioner\nresults in speedups over the original solver by a factor of approximately 2–3.\nYang et al. (2019) implement in TensorFlow a Monte Carlo simulation of the\nIsing model on a two-dimensional lattice and run it on Google Tensor Processing\nUnits (TPUs). They ﬁnd that single precision can be replaced by half precision\n(bﬂoat16) without any loss of accuracy, enabling larger lattices to be simulated\nbecause of the lower memory requirement of half precision.\n1.2.2. Climate modelling and weather forecasting\nIn climate modelling and weather forecasting, codes have traditionally used double\nprecision variables, but in recent years the use of lower precisions has been extensively investigated (Palmer 2014). The lesser data movement and faster execution\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n350\nN. J. Higham and T. Mary\nof lower precision arithmetic oﬀers the possibility of using reﬁned spatial grids\nrepresented in lower precision, allowing higher resolution simulations with no increase in run time, potentially improving the output of a model. The observations\non which a model is built have low precision, so it can be argued that variables\ndo not need to be represented in double precision (Tintó Prims et al. 2019), and\nthis argument is also supported by the notion that the grid parametrizations should\nbe stochastic (Palmer 2020). Moreover, many model components have uncertainties that can be much larger than the level of double precision (Dawson, Düben,\nMacLeod and Palmer 2018). Codes in this area can consist of millions of lines of\nFortran (Bauer et al. 2021), so changing the precisions of variables and assessing\nthe eﬀect of the changes is not an easy task.\nVáňa et al. (2017) show that almost all double precision variables in the Integrated Forecast System of the European Centre for Medium-Range Weather\nForecasts can be converted to single precision with no noticeable loss in accuracy\nand a gain in speed of about 40%. A beneﬁt of running the code in lower precision\nwas found to be that it revealed places where the code could be made more robust.\nHarvey and Verseghy (2015) had a diﬀerent experience with their code for a land\nsurface model, where running in single precision instead of double did not provide\nsuﬃcient accuracy for some of the land depths and timescales of interest.\nA weather and climate simulation code called the Uniﬁed Model (UM) is used\nby the Met Oﬃce for both operational numerical weather prediction and climate\nmodelling. The code carries out time integration of a system of PDEs, which\ninvolves at each time step the solution of a linear system with a banded, timevarying non-symmetric matrix of size 3.5 × 108. The system is solved by the\npreconditioned BiCGstab algorithm, with a convergence test requiring a residual\nof norm 10−4 relative to the right-hand side. The UM is coded in double precision\nand is memory-bound (that is, its execution time is determined by the speed at which\ndata is transferred from main memory to the arithmetic units rather than by the speed\nof the ﬂoating-point arithmetic). Maynard and Walters (2019) implemented the\nlinear system solution almost entirely in single precision, with the same convergence\ntolerance, obtaining close to a factor 2 speedup of the solver, which they attribute\nto the reduction in data movement costs. To alleviate some stability issues they use\na mixed precision summation algorithm that is essentially the FABsum algorithm\nof Blanchard, Higham and Mary (2020a), which is discussed in Section 4.2. The\nmixed precision solver is now used in operational forecasts.\n1.2.3. Machine learning\nLow precision arithmetic has become widely used in machine learning in the last\nfew years because it has been found experimentally that algorithms can run faster\nwith certain parts executed in low precision, with little or no deterioration in the\nquality of the results. Dean (2020) gives three characteristics of deep learning\nmodels that make specialized hardware suitable for running them.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n351\nFirst, they are very tolerant of reduced-precision computations. Second, the computations\nperformed by most models are simply diﬀerent compositions of a relatively small handful\nof operations like matrix multiplies, vector operations, application of convolutional kernels,\nand other dense linear algebra calculations . . . Third, many of the mechanisms developed\nover the past 40 years to enable general-purpose programs to run with high performance on\nmodern CPUs . . . are unnecessary for machine learning computations. So, the opportunity\nexists to build computational hardware that is specialized for dense, low-precision linear\nalgebra, and not much else, but is still programmable at the level of specifying programs\nas diﬀerent compositions of mostly linear algebra-style operations.\nThe special-purpose hardware being referred to here can be classiﬁed as either\nﬁeld programmable gate arrays (FPGAs) or application-speciﬁc integrated circuits\n(ASICs), and it may use ﬁxed-point arithmetic, ﬂoating-point arithmetic or an\nintermediate between the two called block ﬂoating-point arithmetic (which was in\nuse in the 1960s (Wilkinson 1963, Wang et al. 2019)).\nVariables of diﬀerent precisions arise in machine learning algorithms from quantization, the process of reducing the number of bits per operand. The limiting case is\nbinarization, in which a number has just two possible values, 0 and 1. Quantization\nis applied in various ways, including during training or on a trained model.\nOne of the ﬁrst papers to popularize the use of low precision arithmetic in deep\nlearning is by Courbariaux, Bengio and David (2015), who ﬁnd that ‘very low\nprecision is suﬃcient not just for running trained networks but also for training\nthem.’\nSeveral reasons have been suggested by numerical analysts for the success of low\nprecision ﬂoating-point arithmetic in machine learning. Scheinberg (2016) argues\nthat in machine learning we are solving the wrong problem, namely a surrogate\nfor the original optimization problem, so we do not need an accurate solution. It\ncan also be argued that low precision arithmetic provides regularization and that\nthis is beneﬁcial to machine learning, perhaps by leading to ﬂat minima rather than\nnarrow minima.\nIn machine learning one often updates a parameter φ by a sequence of small\nquantities hi through a recurrence φ(i+1) ←φ(i) + hi, i = 1: n. If hi is of absolute\nvalue less than half the spacing of the ﬂoating-point numbers around φ(i), which is\nmore likely in low precision arithmetic, then φ(i) + hi rounds to φ(i) with round to\nnearest, so the information in hi is lost, and if this happens for many i then the error\nin φ(n+1) can be large. This phenomenon is called stagnation. It can be avoided by\nusing stochastic rounding in place of round to nearest (Connolly, Higham and Mary\n2021, Croci et al. 2022). Stochastic rounding is a randomized form of rounding\nthat rounds to the next larger or next smaller ﬂoating-point number with probability\nproportional to 1 minus the distance to those ﬂoating-point numbers. An early use in\ndeep learning was by Gupta, Agrawal, Gopalakrishnan and Narayanan (2015), who\nﬁnd that ‘deep networks can be trained using only 16-bit wide ﬁxed-point number\nrepresentation when using stochastic rounding, and incur little to no degradation\nin the classiﬁcation accuracy.’\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n352\nN. J. Higham and T. Mary\n1.2.4. HPL-AI Mixed Precision Benchmark\nThe HPL-AI Mixed Precision Benchmark1 is intended to measure supercomputer\nperformance on AI-type workloads. It solves a double precision non-singular linear\nsystem Ax = b of order n using an LU factorization without pivoting computed in\nhalf precision and it reﬁnes the solution using preconditioned GMRES in double\nprecision. As of November 2021, the world record execution rate for the benchmark\nis 2.0 ExaFlop/s (2 × 1018 ﬂoating-point operations per second), where most of\nthe operations are half precision ones, for a matrix of size 16 957 440, which was\nachieved by the Fugaku supercomputer in Japan (Kudo, Nitadori, Ina and Imamura\n2020a,b). The choice of matrix A for the benchmark is critical, as it must be cheap\nto compute, have a controlled condition number, and have a numerically stable\nLU factorization without pivoting; a class of matrices having these properties is\nderived by Fasi and Higham (2021).\n2. Floating-point arithmetics\nSupport for more than one precision of ﬂoating-point arithmetic, provided in hardware or software, has existed throughout the history of digital computing.\nA\nlandmark was the Fortran 66 standard (ANSI 1966), which included the real and\ndouble precision data types and so made it possible to write portable programs that\nused two precisions.\nSome early machines that supported d-digit but not 2d-digit arithmetic oﬀered\nthe ability to accumulate an inner product of d-digit vectors in a 2d-digit accumulator, only rounding back to d digits after the ﬁnal addition. This mode of\ncomputation was discussed by von Neumann and Goldstine (1947, Section 2.3)\nand was exploited by Wilkinson (1948, 1961) on the ACE computer and by Moler\n(1967) on the IBM 7094. Even earlier, desk calculating machines such as the\nBrunsviga oﬀered accumulators with more digits than the input or the registers\n(Croarken 1985, Section 1.2.1).\nUp to the mid 1980s, most computers used for scientiﬁc computing oﬀered both\nsingle precision and double precision ﬂoating-point arithmetic, but the formats of\nthe precisions varied greatly between machines. For example, a double precision\nnumber had a 96-bit signiﬁcand on the Cray-1, a 53-bit signiﬁcand on the DEC VAX\n(G format) and a 14-hexadecimal digit signiﬁcand on the IBM 3090. This lack\nof uniformity, and more importantly the diﬀering properties of the arithmetics,\nhindered the development of software intended to perform consistently across\ndiﬀerent machines.\n2.1. IEEE arithmetics\nA major breakthrough for scientiﬁc computing was the publication of the ANSI/\nIEEE Standard 754-1985 for Binary Floating-Point Arithmetic (IEEE 1985), which\n1 https://icl.bitbucket.io/hpl-ai/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n353\nTable 2.1. Parameters for ﬁve ﬂoating-point arithmetics: number of bits in signiﬁcand (including implicit most signiﬁcant bit) and exponent (sig, exp); unit\nroundoﬀu; smallest positive (subnormal) number xs\nmin; smallest positive normalized number xmin; and largest ﬁnite number xmax. The last four columns are given\nto three signiﬁcant ﬁgures. In Intel’s bﬂoat16 speciﬁcation subnormal numbers are\nnot supported (Intel Corporation 2018).\n(sig, exp)\nu\nxs\nmin\nxmin\nxmax\nbﬂoat16\n(8, 8)\n3.91 × 10−3\n9.18 × 10−41\n1.18 × 10−38\n3.39 × 1038\nfp16\n(11, 5)\n4.88 × 10−4\n5.96 × 10−8\n6.10 × 10−5\n6.55 × 104\nfp32\n(24, 8)\n5.96 × 10−8\n1.40 × 10−45\n1.18 × 10−38\n3.40 × 1038\nfp64\n(53, 11)\n1.11 × 10−16\n4.94 × 10−324\n2.22 × 10−308\n1.80 × 10308\nfp128\n(113, 15)\n9.63 × 10−35\n6.48 × 10−4966\n3.36 × 10−4932\n1.19 × 104932\nprovided binary ﬂoating-point formats and precise rules for carrying out arithmetic\non them. The standard had been carefully designed over several years by a committee of experts from academia and industry, and it brought much-needed order\nto computer arithmetic (Kahan 1981). Within a few years virtually all computer\nmanufacturers had adopted it.\nThe 1985 standard prescribed two ﬂoating-point number formats: 32-bit single\nprecision and 64-bit double precision. A 2008 revision (IEEE 2008) added a 128bit quadruple precision format and a 16-bit half precision format, the latter deﬁned\nas a storage format only rather than for computation. The half precision format was\nmotivated by the emergence of support for 16-bit formats on graphical processing\nunits (GPUs), where these formats were used for graphics and gaming.\nTo deﬁne the IEEE formats we recall that a ﬂoating-point number system is a\nﬁnite subset F = F(β, t, emin, emax) of R whose elements have the form\nx = ±m × βe−t+1.\n(2.1)\nHere, β is the base, which is 2 on virtually all current computers. The integer t is the\nprecision and the integer e is the exponent, which lies in the range emin ≤e ≤emax,\nand the IEEE standard requires that emin = 1−emax. The signiﬁcand m is an integer\nsatisfying 0 ≤m ≤βt −1. To ensure a unique representation for each non-zero\nx ∈F it is assumed that m ≥βt−1 if x , 0, so that the system is normalized.\nThe largest and smallest positive numbers in the system are xmax = βemax(β−β1−t)\nand xmin = βemin, respectively. Two other important quantities are u = 1\n2 β1−t, the\nunit roundoﬀ, and ϵ = β1−t, the machine epsilon, which is the distance from 1 to\nthe next larger ﬂoating-point number.\nNumbers with e = emin and 0 < m < βt−1 are called subnormal numbers. They\nhave the minimal exponent but fewer than t digits of precision. They form an\nequally spaced grid between 0 and the smallest normalized number.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n354\nN. J. Higham and T. Mary\nSign\nExponent\nSigniﬁcand\n5 bits\n10 (+1) bits\nfp16\nSign\nExponent\nSigniﬁcand\n8 bits\n7 (+1) bits\nbﬂoat16\nFigure 2.1. Comparison of the 16-bit bﬂoat16 and fp16 ﬂoating-point number\nformats. The ‘+1’ refers to the implicit leading bit of the signiﬁcand, which is not\nstored.\nThe parameters for the IEEE formats are given in Table 2.1. We refer to these\nformats as ‘fpxy’, where the integer ‘xy’ speciﬁes the number of bits in a ﬂoatingpoint number (the IEEE standard uses the terminology ‘binaryxy’).\nWhat is perhaps most striking is the great diﬀerence in ranges [xs\nmin, xmax]\nbetween the formats, and especially the narrow range [xmin, xmax] ≈[6 × 10−5, 6 ×\n104] for fp16. This means that a large proportion of fp32 and fp64 numbers are\nnot representable as ﬁnite, non-zero fp16 numbers; as we will see, this means that\ncareful scaling is needed in mixed precision algorithms that use fp16.\n2.2. Other arithmetics\nA ﬂoating-point number format called bﬂoat16 was proposed by researchers in the\nGoogle Brain artiﬁcial intelligence research group. Like fp16, it is a 16-bit format,\nbut it allocates bits between the signiﬁcand and exponent diﬀerently: as illustrated\nin Figure 2.1, bﬂoat16 allocates 8 bits for the signiﬁcand and 8 bits for the exponent\nversus 11 bits for the signiﬁcand 5 bits for the exponent for fp16. As shown in\nTable 2.1, the range of bﬂoat16 is very similar to that of fp32 (but not identical,\nbecause of its narrower signiﬁcand), which means that overﬂow in converting to\nbﬂoat16 from higher precisions is much less likely than for fp16. The drawback\nof bﬂoat16 is its low precision: about three decimal digits of precision versus four\nfor fp16. Bﬂoat16 has been taken up by Intel (Intel Corporation 2018), Arm and\nNVIDIA (beginning with the Ampere architecture).\nThere is no generally accepted 8-bit quarter precision format, though suggestions\nhave been made by Moler (2017) (and implemented in MATLAB by Moler2),\nTagliavini et al. (2018) and Wang et al. (2018).\nDouble-double arithmetic is a form of quadruple precision arithmetic in which\na quadruple precision number is represented as the unevaluated sum of two double\nprecision numbers, one representing the higher-order bits of the signiﬁcand and\n2 http://mathworks.com/matlabcentral/ﬁleexchange/59085-cleve-laboratory\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n355\nthe other the lower-order bits (Muller et al. 2018, Section 14.1). Double-double\narithmetic has a long history going back to the 1960s (Li et al. 2002). It is a\n‘poor man’s quadruple precision’ in that it is slightly less accurate than quadruple\nprecision, has roughly the same range as double precision, and does not inherit\nall the desirable properties of the underlying double precision arithmetic (Joldes,\nMuller and Popescu 2017).\nOther ﬂoating-point formats have been proposed for speciﬁc applications. For\nexample, a 9-bit format with a 4-bit signiﬁcand and a 5-bit exponent is proposed\nby O’uchi et al. (2018) for use in deep learning.\n2.3. Availability in hardware and software\nIEEE single and double precision arithmetic began to be widely supported in\nhardware in the late 1980s and early 1990s. In fact, the Intel 8087 coprocessor,\nproduced before the standard was published, partly supported it. In principle, single\nprecision arithmetic operations should run twice as fast as their double precision\ncounterparts, and single precision variables have the beneﬁt of requiring half the\nstorage of double precision ones, resulting in less data movement, but on Intel chips\nsingle precision had no speed advantage over double precision until the late 1990s,\nwhen Streaming SIMD Extensions (SSE) instructions were introduced (Langou\net al. 2006).\nIEEE fp16 arithmetic began to be supported on NVIDIA GPUs in the Maxwell architectures (Jetson TX1, 2014) and was included in the subsequent Pascal\n(P100, 2016), Volta (V100, 2017), Turing (T4, 2018) and Ampere (A100, 2020)\narchitectures. Fp16 is also supported on AMD GPUs in the GCN and CDNA\narchitectures.\nBﬂoat16 is supported on Google’s TPUs (Norrie et al. 2021), the NVIDIA\nA100 GPU (Choquette et al. 2021, NVIDIA Corporation 2020), the ARM NEON\narchitecture (ARM 2018) and Armv8-A architecture (ARM 2019), the Fujitsu\nA64FX ARM processor (Dongarra 2020, Sato et al. 2020) and the Intel Xeon\nCooper Lake processors. It is not only high-end devices that support half precision:\nthe Raspberry Pi, which uses the Armv8-A architecture, supports Bﬂoat16 (Groote,\nMorel, Schmaltz and Watkins 2021, Section 7.2.1).\nThe future Chinese Sunway exascale computer is scheduled to have double\nprecision arithmetic running at 1 ExaFlop/s and half precision arithmetic running\nat 4 ExaFlop/s (Gao et al. 2021, Section 4).\nQuadruple precision arithmetic is available almost exclusively in software. It\nis supported by some compilers, such as the GNU Compiler Collection (GCC),3\nand in MATLAB through the Symbolic Math Toolbox4 and the Multiprecision\nComputing Toolbox.5 Quadruple precision arithmetic is supported in hardware on\n3 https://gcc.gnu.org/\n4 http://www.mathworks.co.uk/products/symbolic/\n5 http://www.advanpix.com\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n356\nN. J. Higham and T. Mary\nthe IBM Power9 processor (Trader 2016) and the IBM z13 processor (Lichtenau,\nCarlough and Mueller 2016).\nA set of extended and mixed precision Basic Linear Algebra Subprograms\n(BLAS) known as the XBLAS6 provides extended and mixed precision counterparts of selected level 1, 2 and 3 BLAS (Li et al. 2002).\nThey use extended\nprecision internally, deﬁned to mean a precision at least 1.5 times as accurate as\ndouble precision and wider than 80 bits. The input and output arguments remain\nsingle or double precision variables, but some arguments can be of mixed type (real\nor complex) as well as mixed precision (single or double), and the main visible\ndiﬀerence is an extra input argument that speciﬁes the precision at which internal\ncomputations are to be performed. A reference implementation is provided that\nemploys the double-double format described in the previous subsection.\n2.4. Block fused multiply–adds\nSince the 1990s some processors have provided a fused multiply–add (FMA)\noperation that computes x + yz with just one rounding error instead of two: x + yz\nis essentially computed exactly and then rounded. The motivation for an FMA is\nspeed, as it can be implemented in a pipelined fashion so that it takes about the\nsame time as a single multiplication or addition (Muller et al. 2018, Section 3.4.2).\nIn recent years, mixed precision block FMAs (also known as mixed precision\nmatrix multiply–accumulate accelerators) have become available in hardware. In\ngeneral, such a device takes as input matrices A ∈Rb1×b, B ∈Rb×b2 and C ∈\nRb1×b2, where A and B are provided in a given precision ulow and C is either in\nprecision ulow or in a higher precision uhigh, and computes\nD\n|{z}\nulow or uhigh\n=\nC\n|{z}\nulow or uhigh\n+\nA\n|{z}\nulow\nB\n|{z}\nulow\n,\n(2.2)\nreturning D in precision ulow or uhigh.\nThe tensor cores in the NVIDIA Volta and Turing architectures have b1 = b =\nb2 = 4. They require the matrices A and B to be in the fp16 format, C and the result\nD can be in fp16 or fp32, and internal computations are done in fp32 (Appleyard\nand Yokim 2017). Pictorially, we have\nD\n=\nC\n+\nA\nB\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|     {z     }\nfp16 or fp32\n=\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|     {z     }\nfp16 or fp32\n+\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|     {z     }\nfp16\n\n× × × ×\n× × × ×\n× × × ×\n× × × ×\n\n|     {z     }\nfp16\n.\n6 https://netlib.org/xblas/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n357\nTable 2.2. Processing units or architectures equipped with mixed precision block\nfused multiply–add accelerators. Matrix dimensions are expressed as b1 × b × b2,\nwhere the matrix product is of a b1 × b matrix with a b × b2 matrix. The input\nand output precisions ulow and uhigh are deﬁned in (2.2). Sources: ARM (2020),\nChoquette et al. (2021), Jouppi et al. (2020, 2021), Norrie et al. (2021).\nYear of release\nDevice\nMatrix dimensions\nulow\nuhigh\n2016\nGoogle TPU v2\n128 × 128 × 128\nbﬂoat16\nfp32\n2017\nGoogle TPU v3\n128 × 128 × 128\nbﬂoat16\nfp32\n2020\nGoogle TPU v4i\n128 × 128 × 128\nbﬂoat16\nfp32\n2017\nNVIDIA V100\n4 × 4 × 4\nfp16\nfp32\n2018\nNVIDIA T4\n4 × 4 × 4\nfp16\nfp32\n2019\nARMv8.6-A\n2 × 4 × 2\nbﬂoat16\nfp32\n2020\nNVIDIA A100\n8 × 8 × 4\nbﬂoat16\nfp32\n8 × 8 × 4\nfp16\nfp32\n8 × 4 × 4\nTensorFloat-32\nfp32\n2 × 4 × 2\nfp64\nfp64\nThe Ampere architecture oﬀers a wider choice of input data types for the tensor\ncores, including bﬂoat16 and fp32 (NVIDIA Corporation 2020).\nOther instances of block FMAs are the matrix units (MXUs) available on Google\nTPUs (Jouppi et al. 2020, 2021, Wang and Kanwar 2019). They use bﬂoat16 rather\nthan fp16 as the low precision format and operate on square matrices of dimension\n128. Google TPUs are not commercially available.\nTable 2.2 summarizes the properties of block FMAs available in some current\nhardware. We note that the details of the computations, such as rounding modes,\nnormalization of intermediate results and whether subnormal numbers are supported, are generally not available and so must be inferred from experimentation. Fasi,\nHigham, Mikaitis and Pranesh (2021) investigate NVIDIA tensor cores; among\ntheir ﬁndings is that the inner products within the matrix multiplications use round\ntowards zero for the additions and can be non-monotonic.\n2.5. Multiprecision arithmetic\nMultiprecision ﬂoating-point arithmetic is a built-in feature of Maple7 and Mathematica8 as well as the open-source PARI/GP9 and Sage10 computer algebra systems.\n7 http://www.maplesoft.com\n8 http://www.wolfram.com\n9 http://pari.math.u-bordeaux.fr\n10 http://www.sagemath.org\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n358\nN. J. Higham and T. Mary\nIt is available in MATLAB through the Symbolic Math Toolbox and the Multiprecision Computing Toolbox.\nThe programming language Julia11 (Bezanson,\nEdelman, Karpinski and Shah 2017) supports multiprecision ﬂoating-point numbers by means of the built-in data type BigFloat. For other languages third-party\nlibraries are available.\nPython: mpmath12 (Johansson et al. 2013) and SymPy13 (Meurer et al. 2017).\nC: the GNU Multiple Precision Arithmetic Library14 and the GNU MPFR Library15 (Fousse et al. 2007).\nC++: the BOOST libraries.16\nC++ and Fortran: the ARPREC library (Bailey, Hida, Li and Thompson 2002).\nC++: the MPFUN2020 library (Bailey 2021).17\nThe GNU MPFR Library is used in some of the software mentioned above, and\ninterfaces to it are available for several programming languages. It was originally\nintended for high precisions, though recent work has improved its eﬃciency for\nfp64 and fp128 (Lefèvre and Zimmermann 2017). As the documentation notes,18\nthe default exponent range is wide and ‘subnormal numbers are not implemented\n(but can be emulated).’\nNakata (2021) has produced a multiprecision version of the LAPACK library\ncalled MPLAPACK by translating the LAPACK source code from Fortran to C++.\nMPLAPACK has several options for the underlying arithmetic, including quadruple precision provided by GCC, double-double arithmetic, quad-double arithmetic (which represents a number as the unevaluated sum of four double precision\nnumbers, so has about twice the precision of quadruple precision), the GNU Multiple Precision Arithmetic Library and the GNU MPFR Library. The test results\nreported in Nakata (2021) indicate a roughly 1:5:10 ratio of the time for double\nprecision arithmetic, double-double arithmetic and quadruple precision arithmetic\nfor matrix multiplication on an AMD Ryzen multicore CPU.\n2.6. Simulating diﬀerent precisions\nWhen developing mixed precision algorithms one may not have access in hardware\nto all the precisions of interest. Or one may wish to experiment with ﬂoatingpoint formats not yet supported in hardware.\nIt is therefore useful to be able\n11 http://julialang.org\n12 http://mpmath.org\n13 http://www.sympy.org\n14 http://gmplib.org/\n15 http://www.mpfr.org\n16 http://www.boost.org\n17 https://www.davidhbailey.com/dhbsoftware/\n18 https://www.mpfr.org/mpfr-current/mpfr.html\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n359\nto simulate arithmetics of diﬀerent precisions using arithmetic of a given higher\nprecision available in hardware.\nThis capability has proved particularly useful\nfor half precision arithmetic, since initially fp16 was available only on GPUs and\nbﬂoat16 on Google TPUs. In addition to half precision, support for other binary\nformats speciﬁed by the user via the number of bits in the signiﬁcand and the\nexponent is desirable, as well as support for diﬀerent rounding modes.\nThese\nfeatures diﬀerentiate the simulations from the multiprecision arithmetics described\nin the previous section, some of which are parametrized by the number of base 10\ndigits.\nThe MATLAB function chop19 of Higham and Pranesh (2019) rounds the elements of a matrix stored in single precision or double precision to a lower precision\nusing one of several forms of rounding, with the result stored in the original precision. The target format for the rounding is speciﬁed by the number of bits in the\nsigniﬁcand and the maximum value of the exponent. The bﬂoat16, fp16 and fp32\nformats are built-in. Subnormal numbers can be included or not. Six rounding\nmodes are supported: round to nearest using round to even last bit to break ties (the\ndefault), round towards plus inﬁnity, round towards minus inﬁnity, round towards\nzero, and two forms of stochastic rounding. The chop function makes it easy to\nadapt existing codes to mixed precision by wrapping statements in calls to chop,\nand since the chop function is vectorized, few calls to it are typically needed for\nlinear algebra codes.\nThe library CPFloat20 by Fasi and Mikaitis (2020) oﬀers similar functionality\nto chop for C. It comes with a MEX interface to MATLAB, and calling CPFloat\ncan be faster than calling chop for large matrices. Fasi and Mikaitis (2020) oﬀer\na comparison with some other available packages for simulating low precision\nﬂoating-point arithmetics.\nAnother approach to simulation is to provide a new storage class and overload\noperators to do arithmetic on the class. The fp16 half precision MATLAB class of\nMoler (2017)21 introduces a new data type fp16 that implements the fp16 storage\nformat and overloads some basic functions for fp16 arguments. Arithmetic in this\nclass is slow because of both the overhead of object orientation in MATLAB and\nthe cost of converting to and from the fp16 storage format. Moler has also written\na class vfp16 that allows the partitioning of a 16-bit word between signiﬁcand\nand exponent to be varied, in particular allowing bﬂoat16 to be simulated (Moler\n2019). This class also allows subnormals to be included or not and FMAs to be\ndone within the inner products inside a matrix multiplication.\n19 https://github.com/higham/chop\n20 https://github.com/mfasi/cpﬂoat\n21 http://mathworks.com/matlabcentral/ﬁleexchange/59085-cleve-laboratory\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n360\nN. J. Higham and T. Mary\nHalf precision simulations are available in some other languages.\nJulia: the built-in ﬂoat16 (fp16) datatype and the bﬂoat16 package22 provide simulations of these half precision arithmetics.\nC++: a header ﬁle for fp16 is available.23\nPython: NumPy provides a ﬂoat16 data type.24\nThe rpe (reduced ﬂoating-point precision) library of Dawson and Düben (2017)\nprovides a derived type and overloaded operators for Fortran and was developed\nfor use in weather and climate modelling. It emulates the speciﬁed precision but\nin general uses the exponent range of double precision.\nWith all these simulations it is important to realize that one might obtain more\naccurate results than for a true low precision computation because certain operations\nmay be done in higher precision. For example, a language that supports matrix\noperations and provides a half precision data type may implement half precision\nmatrix operations by doing them at higher precision and rounding to half precision.\nFor a detailed discussion of the resulting diﬀerences in accuracy, see Higham and\nPranesh (2019, Section 3).\n3. Rounding error analysis model\nWe denote by fl the operator that rounds a real number into the ﬂoating-point\nnumber system F whose elements are given by (2.1). We recall that if x is in the\nrange of F,\nfl(x) = x(1 + δ),\n|δ| ≤u,\nwhere u is the unit roundoﬀ(Higham 2002, Theorem 2.2).\nUnless otherwise\nstated, when the argument of fl is an expression expr, fl(expr) denotes the result of\nevaluating that expression in ﬂoating-point arithmetic.\nWe will use the standard model of ﬂoating-point arithmetic (Higham 2002,\nSec 2.2), which states that\nfl(x op y) = (x op y)(1 + δ),\n|δ| ≤u,\nop = +, −, ∗, /.\n(3.1)\nThis model is certainly satisﬁed by IEEE arithmetic (in the absence of underﬂow\nor overﬂow), which deﬁnes fl(x op y) to be the rounded exact value.\nA constant that appears in rounding error analyses is\nγn =\nnu\n1 −nu\n(nu < 1).\nWe will use the notation u16, u32, u64 and u128 to denote the unit roundoﬀs\ncorresponding to IEEE arithmetics with the indicated word sizes. These values are\ngiven in the third column of Table 2.1.\n22 https://github.com/JuliaMath/BFloat16s.jl\n23 http://half.sourceforge.net/\n24 https://numpy.org/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n361\nThe rounding error bounds we state in this paper are mostly worst-case bounds\nand can be very pessimistic. For blocked algorithms, worst-case bounds that are\nsmaller by a factor equal to the block size can be obtained for many algorithms,\nas explained by Higham (2021). Moreover, under suitable assumptions on the\nrounding errors, probabilistic bounds with constants that are the square roots of\nthe constants in the worst-case bounds can be obtained; see Section 4.3. These\nobservations are important because for low precisions a constant nu (say) in a\nworst-case rounding error bound can exceed 1 even for very modest n.\n4. Matrix multiplication\nIn this section we consider the computation of C = AB, where A ∈Rm×n and\nB ∈Rn×p are two general matrices. If all operations are carried out in a uniform precision u, the computed b\nC satisﬁes the standard bound (Higham 2002,\nSection 3.5)\n| b\nC −C| ≤γn|A||B|,\n(4.1)\nwhere |A| denotes the matrix of absolute values, (|aij|).\nThe presence of the dimension n in bound (4.1), which reﬂects the fact that\nrounding errors accumulate along the inner dimension, may prevent the computation from achieving suﬃcient accuracy when n is large or u is large. Various\napproaches have therefore been proposed to reduce the eﬀect of error accumulation,\nand mixed precision arithmetic is at the heart of several of them.\n4.1. Using block FMAs\nA matrix product can be computed with the aid of a block FMA (2.2). We will\nassume that the internal computations are done at precision uhigh.\nBlock FMAs can be chained together by taking the output D at precision uhigh\nand using it as the input C to a subsequent FMA. Block FMAs thereby provide a\nnatural way to mitigate error accumulation, as accumulation occurs at the level of\nuhigh, not ulow. The required conversion of A and B to ulow is the only source of\nerror of order ulow, and it does not depend on the matrix dimensions.\nAlgorithm 4.1 shows how to use a block FMA to compute a general matrix\nproduct. Three precisions are in play: the working precision u and the precisions\nulow and uhigh, where uhigh ≤ulow.\nThe following error bound, a special case of Blanchard et al. (2020b, Theorem 3.2), describes the result of Algorithm 4.1.\nTheorem 4.1.\nLet the product C = AB of A ∈Rm×n and B ∈Rn×t, given in\nprecision u, be evaluated by Algorithm 4.1, where q = n/b. The computed b\nC\nsatisﬁes\n| b\nC −C| ≤(2ulow + u2\nlow + nuhigh + O(uhighulow))|A||B|.\n(4.2)\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n362\nN. J. Higham and T. Mary\nAlgorithm 4.1. Let A ∈Rm×n and B ∈Rn×t, given in precision u, be partitioned\ninto b1 × b blocks Aij and b× b2 blocks Bij, respectively, where p = m/b1, q = n/b\nand r = t/b2 are assumed to be integers. This algorithm performs the matrix\nmultiplication C = AB using a block FMA.\n1\neA ←fllow(A), eB ←fllow(B)\n2 for i = 1: p\n3\nfor j = 1: r\n4\nCij = 0\n5\nfor ℓ= 1: q\n6\nCompute Cij = Cij + eAiℓeBℓj using a block FMA with output\nat precision uhigh.\n7\nend\n8\nConvert Cij to precision u.\n9\nend\n10 end\nTheorem 4.1 is applicable to NVIDIA Volta and Turing tensor cores with b = 4,\nulow = u16 and uhigh = u32. The theorem is also applicable to the latest Ampere\ngeneration of NVIDIA GPUs, where A and B can also be stored in bﬂoat16 or\ntﬂoat32 arithmetics.25\nWe note that a more general error analysis is given in Blanchard et al. (2020b,\nTheorem 3.2) that allows for a diﬀerent precision in the internal block FMA evaluation.\nOptimized low precision BLAS are available in vendor libraries, such as in\nNVIDIA’s cuBLAS library. Open-source implementations are also available, such\nas that of San Juan et al. (2021), who target the ARM v8.2 architecture, and\nAbdelfattah, Tomov and Dongarra (2019a), who provide batched multiplication\nroutines for NVIDIA GPUs. A batched operation is one in which many independent\noperations on small matrices are grouped together and carried out by a single\nroutine, and the batched BLAS standard described by Abdelfattah et al. (2021b)\nincludes half precision and quadruple precision data types.\n4.2. Blocked summation\nIn the absence of block FMA hardware with internal computations in higher precision, reduced error bounds can still be achieved by changing the summation\nalgorithm used to compute each element cij = Ín\nk=1 aikbk j. In particular, blocked\nalgorithms, which are widely used in numerical linear algebra, compute the sum\ns = Ín\nk=1 xk by grouping summands xk into blocks of size b. Partial sums of b\n25 Tﬂoat32 is a format introduced by NVIDIA for use in tensor cores that has the range of fp32 and\nthe precision of fp16.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n363\nsummands are ﬁrst computed independently, before being combined into the ﬁnal\nresult. By doing so, the term γn in the error bound (4.1) is reduced to γb+n/b−1,\nbecause rounding errors incurred in diﬀerent blocks do not accumulate. Indeed,\nin forming cij, precisely (n/b)(b −1) = n −n/b of the additions are carried out in\ncomputing the partial sums, and these account for the term bu in the bound. Only\nthe last n/b −1 additions account for the error term (n/b −1)u. This observation\ncreates an opportunity for mixed precision arithmetic: by computing these last\nn/b −1 additions in higher precision (say, in precision u2), we can obtain an error\nbound independent of n to ﬁrst order. The next result is a special case of Blanchard\net al. (2020a, Theorem 4.2).\nTheorem 4.2.\nLet the product C = AB of A ∈Rm×n and B ∈Rn×p be evaluated\nby computing the inner products cij = Ín\nk=1 aikbk j by blocks of size b, where\npartial sums of each block are computed in precision u before being combined in\nprecision u2. The computed b\nC satisﬁes\n| b\nC −C| ≤((b + 1)u + (n/b + b2 −1)u2 + O(u3))|A||B|.\n(4.3)\nThis mixed precision summation algorithm is an instance of the FABsum algorithm (Blanchard et al. 2020a), which computes the partial sums with a fast\nsummation and combines them with an accurate summation. The reduction of the\nerror bound is achieved at a modest extra cost, because most of the additions are\nstill carried out in precision u.\nFasi et al. (2022) implement FABsum on NVIDIA GPUs using the CUTLASS26\nlibrary to improve the accuracy of multiword matrix multiplication (see Section 13).\nTheir implementation achieves an improved performance–accuracy tradeoﬀcompared with cuBLAS: depending on the choice of block size and precisions, FABsum\ncan be either as fast as cuBLAS with fp16 tensor cores, but more accurate, or as\naccurate as cuBLAS with fp32 arithmetic, but faster.\n4.3. Probabilistic analyses\nThe bounds (4.1)–(4.3) are worst-case bounds and they do not reﬂect the fact that\nrounding errors of opposite signs can partially cancel each other. Under some\nassumptions on the rounding errors, probabilistic error analyses (Connolly et al.\n2021, Connolly and Higham 2022, Higham and Mary 2019a, 2020, Ipsen and Zhou\n2020) show that the dimension-dependent constants in the bounds can be replaced\nby their square roots. The underlying assumptions of these analyses are not always\nsatisﬁed; one way to enforce them is to use stochastic rounding (Connolly et al.\n2021).\nIn the case where the matrices A and B are generated by sampling their entries\nfrom a random distribution, the sharper analysis of Higham and Mary (2020) shows\nthat the means of the entries play an important role. Speciﬁcally, for zero-mean\n26 https://github.com/nvidia/cutlass\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n364\nN. J. Higham and T. Mary\ndata, the error bound is independent of n. Therefore, given a general matrix, a\nnatural idea is to shift its entries so that they have zero mean. Computing the\nproduct C = AB of the shifted matrices and shifting back the result can provide a\nmuch more accurate result (Higham and Mary 2020, Theorem 4.2). Shifting back\nthe result has negligible cost for large dimensions, but it must be carried out in\nhigher precision.\n4.4. Multiword matrix multiplication with block FMAs\nThe emergence of block FMA hardware that allows for accumulation in higher\nprecision (see Section 2.4) has provided new opportunities to eﬃciently implement\nmatrix multiplication using multiword arithmetic, such as double-fp16 arithmetic,\nwhich approximates fp32 arithmetic by representing numbers as the unevaluated\nsum of two fp16 numbers. These approaches are described in Section 13.\n4.5. Data-driven matrix–vector product\nRecently, various inner product and matrix–vector product algorithms have been\nproposed based on the idea of storing each element in a precision adapted to its\nmagnitude. These approaches are described in Section 14.4.\n5. Non-linear equations\nConsider a system of non-linear algebraic equations\nF(x) = 0,\nF : Rn →Rn.\n(5.1)\nMany problems of interest can be formulated in this form, so we consider the use of\nmixed precision arithmetic in this general context before specializing to particular\nproblems.\nSuppose we have an iteration xk+1 = g(xk) that generates a sequence of vectors\nx0, x1, . . . converging to a solution x∗. An obvious idea is to use on each iteration\narithmetic of the lowest available precision that equals or exceeds the accuracy of\nthe iterates. Therefore we use low precision arithmetic for the early iterations and\nincrease the precision as the iteration proceeds, until the last few iterations are done\nat the working precision. The justiﬁcation is that the rounding errors committed\non the early iterations should be dominated by the inherent iteration errors. If the\niteration is globally convergent this approach should produce a solution of quality\nas good as if the working precision were used throughout, because such an iteration\ndamps out errors, and each iterate xk can be regarded as restarting the iteration\nwith a new starting value.\nThe possible gain in speed from using mixed precision arithmetic in this way\ndepends on the number of iterations required, which in turn depends on the rate\nof convergence and on the cost per iteration. Consider a quadratically convergent\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n365\niteration and a working precision of double. If at the end of the ﬁrst iteration the\nerror is 10−1, the subsequent errors will ideally be 10−2, 10−4, 10−8 and 10−16.\nWe might carry out the ﬁrst three iterations at half precision, the fourth at single\nprecision and the ﬁfth at double precision. Assuming each iteration requires the\nsame number of operations and a ratio of 1:2:4 for the costs of half, single and double\nprecision arithmetic, the overall cost will be a fraction (3/4 + 1/2 + 1)/5 = 9/20\nof the cost of carrying out all the iterations in double precision. A greater speedup\nwill be obtained if a greater proportion of the iterations are carried out at lower\nprecisions, as will be the case if the initial non-asymptotic convergence phase is\nlong, but in this case alternative iterations or methods might be more eﬃcient. The\nassumption that each iteration requires the same number of operations may not\nhold, as we will see in Section 6, and this is why greater speedups are possible.\nVarying the precisions in the way just described does not always work.\nIn\nparticular, it fails for iterations for matrix functions that are not self-correcting,\nsuch as the Newton iteration for the unitary polar factor (a solution to X∗X = I):\nXk+1 = (Xk + X−∗\nk )/2, X0 = A ∈Cn×n (Higham 1986; Higham 2008, Chapter 8).\nThe iteration formula is independent of A, so if we perturb Xk →Xk + Ek with\na general Ek with ∥Ek∥≫u (as opposed to the specially structured errors that\nappear in the exact arithmetic iteration), then important information about A has\nbeen lost and convergence to a matrix with error of order u cannot be obtained.\n5.1. Newton’s method\nNewton’s method is an excellent method for solving (5.1) and it includes various\nparticular methods of interest as special cases. Therefore we will carry out an\nanalysis of Newton’s method in mixed precision arithmetic. Because the analysis\nis general, it may be suboptimal for particular problems, but it will reveal features\ncommon to all.\nWe suppose that F is continuously diﬀerentiable and denote by J its Jacobian\nmatrix (∂Fi/∂xj). Given a starting vector x0, Newton’s method for (5.1) generates\na sequence {xi} deﬁned by\nJ(xi)(xi+1 −xi) = −F(xi),\ni ≥0.\n(5.2)\nAs is well known, under appropriate conditions xi converges to a solution x∗\nfrom any starting vector x0 suﬃciently close to x∗, and the rate of convergence\nis quadratic if J(x∗) is non-singular (Dennis and Schnabel 1983, Theorem 5.2.1).\nWe consider the mixed precision implementation of Newton’s method given in\nAlgorithm 5.1. Here, we evaluate f in a possibly higher precision ur and solve\nfor the update di at a possibly lower precision uℓ(hoping that the resulting errors\nare damped out). In this and the later algorithms, imax is a limit on the number of\niterations and ‘or until converged’ means that the iteration will be terminated if an\nunspeciﬁed convergence test based on the residual or an estimate of the forward\nerror is satisﬁed.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n366\nN. J. Higham and T. Mary\nAlgorithm 5.1. Newton’s method for F(x) = 0 with starting vector x1, in precisions uℓ, u and ur (ur ≤u ≤uℓ).\n1 for i = 1: imax or until converged\n2\nCompute fi = F(xi) in precision ur.\n3\nSolve J(xi)di = −fi in precision uℓ.\n4\nxi+1 = xi + di at precision u.\n5 end\nAccounting for rounding and approximation errors, we can write the computed\niterates bxi as\nbxi+1 = bxi −(J(bxi) + Ei)−1(F(bxi) + ei) + ϵi.\n(5.3)\nThe error terms are explained as follows.\n• ei is the error made in computing F(bxi), and we assume that there is a function\nψ depending on F, bxi, u and ur such that\n∥ei∥≤u∥F(bxi)∥+ ψ(F, bxi, u, ur).\n(5.4)\n• Ei combines the error incurred in forming J(bxi) with the backward error for\nsolving the linear system for di. We assume that\n∥Ei∥≤φ(F, bxi, n, uℓ, u),\n(5.5)\nfor some function φ that reﬂects both the (in)stability of the linear system\nsolver and the error made when approximating or forming J(bxi). In practice,\nwe certainly have φ(F, bxi, n, uℓ, u) ≥u∥J(bxi)∥.\n• ϵi is the rounding error made when adding the correction bdi to bxi, so\n∥ϵi∥≤u(∥bxi∥+ ∥bdi∥).\nThe norm is any absolute vector norm (one for which ∥|v|∥= ∥v∥for all v) and\nthe corresponding subordinate matrix norm.\nNote that (5.3) is a very general model, and with a suitable choice of Ei it\nyields modiﬁed Newton methods, in which the Jacobian is held constant for several\niterations in order to reduce the cost of the method.\nWe wish to know how the precisions aﬀect (a) suﬃcient conditions for convergence and (b) the limiting accuracy and limiting residual, that is, how small\n∥x∗−bxi∥and ∥F(bxi)∥are guaranteed to become as i increases, where x∗is the\nsolution to which the iteration would converge in the absence of errors.\nWe will assume that J is Lipschitz continuous with constant θL, that is,\n∥J(v) −J(w)∥≤θL∥v −w∥\nfor all v, w ∈Rn.\nAnalyses of the eﬀects of diﬀerent sources of error on Newton’s method are\navailable in the literature, for example in Kelley (1995, Section 5.4) and Kelley\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n367\n(2022). Most useful for our purposes are results of Tisseur (2001). The results\nwere originally stated for the situation where just two precisions are in use (uℓ= u),\nbut they are general enough to support a third precision uℓas well. The ﬁrst result\nbounds the limiting accuracy. Here we use the condition number κ(A) = ∥A∥∥A−1∥.\nTheorem 5.1 (Tisseur).\nAssume that there is an x∗such that F(x∗) = 0 and\nJ∗= J(x∗) is non-singular with\nκ(J∗)u ≤1\n8.\n(5.6)\nAssume also that for φ in (5.5),\n∥J(bxi)−1∥φ(F, bxi, n, uℓ, u) ≤1\n8\nfor all i.\n(5.7)\nThen, for all x0 such that\nθL∥J−1\n∗∥∥x0 −x∗∥≤1\n8,\n(5.8)\nNewton’s method in ﬂoating-point arithmetic generates a sequence {bxi} satisfying\n∥bxi+1 −x∗∥≤αi∥bxi −x∗∥+ βi,\n(5.9)\nwhere\nαi ≈∥J(bxi)−1Ei∥+ ∥J−1\n∗∥∥bxi −x∗∥+ κ(J∗)u,\nβi ≈∥J−1\n∗∥∥ψ(F, bxi, u, ur)∥+ u∥x∗∥,\nand the normwise relative error decreases until the ﬁrst i for which\n∥bxi+1 −x∗∥\n∥x∗∥\n≈∥J−1\n∗∥\n∥x∗∥ψ(F, x∗, u, ur) + u.\n(5.10)\nAs a check, we note that in the absence of errors, the terms u, ψ(F, v, u, ur)\nand φ(F, v, n, uℓ, u) are all zero and thus Theorem 5.1 implies local quadratic\nconvergence of Newton’s method.\nIn words, Theorem 5.1 says that if J(x∗) is not too ill-conditioned, the Jacobian\nevaluation and the solver are not too inaccurate, the Lipschitz constant θL is not\ntoo large and the initial guess x0 is not too far from x∗, then the limiting accuracy is\nproportional to the condition of the Jacobian at the solution and the accuracy with\nwhich the residual is evaluated. Note that the function φ does not appear in (5.10),\nwhich shows that errors in forming J and solving the linear system do not aﬀect\nthe limiting accuracy, provided they are not too large. The αi term in (5.9) shows\nthat these errors do, however, aﬀect the rate of convergence, and that this rate is\nessentially independent of ur.\nThe next result bounds the limiting residual.\nTheorem 5.2 (Tisseur).\nUnder the assumptions of Theorem 5.1, if\nθL∥J−1\n∗∥(∥J−1\n∗∥ψ(F, x∗, u, ur) + u∥x∗∥) ≤1\n8,\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n368\nN. J. Higham and T. Mary\nthen, for all x0 such that (5.8) holds, the sequence {∥F(bxi)∥} of residual norms\ngenerated by Newton’s method in ﬂoating-point arithmetic decreases until\n∥F(bxi+1)∥≈ψ(F, bxi, u, ur) + u∥J(bxi)∥∥bxi∥.\n(5.11)\nTheorem 5.2 shows that, under very similar conditions to those in Theorem 5.1,\nthe limiting residual is at the level of the error made in computing the residual plus\nthe term u∥J(bxi)∥∥bxi∥. This latter term is inevitable: from the Taylor series\nF(x∗+ ∆x∗) = F(x∗) + J(x∗)∆x∗+ O(∥∆x∗∥2),\nwe see that merely rounding the exact solution to ex∗= x∗+ ∆x∗, so that ∥∆x∗∥≤\nu∥x∗∥, gives\n∥F(ex∗)∥≲∥J(x∗)∥∥∆x∗∥≤u∥J(x∗)∥∥x∗∥.\nJust as for the limiting accuracy, the limiting residual does not depend on the errors\nin evaluating J or in solving the linear systems.\nSince the limiting accuracy and limiting residual both depend on ψ, Theorems 5.1\nand 5.2 conﬁrm the folklore that Newton’s method must be provided with good\nfunction values if it is to work well in practice.\nAs an application, we consider a linear system F(x) = b −Ax = 0, where\nA ∈Rn×n is non-singular. In principle, Newton’s method converges in one step,\nbut in the presence of errors it becomes an iterative method, namely iterative\nreﬁnement. Here, we have θL = 0. Computing F at precision ur and rounding to\nprecision u gives\nψ(F, bxi, u, ur) ≈γr\nn+1(∥b∥+ ∥A∥∥bxi∥),\nwhere γr\nn+1 = (n + 1)ur/(1 −(n + 1)ur). Hence Theorem 5.1 shows a limiting\naccuracy\n∥bxi+1 −x∗∥\n∥x∗∥\n≈∥A−1∥\n∥x∗∥γr\nn+1(∥b∥+ ∥A∥∥bxi∥) + u\n≲2κ(A)γr\nn+1 + u,\nsince ∥bxi∥≈∥x∗∥and ∥b∥≤∥A∥∥x∗∥. Hence, if ur = u2, the limiting accuracy is\nof order u for κ(A) < u−1. Theorem 5.2 gives a limiting residual\n∥b −Abxi+1∥≈γr\nn+1(∥b∥+ ∥A∥∥bxi∥) + u∥A∥∥bxi∥\n≲(nur + u)(∥b∥+ ∥A∥∥bxi∥),\nwhich means a backward error of order nur + u. Both theorems require (5.7) to\nhold, and since we expect φ to be proportional to uℓfor a solver in precision uℓ,\nthis condition is essentially of the form cnκ(A)uℓ< 1 for some constant cn.\nThis very general analysis of Newton’s method for Ax = b provides signiﬁcant\ninsight into mixed precision iterative reﬁnement, even though we have not speciﬁed\nthe details of the solver. We will derive more speciﬁc and detailed results in the\nnext section.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n369\n6. Iterative reﬁnement for Ax = b\nWe consider the general iterative reﬁnement algorithm given in Algorithm 6.1 for\nsolving a non-singular linear system Ax = b, based on the use of precisions ur ≤u\nand uℓ≥u in addition to the working precision u. The method used to solve for\nthe update vector di on line 3 is arbitrary; we will specialize to particular solvers\nin the following sections.\nFor a discussion of stopping tests see Higham (2002, Section 12.3).\nAlgorithm 6.1. Given a non-singular matrix A ∈Rn×n, b ∈Rn and an initial\napproximation x1, this algorithm uses iterative reﬁnement to solve Ax = b. The\nalgorithm uses three precisions satisfying ur ≤u ≤uℓ.\n1 for i = 1: imax or until converged\n2\nCompute ri = b −Axi in precision ur.\n3\nSolve Adi = ri at precision uℓ.\n4\nUpdate xi+1 = xi + di in precision u.\n5 end\nWe denote the relative error in the solution computed on line 3 by\nξi = ∥di −bdi∥∞\n∥di∥∞\n.\n(6.1)\nLet\nµi =\n∥A(xi −bxi)∥∞\n∥A∥∞∥x1 −bxi∥∞\n≤1,\n(6.2)\nφi = 2 min(cond(A), κ∞(A)µi)uℓ+ ξi,\n(6.3)\nwhere the condition number cond(A) = ∥|A−1||A| ∥∞. We also need the condition\nnumber\ncond(A, x) = ∥|A−1||A||x| ∥∞\n∥x∥∞\n.\nNote that cond(A, x) ≤cond(A) ≤κ∞(A). The next result is by Carson and Higham\n(2018, Corollary 3.3).\nTheorem 6.1.\nLet Algorithm 6.1 be applied with any x1 to a linear system Ax = b,\nwhere A ∈Rn×n is non-singular. As long as φi in (6.3) is suﬃciently less than\n1, the forward error is reduced on the ith iteration by a factor of approximately φi\nuntil an iterate bx is produced for which\n∥bx −x∥∞\n∥x∥∞\n≲u + 4p cond(A, x)ur,\n(6.4)\nwhere p is the maximum number of non-zeros in any row of [A b].\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n370\nN. J. Higham and T. Mary\nTheorem 6.1 shows that the limiting accuracy (6.4) depends on the precisions\nu and ur but does not depend on the precision uℓ, on x1, or on how the system\nAdi = ri at line 3 is solved, provided that it is solved with some relative accuracy\nξi ≪1.\nThe limiting accuracy (6.4) motivates the use of extended precision in the computation of the residual. Indeed, if we set ur = u2, we obtain a limiting accuracy of\norder u, independent of the conditioning of the problem as long as cond(A, x)u ≤1.\n6.1. Historical development\n6.1.1. Traditional iterative reﬁnement\nIterative reﬁnement was programmed on a digital computer by Wilkinson (1948,\np. 111 ﬀ.), using LU factorization with partial pivoting as the solver. Wilkinson,\nand subsequent authors, took advantage in computing the residual of the ability\nof many machines of the time to accumulate inner products at twice the working\nprecision at little or no extra cost (as discussed at the start of Section 2). The\nmethod was also used by Wilkinson and colleagues on desk calculating machines,\nmaking use of their extra length accumulators in computing residuals (Fox, Huskey\nand Wilkinson 1948).\nIterative reﬁnement with extra precision residuals fell out of favour in the 1970s\nbecause machines began to lack the ability to accumulate inner products in extra\nprecision. Indeed the LINPACK library did not include it because it could not be\nimplemented in a portable way in Fortran (Dongarra, Bunch, Moler and Stewart\n1979).\n6.1.2. Fixed precision iterative reﬁnement\nAs the traditional form of iterative reﬁnement declined in popularity, another\nusage came to the fore: ﬁxed precision reﬁnement, in which only one precision is\nused. Jankowski and Woźniakowski (1977) proved that an arbitrary linear equation\nsolver is made normwise backward stable by the use of ﬁxed precision iterative\nreﬁnement, as long as the solver is not too unstable to begin with and A is not too\nill-conditioned. Skeel (1980) analysed ﬁxed precision iterative reﬁnement for LU\nfactorization with partial pivoting and showed that one step of reﬁnement yields a\nsmall componentwise backward error under suitable conditions. Higham (1991)\nextended the componentwise backward error analysis of ﬁxed precision iterative\nreﬁnement to a general solver, and Higham (1997) gave an analysis that covers the\ntraditional and ﬁxed precision forms and a general solver.\n6.1.3. Iterative reﬁnement with lower precision solves\nIn the 2000s, hardware emerged in which fp32 arithmetic was much faster than\nfp64 arithmetic, such as Intel chips with SSE instructions (a factor about 2) and the\nSony/Toshiba/IBM (STI) Cell processor (a factor up to 14) (Kurzak and Dongarra\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n371\n2007). Motivated by this speed diﬀerence, Langou et al. (2006) proposed a new\nusage of iterative reﬁnement in which LU factors computed at a precision lower\nthan the working precision (speciﬁcally, single versus double precision) are used\nto solve on line 3 in Algorithm 6.1.\nCarson and Higham (2017) showed how preconditioned GMRES can be exploited on line 3 in Algorithm 6.1, giving an algorithm called GMRES-based\niterative reﬁnement (GMRES-IR). Carson and Higham (2018) proposed a threeprecision version of iterative reﬁnement, essentially Algorithm 6.1, and gave detailed convergence analysis for the backward error (normwise and componentwise)\nand the forward error.\nAmestoy et al. (2021b) extended the analysis of Carson and Higham (2018) to a\nﬁve-precision form of GMRES-IR.\nMore details of these works are given in Sections 7 and 8.\n6.2. Specialized applications\nMuch work has been done on specializing iterative reﬁnement to particular contexts.\nWe mention just a few examples.\nGovaerts and Pryce (1990) develop and analyse an iterative reﬁnement-based\nalgorithm for solving bordered linear systems Ax = b of order n in which a\nblack box solver is assumed to be available for systems involving the submatrix\nA(1: n −1, 1: n −1). One application is to numerical continuation problems.\nIn some structured problems the elements of A are never formed and so residuals cannot be computed in the usual way via matrix–vector multiplication. An\nexample is when A is a Vandermonde matrix and a fast O(n2) ﬂops algorithm\ntailored to the structure is being used. Higham (1988) develops algorithms for\nsolving Vandermonde-like systems where aij = pi−1(αj), with {pi(x)}n−1\ni=0 a set of\npolynomials satisfying a three-term recurrence relation, such as orthogonal polynomials. The algorithms are numerically unstable for the Chebyshev polynomials,\nbut one step of iterative reﬁnement at the working precision is found to give stability. The residual is evaluated by a nested multiplication algorithm for orthogonal\npolynomials.\nBy steadily increasing the precision during the iterative reﬁnement process it is\npossible to compute solutions to arbitrarily high accuracy, assuming that arithmetic\nof suitable precisions is available.\nThis idea, ﬁrst suggested in an exercise by\nStewart (1973, pp. 206–207) has been investigated by Kiełbasiński (1981) and\nSmoktunowicz and Sokolnicka (1984).\n7. Direct methods for Ax = b\nIn this section we discuss the solution of linear systems by direct methods based\non a factorization of the matrix.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n372\nN. J. Higham and T. Mary\n7.1. LU factorization-based iterative reﬁnement\nAlgorithm 7.1 is a version of Algorithm 6.1 based on an LU factorization of A,\nhereinafter referred to as LU-IR. The LU factorization is computed in precision\nuℓand is used to compute the initial solution x1 and solve the update equation on\nline 5.\nThe only line of the algorithm that costs O(n3) ﬂops is the ﬁrst line, as the\nsubstitutions cost only O(n2) ﬂops. The factorization is carried out at precision uℓ,\nso if uℓ≫u, then if the iteration converges quickly the algorithm is potentially\nsigniﬁcantly faster than solving Ax = b by LU factorization at precision u.\nAlgorithm 7.1 (LU-IR). Given a non-singular matrix A ∈Rn×n and b ∈Rn, this\nalgorithm uses LU factorization-based iterative reﬁnement with three precisions\nsatisfying ur ≤u ≤uℓ, to solve Ax = b.\n1 Compute the factorization A = LU in precision uℓ.\n2 Solve LUx1 = b by substitution in precision uℓ.\n3 for i = 1: imax or until converged\n4\nCompute ri = b −Axi in precision ur.\n5\nSolve LUdi = ri by substitution in precision uℓ.\n6\nUpdate xi+1 = xi + di in precision u.\n7 end\nStandard error analysis (Higham 2002, Theorem 9.4) shows that solving a linear system by substitution in precision uℓwith LU factors computed in precision\nuℓachieves a relative error bounded approximately by 3n∥|A−1||bL|| b\nU|∥uℓ. Theorem 6.1 therefore yields the following result.\nTheorem 7.1.\nLet LU-IR (Algorithm 7.1) be applied to a linear system Ax = b,\nwhere A ∈Rn×n is non-singular. If ∥|A−1||bL|| b\nU|∥uℓis suﬃciently less than 1, then\nthe algorithm produces an iterate bx satisfying (6.4).\nAs mentioned in Section 6, the traditional and ﬁxed precision forms of iterative\nreﬁnement use an LU factorization computed by LU factorization with partial\npivoting with uℓ= u.\nWith such a stable LU factorization, the convergence\ncondition in Theorem 7.1 reduces to κ(A)u ≪1. However, as already mentioned,\nunstable solvers can still lead to convergence. In the context of LU-IR, two main\napproaches have been proposed that introduce potential instability in an attempt to\nincrease speed.\nThe ﬁrst approach is to use a potentially unstable LU factorization in precision\nu, where the instability can come from diﬀerent sources. For example, using a\nweaker form of pivoting to accelerate the factorization and preserve the sparsity of\nthe matrix, such as static pivoting (Li and Demmel 1998, Arioli, Duﬀ, Gratton and\nPralet 2007), can still lead to the convergence of LU-IR. Several sparse direct solvers incorporate static pivoting strategies as an option, such as MUMPS (Amestoy,\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n373\nDuﬀ, L’Excellent and Koster 2001, Amestoy, Buttari, L’Excellent and Mary 2019),\nwhich implements the approach proposed by Duﬀand Pralet (2007), or as the\ndefault, such as SuperLU_DIST (Li and Demmel 2003) and PARDISO (Schenk,\nGärtner, Fichtner and Stricker 2001). Other potentially unstable, but faster, factorizations have been combined with iterative reﬁnement to remedy their instability,\nsuch as incomplete LU factorization (Zlatev 1982) or Cholesky factorization for\nquasideﬁnite systems (Gill, Saunders and Shinnerl 1996).\nThe second approach is to use an LU factorization in lower precision uℓ> u.\nIf the LU factorization algorithm is numerically stable, convergence is guaranteed\nprovided that κ(A)uℓ≪1, as noted above. This approach is attractive because\nmost of the work (O(n3) ﬂops for dense systems) is done in the factorization phase;\nthe iterative phase (O(n2) ﬂops) has negligible cost for large n, as long as the\nnumber of iterations remains reasonable. Thus, asymptotically, we may expect the\nspeed of the entire solution to be determined by the speed of the lower precision\narithmetic. Using the Cell processor, Langou et al. (2006) solve double precision\nlinear systems (u = u64) with speedups of up to a factor of 8 over a double precision\nLU factorization by using LU-IR (Algorithm 7.1) with uℓ= u32 and ur = u.\nFurther experimental results are reported by Buttari et al. (2007) for dense linear\nsystems and by Buttari et al. (2008) for sparse ones. See Baboulin et al. (2009) for\nan overview of the methods developed in this period.\nIterative reﬁnement with LU factorization in lower precision has also been exploited on FPGAs (Sun, Peterson and Storaasli 2008).\nThe popularity of LU-IR with a lower precision factorization grew again with the\nemergence of half precision arithmetic (fp16 and bﬂoat16). Indeed, half precision\narithmetic is at least four times faster than double precision arithmetic, and possibly\nmuch more than that on some hardware, notably on NVIDIA tensor cores (see\nSection 7.3). Haidar, Wu, Tomov and Dongarra (2017) provide the ﬁrst evaluation\nof the potential of half precision for iterative reﬁnement, obtaining speedups of up\nto 2.7 on an NVIDIA P100 GPU using LU-IR with u = ur = u64 and uℓ= u16.\nKudo et al. (2020a,b) implement LU-IR on the Fugaku supercomputer, which is\nequipped with ARM-based Fujitsu A64FX processors that support fp16 arithmetic,\nfor use in the HPL-AI Mixed Precision Benchmark (see Section 1.2.4).\nLU-IR with a half precision factorization can only guarantee convergence for\nwell-conditioned problems: the condition κ(A)uℓ≪1 translates to κ(A) ≪2000\nin fp16 and κ(A) ≪300 in bﬂoat16. Two main approaches have been proposed\nto extend the applicability of half precision iterative reﬁnement to a wider range\nof problems: the ﬁrst uses a more accurate solver on line 5 of Algorithm 7.1 (see\nSection 7.2) and the second uses more accurate hardware such as tensor cores (see\nSection 7.3).\n7.2. GMRES-based iterative reﬁnement\nGMRES-IR (Carson and Higham 2017), mentioned in Section 6.1.3, is described\nin Algorithm 7.2.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n374\nN. J. Higham and T. Mary\nAlgorithm 7.2 (GMRES-IR). Given a non-singular matrix A ∈Rn×n and b ∈Rn,\nthis algorithm solves Ax = b using by GMRES-IR in ﬁve precisions: ur, ug, up, u\nand uℓ.\n1 Compute the factorization A = LU in precision uℓ.\n2 Solve LUx1 = b by substitution in precision uℓ.\n3 for i = 1: imax or until converged\n4\nCompute ri = b −Axi in precision ur.\n5\nSolve U−1L−1Adi = U−1L−1ri by GMRES in precision ug, performing\nthe products with U−1L−1A in precision up.\n6\nCompute xi+1 = xi + di in precision u.\n7 end\nNote that the application of the preconditioner on line 5 involves a multiplication\nby A and substitutions with the LU factors. The results quoted below assume\nthe use of a backward stable implementation of GMRES, such as MGS-GMRES\n(Paige, Rozložník and Strakoš 2006).\n7.2.1. High accuracy solution of ill-conditioned systems\nCarson and Higham (2017) proposed GMRES-IR with two precisions, u = uℓand\nur = ug = up = u2, and were interested in solving ill-conditioned systems to high\naccuracy. They showed that the quantity µi deﬁned in (6.2) tends to be small in\nthe early iterations and gradually grows to order 1 as the iteration proceeds. This\nmeans that in φi in (6.3) the min term is negligible in the early iterations and so\nφi ≈ξi. Carson and Higham (2017) also show that ξi ≈u as long as κ(A) is not\nmuch larger than u−1. Hence Theorem 6.1 guarantees that a limiting accuracy of\norder u will be achieved. In other words, by using a small amount of computation\nat twice the working precision it is possible to solve Ax = b to full accuracy even\nif A is numerically singular!\nIt is important to emphasize that standard methods, such as even the singular\nvalue decomposition (SVD), will not in general yield an accurate solution to an\nill-conditioned system. GMRES-IR computes an accurate solution to the update\nequation, which is relatively well-conditioned thanks to the preconditioning and\nwhich has an accurate right-hand side. The behaviour of µi is also crucial, and it\nhad not been previously been proved or exploited, though Wilkinson (1977) did\nmake an observation that is equivalent to saying that the µi increase with i.\n7.2.2. Exploiting low precision LU factors of an ill-conditioned matrix\nAs mentioned in Section 7.1, one of the main limitations of LU-IR (Algorithm 7.1)\nis that its success is guaranteed only when κ(A)uℓ≪1. If the LU factorization is\ncomputed in low precision, LU-IR is therefore limited to well-conditioned matrices.\nIn this setting, GMRES-IR becomes particularly useful.\nIndeed, even though\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n375\nGMRES-IR was originally intended to solve linear systems nearly singular to the\nworking precision u, as described in the previous subsection, Carson and Higham\n(2018) subsequently proposed using it to exploit LU factors computed in a precision\nuℓ(potentially much) lower than the working precision u. They assume that the\npreconditioner is applied in precision ug = up = u2. Amestoy et al. (2021b) relax\nthis requirement by allowing the products with U−1L−1A to be carried out in a\nprecision up possibly lower than u2, and additionally allow the rest of the GMRES\ncomputations to be performed in a precision ug possibly lower than u. This results\nin the ﬁve-precision algorithm described in Algorithm 7.2. To obtain convergence\nguarantees for this algorithm, Amestoy et al. (2021b) generalize the analysis of\nPaige et al. (2006) on the backward stability of GMRES to a two-precision GMRES\nwith LU preconditioning, and they prove the following theorem.\nTheorem 7.2.\nLet GMRES-IR (Algorithm 7.2) be applied to a linear system\nAx = b, where A ∈Rn×n is non-singular. If κ(A)2u2\nℓ(ug + κ(A)up) is suﬃciently\nless than 1, then the algorithm produces an iterate bx satisfying (6.4).\nAmestoy et al. (2021b) consider the thousands of possible combinations of the\nﬁve precisions in Algorithm 7.2 and narrow down the choice to a few combinations\nof practical interest, from among which one can balance accuracy, robustness and\nperformance. The bounds on κ(A) for convergence guarantees are not always sharp,\nso it can be diﬃcult to decide which variant should be preferred for a particular\nproblem. To address this issue, Oktay and Carson (2022) propose a multistage\niterative reﬁnement that switches to increasingly robust but also more expensive\nvariants by monitoring key quantities during the iterative process.\nHaidar, Tomov, Dongarra and Higham (2018b) and Haidar et al. (2020) implement GMRES-IR with just two precisions: the factorization is in half precision\n(uℓ) and the rest of the operations are in double precision (ug = up = ur = u).\nThey show that for several matrices where LU-IR takes a large number of iterations\nto converge, GMRES-IR can still converge in a small number of iterations and\nthus retains an attractive performance boost compared with LU-IR with a single\nprecision factorization.\nHigham and Mary (2019b) propose a new preconditioner that builds upon the low\nprecision LU factors and exploits a low-rank approximation to speed up GMRESIR.\n7.3. Harnessing tensor cores\nNVIDIA tensor cores present two beneﬁts for iterative reﬁnement compared with\nstandard half precision arithmetic on NVIDIA GPUs. The ﬁrst is that they are\nsigniﬁcantly faster and so computing the LU factorization with tensor cores provides\nmore room to amortize the cost of the iterations in the iterative phase. The second\nbeneﬁt is their improved accuracy since, as discussed in Section 2.4, tensor cores\naccumulate intermediate operations in fp32 arithmetic.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n376\nN. J. Higham and T. Mary\nTensor cores can carry out arbitrarily sized matrix products using Algorithm 4.1\nand so can be naturally exploited by standard blocked LU factorization algorithms,\nwhich mostly consist of matrix–matrix products. Haidar et al. (2018b) propose\nan algorithm that harnesses tensor cores to accelerate the updates of the trailing\nsubmatrix, which account for the O(n3) ﬂops of the factorization; the remaining\nO(n2) ﬂops are carried out by standard ﬂoating-point units in fp32 arithmetic.\nBlanchard et al. (2020b, Theorem 4.4) analyse this algorithm and prove that it\npossesses a reduced backward error bound of order u16 + nu32 instead of the\nstandard bound nu16 of an LU factorization entirely in fp16 arithmetic.\nUsing their mixed precision LU factorizationalgorithmwithinLU-IRorGMRESIR, Haidar et al. (2018b) are able to solve linear systems with fp64 accuracy at a\nspeed of up to 24 TFLOPS on an NVIDIA V100 GPU, which represents a speedup\nof 4 over a double precision solver. Moreover, for some of the more diﬃcult matrices\nin their test set, the solution entirely in fp16 arithmetic requires many iterations\nor does not converge at all, whereas the algorithm using tensor cores maintains a\nfast convergence. This shows that the accuracy boost of tensor cores can strongly\nimprove the convergence of iterative reﬁnement. The use of half precision and/or\ntensor cores also improves the energy eﬃciency of the solution, reducing power\nconsumption by up to a factor of 5 (Haidar et al. 2018a). See Haidar et al. (2020)\nfor a more complete discussion of iterative reﬁnement with tensor cores, and Abdelfattah, Tomov and Dongarra (2019b) for an extension of these approaches to\ncomplex matrices. These ideas are implemented in the MAGMA library,27 in the\nNVIDIA cuSOLVER library28 and also in the SLATE library29 (Charara et al.\n2020), which targets machines with large numbers of cores and multiple hardware\naccelerators per node.\nIn addition to its speed and energy beneﬁts, fp16 arithmetic can also be used to\nreduce memory consumption and data movement. However, special care has to be\ntaken not to lose the accuracy boost of tensor cores. Indeed, tensor cores carry out\ncomputations internally in fp32 arithmetic, and so to beneﬁt from their improved\naccuracy the input matrix C in (2.2) needs to be stored in fp32. Lopez and Mary\n(2020) propose a modiﬁcation of the above approaches of Haidar et al., based on a\nleft-looking Crout factorization that allows them to store the matrix in fp16 while\naccumulating computations in fp32 buﬀers of controlled size. As a result, memory\nconsumption is halved and data movement costs are greatly reduced, making the\nfactorization faster by up to a factor of 2 on NVIDIA V100 GPUs.\n7.4. Scaling strategies\nA limitation of iterative reﬁnement with fp16 as the low precision arithmetic is the\nnarrow range of the arithmetic: as seen in Table 2.1, numbers of magnitude outside\n27 https://icl.utk.edu/magma/\n28 https://developer.nvidia.com/cusolver\n29 https://icl.utk.edu/slate/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n377\nthe interval [xs\nmin, xmax] = [5.96 × 10−8, 6.55 × 104] are not representable and will\nunderﬂow or overﬂow when converted to fp16. Moreover, numbers of magnitude\nsmaller than xmin = 6.10 × 10−5 are often ﬂushed to zero in practice, to avoid the\npossibly heavy performance penalty of handling subnormal numbers.\nIn the LU factorization of a matrix A in fp16 arithmetic, overﬂow and underﬂow\nmay occur during the initial conversion of A to fp16 and also during the LU\nfactorization itself. In particular, note that even LU factorization algorithms using\ntensor cores that keep the original matrix in fp32 are not immune to overﬂow and\nunderﬂow, since the LU factors must be converted to fp16.\nOne way to deal with overﬂow in rounding A to fp16 is to replace any element\naij that overﬂows by sign(aij)θxmax, where θ ∈(0, 1] is a parameter. We will refer\nto this as the overﬂow mapping strategy. This approach is used in Haidar et al.\n(2017, 2018a,b).\nHigham, Pranesh and Zounon (2019) suggest Algorithm 7.3, which uses a twosided diagonal scaling at the working precision and only rounds to fp16 once all\nthe matrix elements do not exceed xmax. The algorithm applies row and column\nequilibration, which produces a matrix eA in which every row and column has\nmaximum element in modulus equal to 1. Then it scales eA so that the maximum\nelement in modulus of the scaled matrix is θxmax and rounds to fp16. Here, θ is\nintended to be reasonably close to 1, in order to maximize the use of the limited\nfp16 range and keep the numbers away from the subnormal zone. If A is symmetric,\na symmetry-preserving two-sided scaling of Knight, Ruiz and Uçar (2014) can be\nused instead of row and column equilibration.\nAlgorithm 7.3. This algorithm rounds A ∈Rn×n to the fp16 matrix A(h), scaling\nall elements to avoid overﬂow. θ ∈(0, 1] is a parameter.\n1 R = diag(∥A(i, : )∥−1\n∞)\n2\neA = RA\n% eA is row equilibrated.\n3 S = diag(∥eA(:, j)∥−1\n∞)\n4\neA = eAS\n% eA is now row and column equilibrated.\n5 Let β be the maximum magnitude of any entry of eA.\n6 µ = θxmax/β\n7 A(h) = flh(µeA)\nHow should θ be chosen? The main requirement is that there is no overﬂow in\nthe LU factorization, which means that we need θ ≤ρ−1\nn , where ρn is the growth\nfactor for LU factorization on A. With partial pivoting, ρn is typically not large,\nso one might take θ = 0.1, as used in Higham et al. (2019).\nHowever, large\ngrowth factors can occur, notably for ‘randsvd matrices’ having one small singular\nvalue (Higham, Higham and Pranesh 2021), and this led to poor performance with\nθ = 0.1 in one of the experiments in Haidar et al. (2020, Section 14(b)).\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n378\nN. J. Higham and T. Mary\nHigham et al. (2019) show experimentally that compared with the overﬂow\nmapping strategy, Algorithm 7.3 leads to faster and more reliable convergence of\nGMRES-IR on badly scaled matrices.\nUnless the LU factors are converted to fp32 precision at the end of the factorization (or are already available in fp32, such as when using tensor cores), the\nsubstitution operations must also be performed in fp16 arithmetic, and are therefore\nvulnerable to overﬂow and underﬂow, especially as the elements of the residual vector ri on line 4 of Algorithm 7.1 must eventually become of order u(∥A∥∥x∥+ ∥b∥),\nand so are likely to underﬂow in fp16. Techniques for avoiding overﬂow in solving\ntriangular systems can be found in Anderson (1991) and Demmel and Li (1994)\n(these are used in the LAPACK subroutine xLATRS), and they can be combined\nwith the simple scaling suggested by Carson and Higham (2018, Section 6) and\nLuszczek, Yamazaki and Dongarra (2019).\n7.5. Exploiting symmetry and positive deﬁniteness\nSuppose, now, that A ∈Rn×n is symmetric positive deﬁnite. In principle, LUIR can be adapted in a straightforward way by replacing LU factorization with\nCholesky factorization. However, there is a problem to overcome: a matrix that\nhas elements stored in a given precision and is symmetric positive deﬁnite may lose\ndeﬁniteness when rounded to a lower precision, and in Algorithm 7.1 we round\nA to precision uℓon the ﬁrst step. We can guarantee to preserve deﬁniteness in\nthe rounding only if κ2(A)uℓ< 1, which is a severe restriction if we are using\nhalf precision. Higham and Pranesh (2021) suggest Algorithm 7.4, which scales\nand shifts in order to ensure a successful Cholesky factorization. The two-sided\nscaling H = D−1AD−1, where D = diag(a1/2\nii ), produces a unit diagonal matrix\nwith oﬀ-diagonal elements bounded in magnitude by 1. This matrix is then shifted\nby an amount intended to lift the smallest eigenvalue suﬃciently above zero, and a\nmultiplicative factor θ is applied that plays the same role as that in Algorithm 7.3.\nAs explained by Higham and Pranesh (2021), shifting H by a multiple of I is better\nthan shifting A by a multiple of I, as it is equivalent to shifting A by a multiple of\ndiag(aii) and so it makes the same relative perturbation to each diagonal element\nof A.\nHigham and Pranesh (2021) give perturbation analysis and error analysis that\nsuggests taking c ≈n2 in Algorithm 7.4, but they ﬁnd this is too pessimistic in\npractice. They recommend taking c as a small constant and found c = 2 to work\nwell in practice, with no need for the doubling on line 8. They use this idea with\nan appropriate modiﬁcation of GMRES-IR in which GMRES is applied to the\npreconditioned update equation M Adi = Mri, where M = µD−1R−1R−T D−1.\nNote that since A is symmetric positive deﬁnite it is more natural to use the\nconjugate gradient (CG) method instead of GMRES, but the supporting rounding\nerror analysis works only for GMRES, because it relies on the backward stability of GMRES and preconditioned CG is not guaranteed to be backward stable\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n379\nAlgorithm 7.4. Given a symmetric positive deﬁnite A ∈Rn×n in precision u, this\nalgorithm computes an approximate Cholesky factorization RTR ≈µD−1AD−1 at\nprecision uℓ> u, where D = diag(a1/2\nii ). The scalar θ ∈(0, 1] and the positive\ninteger c are parameters.\n1 D = diag(a1/2\nii ), H = D−1AD−1\n% Set hii ≡1 instead of computing it.\n2 G = H + cuℓI\n3 β = 1 + cuℓ\n4 µ = θxmax/β\n5 Aℓ= flℓ(µG)\n6 Attempt Cholesky factorization Aℓ= RTR in precision uℓ.\n7 if Cholesky factorization failed\n8\nc ←2c, goto line 2\n9 end\n(Greenbaum 1997, eq. (34)). However, Higham and Pranesh (2021) ﬁnd that in\ntheir experiments CG works as well as GMRES.\nAlgorithm 7.4 has been implemented on an NVIDIA V100 GPU by Abdelfattah,\nTomov and Dongarra (2020), eﬀectively taking uℓ= u16, u = ur = u64, with\nCholesky factorization computed in mixed fp16 and fp32 precisions. With matrices\nof dimensions up to 42 000, they obtained speedups of up to 4.7 over a double\nprecision solver.\n7.6. Sparse matrix considerations\nSparsity presents both opportunities and obstacles to the use of iterative reﬁnement.\nOn the one hand, while LU factorization of dense matrices tends to run twice as\nfast in single precision as in double precision, this speedup may not be attained for\nsparse matrices, for two reasons explained by Zounon, Higham, Lucas and Tisseur\n(2022). The ﬁrst reason is that real-life sparse double precision matrices, such as\nmany of those in the SuiteSparse Matrix Collection30 (Davis and Hu 2011), can\nhave elements of widely varying magnitudes. While the matrix elements usually ﬁt\ninto the range of single precision numbers, LU factorization can generate cascading\nﬁll-ins in which small multipliers combine to produce subnormal numbers. This\ncan cause a signiﬁcant performance loss because ﬂoating-point operations on subnormal numbers can be very slow. A cure is to set a compiler ﬂag to ﬂush subnormal\nnumbers to zero. The second reason why LU factorization of a sparse matrix in\nsingle precision may not give the expected speedup over double precision is that\nthe reordering and analysis phase of the algorithm does not involve ﬂoating-point\narithmetic (Duﬀ, Erisman and Reid 2017) and so does not beneﬁt from reducing\nthe precision. Moreover, if the reordering and analysis is sequential rather than\n30 https://sparse.tamu.edu/. Previously known as the University of Florida Sparse Matrix Collection.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n380\nN. J. Higham and T. Mary\nparallelized, then increasing the number of cores increases the proportion of time\nspent on non-ﬂoating-point arithmetic computations.\nOn the other hand, some of the features of iterative reﬁnement are especially\nattractive when the matrix is sparse. First, as explained by Amestoy et al. (2022),\niterative reﬁnement with a lower precision LU factorization can lead to signiﬁcant\nmemory savings due to the fact that the LU factors of a sparse matrix are typically\nmuch denser, and unlike for dense matrices, the overhead of keeping a high precision\ncopy of the original matrix is negligible. Second, to best preserve the sparsity of\nthe matrix, sparse direct solvers often employ relaxed pivoting strategies, such as\nthreshold partial pivoting (Duﬀet al. 2017, Chapter 7) or the more aggressive static\npivoting (Li and Demmel 1998), which can lead to large growth factors; iterative\nreﬁnement can overcome any resulting numerical instability.\nAmestoy et al. (2022) develop implementations of LU-IR and GMRES-IR based\non a single precision sparse LU factorization computed with the multifrontal solver\nMUMPS and use them to solve with double precision accuracy a range of large and\nill-conditioned sparse systems coming from a variety of applications. They obtain\nreductions of up to a factor of 2 in both execution time and memory consumption\nover the double precision MUMPS solver, with LU-IR being usually faster than\nGMRES-IR, although the latter is more robust and successfully converged for all\ntest problems.\n7.7. Exploiting data sparsity\nIn many applications, the matrix possesses a so-called data sparse structure: many\nof its oﬀ-diagonal blocks have low numerical rank. In the last two decades, several\napproaches have been devised to leverage this property to accelerate the solution\nof linear solvers, such as hierarchical (H) or block low-rank (BLR) methods.\nThe low-rank approximations are computed with a truncation threshold parameter ε, which controls the accuracy of these data sparse solvers, as proved by\nHigham and Mary (2021) in the case of BLR solvers. Thus data sparse solvers\ncan be used either as direct solvers (setting ε to the target accuracy) or as preconditioners to iterative methods. In particular, they can be used in conjunction with\niterative reﬁnement. Amestoy et al. (2022) use the BLR sparse solver MUMPS\n(Amestoy et al. 2019) at low accuracy with LU-IR and GMRES-IR, and obtain\nlarge performance gains with respect to the double precision solver, reducing execution time by up to 5.6× and memory consumption by up to 4.4×. Moreover,\nGMRES-IR can converge for larger values of the parameter ε than LU-IR, which\nleads to increased performance in some cases.\nIn addition to their use in lower precision, data sparse solvers can also beneﬁt\nfrom mixed precision. Indeed, data sparse matrices exhibit blocks of highly unequal\nimportance: those that correspond to weak interactions (and that are usually far\naway from the diagonal) contain less signiﬁcant information and are more resilient\nto the use of reduced precision. As a result, Abdulah et al. (2019) propose to store\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n381\nblocks that are suﬃciently far way from the diagonal in single precision instead of\ndouble precision. They apply this strategy to the Cholesky factorization of data\nsparse covariance matrices arising in geostatistical modelling, obtaining an average\n1.6× speedup. The approach is extended to also include half precision in Abdulah\net al. (2022), leading to an improved 2.6× speedup. Doucet, Ltaief, Gratadour and\nKeyes (2019) use the same approach in a diﬀerent application in computational\nastronomy (tomographic reconstructors) using only single and half precisions on\nNVIDIA V100 GPUs.\nTo go even further, in addition to storing diﬀerent blocks in diﬀerent precisions,\neach block can also use a mixed precision representation. Since most of the blocks\nof data sparse matrices exhibit rapidly decaying singular values, they are amenable\nto the mixed precision low-rank representation proposed by Amestoy et al. (2021a)\nand described in Section 12.2. Amestoy et al. (2021a) apply this approach to the\nLU factorization of BLR matrices and obtain storage and ﬂops reductions of up to\na factor of 3 using fp64, fp32 and bﬂoat16 arithmetics.\n8. Iterative methods for Ax = b\nWe outline three classes of approaches to exploit mixed precision arithmetic in\niterative methods.\nThe ﬁrst approach is to use an inner–outer scheme such as\nGMRES-IR, where the low precision is used by the inner scheme (Section 8.1).\nThe second approach is to use low precision arithmetic to compute and/or apply\nthe preconditioner in a higher precision iterative method (Section 8.2). The third\napproach is to intrinsically use mixed precision within the iterative method, such\nas for inexact Krylov methods (Section 8.3). Finally, we also comment on speciﬁc\nmethods such as communication-avoiding or multigrid methods.\n8.1. GMRES-IR without an LU factorization\nComputing an LU factorization can be expensive, especially for large, sparse\nmatrices. GMRES-IR can also be eﬀective with a cheaper preconditioner M−1,\nor with no preconditioner at all.\nIn this latter case, Algorithm 7.2 reduces to\nAlgorithm 8.1, which has the form of an inner–outer scheme: the outer loop for\niterative reﬁnement (in precision u, with the residual computed at a possibly higher\nprecision ur) and the inner loop for solving the correction equations with GMRES\n(assumed backward stable) in lower precision uℓ.\nBy Theorem 6.1 (or indeed\nTheorem 5.2), convergence to an iterate satisfying (6.4) is guaranteed as long as\nκ(A)uℓ≪1. Note that inner solvers other than GMRES can be used, and, as long\nas they are backward stable, the convergence condition κ(A)uℓ≪1 still holds.\nAlgorithm 8.1 is one form of mixed precision restarted GMRES, although to\nguarantee convergence the GMRES call on line 4 must not terminate after a ﬁxed\nnumber of iterations, but rather when a suﬃciently small residual has been achieved.\nAlgorithm 8.1 was ﬁrst described by Turner and Walker (1992), who perform\nthe inner loop in single precision (uℓ) and the outer loop in double precision\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n382\nN. J. Higham and T. Mary\nAlgorithm 8.1. GMRES-based iterative reﬁnement in three precisions for the solution of Ax = b with no preconditioner.\n1 Choose an initial x1.\n2 for i = 1: imax or until converged\n3\nCompute ri = b −Axi in precision ur.\n4\nSolve Adi = ri by GMRES in precision uℓ.\n5\nCompute xi+1 = xi + di in precision u.\n6 end\n(u and ur), and use a ﬁxed number of inner GMRES iterations. Buttari et al.\n(2008) implement several inner–outer iterative algorithms similar to GMRES-IR\nemploying single and double precisions for the solution of sparse linear systems.\nIn particular, one version uses GMRES for the inner loop and FGMRES for the\nouter loop; this version is also studied by Baboulin et al. (2009).\nMore recent implementations of these methods, still using only single and double\nprecisions, are described by Lindquist, Luszczek and Dongarra (2020, 2022) for\nCPUs and by Loe et al. (2021a,b) for GPUs. Iwashita, Suzuki and Fukaya (2020)\npropose a restarted GMRES where the inner loop uses integer arithmetic and the\nouter loop uses ﬂoating-point arithmetic.\nTo ﬁnd a compromise between computing an LU factorization and using no\npreconditioner at all, cheaper preconditioners can be considered. Algorithm 8.2 is\nobtained by replacing U−1L−1 in Algorithm 7.2 with a general preconditioner M−1.\nAlgorithm 8.2. GMRES-based iterative reﬁnement in ﬁve precisions for the solution of Ax = b with a general preconditioner M−1 ≈A−1 stored in precision uℓ.\n1 Compute x1 = M−1b in precision uℓ.\n2 for i = 1: imax or until converged\n3\nCompute ri = b −Axi in precision ur.\n4\nSolve M−1Adi = M−1ri by GMRES in precision ug, performing the\nproducts with M−1A in precision up.\n5\nCompute xi+1 = xi + di in precision u.\n6 end\nThere is a tradeoﬀinvolved, since a better quality preconditioner will lead to faster\nconvergence but will be more expensive to compute. More subtly, the closer M−1\nis to A−1, the more signiﬁcant the rounding errors incurred in the matrix–vector\nproducts with M−1A become. Indeed, with M−1 = U−1L−1 (LU factorizationbased preconditioner), we have explained in Section 7.2.2 that the products with\nU−1L−1A introduce an extra κ(A) term in the convergence condition, which can be\nattenuated by performing them in higher precision up. This error analysis has not\nbeen extended to a general preconditioner M−1 in the literature, but we can expect\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n383\nκ(A) in the error bound to be replaced by a more general term depending on both\nA and M−1.\nExamples of implementations that use a preconditioner other than a low precision\nLU factorization are found in Lindquist et al. (2020, 2022), who use GMRES-IR\npreconditioned by an incomplete LU factorization, or in Loe et al. (2021a,b), where\nblock Jacobi and polynomial preconditioners are used.\n8.2. Iterative methods with low or mixed precision preconditioner\nAnother approach to exploiting mixed precision arithmetic in iterative methods is\nto use low precision to compute and/or apply the preconditioner. If the iterative\nmethod is iterative reﬁnement, and the preconditioner is a low precision LU factorization, this corresponds to LU-IR (Algorithm 7.1). The idea can be extended to\nother iterative methods or preconditioners.\nFor example, Arioli and Duﬀ(2009) show that FGMRES implemented in double\nprecision and preconditioned with an LU factorization computed in single precision\ncan give backward stability at double precision, even for ill-conditioned systems.\nBuilding on this work, Hogg and Scott (2010) implement an algorithm for symmetric indeﬁnite systems that computes a solution using a direct solver in single\nprecision, performs iterative reﬁnement using the factorization of A, and then uses\nmixed precision FGMRES preconditioned by the direct solver to solve the original\nsystem.\nGiraud, Haidar and Watson (2008) propose an fp32 domain decomposition\npreconditioner applied to an fp64 CG solver. Similarly, Emans and van der Meer\n(2012) propose the use of an fp32 algebraic multigrid method as preconditioner for\nan fp64 CG.\nAnzt et al. (2019a) and Flegar, Anzt, Cojean and Quintana-Ortí (2021) implement a block Jacobi preconditioner within the preconditioned conjugate gradient\nmethod and store the explicitly inverted diagonal blocks of the preconditioner in\nhalf, single or double precision arithmetic according to a criterion based on the\ncondition number of each block. In experiments that use the preconditioner within\na conjugate gradient solver, Flegar et al. (2021) report reductions in run time of\n10%–30% compared with a full precision implementation. Göbel, Grützmacher,\nRibizel and Anzt (2021) apply the same idea to sparse approximate inverse preconditioning with a BiCGSTAB solver. It is worth noting that in these papers the\npreconditioner does not simply use low precision throughout but is itself in mixed\nprecision.\n8.3. Mixed precision GMRES\nThe previously described approaches introduce mixed precision in GMRES either\nin the preconditioner or via an inner–outer iteration scheme. However, there are\nopportunities to exploit multiple precisions even within a non-restarted, unpreconditioned GMRES.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n384\nN. J. Higham and T. Mary\nA ﬁrst approach is to use lower precision in the matrix–vector products with\nA, based on the theory of inexact Krylov methods (Giraud, Gratton and Langou\n2007, Simoncini and Szyld 2003, van den Eshof and Sleijpen 2004), which proves\nthat an increasing level of inexactness as the iteration proceeds can be tolerated\nin the matrix–vector products without degrading the achievable accuracy. This\nwas ﬁrst experimentally observed by Bouras, Frayssé and Giraud (Bouras, Frayssé\nand Giraud 2000, Bouras and Frayssé 2005). The eﬀect of inexactness on the\nconvergence rate of the method is, however, not well understood.\nIn addition, Gratton, Simon, Titley-Peloquin and Toint (2019) prove that the\northonormalization of the Krylov basis can also be performed inexactly.\nThis\nobservation is leveraged by Aliaga et al. (2020), who propose to store the Krylov\nbasis in lower precision.\n8.4. Communication-avoiding iterative methods\nOn modern computers, communication has become a signiﬁcant performance\nbottleneck. Communication-avoiding (CA) methods seek to reduce the communication costs, sometimes at the expense of additional ﬂops, in order to achieve\nhigher performance, especially when scaling to large numbers of processors. In\nparticular, CA iterative methods often compute blocks of s iterations at a time to\nreduce synchronization costs. However, these s-step approaches are known to be\nsometimes unstable. Mixed precision arithmetic has been used to overcome this\npotential instability.\nYamazaki, Tomov, Dong and Dongarra (2015b) and Yamazaki, Tomov and\nDongarra (2015a) propose a mixed precision Cholesky–QR orthonormalization\n(described in Section 9) that they use to stabilize CA-GMRES. They show that the\nuse of this stabilized orthonormalization avoids the need to orthogonalize twice\nand speeds up the convergence of GMRES.\nCarson, Gergelits and Yamazaki (2022a) propose mixed precision s-step Lanczos\nand conjugate gradient methods that compute the Gram matrix in higher precision.\nThis allows for reducing the loss of orthogonality by a factor relating to the condition\nnumber of the s-step Krylov bases, speeding up the convergence of the method at\nthe expense of an increase of the per-iteration cost that is expected to be small in\nlatency-bound applications.\n8.5. Multigrid iterative reﬁnement\nIn addition to Krylov methods, mixed precision has also been investigated for\nmultigrid methods. The most popular approach has been to use a multigrid method\nas the inner solver for iterative reﬁnement, that is, to use Algorithm 6.1 with a\nmultigrid solver on line 3, usually run in lower precision. Single precision multigrid\nmethods have, for example, been used within double precision iterative reﬁnement\nalgorithms by Göddeke, Strzodka and Turek (2007), Goddeke and Strzodka (2011),\nSumiyoshi, Fujii, Nukada and Tanaka (2014) and Kronbichler and Ljungkvist\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n385\n(2019). More recently, Oo and Vogel (2020) also used fp16 arithmetic on V100\nGPUs. The ﬁrst error analysis of multigrid methods in this context was performed\nby McCormick, Benzaken and Tamstorf (2021), who observed that diﬀerent levels\nin the grid hierarchy should use diﬀerent precisions: coarser grids are more resilient\nto lower precisions.\nThis ‘progressive precision’ approach was applied to the\nsolution of elliptic PDEs by Tamstorf, Benzaken and McCormick (2021).\nFor more details of mixed precision multigrid algorithms see Abdelfattah et al.\n(2021a).\n8.6. Other iterative solvers\nClark et al. (2010) gave an early investigation into mixed precision implementation\nof conjugate gradients (CG) and BiCGstab solvers on GPUs, for a lattice quantum\nchromodynamics application. They used half precision for storage only, since half\nprecision computation was not available to them.\nAnzt, Dongarra and Quintana-Ortí (2015) carried out the Jacobi iterative method\nwith diﬀerent solution components represented in diﬀerent precisions, using an\ninexpensive test to decide when to increase precisions during the iteration.\n8.7. Decoupling formats for data storage and processing\nOne speciﬁc feature of exploiting reduced precision in GMRES and iterative methods more generally is that performance is often limited by the memory bandwidth.\nThis leads to the idea of storing the data in compressed form and uncompressing\nit before performing arithmetic operations on the processor. The aim is that the\ncompression reduces the data movement costs enough to outweigh the costs of\ncompressing and uncompressing. Anzt, Flegar, Grützmacher and Quintana-Ortí\n(2019b) propose this approach of decoupling the data storage format from the processing format, and they focus on storing the data at a lower precision than that at\nwhich the computations are performed. This approach is used in the papers mentioned at the end of Section 8.2 and for level 1 and level 2 BLAS by Grützmacher,\nAnzt and Quintana-Ortí (2021). Agullo et al. (2020) propose a similar approach\nfor ﬂexible GMRES, using as compression either reduced precision or the lossy\nﬂoating-point SZ compressor (Di and Cappello 2016).\n9. Mixed precision orthogonalization and QR factorization\nThere exist many algorithms to orthogonalize a set of vectors and to carry out\nthe related task of computing the QR factorization of a matrix A ∈Rm×n, where\nwe assume m ≥n. Householder QR factorization is the most widely used and is\nunconditionally stable: it achieves a backward error and a loss of orthogonality\nof the computed b\nQ (if it is explicitly formed) both of order the unit roundoﬀu\n(Higham 2002, Section 19.3).\nHowever, Householder QR factorization oﬀers\nrelatively little parallelism and requires expensive synchronizations. Alternative\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n386\nN. J. Higham and T. Mary\nalgorithms that are more suitable for parallel computers are unfortunately also less\nstable: for example, the classical and modiﬁed Gram–Schmidt algorithms (CGS and\nMGS) lead to a loss of orthogonality of order κ(A)2u (Giraud, Langou, Rozložník\nand van den Eshof 2005) and κ(A)u, respectively. Developing orthogonalization\nalgorithms that are both parallel and stable is an active ﬁeld of research.\nIn\nthis section we discuss how mixed precision arithmetic can be used to stabilize\nor accelerate these algorithms. We refer to the recent survey by Carson, Lund,\nRozložník and Thomas (2022b) for a description of what is known about the\nstability of block Gram–Schmidt algorithms.\nYang, Fox and Sanders (2021) perform rounding error analysis of Householder\nQR factorization under a model of mixed precision computation that assumes that\ninner products are computed in high precision uhigh and then rounded to lower\nprecision ulow. This model is thus applicable to the use of block FMAs. They show\nthat the bound for the backward error, which is of order mnu in uniform precision u\n(Higham 2002, p. 361), becomes of order nulow+mnuhigh under this mixed precision\nmodel. Unlike the error bound 2ulow + nuhigh of Blanchard et al. (2020b) for LU\nfactorization with block FMAs (see Section 7.3), their bound for QR factorization\nstill grows with n at the ulow level. This is because the model assumes the result of\nthe inner products to be rounded to precision ulow at each step of the factorization.\nYang et al. (2021) also analyse a blocked version of Householder QR assuming\nthat the rounding to precision ulow takes place only once per block-column. They\nshow that the term nulow can then be replaced by Nulow, where N is the number\nof block-columns. Taking advantage of the capability of some block FMAs (such\nas NVIDIA tensor cores) of keeping the result in high precision, one can also\nimagine a Householder QR factorization which starts with the original matrix A in\nhigh precision and rounds its QR factors to low precision on the ﬂy, similarly to\nthe LU factorization algorithm proposed by Haidar et al. (2018b) and analysed by\nBlanchard et al. (2020b). We expect this algorithm to further reduce the constant\nin the error bound at the ulow level by dropping the dependence on N, although this\nis not covered by the analysis of Yang et al. (2021).\nZhang, Baharlouei and Wu (2020) implement Householder QR factorization using NVIDIA tensor cores to accelerate the matrix–matrix products, but obtain only\nmodest speedups due to the panel factorization being the performance bottleneck.\nFor this reason, they propose to switch to a recursive QR factorization employing\nMGS for the orthogonalization, which requires more ﬂops and is potentially less\nstable but makes a more intensive use of matrix–matrix operations. They also\npropose to use communication-avoiding QR (CAQR) for the panel factorizations.\nThese two modiﬁcations allow them to eﬃciently leverage the tensor core performance, signiﬁcantly accelerating the factorization. They demonstrate experimentally\nthat their algorithm can solve least squares problems with double precision accuracy\nby using the computed QR factors to precondition the CGLS iterative method.\nThe Cholesky–QR algorithm computes the QR factorization of A ∈Rm×n by a\nthree-step process.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n387\n1 Compute the matrix product B = ATA.\n2 Compute the Cholesky factorization RTR = B.\n3 Compute Q = AR−1 by a multiple right-hand side triangular solve.\nFor tall, thin matrices (m ≫n), most of the ﬂops take place at steps 1 and 3,\nwhich are very parallel and make intensive use of BLAS-3 operations. However,\nCholesky–QR in uniform precision u leads to a loss of orthogonality of order\nκ(A)2u (Stathopoulos and Wu 2002), and can fail if the Cholesky factorization\nat step 2 breaks down. Cholesky–QR can be partially stabilized by using mixed\nprecision arithmetic: Yamazaki et al. (2015a) show that, if the ﬁrst two steps above\nare carried out at precision uhigh and the third step is carried out at precision u, the\nloss of orthogonality can be bounded by (Yamazaki et al. 2015a, Theorem 3.2)\n∥I −b\nQT b\nQ∥= O\n\u0000κ(A)2(uhigh + u2) + κ(A)u\n\u0001\n.\n(9.1)\nThus, by using doubled precision (that is, uhigh = u2) for the ﬁrst two steps, the\nloss of orthogonality is O(κ(A)2u2 + κ(A)u) and is therefore of order O(κ(A)u)\nas long as κ(A) < u−1. In this context, mixed precision arithmetic can therefore\nbe used not to accelerate the algorithm but to (partially) stabilize it, by reducing\nthe loss of orthogonality by a factor of κ(A). Yamazaki et al. (2015a) implement\nthis mixed precision Cholesky–QR algorithm with fp64 as the working precision\nu, and employ double-double arithmetic for the ﬁrst two steps. Despite requiring\n8.5× more ﬂops due to the use of software-emulated arithmetic, they show that\nthe mixed precision Cholesky–QR algorithm can be only moderately slower than\nin uniform precision (about 1.4× slower in the best case) when the number of\ncolumns n to orthogonalize is small, because in this case the performance of\nCholesky–QR is memory-bound. They apply this mixed precision Cholesky–QR\nalgorithm to the solution of linear systems with a communication-avoiding GMRES\nmethod, and show that the use of this more stable Cholesky–QR algorithm avoids\nthe need for reorthogonalization and allows GMRES to converge faster, leading\nto signiﬁcant speedups. See also Yamazaki et al. (2015b) for early results on this\napproach. When the number of columns to orthogonalize is larger, the performance\nof Cholesky–QR tends to become compute-bound and the overhead associated with\nthe use of double-double arithmetic becomes more signiﬁcant. To overcome this\nissue, Yamazaki et al. (2015c) propose a block MGS method that partitions the\nmatrix into block-columns of smaller size and uses the mixed precision Cholesky–\nQR to orthogonalize each block. This method can be up to seven times faster\nthan applying mixed precision Cholesky–QR to the entire matrix, and numerical\nexperiments show that it can also be as stable, despite the lack of error analysis\nbounding the loss of orthogonality.\nA drawback of Cholesky–QR-based methods is that they can fail if the Cholesky\nfactorization breaks down (because it encounters a non-positive pivot). Breakdown can be avoided by shifting the matrix to ensure the success of the Cholesky\nfactorization (Fukaya et al. 2020).\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n388\nN. J. Higham and T. Mary\nAnother solution is the singular value QR (SVQR) factorization (Stathopoulos\nand Wu 2002), which takes the following steps.\n1 Compute the matrix product B = ATA.\n2 Compute the singular value decomposition UΣUT = B.\n3 Compute the QR factorization ¯QR = Σ1/2UT.\n4 Compute Q = AR−1 by multiple right-hand side triangular solve.\nSteps 2 and 3 require more ﬂops than simply computing the Cholesky factorization\nof B, but if m ≫n the overhead is negligible compared with the ﬂops required\nby steps 1 and 4.\nThe advantage of SVQR is that when B is singular to the\nworking precision, step 2 will directly identify the entire subspace of the nearly\ndependent columns and one can replace all the associated singular values with an\nappropriately large value. Stathopoulos and Wu (2002) suggest replacing singular\nvalues smaller than uσ1 with uσ1, where σ1 is the largest singular value of B. In\nuniform precision u, SVQR also suﬀers from a loss of orthogonality proportional\nto κ(A)2u. Yamazaki, Tomov and Dongarra (2016) propose a mixed precision\nversion of SVQR analogous to their mixed precision Cholesky–QR (Yamazaki\net al. 2015a), where step 4 is carried out in halved precision u1/2 compared with\nthe ﬁrst two steps. The loss of orthogonality can then be bounded by (Yamazaki\net al. 2016, Theorem 5.1)\n∥I −b\nQT b\nQ∥= O(κ(A)2u + κ(A)u1/2).\n(9.2)\nWhen κ(A) is larger than u−1/2, the use of halved precision in step 4 therefore does\nnot signiﬁcantly impact the loss of orthogonality. For smaller values of κ(A), the\nloss of orthogonality is increased but remains a factor κ(A) smaller than if SVQR\nwere carried out entirely in halved precision.\n10. Least squares problems\nConsider the linear least squares (LS) problem minx ∥Ax −b∥2, where A ∈Rm×n\nwith m ≥n has full rank. Recall that the unique LS solution is the solution of the\nnormal equations\nATAx = AT b\n(10.1)\nand that the normal equations can be rewritten as the (m + n) × (m + n) augmented\nsystem\n\u0014 I\nA\nAT 0\n\u0015 \u0014r\nx\n\u0015\n=\n\u0014b\n0\n\u0015\n.\n(10.2)\nBjörck (1967) proposed reﬁning an approximate LS solution by applying iterative\nreﬁnement to the augmented system, with residuals calculated at twice the working\nprecision, and he showed how to eﬃciently solve the augmented system given a QR\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n389\nfactorization of A. He also gave rounding error analysis for the method. Björck’s\nmethod and analysis was extended to constrained and weighted LS problems by\nGulliksson (1994).\nDemmel, Hida, Riedy and Li (2009) discuss practical implementation details\nsuch as convergence tests and how to compute error bounds, and they exploit the\nXBLAS.\nFor more on traditional and ﬁxed precision forms of iterative reﬁnement for the\nLS problem, see Björck (1996) and Higham (2002, Chapter 20).\nRecently, mixed precision algorithms for solving the LS problem have been\ndeveloped by building on GMRES-IR for square linear systems.\nHigham and Pranesh (2021) assume that A is well-conditioned and make use\nof the normal equations (10.1). Their idea is a modiﬁcation of the algorithm of\nSection 7.5 that uses GMRES-IR with Cholesky preconditioning. It chooses a\ndiagonal matrix S so that B = AS has columns of unit 2-norm, forms C = BTB\nat precision uℓ, computes the Cholesky factorization of a shifted C at precision\nuℓ, then applies GMRES-IR to the normal equations, computing the residual in\nprecision ur as ri = AT(b−Axi) and applying GMRES to the preconditioned update\nequation M ATAdi = Mri, where M = SR−1R−T S. Solving the normal equations\nis usually avoided by numerical analysts because it gives a backward error bound\nof order κ2(A)u (Higham 2002, Section 20.4) and the Cholesky factorization can\nbreak down for κ2(A) > u−1/2. Its use here is justiﬁed by the facts that A is assumed\nto be well-conditioned, the Cholesky factorization of the cross-product matrix is\nbeing used as a preconditioner rather than to compute the solution directly, and if\na block FMA is available it can be exploited in forming C, boosting the speed and\naccuracy.\nCarson, Higham and Pranesh (2020) make use of the augmented system (10.2).\nTheir method computes a QR factorization at precision uℓ, then applies GMRESIR to the augmented system with a left preconditioner constructed in one of two\npossible ways from the QR factors.\nBackward error analysis given in Carson\net al. (2020), combined with the analysis of Carson and Higham (2017, 2018)\nand Amestoy et al. (2021b), shows that the method yields a forward error, and a\nbackward error for the augmented system, of order the working precision under\nreasonable assumptions.\nNumerical experiments in Carson et al. (2020) with\nvarious combinations of the three precisions show that the method behaves as\npredicted by the theory.\n11. Eigenvalue decomposition\nA natural way to reﬁne approximate solutions to the eigenvalue problem is by\nNewton’s method, and it presents opportunities for exploiting diﬀerent arithmetic\nprecisions.\nEarly references developing Newton’s method for mixed precision\niterative reﬁnement for the standard eigenvalue problem are Dongarra (1980, 1982)\nand Dongarra, Moler and Wilkinson (1983).\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n390\nN. J. Higham and T. Mary\nWe consider the generalized eigenvalue problem Ax = λBx, where A, B ∈Rn×n.\nSetting B = I gives the standard eigenvalue problem. We suppose that we have an\napproximate eigenpair that we wish to improve. We will use Newton’s method, so\nwe need to put the problem in the form of a non-linear system.\nSince an eigenvector remains an eigenvector when multiplied by a non-zero\nscalar, we need to normalize x, which we will do by requiring that eT\ns x = 1 for\nsome chosen s, where es is the unit vector with a 1 in position s. Deﬁne\nF(v) =\n\u0014\n(A −λB)x\neT\ns x −1\n\u0015\n: Cn+1 →Cn+1,\nv =\n\u0014\nx\nλ\n\u0015\n.\nThe Jacobian is\nJ(v) =\n\u0012∂Fi\n∂vj\n\u0013\n=\n\u0014\nA −λB\n−Bx\neT\ns\n0\n\u0015\n.\nIt is easy to see that ∥J(w)−J(v)∥∞≤2∥B∥∞∥w−v∥∞, so J is Lipschitz continuous\nwith constant 2∥B∥∞. Moreover, it can be shown that J is non-singular when λ is\na simple (non-multiple) eigenvalue (Tisseur 2001, Lemma 3.3).\nBy applying Theorems 5.1 and 5.2, Tisseur (2001, Section 3.2) shows that if\n(x0, λ0) is a suﬃciently good approximation to an eigenpair (x∗, λ∗), λ∗is simple, J\nis not too ill-conditioned at (x∗, λ∗), and the linear system solver is not too unstable,\nthen Newton’s method is well-deﬁned and the limiting forward error is bounded by\n∥(bxT, λ)T −(xT\n∗, λ∗)T ∥∞\n∥(xT∗, λ∗)T ∥∞\n≲cnur ∥J(v∗)−1∥∞max(∥A∥∞, ∥B∥∞) + u,\nwhere c is a small integer constant and ur is the precision in which the residual\nF(v) is evaluated. If ur = u2 this bound can be shown to reduce to cnu. Moreover,\nthe limiting backward error is bounded by\nη∞(bx, bλ) ≲cnur + u(3 + |λ|) max\n\u0012 ∥A∥∞\n∥B∥∞\n, ∥B∥∞\n∥A∥∞\n\u0013\n.\n(11.1)\nNote that as for linear systems, instability in the linear system solver does not aﬀect\nthe bounds for the limiting forward error and backward error.\nEach Newton iteration involves the solution of a linear system with the Jacobian\nmatrix evaluated at the current iterate.\nIf this is done using LU factorization\nof J(v) it costs O(n3) ﬂops per step, which is expensive.\nIf an approximate\neigendecomposition is available then this cost can be reduced to O(n2) ﬂops per\niteration. We specialize to the symmetric deﬁnite generalized eigenvalue problem\nin which A is symmetric and B is symmetric positive deﬁnite. Algorithm 11.1\nis given by Tisseur (2001, Algorithm 4.2) and is used by Davies, Higham and\nTisseur (2001) to reﬁne solutions from the Cholesky–Jacobi method, which uses\na Cholesky decomposition of B to reduce the problem to a standard symmetric\neigenvalue problem and then applies the Jacobi method.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n391\nAlgorithm 11.1. Given a symmetric A ∈Rn×n, a symmetric positive deﬁnite\nB ∈Rn×n, X ∈Rn×n and a diagonal Λ ∈Rn×n such that XTAX ≈Λ and XT BX ≈I,\nand an approximate eigenpair (x, λ) with ∥x∥∞= xs = 1, this algorithm applies\niterative reﬁnement to λ and x at a cost of O(n2) ﬂops per iteration. Computations\nare at precision u unless otherwise stated.\n1 repeat until converged\n2\nCompute r = λBx −Ax in precision ur.\n3\nDλ = Λ −λI\n4\nd = −Bx −cs, where cs is the sth column of A −λB.\n5\nv = XT d, f = XT es\n6\nCompute Givens rotations Jk in the (k, k + 1) plane, such that\nQT\n1 v : = JT\n1 . . . JT\nn−1v = ∥v∥2e1.\n7\nCompute orthogonal Q2 such that\nT = QT\n2 QT\n1 (Dλ + v f T) is upper triangular.\n8\nz = QT\n2 QT\n1 XTr\n9\nSolve Tw = z for w.\n10\nδ = Xw\n11\nλ = λ + δs, δs = 0\n12\nx = x + δ\n13 end\nNewton’s method is well suited to reﬁning a small number of eigenpairs but not\na complete eigensystem, as in the latter case it is expensive and may not converge\nfor all eigenpairs.\nTsai, Luszczek and Dongarra (2021) revisit the Newton method for the standard\nsymmetric eigenvalue problem and develop a mixed precision algorithm that transforms the matrix to tridiagonal form in single precision, computes the eigensystem\nby divide and conquer in double precision, then reﬁnes the eigenpairs in double\nprecision.\nOgita and Aishima (2018) develop an iteration for reﬁning the whole eigensystem\nof a symmetric matrix. It requires four matrix multiplications per iteration, all\nexecuted in a higher precision than the working precision. Quadratic convergence\nis proved for suﬃciently good initial approximations. The algorithm does not work\nwell when there are nearly multiple eigenvalues. The latter limitation is addressed\nin Ogita and Aishima (2019) by using further steps that work with clusters of\neigenvalues.\nPetschow, Quintana-Ortí and Bientinesi (2014) use extra precision to improve\nthe accuracy of the multiple relatively robust representations (MRRR) method for\nthe symmetric tridiagonal eigenvalue problem without sacriﬁcing performance.\nRalha (2018) considers carrying out the bisection method for symmetric tridiagonal matrices with early iterations in single precision before switching to the\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n392\nN. J. Higham and T. Mary\nworking precision of double, and develops criteria for deciding when to make the\nswitch.\nStor, Slapničar and Barlow (2015) give an algorithm for the eigendecomposition\nof symmetric arrowhead matrices that employs bisection and a shift and invert\ntechnique, and in the latter it uses arithmetic at twice the working precision for one\nelement of the inverse in order to ensure forward stability.\nTsuchida and Choe (2012) consider a trace minimization method for computing\nthe complete eigensystem of a symmetric matrix and explore running diﬀerent\nparts of the method at half the working precision. Gains of over 30% in execution\ntime are reported with little loss of accuracy.\nAlvermann et al. (2019) report on two projects that are developing eigensolvers\nbased on the (block) Jacobi–Davidson method, subspace iteration and other methods, and are using lower precision in early iterations for speed and higher precision\nwithin the orthogonalizations for robustness.\n12. Singular value decomposition\nWe now consider the singular value decomposition (SVD) of A ∈Rm×n with m ≥n:\nA = UΣVT with U ∈Rm×m and V ∈Rn×n orthogonal and Σ = diag(σi) ∈Rm×n.\n12.1. Iterative reﬁnement\nThe Newton approach to reﬁning eigenpairs can be extended to singular value\ntriples of A ∈Rm×n by using the function\nF(x) =\n\nAv −µ1u\nATu −µ2v\nuTu −1\nvTv −1\n\n,\nx =\n\nu\nv\nµ1\nµ2\n\n.\nThe Jacobian of f is\nJ(x) =\n\n−µ1I\nA\n−u\n0\nAT\n−µ2I\n0\n−v\n2uT\n0\n0\n0\n0\n2vT\n0\n0\n\n.\nThe approximate singular value is updated by (µ1 + µ2)/2.\nDongarra (1983),\nextending the work in Dongarra et al. (1983), shows how to solve systems with\nJ(x) in O(mn) ﬂops, given an SVD or bidiagonal factorization of A. Again, the\nNewton theory of Section 5 applies.\nOgita and Aishima (2020) extend their algorithm for the symmetric eigenvalue\nproblem, mentioned in the previous section, to the SVD in order to reﬁne the\ncomplete SVD; the algorithm uses higher precision and is dominated by matrix\nmultiplication.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n393\n12.2. SVD with rapidly decaying singular values\nAnother opportunity for mixed precision arithmetic arises in the case of matrices\nwith rapidly decaying singular values. Given a target accuracy ε, it is well known\nthat singular values smaller than ε and the corresponding singular vectors can be\ndropped to provide a low-rank approximation to the matrix with an error bound of\norder ε. Amestoy et al. (2021a) explain that among the singular values that remain,\nthose that are small enough can be represented, along with their associated singular\nvectors, in lower precision. For example, singular vectors associated with singular\nvalues less than ε/us, where us = 2−24 is the unit roundoﬀfor single precision, can\nbe stored in single precision, even when ε ≪us. They introduce a mixed precision\nSVD representation that uses p precisions,\nA = UΣVT = [U1 U2 . . . Up]Σ[V1 V2 . . . Vp]T,\n(12.1)\nwhere Ui and Vi are stored in precision ui, with u1 < u2 < · · · < up. They give an\nexplicit rule on how to partition U andV in order to guarantee an overall accuracy of\norder ε (Amestoy et al. 2021a, Theorem 2.2). Note that this approach is applicable\nnot only to the SVD but also to other types of rank-revealing decompositions, such\nas QR factorization with column pivoting.\nOoi et al. (2020) propose three diﬀerent methods to introduce mixed precision\narithmetic in the product of a low-rank matrix with a vector. Their method 3 is\nsimilar to the representation (12.1), which they use with fp64 and fp32 arithmetics.\nThey apply this approach to the solution of linear systems exploiting products of a\nhierarchical (H) matrix with a vector, using the iterative BiCGstab solver.\n13. Multiword arithmetic\nMultiword arithmetic is a well-known approach to enhance the accuracy of computations while employing fast arithmetic supported in hardware. It consists of\nrepresenting high precision numbers by the unevaluated sum of lower precision\nnumbers. An example is double-double arithmetic which, as mentioned in Section 2.2, approximates an fp128 number as the sum of two fp64 numbers and\nreplaces fp128 operations with fp64 operations.\nThe emergence of specialized hardware supporting low precision matrix multiplication with high precision accumulators, such as the NVIDIA GPU tensor cores,\nprovides new opportunities for multiword arithmetic. Indeed, these units are much\nfaster than standard fp32 arithmetic (up to 8 and 16 times faster on the Volta and\nAmpere GPUs, for example). Therefore an approach to accelerate the computation\nof an fp32 matrix product C = AB is to approximate A ≈A1 + A2 as the sum\nof two fp16 matrices, and similarly B ≈B1 + B2. Then C can be computed as\nC ≈A1B1 + A1B2 + A2B1 + A2B2 using block FMAs to compute each of the AiBj\nterms using internal tensor core arithmetic at fp32 accuracy. Since there are only\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n394\nN. J. Higham and T. Mary\nfour terms (and in fact, we can reduce that number to three, as explained below),\nthis approach can potentially be much faster than standard fp32 arithmetic.\nThis approach was ﬁrst used with NVIDIA tensor cores by Markidis et al.\n(2018) to accelerate matrix products, and by Sorna et al. (2018) to accelerate\nthe fast Fourier transform (FFT). Pisha and Ligowski (2021) similarly use the\nTensorFloat32 format in computing the FFT on the NVIDIA A100 GPUs. Henry,\nTang and Heinecke (2019) describe an approach based on block FMA hardware\nusing the bﬂoat16 format instead of the fp16 one, where A and B are split into\nthree bﬂoat16 matrices, which requires nine products to compute C = AB. Finally,\nMukunoki, Ozaki, Ogita and Imamura (2020) explain how to achieve not only fp32\naccuracy but also fp64 accuracy with this approach, by using the Ozaki scheme.\nTheir approach, however, requires splitting both A and B a large number of times,\nwhich leads to several dozens if not hundreds of products. Their algorithm is\ntherefore only beneﬁcial on GPUs on which fp64 arithmetic is very slow, such as\nsome of the Turing models.\nFasi et al. (2022) generalize these approaches by considering any low precision\nulow and any number of splits p. They give Algorithm 13.1.\nAlgorithm 13.1 (multiword matrix multiplication). Thisalgorithmcomputesthe\nmatrix–matrix product C = AB using p-word arithmetic with a mixed precision\nblock FMA with precisions ulow and uhigh.\n1 for i = 1: p\n2\nAi = fllow(A −Íi−1\nk=1 Ak)\n3\nBi = fllow(B −Íi−1\nk=1 Bk)\n4 end\n5 for i = 1: p\n6\nfor j = 1: p\n7\nCompute Cij = AiBj with Algorithm 4.1.\n8\nC ←C + Cij\n9\nend\n10 end\nThe algorithm recursively computes Ai (and similarly Bj) as the residual from\nthe (i −1)-way split A ≈A1 + · · · + Ai−1 and rounds it to precision ulow, that is,\nAi = fllow\n\u0012\nA −\ni−1\nÕ\nk=1\nAk\n\u0013\nBi = fllow\n\u0012\nB −\ni−1\nÕ\nk=1\nBk\n\u0013\n\n\ni = 1: p.\n(13.1)\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n395\nThis gives the approximations\nA =\np\nÕ\ni=1\nAi + ∆A,\n|∆A| ≤up\nlow|A|,\n(13.2)\nB =\np\nÕ\ni=1\nAi + ∆B,\n|∆B| ≤up\nlow|B|.\n(13.3)\nThen, if C is approximated by the sum of the p2 products AiBj, which are computed\nby chaining calls to a block FMA with internal precision uhigh, by Theorem 4.1 we\nobtain a computed b\nC satisfying (Fasi et al. 2022)\nb\nC = AB + E,\n|E| ≲\n\u00002up\nlow + u2p\nlow + (n + p2 −1)uhigh\n\u0001\n|A||B|.\n(13.4)\nClearly, for practical choices of ulow and uhigh a small value of p is suﬃcient. For\nexample, for fp16 (ulow = 2−11) and fp32 (uhigh = 2−24), p = 2 is enough since in\nthis case u2\nlow = 4uhigh. Taking larger values of p will not signiﬁcantly improve\nthe bound (13.4) since the term (n + p2)uhigh will then dominate. For bﬂoat16\n(ulow = 2−8) and fp32, the case p = 3 is also of interest because the signiﬁcand of\none fp32 number ﬁts exactly into the signiﬁcands of three bﬂoat16 numbers.\nImportantly, in practice not all p2 products AiBj need be computed. As a result\nof the construction (13.1), the magnitude of the elements of Ai and Bj rapidly\ndecreases as we increase i and j. More precisely, we have\n|Ai| ≤ui−1\nlow(1 + ulow)|A|,\n|Bi| ≤ui−1\nlow(1 + ulow)|B|,\ni = 1: p,\nand thus\n|Ai||Bj| ≤ui+j−2\nlow\n(1 + ulow)2|A||B|.\n(13.5)\nTherefore, ignoring any product AiBj such that i + j > p + 1 only introduces an\nerror of order up\nlow or higher, which does not signiﬁcantly impact the bound (13.4).\nIndeed, by only computing the products AiBj such that i + j ≤p + 1, we obtain the\nmodiﬁed bound\nb\nC = AB + E,\n|E| ≲\n\u0012\n2up\nlow + u2p\nlow + (n + p2)uhigh +\np−1\nÕ\ni=1\n(p −i)up+i−1\nlow\n(1 + ulow)2\n\u0013\n|A||B|.\nThe constant in this bound is (p + 1)up\nlow plus higher-order terms, so to order up\nlow\nwe have only increased the constant 2 from (13.4) to p+1, and we have reduced the\nnumber of products from p2 to p(p+1)/2. Concretely, with fp32 and fp16 (p = 2),\nwe only need three products, which is less than the four used by Markidis et al.\n(2018), and with bﬂoat16 and fp32 (p = 3), we can reduce the number of products\nfrom nine to six, as already suggested by Henry et al. (2019).\nNote that further reducing the number of products (such as using two products\nfor p = 2, as attempted by Markidis et al. 2018) is possible, but the analysis tells us\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n396\nN. J. Higham and T. Mary\nit should not be beneﬁcial. Indeed, ignoring any product AiBj such that i+ j ≤p+1\nwould introduce an error of order at least up−1\nlow , and so could not be signiﬁcantly\nmore accurate than simply using p −1 splits rather than p.\nThe above analysis encompasses previously proposed algorithms, and also includes new cases. For example, we may use a 2-way split (p = 2) with bﬂoat16\nand fp32, which requires three products rather than six (when p = 3) and delivers\nan accuracy of order 2−16 rather than 2−24.\nNote that this analysis deals with worst-case error bounds and so does not\nguarantee that multiword arithmetic with low precision block FMAs will be as\naccurate as higher precision standard arithmetic in the case where the latter does\nnot attain its worst-case error. In fact, in their experiments with NVIDIA tensor\ncores, Fasi et al. (2022) ﬁnd that double-fp16 arithmetic can be much less accurate\nthan fp32 arithmetic due to the rounding mode of these devices, which can make\nthe worst-case bounds for double-fp16 sharp. To overcome this issue, Fasi et al.\n(2022) propose the use of FABsum (see Section 4.2) to reduce the worst-case error\nbound.\n14. Adaptive precision algorithms\nSeveral mixed precision algorithms described in the previous sections share the\nsame foundation: adapt the precision to the data by using lower precisions to\nrepresent the less important or signiﬁcant parts of the data. As an example, consider\nthe computation of the sum a + b, where |b| ≪|a|. Because of the widely diﬀerent\nmagnitudes of a and b, the least signiﬁcant bits of b do not play a signiﬁcant role in\nthe computed value of the result. Indeed if we round b to eb = fllow(b) = b(1+δlow),\nwhere |δlow| ≤ulow, then\nfl(a + eb) = (a + eb)(1 + δ)\n(|δ| ≤u)\n= (a + b(1 + δlow))(1 + δ)\n= (a + b)(1 + δ)\n\u0012\n1 +\nb\na + bδlow\n\u0013\n,\nand so we have an extra term 1 + bδlow/(a + b), which is insigniﬁcant as long\nas |b|ulow ≪|a + b|u.\nTherefore b can be stored in lower precision without\nsigniﬁcantly impacting the accuracy of the computation. Moreover, if b is the\nresult of a previous computation, that computation can also be carried out in lower\nprecision. This example illustrates that computations performed on data of small\nmagnitude need not use very high precision. This is a simple but fundamental\nobservation that has given birth to several adaptive precision algorithms.\nThe\nobject of this section is to show that these algorithms share strong connections.\nAdaptive precision algorithms seek to exploit this observation by adapting the\nprecision to be inversely proportional to the weight of the data, where the weight is\ndeﬁned by some metric such as the maximum magnitude or the norm of the data.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n397\nIn numerical linear algebra algorithms, this can be done at diﬀerent levels of the\ncomputation: at the element, block, column/row or matrix levels.\n14.1. At the matrix level\nIn computations involving several matrices, we may choose to compute and store\nsome of them in lower precision. For example, in computing C = A1B1 + A2B2\nwhere |A1| ≥|A2| and |B1| ≥|B2|, if |A2||B2| ≪|A1||B1| then the matrix product\nA2B2 can be computed in lower precision than A1B1. An example where this\nsituation arises is the use of multiword arithmetic, as illustrated by (13.5). In fact\nwe have already explained that the products AiBj of highest order can be ignored;\ndata-driven analysis also shows that most of the products that cannot be ignored\ncan however be computed in lower precision. For example, with ulow as fp16 and\np = 2, the products A1B2 and A2B1 can be computed in fp16 arithmetic, because\nthe magnitude of their entries is proportional to ulow. Only the ﬁrst term A1B1\nactually needs to be computed in fp32 arithmetic. This observation is especially\nimportant when implementing multiword arithmetic on GPU tensor cores, which\nlead to heavy rounding error accumulation in the products AiBj because of their\nrounding mode: Fasi et al. (2022) explain that it is only necessary to take care of\nreducing the eﬀect of error accumulation on the A1B1 term.\n14.2. At the column level (or, equivalently, at the row level)\nGiven a matrix, we may think of storing each of its columns (or rows) in a diﬀerent\nprecision. This approach makes the most sense when dealing with matrices that\ncan be decomposed as low-rank components of rapidly decreasing norm. This can\nbe the case, for example, of SVDs or rank-revealing factorizations. In fact, the\nmixed precision truncated SVD approaches described in Section 12.2 (Amestoy\net al. 2021a, Ooi et al. 2020) are precisely based on this property: rounding errors\nintroduced by converting singular vectors to lower precision are demagniﬁed by\nthe associated singular value, and so the precision of each vector should be selected\nbased on its associated singular value.\nNote that, given the SVD UΣVT of a matrix A, we can express the matrix as\nA = Í\ni Ai with Ai = uiσivT\ni , where ∥Ai+1∥F ≤∥Ai∥F. Thus, even though the\nmatrices Ai are never formed or manipulated explicitly, the link with the matrixlevel case is clear.\n14.3. At the block level\nIn some applications it pays to partition a matrix into several blocks and adapt the\nprecision to each block. For example, in Section 7.7 we described approaches where\nthe precision of each block is based on its distance to the diagonal (Abdulah et al.\n2019, 2022, Doucet et al. 2019). The success of these approaches is explained by\nthe fact that, for many data-sparse matrices, blocks distant from the diagonal tend to\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n398\nN. J. Higham and T. Mary\nhave smaller norm. Indeed, storing each block in a precision inversely proportional\nto its norm can allow for signiﬁcant gains with potentially little accuracy loss. As\nan example, consider a matrix A ∈Rpb×pb partitioned into p2 blocks Aij ∈Rb×b\nand assume we have two precisions uhigh and ulow at our disposal. Then the matrix\nbA obtained by storing blocks Aij of Frobenius norm less than uhigh∥A∥F/(pulow) in\nprecision ulow satisﬁes ∥bA −A∥F ≤uhigh∥A∥F. Thus we can store selected blocks\nof A in precision ulow and still recover a global approximation at accuracy uhigh.\nThis example trivially extends to more than two precisions.\nAnother example of an adaptive precision algorithm at the block level is the\nadaptive precision block Jacobi preconditioner discussed in Section 8.2 (Anzt et al.\n2019a, Flegar et al. 2021). In this case the precisions of the blocks are selected\nbased on their condition number rather than their norm, because this is the relevant\nmetric when applying the inverse of the blocks as part of the preconditioner.\n14.4. At the element level\nThe adaptive precision algorithms described above seek to exploit the underlying\nstructure of the data. However, the question arises as to whether it can be beneﬁcial\nto adapt the precision at the element level: that is, to allow each variable in the\ncomputation to have its own precision, without any special structure (by blocks or\nby columns, for instance). This is similar in goal to transprecision computing and\nprecision auto-tuning tools, which we brieﬂy discuss in Section 15.1.\nWhile this approach maximizes the use of reduced precision, it also destroys\nthe granularity of the computation and should therefore only be used for memorybound applications, such as for sparse matrix–vector products (SpMV) y = Ax. In\nparticular, Ahmad, Sundar and Hall (2019) propose to split A as Ad + As, where\nAs contains the small non-zero elements of A and is stored in single precision,\nwhereas Ad is kept in double precision.\nMore generally, given p precisions, one could split the elements of A into\np diﬀerent matrices and compute p independent products in the corresponding\nprecision. This idea is then similar to bucket summation (Demmel and Hida 2004,\nZhu and Hayes 2009), in which summands are split into buckets based on their\nexponent. The novelty comes from summing each bucket in a diﬀerent precision.\nDiﬀenderfer, Osei-Kuﬀuor and Menon (2021) propose such a bucket algorithm for\nthe inner product that uses the four IEEE arithmetics as well as ‘perforation’, that is,\nthe option to ignore some of the smallest summands. Graillat, Jézéquel, Mary\nand Molina (2022) propose an adaptive precision sparse matrix–vector product\nalgorithm of similar spirit that, given p precisions u1 < · · · < up, splits A into p\nbuckets based on the magnitude of the elements: bucket number i contains all the\nelements whose absolute value lies in the interval [ε/ui, ε/ui+1], for a given target\naccuracy ε. They obtain speedups of up to an order of magnitude compared with\na standard product in uniform precision.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n399\n15. Miscellany\n15.1. Tuning precisions\nA very diﬀerent approach to mixed precision computing is to focus on the code\nrather than the algorithm. Given a code to solve a particular problem and a set of\narithmetics of diﬀerent precisions, one can ask what is the best selection of precision\nin which to store each variable. Here, ‘best’ means a choice that minimizes some\nsuitable performance metric subject to achieving a computed result of acceptable\nquality. The motivation is the assumption that reducing precisions means faster\nexecution and lower storage and energy requirements, though conversion between\ndiﬀerent precisions is required and adds overhead costs.\nThis problem is clearly combinatorial in nature, as if there are n variables and p\nprecisions there are pn possible implementations. Ensuring results of acceptable\nquality requires, in principle, a parametrized rounding error analysis that encapsulates all the possible input data.\nMuch research has been done on algorithms that attempt to solve this problem.\nUsually, optimization of code is done for a ‘representative’ data set, with the\nassumption that the code will be used on related data for which the quality of the\nresults will be similar. At best a local minimum of the objective function can be\nexpected. No guarantee is provided that the code with the chosen precisions will\nsatisfy error requirements across all possible input data.\nTools can be categorized as using static analysis (carried out before the code is\nrun) or dynamic analysis. Dynamic analysis tools typically instrument the compiled\ncode in order to try diﬀerent combinations of precisions, and a popular way to do\nso is via the LLVM compiler infrastructure.31\nAny attempt to reduce precisions of variables must ensure that suﬃcient range\nis maintained to avoid overﬂow and harmful underﬂow, which is particularly important if fp16 is one of the formats, given its narrow range.\nAn example of such work is the tool Precimonious, by Rubio-González et al.\n(2013), which uses execution time as the performance metric. It takes a C program\nas input and outputs a description of the precisions to be assigned to the variables.\nThe experiments in Rubio-González et al. (2013) demonstrate a speedup of up to a\nfactor of 1.4 by replacing certain double precision variables with single precision\nones in the benchmark codes tested.\nA more recent example, focused on GPUs, is GRAM, which chooses the precisions at run time (Ho, De Silva and Wong 2021). Each block of threads is kept at\nthe same precision and a proportion α of the blocks is assigned a lower precision,\nwith a binary search used to select α. Speedups on an NVIDIA GPU of up to 1.8\nover single precision are reported, by exploiting half precision arithmetic. GRAM\ndoes not support the use of tensor cores.\n31 https://llvm.org/\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n400\nN. J. Higham and T. Mary\nBrun et al. (2021) develop a tool that uses a heuristic search strategy to select\nthe precisions at which elementary functions are evaluated in a code, aiming to\nminimize the precisions subject to achieving output of a given accuracy. They\nuse the Intel Vector Mathematics functions (VM) in the Intel oneAPI Math Kernel\nLibrary, which have an input argument that allows the user to select ‘high accuracy’,\n‘low accuracy’ or ‘enhanced performance accuracy’ modes for the functions. They\nare able to obtain up to an approximate halving of the execution time on a Monte\nCarlo code that spends 70% of its time in mathematical library functions.\nPrecision tuning has also been used in climate and weather models. Tintó Prims\net al. (2019) use the rpe Fortran library that emulates reduced precision, which\nwe mentioned in Section 2.6. For two widely used ocean model codes they use a\ndivide and conquer approach to ﬁnd assignments of precisions to variables, ﬁnding\nthat half precision or single precision can be used for large portions of the codes.\nSimilar ﬁndings were made by Düben, Subramanian, Dawson and Palmer (2017)\nfor a cloud-resolving model within a general circulation model.\nIt needs to be kept in mind that simply lowering the precisions of variables in a\ncode may not be all that can be done. In some problems the choice of algorithm,\nor the algorithm itself, is precision-dependent.\nFor example, an algorithm for\ncomputing an elementary function may be built upon a rational approximation that\ndepends on the target accuracy, so that diﬀerent approximations can be used for\nhalf, single and double precision.\n15.2. Multiprecision algorithms\nMultiprecision algorithms for the matrix logarithm and the matrix exponential are\ndeveloped by Fasi and Higham (2018, 2019). These algorithms take as input the\nunit roundoﬀu of the arithmetic and then determine a suitable level of (inverse)\nscaling and squaring transformations and degree of Taylor or Padé approximants\nsuch that the functions are approximated to precision u.\nThe key algorithmic\nparameters are determined at run time, which contrasts with the state-of-the-art\nalgorithms for double precision arithmetic, where some of the parameters have\nbeen determined in advance. A similar strategy is followed by Al-Mohy, Higham\nand Liu (2022) in a multiprecision algorithm for computing for the matrix cosine\nand its Fréchet derivative.\nHigham and Liu (2021) develop a multiprecision version of the Schur–Parlett\nalgorithm for computing general analytic functions at a matrix argument. It avoids\nthe need for derivatives by computing the function of the diagonal blocks of the\nreordered and blocked Schur form by diagonalizing, at a suitable precision, a small\nrandom perturbation of each block.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n401\nAcknowledgements\nThe work of the ﬁrst author was supported by Engineering and Physical Sciences\nResearch Council grant EP/P020720/1, the Royal Society and the Exascale Computing Project (17-SC-20-SC), a collaborative eﬀort of the US Department of Energy\nOﬃce of Science and the National Nuclear Security Administration. The work of\nthe second author was supported by the InterFLOP project (ANR-20-CE46-0009)\nof the French National Agency for Research.\nWe thank Massimiliano Fasi, Sven Hammarling, Claude-Pierre Jeannerod, Mantas Mikaitis and Françoise Tisseur for their comments on a draft manuscript.\nReferences\nA. Abdelfattah, H. Anzt, E. G. Boman, E. Carson, T. Cojean, J. Dongarra, A. Fox, M. Gates,\nN. J. Higham, X. S. Li, J. Loe, P. Luszczek, S. Pranesh, S. Rajamanickam, T. Ribizel,\nB. F. Smith, K. Swirydowicz, S. Thomas, S. Tomov, Y. M. Tsai and U. M. Yang (2021a),\nA survey of numerical linear algebra methods utilizing mixed-precision arithmetic, Int.\nJ. High Perform. Comput. Appl. 35, 344–369.\nA. Abdelfattah, T. Costa, J. Dongarra, M. Gates, A. Haidar, S. Hammarling, N. J. Higham,\nJ. Kurzak, P. Luszczek, S. Tomov and M. Zounon (2021b), A set of Batched Basic\nLinear Algebra Subprograms and LAPACK routines, ACM Trans. Math. Software 47,\n21.\nA. Abdelfattah, S. Tomov and J. Dongarra (2019a), Fast batched matrix multiplication\nfor small sizes using half-precision arithmetic on GPUs, in 2019 IEEE International\nParallel and Distributed Processing Symposium (IPDPS), IEEE, pp. 111–122.\nA. Abdelfattah, S. Tomov and J. Dongarra (2019b), Towards half-precision computation\nfor complex matrices: A case study for mixed-precision solvers on GPUs, in 2019\nIEEE/ACM 10th Workshop on Latest Advances in Scalable Algorithms for Large-Scale\nSystems (ScalA), IEEE, pp. 17–24.\nA. Abdelfattah, S. Tomov and J. Dongarra (2020), Investigating the beneﬁt of FP16enabled mixed-precision solvers for symmetric positive deﬁnite matrices using GPUs,\nin Computational Science – ICCS 2020 (V. V. Krzhizhanovskaya et al., eds), Vol. 12138\nof Lecture Notes in Computer Science, Springer, pp. 237–250.\nS. Abdulah, Q. Cao, Y. Pei, G. Bosilca, J. Dongarra, M. G. Genton, D. E. Keyes, H. Ltaief\nand Y. Sun (2022), Accelerating geostatistical modeling and prediction with mixedprecision computations: A high-productivity approach with PaRSEC, IEEE Trans.\nParallel Distrib. Syst. 33, 964–976.\nS. Abdulah, H. Ltaief, Y. Sun, M. G. Genton and D. E. Keyes (2019), Geostatistical\nmodeling and prediction using mixed precision tile Cholesky factorization, in 2019 IEEE\n26th International Conference on High Performance Computing, Data, and Analytics\n(HiPC), IEEE, pp. 152–162.\nE. Agullo, F. Cappello, S. Di, L. Giraud, X. Liang and N. Schenkels (2020), Exploring\nvariable accuracy storage through lossy compression techniques in numerical linear algebra: A ﬁrst application to ﬂexible GMRES. Research report RR-9342, Inria Bordeaux\nSud-Ouest. Available at hal-02572910v2.\nK. Ahmad, H. Sundar and M. Hall (2019), Data-driven mixed precision sparse matrix\nvector multiplication for GPUs, ACM Trans. Archit. Code Optim. 16, 51.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n402\nN. J. Higham and T. Mary\nA. H. Al-Mohy, N. J. Higham and X. Liu (2022), Arbitrary precision algorithms for\ncomputing the matrix cosine and its Fréchet derivative, SIAM J. Matrix Anal. Appl. 43,\n233–256.\nJ. I. Aliaga, H. Anzt, T. Grützmacher, E. S. Quintana-Ortí and A. E. Tomás (2020),\nCompressed basis GMRES on high performance GPUs. Available at arXiv:2009.12101.\nA. Alvermann, A. Basermann, H.-J. Bungartz, C. Carbogno, D. Ernst, H. Fehske,\nY. Futamura, M. Galgon, G. Hager, S. Huber, T. Huckle, A. Ida, A. Imakura, M. Kawai,\nS. Köcher, M. Kreutzer, P. Kus, B. Lang, H. Lederer, V. Manin, A. Marek, K. Nakajima,\nL. Nemec, K. Reuter, M. Rippl, M. Röhrig-Zöllner, T. Sakurai, M. Scheﬄer, C. Scheurer,\nF. Shahzad, D. Simoes Brambila, J. Thies and G. Wellein (2019), Beneﬁts from using\nmixed precision computations in the ELPA-AEO and ESSEX-II eigensolver projects,\nJapan J. Indust. Appl. Math. 36, 699–717.\nP. Amestoy, O. Boiteau, A. Buttari, M. Gerest, F. Jézéquel, J.-Y. L’Excellent and T. Mary\n(2021a), Mixed precision low rank approximations and their application to block low\nrank LU factorization. Available at hal-03251738.\nP. Amestoy, A. Buttari, N. J. Higham, J.-Y. L’Excellent, T. Mary and B. Vieublé (2021b),\nFive-precision GMRES-based iterative reﬁnement. MIMS EPrint 2021.5, Manchester\nInstitute for Mathematical Sciences, The University of Manchester, UK.\nP. Amestoy, A. Buttari, N. J. Higham, J.-Y. L’Excellent, T. Mary and B. Vieublé (2022),\nCombining sparse approximate factorizations with mixed precision iterative reﬁnement.\nMIMS EPrint 2022.2, Manchester Institute for Mathematical Sciences, The University\nof Manchester, UK.\nP. Amestoy, A. Buttari, J.-Y. L’Excellent and T. Mary (2019), Performance and scalability\nof the block low-rank multifrontal factorization on multicore architectures, ACM Trans.\nMath. Software 45, 2.\nP. Amestoy, I. S. Duﬀ, J.-Y. L’Excellent and J. Koster (2001), A fully asynchronous\nmultifrontal solver using distributed dynamic scheduling, SIAM J. Matrix Anal. Appl.\n23, 15–41.\nE. Anderson (1991), Robust triangular solves for use in condition estimation. Technical\nreport CS-91-142, Department of Computer Science, The University of Tennessee,\nKnoxville, TN, USA. LAPACK Working Note 36.\nANSI (1966), American National Standard FORTRAN, American National Standards Institute, New York.\nH. Anzt, J. Dongarra and E. S. Quintana-Ortí (2015), Adaptive precision solvers for sparse\nlinear systems, in Proceedings of the 3rd International Workshop on Energy Eﬃcient\nSupercomputing (E2SC ’15), ACM Press, article 2.\nH. Anzt, J. Dongarra, G. Flegar, N. J. Higham and E. S. Quintana-Ortí (2019a), Adaptive precision in block-Jacobi preconditioning for iterative sparse linear system solvers,\nConcurrency Comput. Pract. Exper. 31, e4460.\nH. Anzt, G. Flegar, T. Grützmacher and E. S. Quintana-Ortí (2019b), Toward a modular\nprecision ecosystem for high-performance computing, Int. J. High Perform. Comput.\nAppl. 33, 1069–1078.\nJ. Appleyard and S. Yokim (2017), Programming tensor cores in CUDA 9. Available at\nhttps://devblogs.nvidia.com/programming-tensor-cores-cuda-9/.\nM. Arioli and I. S. Duﬀ(2009), Using FGMRES to obtain backward stability in mixed\nprecision, Electron. Trans. Numer. Anal. 33, 31–44.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n403\nM. Arioli, I. S. Duﬀ, S. Gratton and S. Pralet (2007), A note on GMRES preconditioned\nby a perturbed LDLT decomposition with static pivoting, SIAM J. Sci. Comput. 29,\n2024–2044.\nARM (2018), ARM Architecture Reference Manual. ARMv8, for ARMv8-A Architecture\nProﬁle, ARM Limited, Cambridge, UK. Version dated 31 October 2018. Original\nrelease dated 30 April 2013.\nARM (2019), Arm A64 Instruction Set Architecture Armv8, for Armv8-A Architecture\nProﬁle, ARM Limited, Cambridge, UK.\nARM (2020), Arm Architecture Reference Manual. Armv8, for Armv8-A Architecture\nProﬁle, ARM Limited, Cambridge, UK. ARM DDI 0487F.b (ID040120).\nM. Baboulin, A. Buttari, J. Dongarra, J. Kurzak, J. Langou, J. Langou, P. Luszczek and\nS. Tomov (2009), Accelerating scientiﬁc computations with mixed precision algorithms,\nComput. Phys. Comm. 180, 2526–2533.\nD. H. Bailey (2021), MPFUN2020: A new thread-safe arbitrary precision package (full\ndocumentation). Availableat https://www.davidhbailey.com/dhbpapers/mpfun2020.pdf.\nD. H. Bailey, Y. Hida, X. S. Li and B. Thompson (2002), ARPREC: An arbitrary precision\ncomputation package.\nTechnical report LBNL-53651, Lawrence Berkeley National\nLaboratory, Berkeley, CA, USA.\nP. Bauer, P. D. Dueben, T. Hoeﬂer, T. Quintino, T. C. Schulthess and N. P. Wedi (2021),\nThe digital revolution of earth-system science, Nature Comput. Sci. 1, 104–113.\nJ. Bezanson, A. Edelman, S. Karpinski and V. B. Shah (2017), Julia: A fresh approach to\nnumerical computing, SIAM Rev. 59, 65–98.\nÅ. Björck (1967), Iterative reﬁnement of linear least squares solutions I, BIT 7, 257–278.\nÅ. Björck (1996), Numerical Methods for Least Squares Problems, SIAM.\nP. Blanchard, N. J. Higham and T. Mary (2020a), A class of fast and accurate summation\nalgorithms„ SIAM J. Sci. Comput. 42, A1541–A1557.\nP. Blanchard, N. J. Higham, F. Lopez, T. Mary and S. Pranesh (2020b), Mixed precision\nblock fused multiply-add: Error analysis and application to GPU tensor cores, SIAM J.\nSci. Comput. 42, C124–C141.\nA. Bouras and V. Frayssé (2005), Inexact matrix-vector products in Krylov methods for\nsolving linear systems: A relaxation strategy, SIAM J. Matrix Anal. Appl. 26, 660–678.\nA. Bouras, V. Frayssé and L. Giraud (2000), A relaxation strategy for inner–outer linear\nsolvers in domain decomposition methods. Technical report TR/PA/00/17, CERFACS,\nToulouse, France.\nE. Brun, D. Defour, P. De Oliveira Castro, M. Iştoan, D. Mancusi, E. Petit and A. Vaquet\n(2021), A study of the eﬀects and beneﬁts of custom-precision mathematical libraries\nfor HPC codes, IEEE Trans. Emerg. Topics Comput. 9, 1467–1478.\nA. Buttari, J. Dongarra, J. Kurzak, P. Luszczek and S. Tomov (2008), Using mixed precision\nfor sparse matrix computations to enhance the performance while achieving 64-bit\naccuracy, ACM Trans. Math. Software 34, 17.\nA. Buttari, J. Dongarra, J. Langou, J. Langou, P. Luszczek and J. Kurzak (2007), Mixed\nprecision iterative reﬁnement techniques for the solution of dense linear systems, Int. J.\nHigh Perform. Comput. Appl. 21, 457–466.\nE. Carson and N. J. Higham (2017), A new analysis of iterative reﬁnement and its application to accurate solution of ill-conditioned sparse linear systems, SIAM J. Sci. Comput.\n39, A2834–A2856.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n404\nN. J. Higham and T. Mary\nE. Carson and N. J. Higham (2018), Accelerating the solution of linear systems by iterative\nreﬁnement in three precisions, SIAM J. Sci. Comput. 40, A817–A847.\nE. Carson, T. Gergelits and I. Yamazaki (2022a), Mixed precision s-step Lanczos and\nconjugate gradient algorithms, Numer. Linear Algebra Appl. 29, e2425.\nE. Carson, N. J. Higham and S. Pranesh (2020), Three-precision GMRES-based iterative\nreﬁnement for least squares problems, SIAM J. Sci. Comput. 42, A4063–A4083.\nE. Carson, K. Lund, M. Rozložník and S. Thomas (2022b), Block Gram–Schmidt algorithms and their stability properties, Linear Algebra Appl. 638, 150–195.\nA. Charara, M. Gates, J. Kurzak, A. YarKhan and J. Dongarra (2020), SLATE developers’\nguide. SLATE Working Note 11, Innovative Computing Laboratory, The University of\nTennessee, Knoxville, TN, US.\nJ. Choquette, W. Gandhi, O. Giroux, N. Stam and R. Krashinsky (2021), NVIDIA A100\ntensor core GPU: Performance and innovation, IEEE Micro 41, 29–35.\nM. A. Clark, R. Babich, K. Barros, R. C. Brower and C. Rebbi (2010), Solving lattice QCD\nsystems of equations using mixed precision solvers on GPUs, Comput. Phys. Comm.\n181, 1517–1528.\nM. P. Connolly and N. J. Higham (2022), Probabilistic rounding error analysis of Householder QR factorization. MIMS EPrint 2022.5, Manchester Institute for Mathematical\nSciences, The University of Manchester, UK.\nM. P. Connolly, N. J. Higham and T. Mary (2021), Stochastic rounding and its probabilistic\nbackward error analysis, SIAM J. Sci. Comput. 43, A566–A585.\nM. Courbariaux, Y. Bengio and J.-P. David (2015), Training deep neural networks with\nlow precision multiplications. Available at arXiv:1412.7024v5.\nM. G. Croarken (1985), The centralization of scientiﬁc computation in Britain 1925–1955.\nPhD thesis, University of Warwick, Coventry, UK.\nM. Croci, M. Fasi, N. J. Higham, T. Mary and M. Mikaitis (2022), Stochastic rounding:\nImplementation, error analysis, and applications, Roy. Soc. Open Sci. 9, 1–25.\nP. I. Davies, N. J. Higham and F. Tisseur (2001), Analysis of the Cholesky method with\niterative reﬁnement for solving the symmetric deﬁnite generalized eigenproblem, SIAM\nJ. Matrix Anal. Appl. 23, 472–493.\nT. A. Davis and Y. Hu (2011), The University of Florida Sparse Matrix Collection, ACM\nTrans. Math. Software 38, 1.\nA. Dawson and P. D. Düben (2017), rpe v5: An emulator for reduced ﬂoating-point\nprecision in large numerical simulations, Geosci. Model Dev. 10, 2221–2230.\nA. Dawson, P. D. Düben, D. A. MacLeod and T. N. Palmer (2018), Reliable low precision\nsimulations in land surface models, Climate Dynam. 51, 2657–2666.\nJ. Dean (2020), The deep learning revolution and its implications for computer architecture\nand chip design, in 2020 IEEE International Solid-State Circuits Conference (ISSCC),\nIEEE, pp. 8–14.\nJ. Demmel and Y. Hida (2004), Accurate and eﬃcient ﬂoating point summation, SIAM J.\nSci. Comput. 25, 1214–1248.\nJ. Demmel and X. Li (1994), Faster numerical algorithms via exception handling, IEEE\nTrans. Comput. 43, 983–992.\nJ. Demmel, Y. Hida, E. J. Riedy and X. S. Li (2009), Extra-precise iterative reﬁnement for\noverdetermined least squares problems, ACM Trans. Math. Software 35, 28.\nJ. E. Dennis, Jr and R. B. Schnabel (1983), Numerical Methods for Unconstrained Optimization and Nonlinear Equations, Prentice Hall. Reprinted by SIAM, 1996.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n405\nS. Di and F. Cappello (2016), Fast error-bounded lossy HPC data compression with SZ,\nin 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS),\nIEEE, pp. 730–739.\nJ. Diﬀenderfer, D. Osei-Kuﬀuor and H. Menon (2021), QDOT: Quantized dot product\nkernel for approximate high-performance computing. Available at arXiv:2105.00115.\nJ. J. Dongarra (1980), Improving the accuracy of computed matrix eigenvalues. Preprint\nANL-80-84, Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, USA.\nJ. J. Dongarra (1982), Algorithm 589 SICEDR: A FORTRAN subroutine for improving\nthe accuracy of computed matrix eigenvalues, ACM Trans. Math. Software 8, 371–375.\nJ. J. Dongarra (1983), Improving the accuracy of computed singular values, SIAM J. Sci.\nStatist. Comput. 4, 712–719.\nJ. J. Dongarra (2020), Report on the Fujitsu Fugaku system. Technical report ICL-UT20-06, Innovative Computing Laboratory, The University of Tennessee, Knoxville, TN,\nUSA.\nJ. J. Dongarra, J. R. Bunch, C. B. Moler and G. W. Stewart (1979), LINPACK Users’ Guide,\nSIAM.\nJ. J. Dongarra, C. B. Moler and J. H. Wilkinson (1983), Improving the accuracy of computed\neigenvalues and eigenvectors, SIAM J. Numer. Anal. 20, 23–45.\nN. Doucet, H. Ltaief, D. Gratadour and D. Keyes (2019), Mixed-precision tomographic\nreconstructor computations on hardware accelerators, in 2019 IEEE/ACM 9th Workshop\non Irregular Applications: Architectures and Algorithms (IA3), IEEE, pp. 31–38.\nP. D. Düben, A. Subramanian, A. Dawson and T. N. Palmer (2017), A study of reduced\nnumerical precision to make superparameterization more competitive using a hardware\nemulator in the OpenIFS model, J. Adv. Model. Earth Syst. 9, 566–584.\nI. S. Duﬀand S. Pralet (2007), Towards stable mixed pivoting strategies for the sequential\nand parallel solution of sparse symmetric indeﬁnite systems, SIAM J. Matrix Anal. Appl.\n29, 1007–1024.\nI. S. Duﬀ, A. M. Erisman and J. K. Reid (2017), Direct Methods for Sparse Matrices,\nsecond edition, Oxford University Press.\nM. Emans and A. van der Meer (2012), Mixed-precision AMG as linear equation solver\nfor deﬁnite systems, Procedia Comput. Sci. 1, 175–183.\nM. Fasi and N. J. Higham (2018), Multiprecision algorithms for computing the matrix\nlogarithm, SIAM J. Matrix Anal. Appl. 39, 472–491.\nM. Fasi and N. J. Higham (2019), An arbitrary precision scaling and squaring algorithm\nfor the matrix exponential, SIAM J. Matrix Anal. Appl. 40, 1233–1256.\nM. Fasi and N. J. Higham (2021), Matrices with tunable inﬁnity-norm condition number\nand no need for pivoting in LU factorization, SIAM J. Matrix Anal. Appl. 42, 417–435.\nM. Fasi and M. Mikaitis (2020), CPFloat: A C library for emulating low-precision arithmetic. MIMS EPrint 2020.22, Manchester Institute for Mathematical Sciences, The\nUniversity of Manchester, UK.\nM. Fasi, N. J. Higham, F. Lopez, T. Mary and M. Mikaitis (2022), Matrix multiplication in\nmultiword arithmetic: Error analysis and application to GPU tensor cores. MIMS EPrint\n2022.3, Manchester Institute for Mathematical Sciences, The University of Manchester,\nUK.\nM. Fasi, N. J. Higham, M. Mikaitis and S. Pranesh (2021), Numerical behavior of NVIDIA\ntensor cores, PeerJ Comput. Sci. 7, e330.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n406\nN. J. Higham and T. Mary\nG. Flegar, H. Anzt, T. Cojean and E. S. Quintana-Ortí (2021), Adaptive precision blockJacobi for high performance preconditioning in the Ginkgo linear algebra software, ACM\nTrans. Math. Software 47, 1–28.\nL. Fousse, G. Hanrot, V. Lefèvre, P. Pélissier and P. Zimmermann (2007), MPFR: A\nmultiple-precision binary ﬂoating-point library with correct rounding, ACM Trans. Math.\nSoftware 33, 13.\nL. Fox, H. D. Huskey and J. H. Wilkinson (1948), The solution of algebraic linear simultaneous equations by punched card methods. Report, Mathematics Division, Department\nof Scientiﬁc and Industrial Research, National Physical Laboratory, Teddington, UK.\nT. Fukaya, R. Kannan, Y. Nakatsukasa, Y. Yamamoto and Y. Yanagisawa (2020), Shifted\nCholesky QR for computing the QR factorization of ill-conditioned matrices, SIAM J.\nSci. Comput. 42, A477–A503.\nJ. Gao, F. Zheng, F. Qi, Y. Ding, H. Li, H. Lu, W. He, H. Wei, L. Jin, X. Liu, D. Gong,\nF. Wang, Y. Zheng, H. Sun, Z. Zhou, Y. Liu and H. You (2021), Sunway supercomputer\narchitecture towards exascale computing: Analysis and practice, Sci. China Inform. Sci.\n64, 141101.\nP. E. Gill, M. A. Saunders and J. R. Shinnerl (1996), On the stability of Cholesky factorization for symmetric quasideﬁnite systems, SIAM J. Matrix Anal. Appl. 17, 35–46.\nL. Giraud, S. Gratton and J. Langou (2007), Convergence in backward error of relaxed\nGMRES, SIAM J. Sci. Comput. 29, 710–728.\nL. Giraud, A. Haidar and L. T. Watson (2008), Mixed-precision preconditioners in parallel\ndomain decomposition solvers, in Domain Decomposition Methods in Science and\nEngineering XVII (U. Langer et al., eds), Vol. 60 of Lecture Notes in Computational\nScience and Engineering, Springer, pp. 357–364.\nL. Giraud, J. Langou, M. Rozložník and J. van den Eshof (2005), Rounding error analysis\nof the classical Gram–Schmidt orthogonalization process, Numer. Math. 101, 87–100.\nF. Göbel, T. Grützmacher, T. Ribizel and H. Anzt (2021), Mixed precision incomplete and\nfactorized sparse approximate inverse preconditioning on GPUs, in Euro-Par 2021: Parallel Processing, Vol. 12820 of Lecture Notes in Computer Science, Springer, pp. 550–\n564.\nD. Goddeke and R. Strzodka (2011), Cyclic reduction tridiagonal solvers on GPUs applied\nto mixed-precision multigrid, IEEE Trans. Parallel Distrib. Syst. 22, 22–32.\nD. Göddeke, R. Strzodka and S. Turek (2007), Performance and accuracy of hardwareoriented native-, emulated- and mixed-precision solvers in FEM simulations, Int. J.\nParallel Emergent Distrib. Syst. 22, 221–256.\nW. Govaerts and J. D. Pryce (1990), Block elimination with one iterative reﬁnement solves\nbordered linear systems accurately, BIT 30, 490–507.\nS. Graillat, F. Jézéquel, T. Mary and R. Molina (2022), Adaptive precision matrix–vector\nproduct. Available at hal-03561193.\nS. Gratton, E. Simon, D. Titley-Peloquin and P. Toint (2019), Exploiting variable precision\nin GMRES. Available at arXiv:1907.10550.\nA. Greenbaum (1997), Estimating the attainable accuracy of recursively computed residual\nmethods, SIAM J. Matrix Anal. Appl. 18, 535–551.\nJ. F. Groote, R. Morel, J. Schmaltz and A. Watkins (2021), Logic Gates, Circuits, Processors, Compilers and Computers, Springer.\nT. Grützmacher, H. Anzt and E. S. Quintana-Ortí (2021), Using Ginkgo’s memory accessor\nfor improving the accuracy of memory-bound low precision BLAS, Software Pract.\nExper. Available at doi:10.1002/spe.3041.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n407\nM. Gulliksson (1994), Iterative reﬁnement for constrained and weighted linear least squares,\nBIT 34, 239–253.\nS. Gupta, A. Agrawal, K. Gopalakrishnan and P. Narayanan (2015), Deep learning with\nlimited numerical precision, in Proceedings of the 32nd International Conference on\nMachine Learning (F. Bach and D. Blei, eds), Vol. 37 of Proceedings of Machine\nLearning Research, PMLR, pp. 1737–1746.\nA. Haidar, A. Abdelfattah, M. Zounon, P. Wu, S. Pranesh, S. Tomov and J. Dongarra\n(2018a), The design of fast and energy-eﬃcient linear solvers: On the potential of\nhalf-precision arithmetic and iterative reﬁnement techniques, in Computational Science\n– ICCS 2018 (Y. Shi et al., eds), Vol. 10860 of Lecture Notes in Computer Science,\nSpringer, pp. 586–600.\nA. Haidar, H. Bayraktar, S. Tomov, J. Dongarra and N. J. Higham (2020), Mixed-precision\niterative reﬁnement using tensor cores on GPUs to accelerate solution of linear systems,\nProc. Roy. Soc. London A 476 (2243), 20200110.\nA. Haidar, S. Tomov, J. Dongarra and N. J. Higham (2018b), Harnessing GPU tensor\ncores for fast FP16 arithmetic to speed up mixed-precision iterative reﬁnement solvers, in Proceedings of the International Conference for High Performance Computing,\nNetworking, Storage, and Analysis (SC18), IEEE, article 47.\nA. Haidar, P. Wu, S. Tomov and J. Dongarra (2017), Investigating half precision arithmetic\nto accelerate dense linear system solvers, in Proceedings of the 8th Workshop on Latest\nAdvances in Scalable Algorithms for Large-Scale Systems (ScalA ’17), ACM Press,\narticle 10.\nR. Harvey and D. L. Verseghy (2015), The reliability of single precision computations in\nthe simulation of deep soil heat diﬀusion in a land surface model, Climate Dynam. 16,\n3865–3882.\nG. Henry, P. T. P. Tang and A. Heinecke (2019), Leveraging the bﬂoat16 artiﬁcial intelligence datatype for higher-precision computations, in 2019 IEEE 26th Symposium on\nComputer Arithmetic (ARITH), IEEE, pp. 69–76.\nD. J. Higham, N. J. Higham and S. Pranesh (2021), Random matrices generating large\ngrowth in LU factorization with pivoting, SIAM J. Matrix Anal. Appl. 42, 185–201.\nN. J. Higham (1986), Computing the polar decomposition: With applications, SIAM J. Sci.\nStatist. Comput. 7, 1160–1174.\nN. J. Higham (1988), Fast solution of Vandermonde-like systems involving orthogonal\npolynomials, IMA J. Numer. Anal. 8, 473–486.\nN. J. Higham (1991), Iterative reﬁnement enhances the stability of QR factorization methods for solving linear equations, BIT 31, 447–468.\nN. J. Higham (1997), Iterative reﬁnement for linear systems and LAPACK, IMA J. Numer.\nAnal. 17, 495–509.\nN. J. Higham (2002), Accuracy and Stability of Numerical Algorithms, second edition,\nSIAM.\nN. J. Higham (2008), Functions of Matrices: Theory and Computation, SIAM.\nN. J. Higham (2021), Numerical stability of algorithms at extreme scale and low precisions.\nMIMS EPrint 2021.14, Manchester Institute for Mathematical Sciences, The University\nof Manchester, UK. To appear in Proc. Int. Cong. Math.\nN. J. Higham and X. Liu (2021), A multiprecision derivative-free Schur–Parlett algorithm\nfor computing matrix functions, SIAM J. Matrix Anal. Appl. 42, 1401–1422.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n408\nN. J. Higham and T. Mary\nN. J. Higham and T. Mary (2019a), A new approach to probabilistic rounding error analysis,\nSIAM J. Sci. Comput. 41, A2815–A2835.\nN. J. Higham and T. Mary (2019b), A new preconditioner that exploits low-rank approximations to factorization error, SIAM J. Sci. Comput. 41, A59–A82.\nN. J. Higham and T. Mary (2020), Sharper probabilistic backward error analysis for basic\nlinear algebra kernels with random data, SIAM J. Sci. Comput. 42, A3427–A3446.\nN. J. Higham and T. Mary (2021), Solving block low-rank linear systems by LU factorization\nis numerically stable, IMA J. Numer. Anal. Available at doi:10.1093/imanum/drab020.\nN. J. Higham and S. Pranesh (2019), Simulating low precision ﬂoating-point arithmetic,\nSIAM J. Sci. Comput. 41, C585–C602.\nN. J. Higham and S. Pranesh (2021), Exploiting lower precision arithmetic in solving\nsymmetric positive deﬁnite linear systems and least squares problems, SIAM J. Sci.\nComput. 43, A258–A277.\nN. J. Higham, S. Pranesh and M. Zounon (2019), Squeezing a matrix into half precision,\nwith an application to solving linear systems, SIAM J. Sci. Comput. 41, A2536–A2551.\nN.-M. Ho, H. De Silva and W.-F. Wong (2021), GRAM: A framework for dynamically\nmixing precisions in GPU applications, ACM Trans. Archit. Code Optim. 18, 1–24.\nJ. D. Hogg and J. A. Scott (2010), A fast and robust mixed-precision solver for the solution\nof sparse symmetric linear systems, ACM Trans. Math. Software 37, 17.\nY. Idomura, T. Ina, Y. Ali and T. Imamura (2020), Acceleration of fusion plasma turbulence simulations using the mixed-precision communication-avoiding Krylov method,\nin International Conference for High Performance Computing, Networking, Storage and\nAnalysis (SC20), IEEE, pp. 1–13.\nIEEE (1985), IEEE Standard for Binary Floating-Point Arithmetic, ANSI/IEEE Standard\n754-1985, Institute of Electrical and Electronics Engineers.\nIEEE (2008), IEEE Standard for Floating-Point Arithmetic, IEEE Std 754-2008 (Revision\nof IEEE 754-1985), Institute of Electrical and Electronics Engineers.\nIntel Corporation (2018), BFLOAT16: Hardware Numerics Deﬁnition.\nWhite paper.\nDocument number 338302-001US.\nI. C. F. Ipsen and H. Zhou (2020), Probabilistic error analysis for inner products, SIAM J.\nMatrix Anal. Appl. 41, 1726–1741.\nT. Iwashita, K. Suzuki and T. Fukaya (2020), An integer arithmetic-based sparse linear\nsolver using a GMRES method and iterative reﬁnement, in 2020 IEEE/ACM 11th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA), IEEE,\npp. 1–8.\nM. Jankowski and H. Woźniakowski (1977), Iterative reﬁnement implies numerical stability, BIT 17, 303–311.\nF. Johansson et al. (2013), Mpmath: A Python library for arbitrary-precision ﬂoating-point\narithmetic. Available at http://mpmath.org.\nM. Joldes, J.-M. Muller and V. Popescu (2017), Tight and rigorous error bounds for basic\nbuilding blocks of double-word arithmetic, ACM Trans. Math. Software 44, 15res.\nN. P. Jouppi, D. H. Yoon, M. Ashcraft, M. Gottscho, T. B. Jablin, G. Kurian, J. Laudon,\nS. Li, P. Ma, X. Ma, T. Norrie, N. Patil, S. Prasad, C. Young, Z. Zhou and D. Patterson\n(2021), Ten lessons from three generations shaped Google’s TPUv4i: Industrial product,\nin 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture\n(ISCA), IEEE, pp. 1–14.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n409\nN. P. Jouppi, D. H. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young and D. Patterson\n(2020), A domain-speciﬁc supercomputer for training deep neural networks, Comm.\nAssoc. Comput. Mach. 63, 67–78.\nW. Kahan (1981), Why do we need a ﬂoating-point arithmetic standard? Technical report,\nUniversity of California, Berkeley, CA, USA.\nC. T. Kelley (1995), Iterative Methods for Linear and Nonlinear Equations, SIAM.\nC. T. Kelley (2022), Newton’s method in mixed precision, SIAM Rev. 64, 191–211.\nA. Kiełbasiński (1981), Iterative reﬁnement for linear systems in variable-precision arithmetic, BIT 21, 97–103.\nP. A. Knight, D. Ruiz and B. Uçar (2014), A symmetry preserving algorithm for matrix\nscaling, SIAM J. Matrix Anal. Appl. 35, 931–955.\nM. Kronbichler and K. Ljungkvist (2019), Multigrid for matrix-free high-order ﬁnite\nelement computations on graphics processors, ACM Trans. Parallel Comput. 6, 2.\nS. Kudo, K. Nitadori, T. Ina and T. Imamura (2020a), Implementation and numerical\ntechniques for one EFlop/s HPL-AI benchmark on Fugaku, in Proceedings of the 11th\nIEEE/ACM Workshop on Latest Advances in Scalable Algorithms for Large-Scale, Vol. 1,\nIEEE, pp. 69–76.\nS. Kudo, K. Nitadori, T. Ina and T. Imamura (2020b), Prompt report on exa-scale HPL-AI\nbenchmark, in 2020 IEEE International Conference on Cluster Computing (CLUSTER),\nIEEE, pp. 418–419.\nJ. Kurzak and J. Dongarra (2007), Implementation of mixed precision in solving systems\nof linear equations on the Cell processor, Concurrency Comput. Pract. Exper. 19, 1371–\n1385.\nJ. Langou, J. Langou, P. Luszczek, J. Kurzak, A. Buttari and J. Dongarra (2006), Exploiting\nthe performance of 32 bit ﬂoating point arithmetic in obtaining 64 bit accuracy (revisiting iterative reﬁnement for linear systems), in Proceedings of the 2006 ACM/IEEE\nConference on Supercomputing (SC ’06), IEEE.\nV. Lefèvre and P. Zimmermann (2017), Optimized binary64 and binary128 arithmetic with\nGNU MPFR, in 2017 IEEE 24th Symposium on Computer Arithmetic (ARITH), IEEE,\npp. 18–26.\nX. S. Li and J. W. Demmel (1998), Making sparse Gaussian elimination scalable by static\npivoting, in Proceedings of the 1998 ACM/IEEE Conference on Supercomputing, IEEE,\npp. 1–17.\nX. S. Li and J. W. Demmel (2003), SuperLU_DIST: A scalable distributed-memory sparse\ndirect solver for unsymmetric linear systems, ACM Trans. Math. Software 29, 110–140.\nX. S. Li, J. W. Demmel, D. H. Bailey, G. Henry, Y. Hida, J. Iskandar, W. Kahan, S. Y.\nKang, A. Kapur, M. C. Martin, B. J. Thompson, T. Tung and D. J. Yoo (2002), Design,\nimplementation and testing of extended and mixed precision BLAS, ACM Trans. Math.\nSoftware 28, 152–205.\nC. Lichtenau, S. Carlough and S. M. Mueller (2016), Quad precision ﬂoating point on\nthe IBM z13, in 2016 IEEE 23rd Symposium on Computer Arithmetic (ARITH), IEEE,\npp. 87–94.\nN. Lindquist, P. Luszczek and J. Dongarra (2020), Improving the performance of the\nGMRES method using mixed-precision techniques, in Communications in Computer\nand Information Science (J. Nichols et al., eds), Springer, pp. 51–66.\nN. Lindquist, P. Luszczek and J. Dongarra (2022), Accelerating restarted GMRES with\nmixed precision arithmetic, IEEE Trans. Parallel Distrib. Syst. 33, 1027–1037.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n410\nN. J. Higham and T. Mary\nJ. A. Loe, C. A. Glusa, I. Yamazaki, E. G. Boman and S. Rajamanickam (2021a), Experimental evaluation of multiprecision strategies for GMRES on GPUs, in 2021 IEEE\nInternational Parallel and Distributed Processing Symposium Workshops (IPDPSW),\nIEEE, pp. 469–478.\nJ. A. Loe, C. A. Glusa, I. Yamazaki, E. G. Boman and S. Rajamanickam (2021b), A study\nof mixed precision strategies for GMRES on GPUs. Available at arXiv:2109.01232.\nF. Lopez and T. Mary (2020), Mixed precision LU factorization on GPU tensor cores:\nReducing data movement and memory footprint. MIMS EPrint 2020.20, Manchester\nInstitute for Mathematical Sciences, The University of Manchester, UK.\nP. Luszczek, I. Yamazaki and J. Dongarra (2019), Increasing accuracy of iterative reﬁnement in limited ﬂoating-point arithmetic on half-precision accelerators, in 2019 IEEE\nHigh Performance Extreme Computing Conference (HPEC), IEEE, pp. 1–6.\nS. Markidis, S. Wei Der Chien, E. Laure, I. B. Peng and J. S. Vetter (2018), NVIDIA tensor\ncore programmability, performance & precision, in 2018 IEEE International Parallel\nand Distributed Processing Symposium Workshops (IPDPSW), IEEE, pp. 522–531.\nC. M. Maynard and D. N. Walters (2019), Mixed-precision arithmetic in the ENDGame\ndynamical core of the uniﬁed model, a numerical weather prediction and climate model\ncode, Comput. Phys. Comm. 244, 69–75.\nS. F. McCormick, J. Benzaken and R. Tamstorf (2021), Algebraic error analysis for mixedprecision multigrid solvers, SIAM J. Sci. Comput. 43, S392–S419.\nA. Meurer, C. P. Smith, M. Paprocki, O. ˘Certik, S. B. Kirpichev, M. Rocklin, A. Kumar,\nS. Ivanov, J. K. Moore, S. Singh, T. Rathnayake, S. Vig, B. E. Granger, R. P. Muller,\nF. Bonazzi, H. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R. Terrel,\nŠ. Roučka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman and A. Scopatz (2017), SymPy:\nSymbolic computing in Python, PeerJ Comput. Sci. 3, e103.\nC. B. Moler (1967), Iterative reﬁnement in ﬂoating point, J. Assoc. Comput. Mach. 14,\n316–321.\nC. B. Moler (2017), ‘Half precision’ 16-bit ﬂoating point arithmetic.\nAvailable\nat\nhttp://blogs.mathworks.com/cleve/2017/05/08/half-precision-16-bit-ﬂoating-pointarithmetic/.\nC. B. Moler (2019), Variable format half precision ﬂoating point arithmetic.\nAvailable at https://blogs.mathworks.com/cleve/2019/01/16/variable-format-half-precisionﬂoating-point-arithmetic/.\nD. Mukunoki, K. Ozaki, T. Ogita and T. Imamura (2020), DGEMM using tensor cores, and\nits accurate and reproducible versions, in High Performance Computing (P. Sadayappan\net al., eds), Springer, pp. 230–248.\nJ.-M. Muller, N. Brunie, F. de Dinechin, C.-P. Jeannerod, M. Joldes, V. Lefèvre,\nG. Melquiond, N. Revol and S. Torres (2018), Handbook of Floating-Point Arithmetic,\nsecond edition, Birkhäuser.\nM. Nakata (2021), MPLAPACK version 1.0.0 user manual. Available at arXiv:2109.13406.\nT. Norrie, N. Patil, D. H. Yoon, G. Kurian, S. Li, J. Laudon, C. Young, N. Jouppi and\nD. Patterson (2021), The design process for Google’s training chips: TPUv2 and TPUv3,\nIEEE Micro 41, 56–63.\nNVIDIA Corporation (2020), NVIDIA A100 Tensor Core GPU Architecture, v1.0.\nT. Ogita and K. Aishima (2018), Iterative reﬁnement for symmetric eigenvalue decomposition, Japan J. Indust. Appl. Math. 35, 1007–1035.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n411\nT. Ogita and K. Aishima (2019), Iterative reﬁnement for symmetric eigenvalue decomposition II: Clustered eigenvalues, Japan J. Indust. Appl. Math. 36, 435–459.\nT. Ogita and K. Aishima (2020), Iterative reﬁnement for singular value decomposition\nbased on matrix multiplication, J. Comput. Appl. Math. 369, 112512.\nE. Oktay and E. Carson (2022), Multistage mixed precision iterative reﬁnement, Numer.\nLinear Algebra Appl. Available at doi:10.1002/nla.2434.\nK. L. Oo and A. Vogel (2020), Accelerating geometric multigrid preconditioning with\nhalf-precision arithmetic on GPUs. Available at arXiv:2007.07539.\nR. Ooi, T. Iwashita, T. Fukaya, A. Ida and R. Yokota (2020), Eﬀect of mixed precision\ncomputing on H-matrix vector multiplication in BEM analysis, in Proceedings of the\nInternational Conference on High Performance Computing in Asia-Paciﬁc Region, ACM\nPress.\nS.-i. O’uchi, H. Fuketa, T. Ikegami, W. Nogami, T. Matsukawa, T. Kudoh and R. Takano\n(2018), Image-classiﬁer deep convolutional neural network training by 9-bit dedicated\nhardware to realize validation accuracy and energy eﬃciency superior to the half precision ﬂoating point format, in 2018 IEEE International Symposium on Circuits and\nSystems (ISCAS), IEEE, pp. 1–5.\nC. C. Paige, M. Rozložník and Z. Strakoš (2006), Modiﬁed Gram–Schmidt (MGS), least\nsquares, and backward stability of MGS-GMRES, SIAM J. Matrix Anal. Appl. 28,\n264–284.\nT. N. Palmer (2014), More reliable forecasts with less precise computations: A fasttrack route to cloud-resolved weather and climate simulators?, Phil. Trans. R. Soc. A\n372 (2018), 1–14.\nT. N. Palmer (2020), The physics of numerical analysis: A climate modelling case study,\nPhil. Trans. R. Soc. A 378 (2166), 1–6.\nM. Petschow, E. Quintana-Ortí and P. Bientinesi (2014), Improved accuracy and parallelism\nfor MRRR-based eigensolvers: A mixed precision approach, SIAM J. Sci. Comput. 36,\nC240–C263.\nL. Pisha and L. Ligowski (2021), Accelerating non-power-of-2 size Fourier transforms\nwith GPU tensor cores, in 2021 IEEE International Parallel and Distributed Processing\nSymposium (IPDPS), IEEE, pp. 507–516.\nR. Ralha (2018), Mixed precision bisection, Math. Comput. Sci. 12, 173–181.\nC. Rubio-González, C. Nguyen, H. D. Nguyen, J. Demmel, W. Kahan, K. Sen, D. H.\nBailey, C. Iancu and D. Hough (2013), Precimonious: Tuning assistant for ﬂoatingpoint precision, in Proceedings of the International Conference on High Performance\nComputing, Networking, Storage and Analysis (SC ’13), ACM Press, article 27.\nP. San Juan, R. Rodríguez-Sánchez, F. D. Igual, P. Alonso-Jordá and E. S. QuintanaOrtí (2021), Low precision matrix multiplication for eﬃcient deep learning in NVIDIA\ncarmel processors, J. Supercomput. 77, 11257–11269.\nM. Sato, Y. Ishikawa, H. Tomita, Y. Kodama, T. Odajima, M. Tsuji, H. Yashiro, M. Aoki,\nN. Shida, I. Miyoshi, K. Hirai, A. Furuya, A. Asato, K. Morita and T. Shimizu (2020),\nCo-design for A64FX manycore processor and ‘Fugaku’, in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\n(SC ’20), IEEE.\nK. Scheinberg (2016), Evolution of randomness in optimization methods for supervised\nmachine learning, SIAG/OPT Views and News 24, 1–8.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n412\nN. J. Higham and T. Mary\nO. Schenk, K. Gärtner, W. Fichtner and A. Stricker (2001), PARDISO: A high-performance\nserial and parallel sparse linear solver in semiconductor device simulation, Future Gener.\nComput. Syst. 18, 69–78.\nV. Simoncini and D. B. Szyld (2003), Theory of inexact Krylov subspace methods and\napplications to scientiﬁc computing, SIAM J. Sci. Comput. 25, 454–477.\nR. D. Skeel (1980), Iterative reﬁnement implies numerical stability for Gaussian elimination, Math. Comp. 35, 817–832.\nA. Smoktunowicz and J. Sokolnicka (1984), Binary cascades iterative reﬁnement in\ndoubled-mantissa arithmetics, BIT 24, 123–127.\nA. Sorna, X. Cheng, E. D’Azevedo, K. Won and S. Tomov (2018), Optimizing the fast\nFourier transform using mixed precision on tensor core hardware, in 2018 IEEE 25th\nInternational Conference on High Performance Computing Workshops (HiPCW), IEEE,\npp. 3–7.\nA. Stathopoulos and K. Wu (2002), A block orthogonalization procedure with constant\nsynchronization requirements, SIAM J. Sci. Comput. 23, 2165–2182.\nG. W. Stewart (1973), Introduction to Matrix Computations, Academic Press.\nN. J. Stor, I. Slapničar and J. L. Barlow (2015), Accurate eigenvalue decomposition of real\nsymmetric arrowhead matrices and applications, Linear Algebra Appl. 464, 62–89.\nY. Sumiyoshi, A. Fujii, A. Nukada and T. Tanaka (2014), Mixed-precision AMG method\nfor many core accelerators, in Proceedings of the 21st European MPI Users’ Group\nMeeting (EuroMPI/ASIA ’14), ACM Press, pp. 127–132.\nJ. Sun, G. D. Peterson and O. O. Storaasli (2008), High-performance mixed-precision\nlinear solver for FPGAs, IEEE Trans. Comput. 57, 1614–1623.\nG. Tagliavini, S. Mach, D. Rossi, A. Marongiu and L. Benin (2018), A transprecision\nﬂoating-point platform for ultra-low power computing, in 2018 Design, Automation and\nTest in Europe Conference and Exhibition (DATE), pp. 1051–1056.\nR. Tamstorf, J. Benzaken and S. F. McCormick (2021), Discretization-error-accurate\nmixed-precision multigrid solvers, SIAM J. Sci. Comput. 43, S420–S447.\nO. Tintó Prims, M. C. Acosta, A. M. Moore, M. Castrillo, K. Serradell, A. Cortés and\nF. J. Doblas-Reyes (2019), How to use mixed precision in ocean models: Exploring a\npotential reduction of numerical precision in NEMO 4.0 and ROMS 3.6, Geosci. Model\nDev. 12, 3135–3148.\nF. Tisseur (2001), Newton’s method in ﬂoating point arithmetic and iterative reﬁnement of\ngeneralized eigenvalue problems, SIAM J. Matrix Anal. Appl. 22, 1038–1057.\nT. Trader (2016), IBM advances against x86 with Power9.\nAvailable at https://www.\nhpcwire.com/2016/08/30/ibm-unveils-power9-details/.\nY. M. Tsai, P. Luszczek and J. Dongarra (2021), Mixed-precision algorithm for ﬁnding\nselected eigenvalues and eigenvectors of symmetric and Hermitian matrices. Technical\nreport ICL-UT-21-05, Innovative Computing Laboratory, The University of Tennessee,\nKnoxville, TN, USA.\nE. Tsuchida and Y.-K. Choe (2012), Iterative diagonalization of symmetric matrices in\nmixed precision and its application to electronic structure calculations, Comput. Phys.\nComm. 183, 980–985.\nK. Turner and H. F. Walker (1992), Eﬃcient high accuracy solutions with GMRES(m),\nSIAM J. Sci. Statist. Comput. 12, 815–825.\nJ. van den Eshof and G. L. G. Sleijpen (2004), Inexact Krylov subspace methods for linear\nsystems, SIAM J. Matrix Anal. Appl. 26, 125–153.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\nMixed precision algorithms in numerical linear algebra\n413\nF. Váňa, P. Düben, S. Lang, T. Palmer, M. Leutbecher, D. Salmond and G. Carver (2017),\nSingle precision in weather forecasting models: An evaluation with the IFS, Mon.\nWeather Rev. 145, 495–502.\nJ. von Neumann and H. H. Goldstine (1947), Numerical inverting of matrices of high order,\nBull. Amer. Math. Soc. 53, 1021–1099.\nE. Wang, J. J. Davis, R. Zhao, H.-C. Ng, X. Niu, W. Luk, P. Y. K. Cheung and G. A.\nConstantinides (2019), Deep neural network approximation for custom hardware, ACM\nComput. Surv. 52, 1–39.\nN. Wang, J. Choi, D. Brand, C.-Y. Chen and K. Gopalakrishnan (2018), Training deep\nneural networks with 8-bit ﬂoating point numbers, in Advances in Neural Information\nProcessing Systems 31 (S. Bengio et al., eds), Curran Associates, pp. 7686–7695.\nS. Wang and P. Kanwar (2019), BFloat16: The secret to high performance on cloud TPUs.\nAvailable at https://cloud.google.com/blog/products/ai-machine-learning/bﬂoat16-thesecret-to-high-performance-on-cloud-tpus.\nJ. H. Wilkinson (1948), Progress report on the Automatic Computing Engine. Report\nMA/17/1024, Mathematics Division, Department of Scientiﬁc and Industrial Research,\nNational Physical Laboratory, Teddington, UK.\nJ. H. Wilkinson (1961), Error analysis of direct methods of matrix inversion, J. Assoc.\nComput. Mach. 8, 281–330.\nJ. H. Wilkinson (1963), Rounding Errors in Algebraic Processes, Notes on Applied Science No. 32, Her Majesty’s Stationery Oﬃce. Also published by Prentice Hall, USA.\nReprinted by Dover, 1994.\nJ. H. Wilkinson (1977), The use of the single-precision residual in the solution of linear\nsystems. Unpublished manuscript.\nI. Yamazaki, S. Tomov and J. Dongarra (2015a), Mixed-precision Cholesky QR factorization and its case studies on multicore CPU with multiple GPUs, SIAM J. Sci. Comput.\n37, C307–C330.\nI. Yamazaki, S. Tomov and J. Dongarra (2016), Stability and performance of various\nsingular value QR implementations on multicore CPU with a GPU, ACM Trans. Math.\nSoftware 43, 10.\nI. Yamazaki, S. Tomov, T. Dong and J. Dongarra (2015b), Mixed-precision orthogonalization scheme and adaptive step size for improving the stability and performance\nof CA-GMRES on GPUs, in High Performance Computing for Computational Science\n(VECPAR 2014) (M. Daydé et al., eds), Vol. 8969 of Lecture Notes in Computer Science,\nSpringer, pp. 17–30.\nI. Yamazaki, S. Tomov, J. Kurzak, J. Dongarra and J. Barlow (2015c), Mixed-precision\nblock Gram Schmidt orthogonalization, in Proceedings of the 6th Workshop on Latest\nAdvances in Scalable Algorithms for Large-Scale Systems (ScalA ’15), ACM Press.\nK. Yang, Y.-F. Chen, G. Roumpos, C. Colby and J. Anderson (2019), High performance\nMonte Carlo simulation of Ising model on TPU clusters, in Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis\n(SC ’19), ACM Press.\nL. M. Yang, A. Fox and G. Sanders (2021), Rounding error analysis of mixed precision\nblock Householder QR algorithms, SIAM J. Sci. Comput. 43, A1723–A1753.\nS. Zhang, E. Baharlouei and P. Wu (2020), High accuracy matrix computations on neural\nengines: A study of QR factorization and its applications, in Proceedings of the 29th\nInternational Symposium on High-Performance Parallel and Distributed Computing,\nACM Press.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press\n\n414\nN. J. Higham and T. Mary\nY.-K. Zhu and W. B. Hayes (2009), Correct rounding and a hybrid approach to exact\nﬂoating-point summation, SIAM J. Sci. Comput. 31, 2981–3001.\nZ. Zlatev (1982), Use of iterative reﬁnement in the solution of sparse linear systems, SIAM\nJ. Numer. Anal. 19, 381–399.\nM. Zounon, N. J. Higham, C. Lucas and F. Tisseur (2022), Performance impact of precision\nreduction in sparse linear systems solvers, PeerJ Comput. Sci. 8, e778.\nhttps://doi.org/10.1017/S0962492922000022 Published online by Cambridge University Press",
  "stats": {
    "raw_length": 191790,
    "normalized_length": 191398,
    "raw_lines": 3510,
    "normalized_lines": 3300
  }
}